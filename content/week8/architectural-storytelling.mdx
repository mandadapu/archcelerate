---
title: "Architectural Storytelling: System Design Documents"
week: 8
concept: 4
description: "Packaging technical complexity for stakeholders: authoring System Design Documents that justify every architectural decision"
estimatedMinutes: 35
objectives:
  - Author comprehensive System Design Documents (SDD) explaining the 'Why' behind tech stack
  - Justify vector database, model, and orchestration framework selections against requirements
  - Generate Skill Diagnosis Brief proving mastery across 7 core domains
  - Package architectural authority for hiring managers and technical stakeholders
---

# Architectural Storytelling

Packaging technical complexity for stakeholders and hiring managers.

## The Core Challenge

**You built a production-ready AI system. Can you explain WHY you made those choices?**

Without documentation:
- Stakeholders don't understand why you chose Pinecone over pgvector
- Hiring managers can't evaluate your architectural thinking
- Future maintainers don't know the constraints that drove decisions

**Architect's Mandate**: Document decisions **with justification**, not just implementation details.

---

## Pattern 1: The System Design Document (SDD)

**The Pattern**: A technical brief that explains **WHY** behind every major architectural decision, justified against requirements and constraints.

### SDD Template

```markdown
# System Design Document: [Project Name]

**Author**: [Your Name]
**Date**: [Date]
**Version**: 1.0
**Status**: Production-Ready

---

## Executive Summary

**Problem**: [What business problem does this solve?]

**Solution**: [High-level architecture in 2-3 sentences]

**Key Results**:
- **Performance**: P95 latency &lt;2s, 99.5% availability
- **Quality**: 95% faithfulness on golden dataset, 0 safety violations
- **Cost**: $0.08 per request (70% below budget target)

---

## Requirements & Constraints

### Functional Requirements
1. **Multi-tenant SaaS**: Support 100+ customers with strict data isolation
2. **RAG Pipeline**: Answer queries using proprietary document corpus
3. **Safety Guardrails**: Block jailbreaks and prompt injections (100% rate)
4. **Audit Trail**: Complete logs for regulatory compliance (HIPAA)

### Non-Functional Requirements
1. **Latency**: P95 &lt;3s (user abandonment threshold)
2. **Cost**: &lt;$0.10 per request (commercial viability)
3. **Availability**: 99.5% uptime (4.4 hours downtime/year max)
4. **Scale**: Handle 10x traffic spike without degradation

### Constraints
1. **Regulatory**: HIPAA compliance (PHI must stay in VPC)
2. **Budget**: $2K/month infrastructure (seed-stage startup)
3. **Team**: 2 engineers (minimal ops overhead)

---

## Architecture Overview

```
User Request
    ‚Üì
[API Gateway + Auth]
    ‚Üì
[Tenant Context Extraction]
    ‚Üì
[Safety Proxy] ‚Üí [Jailbreak Detection]
    ‚Üì
[RAG Pipeline]
    ‚îú‚îÄ [Query Rewriting]
    ‚îú‚îÄ [Hybrid Search: Vector + BM25]
    ‚îú‚îÄ [Re-ranking (top 100 ‚Üí top 10)]
    ‚îî‚îÄ [Context Window Optimization]
    ‚Üì
[LLM: Claude 4.5 Sonnet]
    ‚Üì
[Output Validation]
    ‚Üì
[Audit Logging]
    ‚Üì
Response
```

---

## Decision Log (ADRs)

### ADR-001: Vector Database Selection

**Decision**: Pinecone over pgvector

**Context**:
- Need multi-tenant namespaces for data isolation
- Expecting 10M+ vectors across all tenants
- Team has no PostgreSQL ops expertise

**Options Considered**:

| Option | Pros | Cons | Cost (10M vectors) |
|--------|------|------|-------------------|
| **Pinecone** | Managed service, namespaces, auto-scaling | Higher cost, vendor lock-in | $200/mo |
| **pgvector** | Open-source, full control, lower cost | Requires Postgres ops, no native multi-tenancy | $100/mo + ops time |
| **Weaviate** | Open-source, multi-tenant, rich features | Self-hosted complexity, learning curve | $150/mo + ops |

**Quantitative Trade-off Matrix (Requirement ‚Üí Constraint Mapping)**:

| Requirement | Constraint | Pinecone | pgvector | Weaviate | Winner |
|---|---|---|---|---|---|
| Multi-tenant isolation | Security: Physical namespace separation | ‚úÖ Native namespaces (0 custom code) | ‚ùå Row-level security (custom auth layer) | ‚úÖ Native tenants (self-hosted) | Pinecone |
| 10x traffic spike | Scale: Auto-scaling under load | ‚úÖ Managed auto-scale | ‚ùå Manual Postgres scaling | ‚ö†Ô∏è Kubernetes HPA required | Pinecone |
| 2-engineer team | Cost: Ops overhead | ‚úÖ 0 ops hours/week | ‚ùå ~8 ops hours/week | ‚ùå ~12 ops hours/week (K8s) | Pinecone |
| $2K/month budget | Cost: Infrastructure spend | ‚úÖ $200/mo (10% of budget) | ‚úÖ $100/mo (5% of budget) | ‚ö†Ô∏è $150/mo + K8s infra | Pinecone |
| HIPAA compliance | Regulatory: PHI in VPC | ‚ö†Ô∏è SOC2 certified, BAA available | ‚úÖ Self-hosted in VPC | ‚úÖ Self-hosted in VPC | pgvector |

**Justification**:
- Chose **Pinecone** because:
  1. Multi-tenancy isolation requirement demanded **physical namespace separation** ‚Äî Pinecone supports this natively, reducing security engineering overhead by ~40% vs. building custom row-level security on pgvector
  2. Managed service = 0 ops overhead (meets constraint: 2-engineer team cannot absorb 8+ hours/week of database operations)
  3. $200/mo fits within $2K infrastructure budget (10%)
  4. Auto-scaling handles 10x traffic spike requirement without Kubernetes expertise

**Architect's Tip**: *"A Director doesn't say 'I liked Pinecone.' They say, 'We chose Pinecone over pgvector because our Multi-tenancy isolation requirement demanded physical namespace separation, which Pinecone supports natively, reducing our security engineering overhead by 40%.' Every choice must be a direct response to a constraint (Cost, Latency, Security, or Scale)."*

**Trade-offs Accepted**:
- Higher cost vs pgvector (2x) ‚Äî justified by 0 ops overhead for 2-person team
- Vendor lock-in risk (mitigated: standard embedding format enables migration in &lt;1 week)
- HIPAA gap: Pinecone is SOC2-certified with BAA, but PHI does leave VPC ‚Äî mitigated by encrypting embeddings and never storing raw PHI in vector metadata

---

### ADR-002: Model Selection

**Decision**: Claude 4.5 Sonnet over GPT-4o

**Context**:
- Medical domain requires high accuracy (faithfulness &gt;95%)
- Latency budget: &lt;3s end-to-end
- Cost target: &lt;$0.10 per request

**Benchmarks**:

| Model | Faithfulness | Avg Latency | Cost/Request | Decision |
|-------|-------------|-------------|--------------|----------|
| **Claude Opus** | 98% | 2.8s | $0.15 | ‚ùå Over budget |
| **Claude Sonnet** | 96% | 1.2s | $0.08 | ‚úÖ Selected |
| **GPT-4o** | 94% | 1.5s | $0.06 | ‚ùå Below accuracy threshold |
| **Haiku** | 89% | 0.6s | $0.02 | ‚ùå Too many hallucinations |

**Justification**:
- **Sonnet** is the only model meeting ALL requirements:
  1. Faithfulness 96% > threshold 95%
  2. Latency 1.2s ‚Üí end-to-end 2.5s &lt; 3s SLA
  3. Cost $0.08 < $0.10 budget

**Fallback Strategy**:
- Use **Opus** for high-stakes queries (risk score &gt;80)
- Use **Haiku** for simple lookups (caching hit rate &gt;70%)
- Projected blended cost: $0.09 per request

---

### ADR-003: Agent Orchestration Framework

**Decision**: LangGraph over CrewAI

**Context**:
- Need state checkpointing for long-running tasks
- Deterministic execution required for audit compliance
- Cost control critical (token budget: 8K per request)

**Comparison**:

| Framework | Checkpointing | Deterministic | Overhead | Decision |
|-----------|--------------|---------------|----------|----------|
| **LangGraph** | ‚úÖ Built-in | ‚úÖ Explicit graph | 1.4% tokens | ‚úÖ Selected |
| **CrewAI** | ‚ùå None | ‚ùå Opaque routing | 6.5% tokens | ‚ùå Too much overhead |
| **Custom** | Manual | ‚úÖ Full control | 0% | ‚ùå Too much eng time |

**Justification**:
- **LangGraph** meets requirements:
  1. Built-in checkpointing ‚Üí resumable workflows (crash recovery)
  2. Explicit state machine ‚Üí audit trail visibility
  3. 1.4% overhead ‚Üí minimal cost impact ($0.001 per request)

**Trade-offs Accepted**:
- Steeper learning curve vs CrewAI
- More setup code (20 lines for graph definition)

---

## Security Architecture

### Threat Model

| Threat | Mitigation | Status |
|--------|-----------|--------|
| Prompt Injection | Safety Proxy with jailbreak detection | ‚úÖ Tested (100% block rate) |
| Data Leakage (Cross-tenant) | Namespace isolation + tenantId filtering | ‚úÖ Verified (red-team passed) |
| PHI Exposure in Logs | PII/PHI redaction before logging | ‚úÖ HIPAA compliant |
| Recursive Loops | Max iteration limits + cycle detection | ‚úÖ Tested (no infinite loops) |

### Compliance Certifications
- **HIPAA**: PHI stays in VPC, audit logs maintained for 7 years
- **SOC 2 Type II**: In progress (expected Q3 2025)

---

## Performance Validation

### Load Testing Results

| Metric | Baseline (10 users) | Spike (100 users) | SLA | Status |
|--------|-------------------|-------------------|-----|--------|
| P50 Latency | 1.2s | 1.8s | &lt;2s | ‚úÖ |
| P95 Latency | 2.1s | 2.9s | &lt;3s | ‚úÖ |
| Error Rate | 0.1% | 0.3% | &lt;1% | ‚úÖ |
| Throughput | 8.3 req/s | 83 req/s | 10x scale | ‚úÖ |

### NFR Benchmark Dashboard

Don't just list results ‚Äî present them as **Non-Functional Requirement (NFR) Benchmarks** that map each metric to the requirement it satisfies and the stress condition under which it was validated.

**Architect's Tip**: *"Don't just state your P95 latency. Show the Degradation Curve. An Architect's SDD should include a chart showing how latency held up during the 10x Load Spike test from Concept 2. Proving your system survived a failure state is more impressive to a stakeholder than proving it works under ideal conditions."*

| NFR | Target | Baseline (10 users) | 10x Spike (100 users) | Degradation | Verdict |
|---|---|---|---|---|---|
| P95 Latency | &lt;3s | 2.1s | 2.9s | +38% | ‚úÖ Within SLA under stress |
| P99 Latency | &lt;5s | 2.8s | 4.2s | +50% | ‚úÖ Within SLA under stress |
| Error Rate | &lt;1% | 0.1% | 0.3% | +0.2pp | ‚úÖ 3x headroom |
| Throughput | &gt;80 req/s at spike | 8.3 req/s | 83 req/s | Linear scale | ‚úÖ Scales with load |
| Availability | &gt;99.5% | 99.9% | 99.7% | -0.2pp | ‚úÖ Above target |
| Faithfulness | &gt;95% | 96% | 95.8% | -0.2pp | ‚úÖ Quality maintained under load |

**Latency Degradation Curve (Step-Function Ramp-Up)**:

```
Latency (ms)
3000 ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ SLA ‚îÄ ‚îÄ ‚îÄ ‚îÄ
     |                                          ‚ï±
2500 |                                       ‚ï±
     |                                    ‚ï±
2000 |                              ‚ï±‚îÄ‚îÄ‚ï±
     |                        ‚ï±‚îÄ‚îÄ‚ï±
1500 |                  ‚ï±‚îÄ‚îÄ‚ï±
     |            ‚ï±‚îÄ‚îÄ‚ï±
1000 |      ‚ï±‚îÄ‚îÄ‚ï±
     | ‚ï±‚îÄ‚îÄ‚ï±
 500 |‚ï±
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ
          10   20   30   40   50   70   100
                   Concurrent Users

 ‚úÖ System maintained P95 < SLA through 100 concurrent users
 üéØ Kneepoint identified at 120 users (auto-scale trigger: 96)
```

**Key Insight**: The degradation curve proves the system doesn't just **work** ‚Äî it **degrades gracefully**. The latency increase is linear, not exponential, which means the architecture has no hidden bottlenecks that collapse under load.

### Golden Dataset Evaluation

| Category | Pass Rate | Target | Status |
|----------|-----------|--------|--------|
| Happy Path | 98% | &gt;95% | ‚úÖ |
| Edge Cases | 92% | &gt;85% | ‚úÖ |
| Adversarial | 100% | 100% | ‚úÖ |
| Regressions | 100% | 100% | ‚úÖ |

---

## Cost Analysis

### Monthly Infrastructure Costs

| Service | Usage | Cost | % of Budget |
|---------|-------|------|-------------|
| Anthropic API | 100K requests √ó $0.08 | $8,000 | 80% |
| Pinecone | 10M vectors | $200 | 2% |
| Postgres (Supabase) | 50GB storage | $100 | 1% |
| Redis (Upstash) | 1GB cache | $50 | 0.5% |
| Vercel (hosting) | Pro plan | $20 | 0.2% |
| **Total** | | **$8,370** | 84% |

### Unit Economics

- **Cost per request**: $0.08
- **Revenue per request**: $0.15 (avg across tiers)
- **Gross margin**: 47%
- **Break-even**: 56K requests/month

---

## Operational Runbook

### Deployment Process
1. Run golden dataset evaluation ‚Üí must pass &gt;90%
2. Red-team security tests ‚Üí 100% block rate required
3. Load test (10x spike) ‚Üí P95 &lt;3s required
4. Deploy to staging ‚Üí smoke tests
5. Canary deployment (5% traffic) ‚Üí monitor for 1 hour
6. Full rollout

### Monitoring & Alerts

| Alert | Threshold | Action |
|-------|-----------|--------|
| P95 Latency | &gt;3s for 5 minutes | Page on-call engineer |
| Error Rate | &gt;1% for 10 minutes | Auto-rollback |
| Cost Spike | >$15K/day | Circuit breaker triggers |
| Security Violation | Any jailbreak success | Immediate incident response |

### Incident Response
1. Rollback to previous version (1-click)
2. Notify customers (status page update)
3. Debug using time-travel checkpoints
4. Add regression test to golden dataset
5. Post-mortem within 24 hours

---

## Future Improvements (Tech Debt)

| Priority | Item | Est. Effort | Impact |
|----------|------|-------------|--------|
| P0 | Add semantic caching (10% cost reduction) | 3 days | High |
| P1 | Migrate to self-hosted Weaviate (save $100/mo) | 2 weeks | Medium |
| P2 | Implement adaptive routing (Opus for hard queries) | 1 week | High |
| P3 | A/B test GPT-4.5 Turbo (if faithfulness improves) | 2 days | Medium |

---

## Appendix: Alternatives Considered

### Why NOT OpenAI Embeddings?
- Anthropic's models showed 8% better faithfulness in our domain
- Cost difference negligible ($0.0001 vs $0.00013 per 1K tokens)
- Reduced vendor risk (multi-LLM strategy)

### Why NOT Chroma?
- No managed hosting option (ops overhead)
- Multi-tenancy requires custom implementation
- Team unfamiliar with Chroma's query language

---

## Context-Aware Architecture Decision Records (ADRs)

An SDD is a **snapshot** ‚Äî it captures the system as it exists today. An ADR is a **history** ‚Äî it captures **why** the system looks this way, including the specific market conditions, tool limitations, and domain constraints that existed at the time.

**Architect's Tip**: *"Teach students to document the 'Context' ‚Äî the specific market conditions or tool limitations that existed at the time. For example: 'We chose a Re-ranker because the base embedding model had a 20% recall gap on clinical jargon.' This prevents future teams from 'optimizing' away a critical safety component they don't understand."*

### ADR Template: Context-Aware Format

```markdown
# ADR-004: Cross-Encoder Re-Ranking Pipeline

**Status**: Accepted
**Date**: 2026-01-15
**Deciders**: [Lead Architect, ML Engineer]

## Context (Why This Decision Exists)

### Market Conditions (January 2026)
- text-embedding-3-small (OpenAI) is the cost-effective default
  for most RAG applications, BUT has a documented 20% recall gap
  on domain-specific clinical jargon (e.g., "CABG" vs.
  "Coronary Artery Bypass Grafting")
- Cross-encoder re-rankers (e.g., ms-marco-MiniLM) add ~150ms
  latency but recover 15-18% of missed relevant documents
- No medical-specific embedding model exists at production scale
  below $0.01/1K tokens (as of Jan 2026)

### Domain Constraint
- SurgiSearch users are surgeons querying during active operations
- A missed relevant document is a SAFETY FAILURE, not just a
  relevance degradation
- False negatives (missed protocols) are 10x more dangerous
  than false positives (extra context)

### Tool Limitation
- Pinecone's built-in similarity search returns top-K by cosine
  distance, but cosine distance CANNOT distinguish between
  "semantically close" and "clinically equivalent"
- Example: "post-op bleeding protocol" and "hemostasis management
  guidelines" score 0.72 similarity ‚Äî below the 0.75 retrieval
  threshold ‚Äî despite being clinically synonymous

## Decision
Add a Cross-Encoder Re-Ranking stage after vector retrieval:
1. Retrieve top-100 by vector similarity (fast, cheap)
2. Re-rank to top-10 using cross-encoder (slow, accurate)
3. 15% accuracy improvement on clinical jargon queries

## Consequences
- +150ms latency (within 3s SLA budget: 1.2s LLM + 0.3s vector
  + 0.15s re-rank + 0.35s overhead = 2.0s total)
- +$0.002/request cost (cross-encoder inference)
- Requires model versioning (re-ranker model updates must be
  regression-tested against golden dataset)

## ‚ö†Ô∏è WARNING TO FUTURE MAINTAINERS
DO NOT REMOVE the re-ranking stage to "reduce latency." The base
embedding model has a 20% recall gap on clinical jargon that makes
vector-only retrieval UNSAFE for surgical protocols. If you need
lower latency, optimize the vector retrieval stage or upgrade to a
medical-specific embedding model ‚Äî do not remove the safety net.
```

### Why Context-Aware ADRs Matter

| Standard ADR | Context-Aware ADR |
|---|---|
| "We chose a re-ranker for better accuracy" | "We chose a re-ranker because text-embedding-3-small has a 20% recall gap on clinical jargon, and missed surgical protocols are a safety failure" |
| "We chose Pinecone for multi-tenancy" | "We chose Pinecone because our 2-engineer team cannot absorb the 8 hrs/week of Postgres ops that pgvector requires, and namespace isolation is a HIPAA requirement" |
| "We chose LangGraph for checkpointing" | "We chose LangGraph because CrewAI's opaque routing made it impossible to produce the deterministic audit trail required by our HIPAA compliance officer" |

**The difference**: A standard ADR tells you **what** was decided. A context-aware ADR tells you **what would break if you reversed it** ‚Äî which is the only information a future maintainer actually needs.

---

## References

- [HIPAA Technical Safeguards](https://www.hhs.gov/hipaa/for-professionals/security/laws-regulations/index.html)
- [Pinecone Namespaces Documentation](https://docs.pinecone.io/docs/namespaces)
- [LangGraph Checkpointing](https://langchain-ai.github.io/langgraph/concepts/#checkpointing)
```

---

## Pattern 2: Skill Diagnosis Brief

**The Pattern**: Generate a verified telemetry report proving mastery across the 7 core domains.

### Skill Diagnosis Report Template

```markdown
# AI Architect Skill Diagnosis

**Candidate**: [Your Name]
**Assessment Date**: [Date]
**Overall Score**: 87/100 (Senior-level proficiency)

---

## Domain Scores

| Domain | Score | Level | Evidence |
|--------|-------|-------|----------|
| **LLM Fundamentals** | 92/100 | Expert | Implemented multi-model fallback (Opus‚ÜíSonnet‚ÜíHaiku) with cost optimization |
| **Prompt Engineering** | 88/100 | Advanced | Designed system prompts as immutable policy with semantic tracing |
| **RAG Systems** | 90/100 | Expert | Built hybrid search (vector + BM25) with re-ranking achieving 95% faithfulness |
| **Agentic Systems** | 85/100 | Advanced | Implemented Supervisor pattern with 3 specialists + state checkpointing |
| **Safety & Governance** | 95/100 | Expert | 100% adversarial attack block rate, HIPAA-compliant audit trails |
| **Production Engineering** | 82/100 | Advanced | Load tested at 10x spike, P95 &lt;3s, 99.5% availability |
| **Multimodal AI** | 75/100 | Intermediate | Image analysis for document OCR (limited deployment experience) |

**Overall Assessment**: **Senior AI Architect** - Ready for production-grade system ownership

---

## Project Portfolio

### Project 1: Multi-Tenant Medical AI Assistant
- **Tech Stack**: Claude 4.5 Sonnet, Pinecone, LangGraph, Next.js
- **Scale**: 100+ tenants, 10M+ vectors, 100K requests/month
- **Key Achievements**:
  - 96% faithfulness on medical queries (golden dataset: 100+ cases)
  - &lt;3s P95 latency under 10x load spike
  - 100% HIPAA compliance (PHI redaction, audit trails)
  - $0.08 cost per request (20% under budget)

### Project 2: Legal Document Analysis Platform
- **Tech Stack**: GPT-4o, pgvector, Custom React Dashboard
- **Scale**: 50K documents, 5M embeddings
- **Key Achievements**:
  - Jurisdiction-specific precedent matching (98% accuracy)
  - Risky term detection (92% recall on test set)
  - Sub-200ms vector search latency

---

## Architectural Decisions (Highlights)

### 1. Vector Database Selection: Pinecone vs pgvector
**Decision**: Pinecone for managed multi-tenancy
**Justification**: Native namespaces + 0 ops overhead justifies 2x cost vs pgvector

### 2. Agent Framework: LangGraph vs CrewAI
**Decision**: LangGraph for production control
**Justification**: Built-in checkpointing + explicit state machine vs 3.4x lower overhead

### 3. Safety Architecture: Multi-Layer Defense
**Implementation**: Safety Proxy ‚Üí Jailbreak Detection ‚Üí Output Validation
**Result**: 100% block rate on 50+ adversarial test cases

---

## Production Metrics (Verified)

| Metric | Value | Industry Benchmark | Percentile |
|--------|-------|-------------------|------------|
| Faithfulness | 96% | 85% | 95th |
| P95 Latency | 2.1s | 3.5s | 80th |
| Cost Efficiency | $0.08/req | $0.12/req | 75th |
| Security Score | 98/100 | 85/100 | 90th |
| Availability | 99.5% | 99.0% | 70th |

---

## Code Samples

### Sample 1: Safety Proxy with Jailbreak Detection
[Link to GitHub: safety-proxy.ts]

**Highlights**:
- Pattern matching for DAN attacks, role hijacking, delimiter confusion
- &lt;5ms detection latency (regex-based fast path)
- 100% block rate on adversarial test suite (50+ cases)

### Sample 2: Hybrid Search with Re-ranking
[Link to GitHub: hybrid-search.ts]

**Highlights**:
- Vector (semantic) + BM25 (keyword) fusion
- Cross-encoder re-ranking (top 100 ‚Üí top 10)
- 15% accuracy improvement vs vector-only search

### Sample 3: State Checkpointing for Agent Threads
[Link to GitHub: checkpointed-agent.ts]

**Highlights**:
- Postgres + Redis hybrid (3.8x faster than Postgres-only)
- Time-travel debugging (replay from any checkpoint)
- 38% cost reduction on crash recovery

---

## Recommendations for Growth

1. **Multimodal Mastery**: Gain production experience with vision models (current: intermediate level)
2. **Fine-Tuning**: Implement domain-specific fine-tuned models for cost reduction
3. **Distributed Systems**: Architect for multi-region deployment (global latency optimization)

---

## References

- Portfolio: [yourportfolio.com]
- GitHub: [github.com/yourname]
- LinkedIn: [linkedin.com/in/yourname]
- System Design Doc: [Attached]
```

---

## Senior Leadership Review Challenge

**Scenario**: An Executive asks: "Why are we spending $2,000 a month on 'Evaluation Models' (LLM-as-a-Judge) when we could just have our interns test the app for free?" How does your SDD justify this cost?

**A)** It explains that interns are too slow.

**B)** It maps the cost to **Risk Mitigation**. The SDD shows that the Automated Evaluation Suite caught a "Faithfulness" regression that would have resulted in a $50,000 compliance fine. You prove that the $24K/year spent on automated QA provides a **2x ROI** compared to manual labor while maintaining a **99.9% safety guarantee** that humans cannot match.

**C)** It says that "Teacher" models are the industry standard.

**D)** It suggests hiring fewer interns to balance the budget.

<details>
<summary>Answer</summary>

**B) Risk Mitigation with quantified ROI** is the architect-level answer.

- **Why not A?** "Interns are slow" is a developer-level argument that doesn't speak the executive's language. Speed is a secondary concern ‚Äî the real issue is **reliability and liability**.
- **Why not C?** "Industry standard" is an appeal to authority, not evidence. An executive will ask "So what?" ‚Äî they want to see the financial case, not a trend report.
- **Why not D?** This completely sidesteps the question and doesn't justify the technology investment at all.
- **Why B?** An Architect's documentation is a **financial and risk-management tool**, not just a technical one:
  1. **Quantified risk**: The evaluation suite caught a faithfulness regression before production. Without it, the incorrect medical guidance would have reached patients ‚Äî triggering a $50K+ HIPAA compliance fine.
  2. **ROI calculation**: $24K/year (automated) vs. $120K/year (3 interns at $40K) = **5x cost reduction** with higher coverage (1,000 test cases vs. ~50 manual checks per day).
  3. **Safety guarantee**: Automated evaluation runs on every deployment, 24/7. Interns test during business hours, miss edge cases, and cannot maintain 99.9% consistency across 1,000+ golden dataset cases.
  4. **The SDD proves this**: Your cost analysis section shows the $2K/month maps directly to the "0 safety violations" metric in Performance Validation. Remove the spend, and you lose the safety guarantee.

**The Architect's Insight**: When an executive challenges a cost, they're not asking "Is this useful?" ‚Äî they're asking "What happens if we don't have it?" Your SDD must answer the second question with numbers.

</details>

---

## Key Takeaways

**System Design Document (SDD)**:
- Explain **WHY** behind every major decision (not just WHAT you built)
- **Use Quantitative Trade-off Matrices** ‚Äî map every choice to the specific Requirement and Constraint (Cost, Latency, Security, Scale) that drove it
- Use Architectural Decision Records (ADRs) for vector DB, model, framework selections
- **Include NFR Benchmark Dashboards** with stress-test evidence and degradation curves, not just happy-path numbers
- Document trade-offs accepted with quantified impact (cost vs accuracy, latency vs quality)

**Context-Aware ADRs**:
- Document the **Context** ‚Äî market conditions, tool limitations, and domain constraints that existed at the time
- Include **warnings to future maintainers** explaining what would break if the decision were reversed
- A standard ADR tells you what was decided; a **context-aware ADR tells you what breaks if you undo it**

**Skill Diagnosis Brief**:
- Prove mastery across 7 domains with concrete evidence
- Portfolio projects with scale metrics (users, vectors, requests)
- Production metrics (faithfulness, latency, cost, security score)
- Code samples demonstrating architectural thinking
- Growth areas showing self-awareness

**Packaging for Stakeholders**:
- **Executives**: Executive summary with business metrics ‚Äî **justify costs as risk mitigation with quantified ROI**
- **Technical Leaders**: Context-aware ADRs with benchmarks and trade-off matrices
- **Hiring Managers**: Skill diagnosis with verified production metrics
- **Engineers**: Code samples with architectural patterns

**The Architect's Responsibility**:
You **own** the story. If stakeholders don't understand your decisions, **you failed to communicate**. If hiring managers can't evaluate your work, **you didn't provide evidence**. If future maintainers break your system, **you didn't document the context that drove your constraints**.

**Impact**:
- **Without SDD**: "It works" (no justification, future maintainers break it)
- **With SDD**: "Here's why Pinecone over pgvector for our multi-tenant use case" (informed decisions, maintainable systems)

**Portfolio ROI**:
- Time investment: 2 days for comprehensive SDD + Skill Diagnosis
- Interview conversion: 3x higher (employers see architectural thinking)
- Salary impact: $20K+ higher offers (demonstrated senior-level capability)

Week 8 Complete: You've proven **Architectural Authority** through production-ready systems, comprehensive stress-testing, automated evaluation, and compelling documentation.
