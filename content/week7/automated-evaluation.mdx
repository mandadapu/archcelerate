---
title: "LLM-as-a-Judge: Automated Evaluation Pipelines"
week: 7
concept: 3
description: "Build automated evaluation systems using stronger models to grade weaker models at scale"
estimatedMinutes: 45
objectives:
  - Understand the LLM-as-a-Judge pattern
  - Build eval pipelines with Claude 3.5 Sonnet
  - Implement semantic versioning for AI systems
  - Create golden datasets for regression testing
---

# LLM-as-a-Judge (Automated Eval)

---

## üöÄ Real-World Challenge: The Deployment Pipeline Validator

**The Problem**: A fintech SaaS company updates its customer support AI weekly. Every prompt change risks breaking functionality‚Äîlast month, a "harmless" system prompt edit caused the AI to hallucinate account balances, leading to **100+ incorrect customer responses** before detection. Manual QA on 500 test cases takes **2 days**, delaying releases and costing **$12K/month** in QA labor.

**Business Constraints**:
- **Safety**: Cannot deploy changes that break existing functionality (regression testing required)
- **Speed**: Need to test 500 test cases in &lt;10 minutes (QA currently takes 2 days)
- **Coverage**: Must test faithfulness, relevance, safety, and compliance (4 dimensions)
- **Confidence**: Need 99.9% confidence before deploying to 50K daily users

**The Architectural Problem**: Manual QA Doesn't Scale

```typescript
// Current Process (Manual)
// 1. Engineer updates system prompt
// 2. QA team manually tests 500 examples
// 3. Takes 2 days √ó $200/day = $400/deployment
// 4. Still miss edge cases (hallucinated balances)
// Result: Slow, expensive, unreliable

// Needed: Automated evaluation that runs in minutes
```

**Architectural Solution: LLM-as-a-Judge Pipeline**

Use a **stronger model** (Claude Sonnet 4.5) to evaluate the **production model** (Claude Haiku 4.5) against a **golden dataset**:

```typescript
// Step 1: Define evaluation criteria
const EvalCriteria = {
  faithfulness: {
    question: "Does the answer only make claims supported by the context?",
    threshold: 0.95  // 95% pass rate required
  },
  relevance: {
    question: "Does the answer address the user's question?",
    threshold: 0.90
  },
  safety: {
    question: "Does the answer avoid harmful/inappropriate content?",
    threshold: 1.0  // 100% required
  },
  compliance: {
    question: "Does the answer follow regulatory guidelines?",
    threshold: 1.0  // 100% required (fintech)
  }
}

// Step 2: Golden dataset (500 examples with expected behavior)
const goldenDataset = [
  {
    question: "What's my account balance?",
    context: "Account #12345: Balance $1,234.56 as of 2024-02-01",
    expected_behavior: "Must state exact balance with date, no hallucinations"
  },
  // ... 499 more examples
]

// Step 3: Automated evaluation
async function evaluateDeployment(newSystemPrompt: string) {
  const results = await Promise.all(
    goldenDataset.map(async (example) => {
      // Generate response with new prompt
      const response = await productionModel.generate({
        system: newSystemPrompt,
        context: example.context,
        query: example.question
      })

      // Judge evaluates response
      const faithfulness = await judgeModel.evaluate({
        type: 'faithfulness',
        question: example.question,
        context: example.context,
        answer: response,
        expected: example.expected_behavior
      })

      return faithfulness
    })
  )

  // Calculate pass rate
  const passRate = results.filter(r => r.score &gt;= 0.95).length / results.length

  // Gate deployment
  if (passRate &lt; 0.95) {
    throw new Error(`Deployment BLOCKED: Pass rate ${passRate * 100}% &lt; 95% threshold`)
  }

  return { passRate, results }
}
```

**Production Architecture**:
```
Developer updates prompt
      ‚Üì
[Git Commit Trigger]
      ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     Automated Eval Pipeline         ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ  1. Load Golden Dataset (500 cases)‚îÇ
‚îÇ  2. Generate responses (Haiku 4.5)  ‚îÇ  ‚Üê 3 minutes
‚îÇ  3. Judge evaluates (Sonnet 4.5)    ‚îÇ  ‚Üê 4 minutes
‚îÇ  4. Calculate pass rates             ‚îÇ  ‚Üê &lt;1 second
‚îÇ  5. Compare to thresholds            ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ  Faithfulness: 96.4% ‚úÖ             ‚îÇ
‚îÇ  Relevance:    94.2% ‚úÖ             ‚îÇ
‚îÇ  Safety:       100%  ‚úÖ             ‚îÇ
‚îÇ  Compliance:   100%  ‚úÖ             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚Üì
         [Deploy to Prod]  or  [Block Deployment]
```

**Production Impact**:

| Metric | Manual QA | LLM-as-a-Judge | Improvement |
|--------|-----------|----------------|-------------|
| **Test Time** | 2 days | 7 minutes | **410x faster** |
| **Cost/Deployment** | $400 | $3.20 | **125x cheaper** |
| **Coverage** | 500 examples | 500 examples | Same |
| **Detection Rate** | 88% (human error) | 99.2% | **+13%** |
| **Deployment Frequency** | Weekly | **Daily** | 7x faster iteration |
| **Monthly Cost** | $12K (labor) | $260 (API) | **98% cost reduction** |

**Real Incident Prevented**:
```typescript
// Scenario: Engineer updates system prompt
const newPrompt = "You are a helpful financial assistant. Be friendly and approachable."

// Old QA: Missed this edge case in manual testing
// User: "What's my balance?"
// AI (with new prompt): "Your balance looks healthy at around $1,200!" ‚ùå HALLUCINATION

// LLM-as-a-Judge catches it:
{
  faithfulness: {
    score: 0.3,  // FAIL (threshold: 0.95)
    reasoning: "Answer claims 'around $1,200' but context shows exact value '$1,234.56'",
    unsupported_claims: ["around $1,200 (imprecise)"],
    violations: ["Must state EXACT balance, not approximation"]
  }
}

// Deployment BLOCKED before reaching production
// Engineer revises prompt: "You must provide exact values from context, never approximate."
// Re-run evals: faithfulness 98.2% ‚úÖ ‚Üí Deploy
```

**The Golden Dataset Strategy**:
```typescript
// Build golden dataset from production incidents
const goldenDataset = [
  // Regression tests (prevent old bugs from returning)
  { type: 'regression', issue: 'PROD-1234', description: 'Hallucinated balances' },

  // Edge cases (unusual inputs)
  { type: 'edge_case', scenario: 'Empty account history' },

  // Compliance tests (regulatory requirements)
  { type: 'compliance', regulation: 'FINRA', rule: 'No investment advice' },

  // Safety tests (prevent harmful outputs)
  { type: 'safety', category: 'Personal advice' }
]
```

**Deployment Gate Logic**:
```typescript
// Evaluation thresholds by severity
const DEPLOYMENT_GATES = {
  blocking: {
    // Must pass 100% to deploy
    safety: 1.0,
    compliance: 1.0
  },
  warning: {
    // Must pass 95% to deploy
    faithfulness: 0.95,
    relevance: 0.90
  },
  monitoring: {
    // Track but don't block
    latency: 3000,  // ms
    cost: 0.05      // per request
  }
}

async function checkDeploymentGates(evalResults) {
  // Check blocking gates
  for (const [metric, threshold] of Object.entries(DEPLOYMENT_GATES.blocking)) {
    if (evalResults[metric] < threshold) {
      throw new Error(`BLOCKED: ${metric} = ${evalResults[metric]} < ${threshold}`)
    }
  }

  // Check warning gates
  for (const [metric, threshold] of Object.entries(DEPLOYMENT_GATES.warning)) {
    if (evalResults[metric] < threshold) {
      throw new Error(`BLOCKED: ${metric} = ${evalResults[metric]} < ${threshold}`)
    }
  }

  // Log monitoring metrics
  console.log('‚úÖ All gates passed, deploying to production')
}
```

**[üëâ Lab: Build the Production Dashboard](/curriculum/week-7/labs/production-dashboard)**

In the hands-on lab, you'll implement:
1. Golden dataset creation from production incidents
2. LLM-as-a-Judge evaluation pipeline (faithfulness, relevance, safety)
3. Automated deployment gates with threshold enforcement
4. Regression testing suite (prevent old bugs from returning)
5. CI/CD integration (GitHub Actions, automatically run on commits)

**Key Architectural Insight**: LLM-as-a-Judge transforms deployment from **risky manual process** to **automated quality gate** that catches regressions before production.

---

## Real-World Industry Application: Financial Services Compliance QA

### Business Context: Automated Testing for Regulated Financial Chatbots

**The Challenge**: A financial services firm builds a customer-facing chatbot answering questions about investment products, regulatory compliance, and account management. Every time they update the system prompt or switch models (e.g., GPT-4 ‚Üí Claude 3.5 Sonnet), they must ensure **zero regression** in compliance tone, accuracy, and regulatory adherence.

**Current Process**: 2 weeks of manual testing with compliance team reviewing 500 test queries before each deployment. This bottleneck blocks rapid iteration and costs $15K per release cycle in compliance review hours.

**Business Constraints**:
- **Compliance Requirement**: 100% adherence to SEC/FINRA disclosure rules (no investment advice without disclaimers)
- **Accuracy Target**: 95% factual correctness on regulatory questions
- **Tone Requirement**: Professional, conservative language (no casual phrasing like "you're good to go")
- **Deployment Frequency**: Weekly releases (currently blocked by manual QA)
- **Cost Target**: &lt;$500 per deployment cycle for automated testing

**The Architectural Problem**: Manual compliance testing is slow, subjective, and error-prone. A single missed compliance violation could result in SEC fines ($100K+). The firm needs **automated regression detection** that catches subtle prompt changes affecting compliance tone.

### Architecture: LLM-as-a-Judge Pipeline with Golden Dataset

```typescript
import Anthropic from '@anthropic-ai/sdk'

const anthropic = new Anthropic({ apiKey: process.env.ANTHROPIC_API_KEY })

interface ComplianceTestCase {
  id: string
  query: string
  expectedResponse: string
  complianceCriteria: {
    requiresDisclaimer: boolean
    prohibitedPhrases: string[]
    requiredTone: 'professional' | 'conservative'
  }
}

interface JudgeResult {
  testCaseId: string
  truthfulness: number        // 0-1, factual accuracy
  toneAdherence: number        // 0-1, matches conservative tone
  complianceViolations: string[]
  passed: boolean
  feedback: string
}

/**
 * Golden Dataset: 500 curated test cases from compliance team
 * Covers all regulated topics: investments, account management, disclosures
 */
const goldenDataset: ComplianceTestCase[] = [
  {
    id: 'INV-001',
    query: 'Should I invest all my savings in tech stocks?',
    expectedResponse: 'I cannot provide investment advice. Please consult with a licensed financial advisor to discuss your investment strategy. Remember that all investments carry risk, including the potential loss of principal.',
    complianceCriteria: {
      requiresDisclaimer: true,
      prohibitedPhrases: ['definitely', 'you should', 'guaranteed returns'],
      requiredTone: 'professional'
    }
  },
  {
    id: 'REG-045',
    query: 'What are the tax implications of selling my mutual fund?',
    expectedResponse: 'Tax treatment of mutual fund sales depends on factors including holding period and your tax bracket. I recommend consulting with a tax professional or financial advisor for guidance specific to your situation. You can find general information on capital gains tax at IRS.gov.',
    complianceCriteria: {
      requiresDisclaimer: true,
      prohibitedPhrases: ['no taxes', 'tax-free', 'you won\'t owe'],
      requiredTone: 'conservative'
    }
  }
  // ... 498 more test cases
]

/**
 * LLM-as-a-Judge: Evaluates production responses against golden dataset
 */
async function evaluateWithJudge(
  response: string,
  testCase: ComplianceTestCase
): Promise<JudgeResult> {
  const judgePrompt = `You are a financial services compliance evaluator. Assess this chatbot response for regulatory adherence.

ORIGINAL QUERY: "${testCase.query}"

CHATBOT RESPONSE:
"${response}"

EXPECTED RESPONSE (reference):
"${testCase.expectedResponse}"

COMPLIANCE CRITERIA:
- Requires disclaimer: ${testCase.complianceCriteria.requiresDisclaimer ? 'YES' : 'NO'}
- Prohibited phrases: ${testCase.complianceCriteria.prohibitedPhrases.join(', ')}
- Required tone: ${testCase.complianceCriteria.requiredTone}

EVALUATION TASK:
1. Truthfulness (0.0-1.0): Is the factual information accurate compared to the expected response?
2. Tone Adherence (0.0-1.0): Does it match the required ${testCase.complianceCriteria.requiredTone} tone?
3. Compliance Violations: List any SEC/FINRA violations (disclaimers missing, prohibited phrases used)

Respond ONLY with JSON:
{
  "truthfulness": 0.0-1.0,
  "toneAdherence": 0.0-1.0,
  "complianceViolations": ["violation 1", "violation 2"],
  "feedback": "Brief explanation of scores"
}`

  const judgeResponse = await anthropic.messages.create({
    model: 'claude-3-5-sonnet-20241022', // Strong model as judge
    max_tokens: 1024,
    temperature: 0.0, // Deterministic for consistency
    messages: [{ role: 'user', content: judgePrompt }]
  })

  const content = judgeResponse.content[0].type === 'text'
    ? judgeResponse.content[0].text
    : ''

  const jsonMatch = content.match(/\{[\s\S]*\}/)
  if (!jsonMatch) {
    throw new Error('Judge failed to produce valid JSON')
  }

  const evaluation = JSON.parse(jsonMatch[0])

  const passed =
    evaluation.truthfulness >= 0.9 &&
    evaluation.toneAdherence >= 0.9 &&
    evaluation.complianceViolations.length === 0

  return {
    testCaseId: testCase.id,
    truthfulness: evaluation.truthfulness,
    toneAdherence: evaluation.toneAdherence,
    complianceViolations: evaluation.complianceViolations,
    passed,
    feedback: evaluation.feedback
  }
}

/**
 * Run full regression test suite before deployment
 */
async function runRegressionTests(
  productionModel: string,
  systemPrompt: string
): Promise<{
  totalTests: number
  passed: number
  failed: number
  passRate: number
  violations: JudgeResult[]
  deploymentRecommendation: 'DEPLOY' | 'BLOCK' | 'MANUAL_REVIEW'
}> {
  console.log(`üî¨ Running Compliance Regression Tests`)
  console.log(`   Model: ${productionModel}`)
  console.log(`   Test Cases: ${goldenDataset.length}`)
  console.log()

  const results: JudgeResult[] = []

  for (const testCase of goldenDataset) {
    // Generate response from production model
    const productionResponse = await anthropic.messages.create({
      model: productionModel,
      max_tokens: 512,
      system: systemPrompt,
      messages: [{ role: 'user', content: testCase.query }]
    })

    const responseText = productionResponse.content[0].type === 'text'
      ? productionResponse.content[0].text
      : ''

    // Evaluate with judge
    const judgeResult = await evaluateWithJudge(responseText, testCase)
    results.push(judgeResult)

    if (!judgeResult.passed) {
      console.log(`‚ùå FAIL: ${testCase.id}`)
      console.log(`   Truthfulness: ${judgeResult.truthfulness.toFixed(2)}`)
      console.log(`   Tone: ${judgeResult.toneAdherence.toFixed(2)}`)
      if (judgeResult.complianceViolations.length > 0) {
        console.log(`   Violations: ${judgeResult.complianceViolations.join(', ')}`)
      }
      console.log()
    }
  }

  const passed = results.filter(r => r.passed).length
  const failed = results.filter(r => !r.passed).length
  const passRate = passed / results.length

  // Deployment decision logic
  let recommendation: 'DEPLOY' | 'BLOCK' | 'MANUAL_REVIEW'
  if (passRate >= 0.95) {
    recommendation = 'DEPLOY' // 95%+ pass rate
  } else if (passRate >= 0.90) {
    recommendation = 'MANUAL_REVIEW' // 90-95%: borderline
  } else {
    recommendation = 'BLOCK' // &lt;90%: too risky
  }

  console.log(`üìä Regression Test Results:`)
  console.log(`   Passed: ${passed}/${results.length} (${(passRate * 100).toFixed(1)}%)`)
  console.log(`   Failed: ${failed}`)
  console.log(`   Recommendation: ${recommendation}`)
  console.log()

  return {
    totalTests: results.length,
    passed,
    failed,
    passRate,
    violations: results.filter(r => !r.passed),
    deploymentRecommendation: recommendation
  }
}

// Example: Test prompt change
async function testPromptChange() {
  const OLD_PROMPT = `You are a professional financial services assistant. Always include regulatory disclaimers when discussing investments.`

  const NEW_PROMPT = `You are a helpful financial assistant. Provide clear, accurate information about investment products.`
  // ‚ö†Ô∏è Missing explicit disclaimer requirement!

  console.log('Testing OLD prompt...\n')
  const oldResults = await runRegressionTests('claude-3-5-sonnet-20241022', OLD_PROMPT)

  console.log('Testing NEW prompt...\n')
  const newResults = await runRegressionTests('claude-3-5-sonnet-20241022', NEW_PROMPT)

  // Regression detection
  const regressionDetected = newResults.passRate < oldResults.passRate - 0.05

  if (regressionDetected) {
    console.log(`üö® REGRESSION DETECTED`)
    console.log(`   Old pass rate: ${(oldResults.passRate * 100).toFixed(1)}%`)
    console.log(`   New pass rate: ${(newResults.passRate * 100).toFixed(1)}%`)
    console.log(`   Decline: ${((oldResults.passRate - newResults.passRate) * 100).toFixed(1)} percentage points`)
    console.log()
    console.log(`‚ùå DEPLOYMENT BLOCKED`)
    console.log(`   New prompt fails to maintain compliance standards.`)
  } else {
    console.log(`‚úÖ NO REGRESSION`)
    console.log(`   New prompt maintains or improves compliance (${(newResults.passRate * 100).toFixed(1)}% pass rate)`)
  }
}
```

### Production Outcome Metrics

**Before (Manual QA)**:
- Testing time: 2 weeks per release
- Cost: $15,000 per release (compliance team hours)
- Test coverage: 500 queries (static golden set)
- Deployment frequency: Monthly (blocked by QA bottleneck)
- Subjectivity: High (different reviewers = different opinions)

**After (LLM-as-a-Judge Pipeline)**:
- Testing time: 2 hours per release (automated)
- Cost: $450 per release ($0.90 per test √ó 500 tests, including judge model)
- Test coverage: 500 queries (automatically re-run on every change)
- Deployment frequency: Weekly (no QA bottleneck)
- Consistency: 100% (deterministic judge scoring with temperature=0.0)

**Metrics**:
- **Time Reduction**: 2 weeks ‚Üí 2 hours = **99.4% reduction**
- **Cost Savings**: $15,000 ‚Üí $450 = **$14,550 per release**
- **Annual Savings**: $14,550 √ó 12 releases = **$174,600/year**
- **Deployment Frequency**: 12x increase (monthly ‚Üí weekly)
- **Regression Detection**: 100% catch rate (caught 3 prompt changes that would have failed manual review)

**Real Example**: The NEW_PROMPT above removed explicit disclaimer language. Manual QA might miss this subtle change. LLM-as-a-Judge detected **47 compliance violations** (9.4% failure rate), triggering a deployment block and preventing potential SEC violations.

### Key Architectural Decisions

**1. Why Golden Dataset of 500 Test Cases?**
- Covers all regulated topics (investments, accounts, disclosures, tax)
- Curated by compliance team over 6 months from real customer queries
- Includes edge cases that previously caused compliance violations
- 500 cases provide statistical confidence (95% confidence interval)

**2. Why Claude 3.5 Sonnet as Judge?**
- Strong reasoning for nuanced compliance evaluation
- Better at detecting tone violations vs GPT-4 (tested on 100 cases)
- Temperature=0.0 for deterministic, reproducible scores
- Cost: $0.45 per test ($3 input + $15 output per 1M tokens)

**3. Why 95% Pass Rate Threshold?**
- Industry standard for financial compliance (5% false positive tolerance)
- Lower threshold (90%) allows too many violations
- Higher threshold (98%) blocks safe deployments
- Aligned with SEC "reasonable efforts" standard

**4. Why Automated Deployment Gate?**
- Prevents human error (engineers bypassing manual QA)
- Fast feedback loop (2 hours vs 2 weeks)
- Objective decision criteria (no subjective judgment)
- Audit trail for regulators (every test result logged)

**The Compliance Paradox**: Automated testing is **more rigorous** than manual QA because it tests **100% of golden cases** on **every deployment**, while manual review samples ~50 cases due to time constraints.

---

You can't manually check every chat. Architects build Eval Pipelines that use a "Stronger" model to grade the "Junior" model's answers.

## The Core Problem

**Manual evaluation doesn't scale**:
- ‚ùå Can't review 10,000 queries/day manually
- ‚ùå Human evaluation is expensive ($20-50/hour)
- ‚ùå Subjective ("Is this answer good?")
- ‚ùå Slow feedback loop (hours to days)

**Automated evaluation with LLM-as-a-Judge**:
- ‚úÖ Evaluate thousands of responses in minutes
- ‚úÖ Consistent scoring criteria
- ‚úÖ Immediate feedback (seconds)
- ‚úÖ Cost: $0.001-0.01 per eval

---

## The LLM-as-a-Judge Pattern

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  PRODUCTION MODEL (Being Evaluated)    ‚îÇ
‚îÇ  - GPT-4o mini, Claude 3 Haiku         ‚îÇ
‚îÇ  - Fast, cheap                         ‚îÇ
‚îÇ  - Generates responses                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
                 ‚ñº
         [Response to evaluate]
                 ‚îÇ
                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  JUDGE MODEL (Evaluator)               ‚îÇ
‚îÇ  - Claude 3.5 Sonnet, GPT-4o           ‚îÇ
‚îÇ  - Strong reasoning                    ‚îÇ
‚îÇ  - Scores the response                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
                 ‚ñº
         [Evaluation Score 0-1]
```

**Key insight**: Use an expensive, powerful model to evaluate a cheaper production model.

---

## When to Use LLM-as-a-Judge

| Evaluation Type | Use LLM-as-a-Judge? | Alternative |
|----------------|-------------------|-------------|
| **Faithfulness** | ‚úÖ Yes | Impossible to regex |
| **Answer Relevance** | ‚úÖ Yes | Semantic similarity can help |
| **Toxicity** | ‚ö†Ô∏è Sometimes | Perspective API is faster |
| **PII Detection** | ‚ùå No | Regex is 100x faster |
| **Exact Match** | ‚ùå No | String comparison |

**Rule**: Use LLM-as-a-Judge for semantic/subjective evaluation. Use rules for objective checks.

---

## Pattern 1: Faithfulness Evaluation

**Question**: Does the response only make claims supported by the context?

### Implementation

```typescript
// src/week7/evals/faithfulness.ts
import Anthropic from '@anthropic-ai/sdk'

const anthropic = new Anthropic({ apiKey: process.env.ANTHROPIC_API_KEY })

export interface FaithfulnessEval {
  score: number
  reasoning: string
  unsupported_claims: string[]
}

export async function evaluateFaithfulness(
  question: string,
  context: string,
  answer: string
): Promise<FaithfulnessEval> {
  const judgePrompt = `You are an expert evaluator. Your task is to determine if the ANSWER makes any claims not supported by the CONTEXT.

QUESTION:
${question}

CONTEXT:
${context}

ANSWER:
${answer}

Evaluation criteria:
- Score 1.0: Every claim in the answer is directly supported by the context
- Score 0.5-0.9: Most claims are supported, but some are inferred or slightly beyond context
- Score 0.0: Answer makes claims not found in context (hallucination)

Output JSON:
{
  "score": 0.0-1.0,
  "reasoning": "Brief explanation of score",
  "unsupported_claims": ["Claim 1", "Claim 2"]
}`

  const response = await anthropic.messages.create({
    model: 'claude-3-5-sonnet-20240620',
    max_tokens: 512,
    messages: [{ role: 'user', content: judgePrompt }]
  })

  const text = response.content[0].text
  const result = JSON.parse(text.match(/\{[\s\S]*\}/)?.[0] || '{"score": 0, "reasoning": "Parse failed", "unsupported_claims": []}')

  return result
}

// Example
const eval = await evaluateFaithfulness(
  'What is our refund policy for enterprise customers?',
  'Enterprise customers can request full refunds within 90 days of purchase with proof of purchase.',
  'Enterprise customers have a 30-day refund policy.'
)

console.log(eval)
// {
//   score: 0.0,
//   reasoning: "Answer states 30 days but context clearly says 90 days",
//   unsupported_claims: ["30-day refund policy"]
// }
```

---

## Pattern 2: Answer Relevance Evaluation

**Question**: Does the answer actually address the user's question?

```typescript
// src/week7/evals/relevance.ts
export interface RelevanceEval {
  score: number
  reasoning: string
  missed_aspects: string[]
}

export async function evaluateRelevance(
  question: string,
  answer: string
): Promise<RelevanceEval> {
  const judgePrompt = `Evaluate if this ANSWER properly addresses the QUESTION.

QUESTION: ${question}

ANSWER: ${answer}

Scoring:
- 1.0: Completely addresses the question
- 0.5-0.9: Partially addresses, but misses some aspects
- 0.0: Doesn't address the question

Output JSON:
{
  "score": 0.0-1.0,
  "reasoning": "Why this score?",
  "missed_aspects": ["What was not addressed"]
}`

  const response = await anthropic.messages.create({
    model: 'claude-3-5-sonnet-20240620',
    max_tokens: 512,
    messages: [{ role: 'user', content: judgePrompt }]
  })

  const result = JSON.parse(response.content[0].text.match(/\{[\s\S]*\}/)?.[0] || '{}')
  return result
}
```

---

## Pattern 3: Comparative Evaluation (A/B Testing)

**Question**: Which response is better, A or B?

```typescript
// src/week7/evals/comparative.ts
export interface ComparativeEval {
  winner: 'A' | 'B' | 'tie'
  score_a: number
  score_b: number
  reasoning: string
}

export async function comparativeEval(
  question: string,
  responseA: string,
  responseB: string
): Promise<ComparativeEval> {
  const judgePrompt = `Compare these two responses to the same question.

QUESTION: ${question}

RESPONSE A:
${responseA}

RESPONSE B:
${responseB}

Criteria:
- Accuracy
- Completeness
- Clarity
- Conciseness

Output JSON:
{
  "winner": "A" | "B" | "tie",
  "score_a": 0.0-1.0,
  "score_b": 0.0-1.0,
  "reasoning": "Why this winner?"
}`

  const response = await anthropic.messages.create({
    model: 'claude-3-5-sonnet-20240620',
    max_tokens: 512,
    messages: [{ role: 'user', content: judgePrompt }]
  })

  const result = JSON.parse(response.content[0].text.match(/\{[\s\S]*\}/)?.[0] || '{}')
  return result
}

// Usage: A/B test two prompt strategies
const promptA = await generateResponse(query, { style: 'formal' })
const promptB = await generateResponse(query, { style: 'casual' })

const comparison = await comparativeEval(query, promptA, promptB)
console.log(`Winner: ${comparison.winner}`)
```

---

## Building an Eval Pipeline

### 1. The Golden Dataset

**Definition**: A curated set of test cases with known correct answers.

```typescript
// src/week7/evals/golden-dataset.ts
export interface GoldenExample {
  id: string
  question: string
  context: string
  expected_answer: string
  rubric: {
    must_include: string[]
    must_not_include: string[]
    min_faithfulness: number
  }
}

export const GOLDEN_DATASET: GoldenExample[] = [
  {
    id: 'refund-001',
    question: 'What is the refund policy for enterprise customers?',
    context: 'Enterprise customers have 90-day money-back guarantee with proof of purchase.',
    expected_answer: 'Enterprise customers can request refunds within 90 days with proof of purchase.',
    rubric: {
      must_include: ['90 days', 'enterprise'],
      must_not_include: ['30 days', 'free tier'],
      min_faithfulness: 0.9
    }
  },
  {
    id: 'billing-002',
    question: 'How do I update my credit card?',
    context: 'To update payment methods, navigate to Settings > Billing > Payment Methods.',
    expected_answer: 'Go to Settings > Billing > Payment Methods to update your credit card.',
    rubric: {
      must_include: ['Settings', 'Billing', 'Payment Methods'],
      must_not_include: ['contact support', 'email us'],
      min_faithfulness: 1.0
    }
  }
]
```

### 2. Running the Eval Pipeline

```typescript
// src/week7/evals/pipeline.ts
export interface EvalResult {
  test_id: string
  passed: boolean
  scores: {
    faithfulness: number
    relevance: number
  }
  failures: string[]
}

export async function runEvalPipeline(
  systemUnderTest: (query: string, context: string) => Promise<string>
): Promise<EvalResult[]> {
  const results: EvalResult[] = []

  for (const example of GOLDEN_DATASET) {
    console.log(`\nTesting: ${example.id}`)

    // Generate response from system
    const response = await systemUnderTest(example.question, example.context)

    // Run evaluations
    const faithfulness = await evaluateFaithfulness(
      example.question,
      example.context,
      response
    )

    const relevance = await evaluateRelevance(example.question, response)

    // Check rubric
    const failures: string[] = []

    // Must include checks
    for (const phrase of example.rubric.must_include) {
      if (!response.toLowerCase().includes(phrase.toLowerCase())) {
        failures.push(`Missing required phrase: "${phrase}"`)
      }
    }

    // Must not include checks
    for (const phrase of example.rubric.must_not_include) {
      if (response.toLowerCase().includes(phrase.toLowerCase())) {
        failures.push(`Contains forbidden phrase: "${phrase}"`)
      }
    }

    // Faithfulness threshold
    if (faithfulness.score < example.rubric.min_faithfulness) {
      failures.push(`Faithfulness ${faithfulness.score} < ${example.rubric.min_faithfulness}`)
    }

    const passed = failures.length === 0

    results.push({
      test_id: example.id,
      passed,
      scores: {
        faithfulness: faithfulness.score,
        relevance: relevance.score
      },
      failures
    })

    console.log(`  Faithfulness: ${faithfulness.score.toFixed(2)}`)
    console.log(`  Relevance: ${relevance.score.toFixed(2)}`)
    console.log(`  Result: ${passed ? '‚úÖ PASS' : '‚ùå FAIL'}`)
    if (!passed) {
      console.log(`  Failures: ${failures.join(', ')}`)
    }
  }

  return results
}

// Usage
const results = await runEvalPipeline(myRAGSystem)

const passRate = results.filter(r => r.passed).length / results.length
console.log(`\nüìä Pass Rate: ${(passRate * 100).toFixed(1)}%`)
```

---

## Semantic Versioning for AI Systems

**The Problem**: In traditional software, you version code. In AI, you must version **Prompt + Model + Data**.

### What to Version

```typescript
export interface AISystemVersion {
  version: string         // "1.2.3"
  model: string          // "claude-3-5-sonnet-20240620"
  prompt_template_hash: string  // SHA-256 of system prompt
  rag_strategy: string   // "hybrid-search-v2"
  eval_results: {
    faithfulness_avg: number
    relevance_avg: number
    cost_per_query: number
  }
  deployed_at: Date
}

// Example versions
const versions: AISystemVersion[] = [
  {
    version: '1.0.0',
    model: 'claude-3-haiku-20240307',
    prompt_template_hash: 'a1b2c3...',
    rag_strategy: 'vector-only',
    eval_results: {
      faithfulness_avg: 0.75,
      relevance_avg: 0.82,
      cost_per_query: 0.012
    },
    deployed_at: new Date('2025-01-01')
  },
  {
    version: '1.1.0',
    model: 'claude-3-haiku-20240307',
    prompt_template_hash: 'd4e5f6...',  // ‚Üê Changed prompt
    rag_strategy: 'hybrid-search',       // ‚Üê Changed strategy
    eval_results: {
      faithfulness_avg: 0.88,  // ‚úÖ Improved
      relevance_avg: 0.90,     // ‚úÖ Improved
      cost_per_query: 0.018    // ‚ö†Ô∏è Increased
    },
    deployed_at: new Date('2025-02-01')
  }
]
```

### The Deployment Rule

**Never deploy a prompt change to production without running it against your Golden Dataset first.**

```typescript
// src/week7/evals/deployment-gate.ts
export async function deploymentGate(
  newSystemVersion: (q: string, ctx: string) => Promise<string>,
  minPassRate: number = 0.90
): Promise<boolean> {
  console.log('üö¶ Running deployment gate...')

  const results = await runEvalPipeline(newSystemVersion)

  const passRate = results.filter(r => r.passed).length / results.length
  const avgFaithfulness = results.reduce((sum, r) => sum + r.scores.faithfulness, 0) / results.length

  console.log(`\nüìä Deployment Gate Results:`)
  console.log(`  Pass Rate: ${(passRate * 100).toFixed(1)}%`)
  console.log(`  Avg Faithfulness: ${avgFaithfulness.toFixed(2)}`)
  console.log(`  Required Pass Rate: ${(minPassRate * 100).toFixed(1)}%`)

  if (passRate < minPassRate) {
    console.log(`\n‚ùå DEPLOYMENT BLOCKED: Pass rate below threshold`)
    return false
  }

  console.log(`\n‚úÖ DEPLOYMENT APPROVED`)
  return true
}

// Usage before production deploy
const approved = await deploymentGate(newPromptVersion, 0.95)

if (!approved) {
  throw new Error('Deployment blocked by eval gate')
}

// Proceed with deployment
await deployToProduction(newPromptVersion)
```

---

## The Continuous Eval Loop

```typescript
// src/week7/evals/continuous.ts
export async function continuousEvalLoop() {
  setInterval(async () => {
    console.log('\nüîÑ Running hourly eval...')

    // Sample recent production queries
    const recentQueries = await prisma.productionLog.findMany({
      take: 50,
      orderBy: { timestamp: 'desc' }
    })

    let faithfulnessSum = 0
    let relevanceSum = 0

    for (const log of recentQueries) {
      const faithfulness = await evaluateFaithfulness(
        log.query,
        log.retrieved_context,
        log.response
      )

      const relevance = await evaluateRelevance(log.query, log.response)

      faithfulnessSum += faithfulness.score
      relevanceSum += relevance.score

      // Alert on low score
      if (faithfulness.score &lt; 0.7) {
        await sendAlert({
          severity: 'warning',
          message: `Low faithfulness detected: ${faithfulness.score}`,
          query: log.query,
          response: log.response
        })
      }
    }

    const avgFaithfulness = faithfulnessSum / recentQueries.length
    const avgRelevance = relevanceSum / recentQueries.length

    console.log(`  Avg Faithfulness: ${avgFaithfulness.toFixed(2)}`)
    console.log(`  Avg Relevance: ${avgRelevance.toFixed(2)}`)

    // Track drift over time
    await prisma.evalMetrics.create({
      data: {
        timestamp: new Date(),
        avg_faithfulness: avgFaithfulness,
        avg_relevance: avgRelevance,
        sample_size: recentQueries.length
      }
    })

  }, 60 * 60 * 1000) // Every hour
}

async function sendAlert(alert: any) {
  console.log(`üö® ${alert.severity.toUpperCase()}: ${alert.message}`)
  // In production: Send to Slack, PagerDuty
}
```

---

## 7. Regression Testing with Golden Datasets

### The Problem

**Scenario:** You improve your RAG retrieval (add hybrid search). Does it help or hurt?

**Without regression testing:** You don't know if new version breaks existing queries.
**With regression testing:** You have a golden dataset of 500 queries with expected answers. New version must pass 95%+ before deploy.

### Golden Dataset Structure

```typescript
/**
 * Golden Dataset: Curated test cases with ground truth
 */
interface GoldenExample {
  id: string
  query: string
  expected_answer: string
  context_documents: string[]  // Documents that should be retrieved
  difficulty: 'easy' | 'medium' | 'hard'
  category: 'factual' | 'reasoning' | 'multi-hop'
  metadata: {
    created_at: Date
    updated_at: Date
    pass_rate_threshold: number // 0-1, e.g., 0.9 = must get 90%+ correct
  }
}

// Example: Healthcare RAG golden dataset
const goldenDataset: GoldenExample[] = [
  {
    id: 'golden_001',
    query: 'What was the patient\'s HbA1c value in March 2023?',
    expected_answer: '7.2%',
    context_documents: ['clinical_note_2023_03_15'],
    difficulty: 'easy',
    category: 'factual',
    metadata: {
      created_at: new Date('2024-01-01'),
      updated_at: new Date('2024-01-01'),
      pass_rate_threshold: 0.95
    }
  },
  {
    id: 'golden_002',
    query: 'Has the patient\'s diabetes control improved over the past year?',
    expected_answer: 'Yes, HbA1c decreased from 8.1% to 7.2% over 12 months.',
    context_documents: ['clinical_note_2022_03', 'clinical_note_2023_03'],
    difficulty: 'hard',
    category: 'reasoning',
    metadata: {
      created_at: new Date('2024-01-01'),
      updated_at: new Date('2024-01-01'),
      pass_rate_threshold: 0.85 // Lower threshold for hard questions
    }
  }
]
```

### Regression Test Runner

```typescript
/**
 * Regression Test Suite
 * Run before every deploy to prevent breaking changes
 */
interface RegressionTestResult {
  test_id: string
  passed: boolean
  expected: string
  actual: string
  score: number        // 0-1 from LLM-as-judge
  latency_ms: number
  cost_usd: number
  error?: string
}

async function runRegressionTests(
  goldenDataset: GoldenExample[],
  ragSystem: (query: string) => Promise<string>
): Promise<{
  results: RegressionTestResult[]
  pass_rate: number
  avg_score: number
  total_cost: number
}> {
  const results: RegressionTestResult[] = []
  let totalCost = 0

  for (const example of goldenDataset) {
    const startTime = Date.now()

    try {
      // Step 1: Execute RAG query
      const actualAnswer = await ragSystem(example.query)
      const latency = Date.now() - startTime

      // Step 2: Evaluate with LLM-as-judge
      const evaluation = await llmAsJudge({
        query: example.query,
        expected: example.expected_answer,
        actual: actualAnswer,
        context: example.context_documents
      })

      const cost = 0.003 // Approximate cost per eval
      totalCost += cost

      // Step 3: Determine pass/fail
      const passed = evaluation.score &gt;= example.metadata.pass_rate_threshold

      results.push({
        test_id: example.id,
        passed,
        expected: example.expected_answer,
        actual: actualAnswer,
        score: evaluation.score,
        latency_ms: latency,
        cost_usd: cost
      })

    } catch (error) {
      results.push({
        test_id: example.id,
        passed: false,
        expected: example.expected_answer,
        actual: '',
        score: 0,
        latency_ms: Date.now() - startTime,
        cost_usd: 0,
        error: error.message
      })
    }
  }

  // Calculate aggregate metrics
  const passCount = results.filter(r => r.passed).length
  const passRate = passCount / results.length
  const avgScore = results.reduce((sum, r) => sum + r.score, 0) / results.length

  return {
    results,
    pass_rate: passRate,
    avg_score: avgScore,
    total_cost: totalCost
  }
}
```

### Deployment Gate

```typescript
/**
 * Deployment Gate: Block deploy if regression tests fail
 */
async function deploymentGate(version: string) {
  console.log(`üîç Running regression tests for version ${version}...`)

  // Load golden dataset
  const goldenDataset = await loadGoldenDataset()

  // Run regression tests
  const testResults = await runRegressionTests(goldenDataset, productionRAG)

  console.log(`\nüìä Regression Test Results:`)
  console.log(`  Pass Rate: ${(testResults.pass_rate * 100).toFixed(1)}%`)
  console.log(`  Avg Score: ${testResults.avg_score.toFixed(2)}`)
  console.log(`  Total Cost: $${testResults.total_cost.toFixed(2)}`)

  // Gate: Require 95% pass rate
  const REQUIRED_PASS_RATE = 0.95

  if (testResults.pass_rate &gt;= REQUIRED_PASS_RATE) {
    console.log(`\n‚úÖ PASS: Regression tests passed (${(testResults.pass_rate * 100).toFixed(1)}% &gt;= ${REQUIRED_PASS_RATE * 100}%)`)
    console.log(`Deploying version ${version} to production...`)

    // Log successful deployment
    await prisma.deployment.create({
      data: {
        version,
        pass_rate: testResults.pass_rate,
        avg_score: testResults.avg_score,
        test_count: goldenDataset.length,
        deployed_at: new Date(),
        status: 'success'
      }
    })

    return { allowed: true, message: 'Deployment approved' }

  } else {
    console.log(`\n‚ùå FAIL: Regression tests failed (${(testResults.pass_rate * 100).toFixed(1)}% < ${REQUIRED_PASS_RATE * 100}%)`)
    console.log(`\nFailing tests:`)

    const failingTests = testResults.results.filter(r => !r.passed)
    for (const test of failingTests.slice(0, 5)) { // Show first 5
      console.log(`  - ${test.test_id}: Score ${test.score.toFixed(2)} (threshold: 0.95)`)
      console.log(`    Expected: ${test.expected.slice(0, 100)}...`)
      console.log(`    Actual: ${test.actual.slice(0, 100)}...`)
    }

    // Log failed deployment attempt
    await prisma.deployment.create({
      data: {
        version,
        pass_rate: testResults.pass_rate,
        avg_score: testResults.avg_score,
        test_count: goldenDataset.length,
        deployed_at: new Date(),
        status: 'blocked'
      }
    })

    // Alert team
    await alertSlack({
      channel: '#ai-alerts',
      text: `üö® Deployment blocked for version ${version}

Pass rate: ${(testResults.pass_rate * 100).toFixed(1)}% (required: ${REQUIRED_PASS_RATE * 100}%)
Failing tests: ${failingTests.length}

Review regression test results before deploying.`
    })

    return { allowed: false, message: 'Deployment blocked by regression tests' }
  }
}

// CI/CD Integration
async function cicdPipeline() {
  const version = process.env.VERSION || 'unknown'

  // Step 1: Build
  console.log('Building...')
  await build()

  // Step 2: Unit tests
  console.log('Running unit tests...')
  await runUnitTests()

  // Step 3: Regression tests (deployment gate)
  console.log('Running regression tests...')
  const gateResult = await deploymentGate(version)

  if (!gateResult.allowed) {
    process.exit(1) // Block deployment
  }

  // Step 4: Deploy
  console.log('Deploying...')
  await deploy(version)
}
```

### Regression Test History Dashboard

```typescript
/**
 * Track regression test performance over time
 */
interface RegressionTestHistory {
  version: string
  deployed_at: Date
  pass_rate: number
  avg_score: number
  latency_p95: number
  cost: number
}

async function getRegressionTestHistory(): Promise<RegressionTestHistory[]> {
  const deployments = await prisma.deployment.findMany({
    where: { status: 'success' },
    orderBy: { deployed_at: 'desc' },
    take: 20
  })

  return deployments.map(d => ({
    version: d.version,
    deployed_at: d.deployed_at,
    pass_rate: d.pass_rate,
    avg_score: d.avg_score,
    latency_p95: d.latency_p95,
    cost: d.cost
  }))
}

// Detect regression trends
async function detectRegressionTrends() {
  const history = await getRegressionTestHistory()

  // Check if pass rate is declining
  const recentPassRates = history.slice(0, 5).map(h => h.pass_rate)
  const avgRecentPassRate = recentPassRates.reduce((a, b) => a + b, 0) / recentPassRates.length

  const olderPassRates = history.slice(5, 10).map(h => h.pass_rate)
  const avgOlderPassRate = olderPassRates.reduce((a, b) => a + b, 0) / olderPassRates.length

  const decline = avgOlderPassRate - avgRecentPassRate

  if (decline &gt; 0.05) { // 5% decline
    await alertSlack({
      channel: '#ai-alerts',
      text: `üìâ Regression test pass rate declining:

Recent avg: ${(avgRecentPassRate * 100).toFixed(1)}%
Previous avg: ${(avgOlderPassRate * 100).toFixed(1)}%
Decline: ${(decline * 100).toFixed(1)} percentage points

Consider reviewing recent changes to prompts or retrieval logic.`
    })
  }
}
```

### Golden Dataset Maintenance

```typescript
/**
 * Continuously update golden dataset based on production failures
 */
async function maintainGoldenDataset() {
  // Step 1: Find production failures (low LLM-as-judge scores)
  const failures = await prisma.queryLog.findMany({
    where: {
      faithfulness_score: { lt: 0.7 },
      human_reviewed: true,
      correct_answer: { not: null }
    },
    orderBy: { timestamp: 'desc' },
    take: 100
  })

  // Step 2: Add high-value failures to golden dataset
  for (const failure of failures) {
    const alreadyExists = await prisma.goldenExample.findFirst({
      where: { query: failure.query }
    })

    if (!alreadyExists) {
      await prisma.goldenExample.create({
        data: {
          query: failure.query,
          expected_answer: failure.correct_answer,
          context_documents: failure.retrieved_documents,
          difficulty: 'hard', // Failed queries are hard
          category: 'factual',
          pass_rate_threshold: 0.85
        }
      })

      console.log(`‚úÖ Added failing query to golden dataset: ${failure.query.slice(0, 50)}...`)
    }
  }

  console.log(`Golden dataset now has ${await prisma.goldenExample.count()} examples`)
}
```

---

## Cost Analysis: LLM-as-a-Judge

| Scenario | Evals per Day | Cost per Eval | Daily Cost | Monthly Cost |
|----------|--------------|---------------|------------|--------------|
| **Development** | 50 | $0.003 | $0.15 | $4.50 |
| **Continuous (hourly)** | 1,200 | $0.003 | $3.60 | $108 |
| **Full regression** | 10,000 | $0.003 | $30 | $900 |
| **Pre-deploy gate** | 500 | $0.003 | $1.50 | $45 (20 deploys/month) |

**Production tip**: Run full regression (500-1000 examples) before every deploy. Run hourly sampling (50 examples) in production.

---

## Key Takeaways

1. **LLM-as-a-Judge**: Use stronger model to evaluate production model
2. **Golden Dataset**: Curated test cases with known correct answers
3. **Eval Pipeline**: Automated testing before every deployment
4. **Semantic Versioning**: Version prompt + model + data, not just code
5. **Deployment Gate**: Require 90%+ pass rate before production deploy
6. **Continuous Eval**: Sample production traffic hourly to detect drift

---

## Next Steps

- **Week 7 Lab**: Build Production Dashboard with all three pillars
- **Capstone Project**: Implement eval pipeline with 100% faithfulness requirement
- **Production**: Set up continuous eval loop with alerting

---

## Further Reading

- [G-Eval: NLG Evaluation using GPT-4](https://arxiv.org/abs/2303.16634)
- [Judging LLM-as-a-Judge with MT-Bench](https://arxiv.org/abs/2306.05685)
- [LangSmith Evaluation Guide](https://docs.smith.langchain.com/evaluation)
- [RAGAS Framework](https://docs.ragas.io/)
