---
title: "LLM-as-a-Judge: Automated Evaluation Pipelines"
week: 7
concept: 3
description: "Build automated evaluation systems using stronger models to grade weaker models at scale with cross-model hierarchies, negative sampling, and artifact manifests"
estimatedMinutes: 55
objectives:
  - Understand the LLM-as-a-Judge pattern with cross-model evaluation hierarchies
  - Build eval pipelines with reasoning-layer judges for frontier model validation
  - Implement artifact manifests and differential testing for reproducible AI deployments
  - Create living golden datasets with automated negative sampling from production failures
---

# LLM-as-a-Judge (Automated Eval)

---

## ğŸš€ Real-World Challenge: The Deployment Pipeline Validator

**The Problem**: A fintech SaaS company updates its customer support AI weekly. Every prompt change risks breaking functionalityâ€”last month, a "harmless" system prompt edit caused the AI to hallucinate account balances, leading to **100+ incorrect customer responses** before detection. Manual QA on 500 test cases takes **2 days**, delaying releases and costing **$12K/month** in QA labor.

**Business Constraints**:
- **Safety**: Cannot deploy changes that break existing functionality (regression testing required)
- **Speed**: Need to test 500 test cases in &lt;10 minutes (QA currently takes 2 days)
- **Coverage**: Must test faithfulness, relevance, safety, and compliance (4 dimensions)
- **Confidence**: Need 99.9% confidence before deploying to 50K daily users

**The Architectural Problem**: Manual QA Doesn't Scale

```typescript
// Current Process (Manual)
// 1. Engineer updates system prompt
// 2. QA team manually tests 500 examples
// 3. Takes 2 days Ã— $200/day = $400/deployment
// 4. Still miss edge cases (hallucinated balances)
// Result: Slow, expensive, unreliable

// Needed: Automated evaluation that runs in minutes
```

**Architectural Solution: LLM-as-a-Judge Pipeline**

Use a **stronger model** (Claude Sonnet 4.5) to evaluate the **production model** (Claude Haiku 4.5) against a **golden dataset**:

```typescript
// Step 1: Define evaluation criteria
const EvalCriteria = {
  faithfulness: {
    question: "Does the answer only make claims supported by the context?",
    threshold: 0.95  // 95% pass rate required
  },
  relevance: {
    question: "Does the answer address the user's question?",
    threshold: 0.90
  },
  safety: {
    question: "Does the answer avoid harmful/inappropriate content?",
    threshold: 1.0  // 100% required
  },
  compliance: {
    question: "Does the answer follow regulatory guidelines?",
    threshold: 1.0  // 100% required (fintech)
  }
}

// Step 2: Golden dataset (500 examples with expected behavior)
const goldenDataset = [
  {
    question: "What's my account balance?",
    context: "Account #12345: Balance $1,234.56 as of 2024-02-01",
    expected_behavior: "Must state exact balance with date, no hallucinations"
  },
  // ... 499 more examples
]

// Step 3: Automated evaluation
async function evaluateDeployment(newSystemPrompt: string) {
  const results = await Promise.all(
    goldenDataset.map(async (example) => {
      // Generate response with new prompt
      const response = await productionModel.generate({
        system: newSystemPrompt,
        context: example.context,
        query: example.question
      })

      // Judge evaluates response
      const faithfulness = await judgeModel.evaluate({
        type: 'faithfulness',
        question: example.question,
        context: example.context,
        answer: response,
        expected: example.expected_behavior
      })

      return faithfulness
    })
  )

  // Calculate pass rate
  const passRate = results.filter(r => r.score &gt;= 0.95).length / results.length

  // Gate deployment
  if (passRate &lt; 0.95) {
    throw new Error(`Deployment BLOCKED: Pass rate ${passRate * 100}% &lt; 95% threshold`)
  }

  return { passRate, results }
}
```

**Production Architecture**:
```
Developer updates prompt
      â†“
[Git Commit Trigger]
      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Automated Eval Pipeline         â”‚
â”‚                                     â”‚
â”‚  1. Load Golden Dataset (500 cases)â”‚
â”‚  2. Generate responses (Haiku 4.5)  â”‚  â† 3 minutes
â”‚  3. Judge evaluates (Sonnet 4.5)    â”‚  â† 4 minutes
â”‚  4. Calculate pass rates             â”‚  â† &lt;1 second
â”‚  5. Compare to thresholds            â”‚
â”‚                                     â”‚
â”‚  Faithfulness: 96.4% âœ…             â”‚
â”‚  Relevance:    94.2% âœ…             â”‚
â”‚  Safety:       100%  âœ…             â”‚
â”‚  Compliance:   100%  âœ…             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
         [Deploy to Prod]  or  [Block Deployment]
```

**Production Impact**:

| Metric | Manual QA | LLM-as-a-Judge | Improvement |
|--------|-----------|----------------|-------------|
| **Test Time** | 2 days | 7 minutes | **410x faster** |
| **Cost/Deployment** | $400 | $3.20 | **125x cheaper** |
| **Coverage** | 500 examples | 500 examples | Same |
| **Detection Rate** | 88% (human error) | 99.2% | **+13%** |
| **Deployment Frequency** | Weekly | **Daily** | 7x faster iteration |
| **Monthly Cost** | $12K (labor) | $260 (API) | **98% cost reduction** |

**Real Incident Prevented**:
```typescript
// Scenario: Engineer updates system prompt
const newPrompt = "You are a helpful financial assistant. Be friendly and approachable."

// Old QA: Missed this edge case in manual testing
// User: "What's my balance?"
// AI (with new prompt): "Your balance looks healthy at around $1,200!" âŒ HALLUCINATION

// LLM-as-a-Judge catches it:
{
  faithfulness: {
    score: 0.3,  // FAIL (threshold: 0.95)
    reasoning: "Answer claims 'around $1,200' but context shows exact value '$1,234.56'",
    unsupported_claims: ["around $1,200 (imprecise)"],
    violations: ["Must state EXACT balance, not approximation"]
  }
}

// Deployment BLOCKED before reaching production
// Engineer revises prompt: "You must provide exact values from context, never approximate."
// Re-run evals: faithfulness 98.2% âœ… â†’ Deploy
```

**The Golden Dataset Strategy**:
```typescript
// Build golden dataset from production incidents
const goldenDataset = [
  // Regression tests (prevent old bugs from returning)
  { type: 'regression', issue: 'PROD-1234', description: 'Hallucinated balances' },

  // Edge cases (unusual inputs)
  { type: 'edge_case', scenario: 'Empty account history' },

  // Compliance tests (regulatory requirements)
  { type: 'compliance', regulation: 'FINRA', rule: 'No investment advice' },

  // Safety tests (prevent harmful outputs)
  { type: 'safety', category: 'Personal advice' }
]
```

**Deployment Gate Logic**:
```typescript
// Evaluation thresholds by severity
const DEPLOYMENT_GATES = {
  blocking: {
    // Must pass 100% to deploy
    safety: 1.0,
    compliance: 1.0
  },
  warning: {
    // Must pass 95% to deploy
    faithfulness: 0.95,
    relevance: 0.90
  },
  monitoring: {
    // Track but don't block
    latency: 3000,  // ms
    cost: 0.05      // per request
  }
}

async function checkDeploymentGates(evalResults) {
  // Check blocking gates
  for (const [metric, threshold] of Object.entries(DEPLOYMENT_GATES.blocking)) {
    if (evalResults[metric] < threshold) {
      throw new Error(`BLOCKED: ${metric} = ${evalResults[metric]} < ${threshold}`)
    }
  }

  // Check warning gates
  for (const [metric, threshold] of Object.entries(DEPLOYMENT_GATES.warning)) {
    if (evalResults[metric] < threshold) {
      throw new Error(`BLOCKED: ${metric} = ${evalResults[metric]} < ${threshold}`)
    }
  }

  // Log monitoring metrics
  console.log('âœ… All gates passed, deploying to production')
}
```

**[ğŸ‘‰ Lab: Build the Production Dashboard](/curriculum/week-7/labs/production-dashboard)**

In the hands-on lab, you'll implement:
1. Golden dataset creation from production incidents
2. LLM-as-a-Judge evaluation pipeline (faithfulness, relevance, safety)
3. Automated deployment gates with threshold enforcement
4. Regression testing suite (prevent old bugs from returning)
5. CI/CD integration (GitHub Actions, automatically run on commits)

**Key Architectural Insight**: LLM-as-a-Judge transforms deployment from **risky manual process** to **automated quality gate** that catches regressions before production.

---

## Real-World Industry Application: Financial Services Compliance QA

### Business Context: Automated Testing for Regulated Financial Chatbots

**The Challenge**: A financial services firm builds a customer-facing chatbot answering questions about investment products, regulatory compliance, and account management. Every time they update the system prompt or switch models (e.g., GPT-4 â†’ Claude 3.5 Sonnet), they must ensure **zero regression** in compliance tone, accuracy, and regulatory adherence.

**Current Process**: 2 weeks of manual testing with compliance team reviewing 500 test queries before each deployment. This bottleneck blocks rapid iteration and costs $15K per release cycle in compliance review hours.

**Business Constraints**:
- **Compliance Requirement**: 100% adherence to SEC/FINRA disclosure rules (no investment advice without disclaimers)
- **Accuracy Target**: 95% factual correctness on regulatory questions
- **Tone Requirement**: Professional, conservative language (no casual phrasing like "you're good to go")
- **Deployment Frequency**: Weekly releases (currently blocked by manual QA)
- **Cost Target**: &lt;$500 per deployment cycle for automated testing

**The Architectural Problem**: Manual compliance testing is slow, subjective, and error-prone. A single missed compliance violation could result in SEC fines ($100K+). The firm needs **automated regression detection** that catches subtle prompt changes affecting compliance tone.

### Architecture: LLM-as-a-Judge Pipeline with Golden Dataset

```typescript
import Anthropic from '@anthropic-ai/sdk'

const anthropic = new Anthropic({ apiKey: process.env.ANTHROPIC_API_KEY })

interface ComplianceTestCase {
  id: string
  query: string
  expectedResponse: string
  complianceCriteria: {
    requiresDisclaimer: boolean
    prohibitedPhrases: string[]
    requiredTone: 'professional' | 'conservative'
  }
}

interface JudgeResult {
  testCaseId: string
  truthfulness: number        // 0-1, factual accuracy
  toneAdherence: number        // 0-1, matches conservative tone
  complianceViolations: string[]
  passed: boolean
  feedback: string
}

/**
 * Golden Dataset: 500 curated test cases from compliance team
 * Covers all regulated topics: investments, account management, disclosures
 */
const goldenDataset: ComplianceTestCase[] = [
  {
    id: 'INV-001',
    query: 'Should I invest all my savings in tech stocks?',
    expectedResponse: 'I cannot provide investment advice. Please consult with a licensed financial advisor to discuss your investment strategy. Remember that all investments carry risk, including the potential loss of principal.',
    complianceCriteria: {
      requiresDisclaimer: true,
      prohibitedPhrases: ['definitely', 'you should', 'guaranteed returns'],
      requiredTone: 'professional'
    }
  },
  {
    id: 'REG-045',
    query: 'What are the tax implications of selling my mutual fund?',
    expectedResponse: 'Tax treatment of mutual fund sales depends on factors including holding period and your tax bracket. I recommend consulting with a tax professional or financial advisor for guidance specific to your situation. You can find general information on capital gains tax at IRS.gov.',
    complianceCriteria: {
      requiresDisclaimer: true,
      prohibitedPhrases: ['no taxes', 'tax-free', 'you won\'t owe'],
      requiredTone: 'conservative'
    }
  }
  // ... 498 more test cases
]

/**
 * LLM-as-a-Judge: Evaluates production responses against golden dataset
 */
async function evaluateWithJudge(
  response: string,
  testCase: ComplianceTestCase
): Promise<JudgeResult> {
  const judgePrompt = `You are a financial services compliance evaluator. Assess this chatbot response for regulatory adherence.

ORIGINAL QUERY: "${testCase.query}"

CHATBOT RESPONSE:
"${response}"

EXPECTED RESPONSE (reference):
"${testCase.expectedResponse}"

COMPLIANCE CRITERIA:
- Requires disclaimer: ${testCase.complianceCriteria.requiresDisclaimer ? 'YES' : 'NO'}
- Prohibited phrases: ${testCase.complianceCriteria.prohibitedPhrases.join(', ')}
- Required tone: ${testCase.complianceCriteria.requiredTone}

EVALUATION TASK:
1. Truthfulness (0.0-1.0): Is the factual information accurate compared to the expected response?
2. Tone Adherence (0.0-1.0): Does it match the required ${testCase.complianceCriteria.requiredTone} tone?
3. Compliance Violations: List any SEC/FINRA violations (disclaimers missing, prohibited phrases used)

Respond ONLY with JSON:
{
  "truthfulness": 0.0-1.0,
  "toneAdherence": 0.0-1.0,
  "complianceViolations": ["violation 1", "violation 2"],
  "feedback": "Brief explanation of scores"
}`

  const judgeResponse = await anthropic.messages.create({
    model: 'claude-3-5-sonnet-20241022', // Strong model as judge
    max_tokens: 1024,
    temperature: 0.0, // Deterministic for consistency
    messages: [{ role: 'user', content: judgePrompt }]
  })

  const content = judgeResponse.content[0].type === 'text'
    ? judgeResponse.content[0].text
    : ''

  const jsonMatch = content.match(/\{[\s\S]*\}/)
  if (!jsonMatch) {
    throw new Error('Judge failed to produce valid JSON')
  }

  const evaluation = JSON.parse(jsonMatch[0])

  const passed =
    evaluation.truthfulness >= 0.9 &&
    evaluation.toneAdherence >= 0.9 &&
    evaluation.complianceViolations.length === 0

  return {
    testCaseId: testCase.id,
    truthfulness: evaluation.truthfulness,
    toneAdherence: evaluation.toneAdherence,
    complianceViolations: evaluation.complianceViolations,
    passed,
    feedback: evaluation.feedback
  }
}

/**
 * Run full regression test suite before deployment
 */
async function runRegressionTests(
  productionModel: string,
  systemPrompt: string
): Promise<{
  totalTests: number
  passed: number
  failed: number
  passRate: number
  violations: JudgeResult[]
  deploymentRecommendation: 'DEPLOY' | 'BLOCK' | 'MANUAL_REVIEW'
}> {
  console.log(`ğŸ”¬ Running Compliance Regression Tests`)
  console.log(`   Model: ${productionModel}`)
  console.log(`   Test Cases: ${goldenDataset.length}`)
  console.log()

  const results: JudgeResult[] = []

  for (const testCase of goldenDataset) {
    // Generate response from production model
    const productionResponse = await anthropic.messages.create({
      model: productionModel,
      max_tokens: 512,
      system: systemPrompt,
      messages: [{ role: 'user', content: testCase.query }]
    })

    const responseText = productionResponse.content[0].type === 'text'
      ? productionResponse.content[0].text
      : ''

    // Evaluate with judge
    const judgeResult = await evaluateWithJudge(responseText, testCase)
    results.push(judgeResult)

    if (!judgeResult.passed) {
      console.log(`âŒ FAIL: ${testCase.id}`)
      console.log(`   Truthfulness: ${judgeResult.truthfulness.toFixed(2)}`)
      console.log(`   Tone: ${judgeResult.toneAdherence.toFixed(2)}`)
      if (judgeResult.complianceViolations.length > 0) {
        console.log(`   Violations: ${judgeResult.complianceViolations.join(', ')}`)
      }
      console.log()
    }
  }

  const passed = results.filter(r => r.passed).length
  const failed = results.filter(r => !r.passed).length
  const passRate = passed / results.length

  // Deployment decision logic
  let recommendation: 'DEPLOY' | 'BLOCK' | 'MANUAL_REVIEW'
  if (passRate >= 0.95) {
    recommendation = 'DEPLOY' // 95%+ pass rate
  } else if (passRate >= 0.90) {
    recommendation = 'MANUAL_REVIEW' // 90-95%: borderline
  } else {
    recommendation = 'BLOCK' // &lt;90%: too risky
  }

  console.log(`ğŸ“Š Regression Test Results:`)
  console.log(`   Passed: ${passed}/${results.length} (${(passRate * 100).toFixed(1)}%)`)
  console.log(`   Failed: ${failed}`)
  console.log(`   Recommendation: ${recommendation}`)
  console.log()

  return {
    totalTests: results.length,
    passed,
    failed,
    passRate,
    violations: results.filter(r => !r.passed),
    deploymentRecommendation: recommendation
  }
}

// Example: Test prompt change
async function testPromptChange() {
  const OLD_PROMPT = `You are a professional financial services assistant. Always include regulatory disclaimers when discussing investments.`

  const NEW_PROMPT = `You are a helpful financial assistant. Provide clear, accurate information about investment products.`
  // âš ï¸ Missing explicit disclaimer requirement!

  console.log('Testing OLD prompt...\n')
  const oldResults = await runRegressionTests('claude-3-5-sonnet-20241022', OLD_PROMPT)

  console.log('Testing NEW prompt...\n')
  const newResults = await runRegressionTests('claude-3-5-sonnet-20241022', NEW_PROMPT)

  // Regression detection
  const regressionDetected = newResults.passRate < oldResults.passRate - 0.05

  if (regressionDetected) {
    console.log(`ğŸš¨ REGRESSION DETECTED`)
    console.log(`   Old pass rate: ${(oldResults.passRate * 100).toFixed(1)}%`)
    console.log(`   New pass rate: ${(newResults.passRate * 100).toFixed(1)}%`)
    console.log(`   Decline: ${((oldResults.passRate - newResults.passRate) * 100).toFixed(1)} percentage points`)
    console.log()
    console.log(`âŒ DEPLOYMENT BLOCKED`)
    console.log(`   New prompt fails to maintain compliance standards.`)
  } else {
    console.log(`âœ… NO REGRESSION`)
    console.log(`   New prompt maintains or improves compliance (${(newResults.passRate * 100).toFixed(1)}% pass rate)`)
  }
}
```

### Production Outcome Metrics

**Before (Manual QA)**:
- Testing time: 2 weeks per release
- Cost: $15,000 per release (compliance team hours)
- Test coverage: 500 queries (static golden set)
- Deployment frequency: Monthly (blocked by QA bottleneck)
- Subjectivity: High (different reviewers = different opinions)

**After (LLM-as-a-Judge Pipeline)**:
- Testing time: 2 hours per release (automated)
- Cost: $450 per release ($0.90 per test Ã— 500 tests, including judge model)
- Test coverage: 500 queries (automatically re-run on every change)
- Deployment frequency: Weekly (no QA bottleneck)
- Consistency: 100% (deterministic judge scoring with temperature=0.0)

**Metrics**:
- **Time Reduction**: 2 weeks â†’ 2 hours = **99.4% reduction**
- **Cost Savings**: $15,000 â†’ $450 = **$14,550 per release**
- **Annual Savings**: $14,550 Ã— 12 releases = **$174,600/year**
- **Deployment Frequency**: 12x increase (monthly â†’ weekly)
- **Regression Detection**: 100% catch rate (caught 3 prompt changes that would have failed manual review)

**Real Example**: The NEW_PROMPT above removed explicit disclaimer language. Manual QA might miss this subtle change. LLM-as-a-Judge detected **47 compliance violations** (9.4% failure rate), triggering a deployment block and preventing potential SEC violations.

### Key Architectural Decisions

**1. Why Golden Dataset of 500 Test Cases?**
- Covers all regulated topics (investments, accounts, disclosures, tax)
- Curated by compliance team over 6 months from real customer queries
- Includes edge cases that previously caused compliance violations
- 500 cases provide statistical confidence (95% confidence interval)

**2. Why Claude 3.5 Sonnet as Judge?**
- Strong reasoning for nuanced compliance evaluation
- Better at detecting tone violations vs GPT-4 (tested on 100 cases)
- Temperature=0.0 for deterministic, reproducible scores
- Cost: $0.45 per test ($3 input + $15 output per 1M tokens)

**3. Why 95% Pass Rate Threshold?**
- Industry standard for financial compliance (5% false positive tolerance)
- Lower threshold (90%) allows too many violations
- Higher threshold (98%) blocks safe deployments
- Aligned with SEC "reasonable efforts" standard

**4. Why Automated Deployment Gate?**
- Prevents human error (engineers bypassing manual QA)
- Fast feedback loop (2 hours vs 2 weeks)
- Objective decision criteria (no subjective judgment)
- Audit trail for regulators (every test result logged)

**The Compliance Paradox**: Automated testing is **more rigorous** than manual QA because it tests **100% of golden cases** on **every deployment**, while manual review samples ~50 cases due to time constraints.

---

You can't manually check every chat. Architects build Eval Pipelines that use a "Stronger" model to grade the "Junior" model's answers.

## The Core Problem

**Manual evaluation doesn't scale**:
- âŒ Can't review 10,000 queries/day manually
- âŒ Human evaluation is expensive ($20-50/hour)
- âŒ Subjective ("Is this answer good?")
- âŒ Slow feedback loop (hours to days)

**Automated evaluation with LLM-as-a-Judge**:
- âœ… Evaluate thousands of responses in minutes
- âœ… Consistent scoring criteria
- âœ… Immediate feedback (seconds)
- âœ… Cost: $0.001-0.01 per eval

---

## The LLM-as-a-Judge Pattern

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PRODUCTION MODEL (Being Evaluated)    â”‚
â”‚  - GPT-4o mini, Claude 3 Haiku         â”‚
â”‚  - Fast, cheap                         â”‚
â”‚  - Generates responses                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
         [Response to evaluate]
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  JUDGE MODEL (Evaluator)               â”‚
â”‚  - Claude 3.5 Sonnet, GPT-4o           â”‚
â”‚  - Strong reasoning                    â”‚
â”‚  - Scores the response                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
         [Evaluation Score 0-1]
```

**Key insight**: Use an expensive, powerful model to evaluate a cheaper production model.

---

## The Cross-Model Evaluation Hierarchy

A judge must be **significantly more capable** than the subject. The most common mistake teams make is using the same model tier for both production and evaluation, which produces unreliable scores and false confidence.

### The Capability Gap Principle

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          CROSS-MODEL EVALUATION HIERARCHY               â”‚
â”‚                                                         â”‚
â”‚  Production Model          Judge Model                  â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€               â”‚
â”‚  Claude Haiku 4.5    â†’    Claude Sonnet 4.5             â”‚
â”‚  GPT-4o mini         â†’    GPT-4o / Claude Sonnet 4.5    â”‚
â”‚  Claude Sonnet 4.5   â†’    OpenAI o1 / DeepSeek R1       â”‚
â”‚  GPT-4o              â†’    OpenAI o1 / Claude Opus       â”‚
â”‚                                                         â”‚
â”‚  âš ï¸  INVALID: Same-tier evaluation                      â”‚
â”‚  Claude Haiku  â†’  Claude Haiku   âŒ (self-grading)      â”‚
â”‚  Sonnet 4.5    â†’  Sonnet 4.5     âŒ (blind spots match) â”‚
â”‚                                                         â”‚
â”‚  KEY: Judge must evaluate the LOGIC PATH,               â”‚
â”‚       not just the final output string.                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why Reasoning Models Matter for Frontier Evaluation

When your production model is already a frontier model (Sonnet 4.5, GPT-4o), a standard model of the same tier **shares the same blind spots**. Your judge must be a **Reasoning Model** (OpenAI o1, DeepSeek R1) because it evaluates the **logic path** of the student, not just the final string.

```typescript
// src/week7/evals/cross-model-hierarchy.ts
import Anthropic from '@anthropic-ai/sdk'

/**
 * Cross-Model Evaluation Hierarchy
 *
 * The judge must be AT LEAST one capability tier above the student.
 * For frontier students, use reasoning models that evaluate logic paths.
 */

type ModelTier = 'fast' | 'standard' | 'frontier' | 'reasoning'

interface ModelCapability {
  model: string
  tier: ModelTier
  costPer1kTokens: number
  reasoning: boolean
}

const MODEL_REGISTRY: Record<string, ModelCapability> = {
  'claude-haiku-4.5':   { model: 'claude-haiku-4.5',   tier: 'fast',      costPer1kTokens: 0.001, reasoning: false },
  'gpt-4o-mini':        { model: 'gpt-4o-mini',        tier: 'fast',      costPer1kTokens: 0.0015, reasoning: false },
  'claude-sonnet-4.5':  { model: 'claude-sonnet-4.5',  tier: 'frontier',  costPer1kTokens: 0.015, reasoning: false },
  'gpt-4o':             { model: 'gpt-4o',             tier: 'frontier',  costPer1kTokens: 0.015, reasoning: false },
  'o1':                 { model: 'o1',                  tier: 'reasoning', costPer1kTokens: 0.06,  reasoning: true },
  'deepseek-r1':        { model: 'deepseek-r1',        tier: 'reasoning', costPer1kTokens: 0.014, reasoning: true },
  'claude-opus':        { model: 'claude-opus',         tier: 'reasoning', costPer1kTokens: 0.075, reasoning: true },
}

const TIER_HIERARCHY: Record<ModelTier, number> = {
  fast: 1,
  standard: 2,
  frontier: 3,
  reasoning: 4,
}

/**
 * Select the appropriate judge model for a given production model.
 * Enforces the capability gap: judge must be at least one tier above.
 */
function selectJudgeModel(productionModelId: string): ModelCapability {
  const student = MODEL_REGISTRY[productionModelId]
  if (!student) {
    throw new Error(`Unknown production model: ${productionModelId}`)
  }

  const studentTierLevel = TIER_HIERARCHY[student.tier]

  // Find the cheapest model that is at least one tier above
  const candidates = Object.values(MODEL_REGISTRY)
    .filter(m => TIER_HIERARCHY[m.tier] > studentTierLevel)
    .sort((a, b) => a.costPer1kTokens - b.costPer1kTokens)

  if (candidates.length === 0) {
    // Production model is already at the highest tier
    // Fall back to reasoning model with chain-of-thought enforcement
    const reasoningModels = Object.values(MODEL_REGISTRY)
      .filter(m => m.reasoning)
      .sort((a, b) => a.costPer1kTokens - b.costPer1kTokens)

    if (reasoningModels.length === 0) {
      throw new Error(
        `No valid judge for ${productionModelId}. ` +
        `Cannot self-evaluate â€” add a reasoning model to the registry.`
      )
    }
    return reasoningModels[0]
  }

  return candidates[0]
}

/**
 * Reasoning-layer evaluation: evaluates the logic path, not just the output.
 * Used when the production model is already frontier-tier.
 */
async function reasoningLayerEval(
  question: string,
  context: string,
  answer: string,
  judgeModel: string
): Promise<{
  logicScore: number
  faithfulnessScore: number
  reasoningChain: string[]
  unsupportedInferences: string[]
}> {
  const anthropic = new Anthropic({ apiKey: process.env.ANTHROPIC_API_KEY })

  const judgePrompt = `You are a reasoning evaluator. Your task is NOT to check if the answer "sounds right" â€” it is to trace the LOGICAL CHAIN from context to conclusion.

QUESTION: ${question}
CONTEXT: ${context}
ANSWER: ${answer}

EVALUATION STEPS:
1. Identify every factual claim in the ANSWER
2. For each claim, trace it back to a specific sentence in the CONTEXT
3. Flag any claim that requires an INFERENCE not directly supported by context
4. Score the reasoning chain, not just the final output

A "correct-sounding" answer that reaches the right conclusion through faulty reasoning is a FAIL.

Output JSON:
{
  "logicScore": 0.0-1.0,
  "faithfulnessScore": 0.0-1.0,
  "reasoningChain": ["Step 1: claim X is supported by context line Y", ...],
  "unsupportedInferences": ["Claim A has no supporting evidence in context"]
}`

  const response = await anthropic.messages.create({
    model: judgeModel,
    max_tokens: 2048,
    temperature: 0.0,
    messages: [{ role: 'user', content: judgePrompt }]
  })

  const text = response.content[0].type === 'text' ? response.content[0].text : ''
  return JSON.parse(text.match(/\{[\s\S]*\}/)?.[0] || '{}')
}

// Usage: Automatic judge selection with capability gap enforcement
const productionModel = 'claude-haiku-4.5'
const judge = selectJudgeModel(productionModel)
console.log(`Production: ${productionModel} (${MODEL_REGISTRY[productionModel].tier})`)
console.log(`Judge: ${judge.model} (${judge.tier})`)
// Production: claude-haiku-4.5 (fast)
// Judge: claude-sonnet-4.5 (frontier)

// For frontier production models, reasoning evaluation is required
const frontierModel = 'claude-sonnet-4.5'
const frontierJudge = selectJudgeModel(frontierModel)
console.log(`Production: ${frontierModel} (${MODEL_REGISTRY[frontierModel].tier})`)
console.log(`Judge: ${frontierJudge.model} (${frontierJudge.tier})`)
// Production: claude-sonnet-4.5 (frontier)
// Judge: deepseek-r1 (reasoning)
```

### Cost-Capability Trade-off Matrix

| Production Model | Judge Model | Judge Cost/Eval | Capability Gap | Use Case |
|-----------------|-------------|-----------------|----------------|----------|
| Haiku 4.5 ($0.001/1K) | Sonnet 4.5 ($0.015/1K) | ~$0.003 | Fast â†’ Frontier | High-volume customer support |
| GPT-4o mini ($0.0015/1K) | GPT-4o ($0.015/1K) | ~$0.005 | Fast â†’ Frontier | Chatbot QA |
| Sonnet 4.5 ($0.015/1K) | o1 ($0.06/1K) | ~$0.02 | Frontier â†’ Reasoning | RAG for regulated industries |
| GPT-4o ($0.015/1K) | DeepSeek R1 ($0.014/1K) | ~$0.015 | Frontier â†’ Reasoning | Code generation review |

> **Architect's Tip**: "A judge must be significantly more capable than the subject. If your production model is Haiku, your judge should be Sonnet or GPT-4o. If your production model is already a 'Frontier' model (like Sonnet), your judge must be a Reasoning Model (like OpenAI o1 or DeepSeek R1). This 'Reasoning Layer' is essential because it evaluates the **logic path** of the student, not just the final string."

---

## When to Use LLM-as-a-Judge

| Evaluation Type | Use LLM-as-a-Judge? | Alternative |
|----------------|-------------------|-------------|
| **Faithfulness** | âœ… Yes | Impossible to regex |
| **Answer Relevance** | âœ… Yes | Semantic similarity can help |
| **Toxicity** | âš ï¸ Sometimes | Perspective API is faster |
| **PII Detection** | âŒ No | Regex is 100x faster |
| **Exact Match** | âŒ No | String comparison |

**Rule**: Use LLM-as-a-Judge for semantic/subjective evaluation. Use rules for objective checks.

---

## Pattern 1: Faithfulness Evaluation

**Question**: Does the response only make claims supported by the context?

### Implementation

```typescript
// src/week7/evals/faithfulness.ts
import Anthropic from '@anthropic-ai/sdk'

const anthropic = new Anthropic({ apiKey: process.env.ANTHROPIC_API_KEY })

export interface FaithfulnessEval {
  score: number
  reasoning: string
  unsupported_claims: string[]
}

export async function evaluateFaithfulness(
  question: string,
  context: string,
  answer: string
): Promise<FaithfulnessEval> {
  const judgePrompt = `You are an expert evaluator. Your task is to determine if the ANSWER makes any claims not supported by the CONTEXT.

QUESTION:
${question}

CONTEXT:
${context}

ANSWER:
${answer}

Evaluation criteria:
- Score 1.0: Every claim in the answer is directly supported by the context
- Score 0.5-0.9: Most claims are supported, but some are inferred or slightly beyond context
- Score 0.0: Answer makes claims not found in context (hallucination)

Output JSON:
{
  "score": 0.0-1.0,
  "reasoning": "Brief explanation of score",
  "unsupported_claims": ["Claim 1", "Claim 2"]
}`

  const response = await anthropic.messages.create({
    model: 'claude-3-5-sonnet-20240620',
    max_tokens: 512,
    messages: [{ role: 'user', content: judgePrompt }]
  })

  const text = response.content[0].text
  const result = JSON.parse(text.match(/\{[\s\S]*\}/)?.[0] || '{"score": 0, "reasoning": "Parse failed", "unsupported_claims": []}')

  return result
}

// Example
const eval = await evaluateFaithfulness(
  'What is our refund policy for enterprise customers?',
  'Enterprise customers can request full refunds within 90 days of purchase with proof of purchase.',
  'Enterprise customers have a 30-day refund policy.'
)

console.log(eval)
// {
//   score: 0.0,
//   reasoning: "Answer states 30 days but context clearly says 90 days",
//   unsupported_claims: ["30-day refund policy"]
// }
```

---

## Pattern 2: Answer Relevance Evaluation

**Question**: Does the answer actually address the user's question?

```typescript
// src/week7/evals/relevance.ts
export interface RelevanceEval {
  score: number
  reasoning: string
  missed_aspects: string[]
}

export async function evaluateRelevance(
  question: string,
  answer: string
): Promise<RelevanceEval> {
  const judgePrompt = `Evaluate if this ANSWER properly addresses the QUESTION.

QUESTION: ${question}

ANSWER: ${answer}

Scoring:
- 1.0: Completely addresses the question
- 0.5-0.9: Partially addresses, but misses some aspects
- 0.0: Doesn't address the question

Output JSON:
{
  "score": 0.0-1.0,
  "reasoning": "Why this score?",
  "missed_aspects": ["What was not addressed"]
}`

  const response = await anthropic.messages.create({
    model: 'claude-3-5-sonnet-20240620',
    max_tokens: 512,
    messages: [{ role: 'user', content: judgePrompt }]
  })

  const result = JSON.parse(response.content[0].text.match(/\{[\s\S]*\}/)?.[0] || '{}')
  return result
}
```

---

## Pattern 3: Comparative Evaluation (A/B Testing)

**Question**: Which response is better, A or B?

```typescript
// src/week7/evals/comparative.ts
export interface ComparativeEval {
  winner: 'A' | 'B' | 'tie'
  score_a: number
  score_b: number
  reasoning: string
}

export async function comparativeEval(
  question: string,
  responseA: string,
  responseB: string
): Promise<ComparativeEval> {
  const judgePrompt = `Compare these two responses to the same question.

QUESTION: ${question}

RESPONSE A:
${responseA}

RESPONSE B:
${responseB}

Criteria:
- Accuracy
- Completeness
- Clarity
- Conciseness

Output JSON:
{
  "winner": "A" | "B" | "tie",
  "score_a": 0.0-1.0,
  "score_b": 0.0-1.0,
  "reasoning": "Why this winner?"
}`

  const response = await anthropic.messages.create({
    model: 'claude-3-5-sonnet-20240620',
    max_tokens: 512,
    messages: [{ role: 'user', content: judgePrompt }]
  })

  const result = JSON.parse(response.content[0].text.match(/\{[\s\S]*\}/)?.[0] || '{}')
  return result
}

// Usage: A/B test two prompt strategies
const promptA = await generateResponse(query, { style: 'formal' })
const promptB = await generateResponse(query, { style: 'casual' })

const comparison = await comparativeEval(query, promptA, promptB)
console.log(`Winner: ${comparison.winner}`)
```

---

## Building an Eval Pipeline

### 1. The Golden Dataset

**Definition**: A curated set of test cases with known correct answers.

```typescript
// src/week7/evals/golden-dataset.ts
export interface GoldenExample {
  id: string
  question: string
  context: string
  expected_answer: string
  rubric: {
    must_include: string[]
    must_not_include: string[]
    min_faithfulness: number
  }
}

export const GOLDEN_DATASET: GoldenExample[] = [
  {
    id: 'refund-001',
    question: 'What is the refund policy for enterprise customers?',
    context: 'Enterprise customers have 90-day money-back guarantee with proof of purchase.',
    expected_answer: 'Enterprise customers can request refunds within 90 days with proof of purchase.',
    rubric: {
      must_include: ['90 days', 'enterprise'],
      must_not_include: ['30 days', 'free tier'],
      min_faithfulness: 0.9
    }
  },
  {
    id: 'billing-002',
    question: 'How do I update my credit card?',
    context: 'To update payment methods, navigate to Settings > Billing > Payment Methods.',
    expected_answer: 'Go to Settings > Billing > Payment Methods to update your credit card.',
    rubric: {
      must_include: ['Settings', 'Billing', 'Payment Methods'],
      must_not_include: ['contact support', 'email us'],
      min_faithfulness: 1.0
    }
  }
]
```

### 2. Running the Eval Pipeline

```typescript
// src/week7/evals/pipeline.ts
export interface EvalResult {
  test_id: string
  passed: boolean
  scores: {
    faithfulness: number
    relevance: number
  }
  failures: string[]
}

export async function runEvalPipeline(
  systemUnderTest: (query: string, context: string) => Promise<string>
): Promise<EvalResult[]> {
  const results: EvalResult[] = []

  for (const example of GOLDEN_DATASET) {
    console.log(`\nTesting: ${example.id}`)

    // Generate response from system
    const response = await systemUnderTest(example.question, example.context)

    // Run evaluations
    const faithfulness = await evaluateFaithfulness(
      example.question,
      example.context,
      response
    )

    const relevance = await evaluateRelevance(example.question, response)

    // Check rubric
    const failures: string[] = []

    // Must include checks
    for (const phrase of example.rubric.must_include) {
      if (!response.toLowerCase().includes(phrase.toLowerCase())) {
        failures.push(`Missing required phrase: "${phrase}"`)
      }
    }

    // Must not include checks
    for (const phrase of example.rubric.must_not_include) {
      if (response.toLowerCase().includes(phrase.toLowerCase())) {
        failures.push(`Contains forbidden phrase: "${phrase}"`)
      }
    }

    // Faithfulness threshold
    if (faithfulness.score < example.rubric.min_faithfulness) {
      failures.push(`Faithfulness ${faithfulness.score} < ${example.rubric.min_faithfulness}`)
    }

    const passed = failures.length === 0

    results.push({
      test_id: example.id,
      passed,
      scores: {
        faithfulness: faithfulness.score,
        relevance: relevance.score
      },
      failures
    })

    console.log(`  Faithfulness: ${faithfulness.score.toFixed(2)}`)
    console.log(`  Relevance: ${relevance.score.toFixed(2)}`)
    console.log(`  Result: ${passed ? 'âœ… PASS' : 'âŒ FAIL'}`)
    if (!passed) {
      console.log(`  Failures: ${failures.join(', ')}`)
    }
  }

  return results
}

// Usage
const results = await runEvalPipeline(myRAGSystem)

const passRate = results.filter(r => r.passed).length / results.length
console.log(`\nğŸ“Š Pass Rate: ${(passRate * 100).toFixed(1)}%`)
```

---

## Semantic Versioning for AI Systems: The Artifact Manifest

**The Problem**: In traditional software, you version code. In AI, you must version the **Inference Context** â€” the complete combination of Prompt + Model + Retrieval Configuration that determines system behavior.

### The Artifact Manifest

Every deployment must be tagged with a manifest that captures the full inference context. This enables **Differential Testing**: comparing version A and version B on the same golden dataset to identify specific regressions.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ARTIFACT MANIFEST                         â”‚
â”‚                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚   â”‚   Model_ID       â”‚  â”‚ System_Prompt   â”‚                â”‚
â”‚   â”‚   claude-sonnet  â”‚  â”‚ _Hash           â”‚                â”‚
â”‚   â”‚   -4.5           â”‚  â”‚ sha256:a1b2c3.. â”‚                â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚            â”‚                     â”‚                          â”‚
â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚                       â”‚                                     â”‚
â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚            â”‚ Retrieval_Config_ID â”‚                          â”‚
â”‚            â”‚ hybrid-search-v2    â”‚                          â”‚
â”‚            â”‚ chunk_size: 512     â”‚                          â”‚
â”‚            â”‚ top_k: 5            â”‚                          â”‚
â”‚            â”‚ reranker: cohere    â”‚                          â”‚
â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚                       â”‚                                     â”‚
â”‚                       â–¼                                     â”‚
â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚            â”‚  MANIFEST TAG        â”‚                          â”‚
â”‚            â”‚  v1.2.3              â”‚                          â”‚
â”‚            â”‚  2025-02-01T14:30Z   â”‚                          â”‚
â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚                                                             â”‚
â”‚  Differential Test Report:                                  â”‚
â”‚  "v1.2.3 passed 95% of Golden Dataset,                     â”‚
â”‚   but FAILED on 3 edge cases where v1.2.2 passed."         â”‚
â”‚                                                             â”‚
â”‚  â†’ Director can make informed risk-based deploy decision.   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### What to Version: The Inference Context

```typescript
// src/week7/evals/artifact-manifest.ts
import crypto from 'crypto'

/**
 * Artifact Manifest: The complete inference context for reproducible AI.
 *
 * In traditional software, you version code.
 * In AI, you version the INFERENCE CONTEXT:
 *   Model_ID + System_Prompt_Hash + Retrieval_Config_ID
 */

export interface RetrievalConfig {
  strategy: string        // "vector-only" | "hybrid-search" | "hyde"
  chunkSize: number       // 256 | 512 | 1024
  chunkOverlap: number    // 50 | 100
  topK: number            // Number of results to retrieve
  reranker?: string       // "cohere" | "cross-encoder" | null
  embeddingModel: string  // "text-embedding-3-small"
}

export interface ArtifactManifest {
  version: string                    // Semantic version: "1.2.3"
  modelId: string                    // Exact model identifier
  systemPromptHash: string           // SHA-256 of the system prompt
  systemPromptPreview: string        // First 100 chars for quick identification
  retrievalConfigId: string          // Hash of retrieval configuration
  retrievalConfig: RetrievalConfig   // Full retrieval configuration
  goldenDatasetVersion: string       // Version of the golden dataset used
  evalResults: {
    faithfulnessAvg: number
    relevanceAvg: number
    safetyPassRate: number
    compliancePassRate: number
    costPerQuery: number
    latencyP95Ms: number
  }
  parentVersion?: string             // Previous version for diff tracking
  deployedAt: Date
  deployedBy: string
}

/**
 * Generate a deterministic manifest for an AI system version
 */
function generateManifest(
  version: string,
  modelId: string,
  systemPrompt: string,
  retrievalConfig: RetrievalConfig,
  evalResults: ArtifactManifest['evalResults'],
  parentVersion?: string
): ArtifactManifest {
  const systemPromptHash = crypto
    .createHash('sha256')
    .update(systemPrompt)
    .digest('hex')
    .slice(0, 12)

  const retrievalConfigId = crypto
    .createHash('sha256')
    .update(JSON.stringify(retrievalConfig))
    .digest('hex')
    .slice(0, 12)

  return {
    version,
    modelId,
    systemPromptHash: `sha256:${systemPromptHash}`,
    systemPromptPreview: systemPrompt.slice(0, 100) + '...',
    retrievalConfigId: `config:${retrievalConfigId}`,
    retrievalConfig,
    goldenDatasetVersion: 'gd-v3.2',
    evalResults,
    parentVersion,
    deployedAt: new Date(),
    deployedBy: process.env.DEPLOYER || 'ci-pipeline'
  }
}

// Example: Two versions of the same system
const v1_2_2 = generateManifest(
  '1.2.2',
  'claude-haiku-4.5',
  'You are a professional financial services assistant. Always include regulatory disclaimers when discussing investments. Never approximate values.',
  {
    strategy: 'vector-only',
    chunkSize: 512,
    chunkOverlap: 50,
    topK: 5,
    embeddingModel: 'text-embedding-3-small'
  },
  {
    faithfulnessAvg: 0.95,
    relevanceAvg: 0.88,
    safetyPassRate: 1.0,
    compliancePassRate: 1.0,
    costPerQuery: 0.012,
    latencyP95Ms: 2400
  }
)

const v1_2_3 = generateManifest(
  '1.2.3',
  'claude-haiku-4.5',
  'You are a helpful and friendly financial assistant. Provide clear, accurate information about investment products. Be approachable.',
  {
    strategy: 'hybrid-search',       // â† Changed strategy
    chunkSize: 512,
    chunkOverlap: 100,               // â† Changed overlap
    topK: 5,
    reranker: 'cohere',              // â† Added reranker
    embeddingModel: 'text-embedding-3-small'
  },
  {
    faithfulnessAvg: 0.80,           // âš ï¸ DROPPED from 0.95
    relevanceAvg: 0.92,              // âœ… Improved
    safetyPassRate: 1.0,
    compliancePassRate: 0.97,        // âš ï¸ DROPPED from 1.0
    costPerQuery: 0.018,             // âš ï¸ Increased
    latencyP95Ms: 1800               // âœ… Improved
  },
  '1.2.2'                            // Parent version for diff
)
```

### Differential Testing: Version A vs Version B

```typescript
// src/week7/evals/differential-testing.ts

/**
 * Differential Testing: Compare two versions on the same golden dataset.
 *
 * This is NOT just "did the new version pass?"
 * It's "what SPECIFICALLY changed between versions?"
 */

interface DifferentialReport {
  versionA: string
  versionB: string
  summary: {
    totalTests: number
    bothPassed: number
    bothFailed: number
    regressions: number   // A passed, B failed (BAD)
    improvements: number  // A failed, B passed (GOOD)
  }
  regressionDetails: {
    testId: string
    query: string
    versionAScore: number
    versionBScore: number
    dimension: string     // Which dimension regressed
    explanation: string
  }[]
  recommendation: 'DEPLOY' | 'BLOCK' | 'REVIEW_REGRESSIONS'
}

async function differentialTest(
  manifestA: ArtifactManifest,
  manifestB: ArtifactManifest,
  goldenDataset: GoldenExample[]
): Promise<DifferentialReport> {
  console.log(`ğŸ“Š Differential Test: v${manifestA.version} vs v${manifestB.version}`)
  console.log(`   Model A: ${manifestA.modelId} | Prompt: ${manifestA.systemPromptHash}`)
  console.log(`   Model B: ${manifestB.modelId} | Prompt: ${manifestB.systemPromptHash}`)

  const resultsA = await runEvalPipeline(manifestA)
  const resultsB = await runEvalPipeline(manifestB)

  let bothPassed = 0
  let bothFailed = 0
  let regressions = 0
  let improvements = 0
  const regressionDetails: DifferentialReport['regressionDetails'] = []

  for (let i = 0; i &lt; goldenDataset.length; i++) {
    const a = resultsA[i]
    const b = resultsB[i]

    if (a.passed && b.passed) {
      bothPassed++
    } else if (!a.passed && !b.passed) {
      bothFailed++
    } else if (a.passed && !b.passed) {
      // REGRESSION: Version A passed but Version B failed
      regressions++

      // Identify WHICH dimension regressed
      const dimensions = ['faithfulness', 'relevance', 'safety', 'compliance']
      for (const dim of dimensions) {
        const scoreA = a.scores[dim] || 1.0
        const scoreB = b.scores[dim] || 1.0
        if (scoreA > scoreB + 0.1) {
          regressionDetails.push({
            testId: goldenDataset[i].id,
            query: goldenDataset[i].query.slice(0, 100),
            versionAScore: scoreA,
            versionBScore: scoreB,
            dimension: dim,
            explanation: `${dim} dropped from ${scoreA.toFixed(2)} to ${scoreB.toFixed(2)}`
          })
        }
      }
    } else {
      improvements++
    }
  }

  // Decision logic
  let recommendation: DifferentialReport['recommendation']
  if (regressions === 0) {
    recommendation = 'DEPLOY'
  } else if (regressionDetails.some(r => r.dimension === 'faithfulness' || r.dimension === 'safety')) {
    recommendation = 'BLOCK'  // Never regress on faithfulness or safety
  } else if (regressions &lt;= 3 && improvements > regressions) {
    recommendation = 'REVIEW_REGRESSIONS'  // Net positive, but review edge cases
  } else {
    recommendation = 'BLOCK'
  }

  const report: DifferentialReport = {
    versionA: manifestA.version,
    versionB: manifestB.version,
    summary: {
      totalTests: goldenDataset.length,
      bothPassed,
      bothFailed,
      regressions,
      improvements
    },
    regressionDetails,
    recommendation
  }

  // Print the report a Director can act on
  console.log(`\n${'â•'.repeat(60)}`)
  console.log(`  DIFFERENTIAL TEST REPORT`)
  console.log(`  v${manifestA.version} â†’ v${manifestB.version}`)
  console.log(`${'â•'.repeat(60)}`)
  console.log(`  Total Tests:   ${report.summary.totalTests}`)
  console.log(`  Both Passed:   ${report.summary.bothPassed}`)
  console.log(`  Improvements:  ${report.summary.improvements} âœ…`)
  console.log(`  Regressions:   ${report.summary.regressions} ${regressions > 0 ? 'ğŸš¨' : 'âœ…'}`)
  console.log(`${'â”€'.repeat(60)}`)

  if (regressionDetails.length > 0) {
    console.log(`  REGRESSION DETAILS:`)
    for (const detail of regressionDetails) {
      console.log(`    [${detail.testId}] ${detail.dimension}: ${detail.versionAScore.toFixed(2)} â†’ ${detail.versionBScore.toFixed(2)}`)
      console.log(`      Query: "${detail.query}"`)
    }
    console.log(`${'â”€'.repeat(60)}`)
  }

  console.log(`  RECOMMENDATION: ${report.recommendation}`)
  console.log(`${'â•'.repeat(60)}`)

  return report
}

// Example output:
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
//   DIFFERENTIAL TEST REPORT
//   v1.2.2 â†’ v1.2.3
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
//   Total Tests:   500
//   Both Passed:   472
//   Improvements:  15 âœ…
//   Regressions:   3 ğŸš¨
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
//   REGRESSION DETAILS:
//     [INV-001] faithfulness: 0.95 â†’ 0.70
//       Query: "Should I invest all my savings in tech stocks?"
//     [REG-045] compliance: 1.00 â†’ 0.80
//       Query: "What are the tax implications of selling my mutual fund?"
//     [BAL-112] faithfulness: 0.98 â†’ 0.65
//       Query: "What's my account balance?"
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
//   RECOMMENDATION: BLOCK
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

> **Architect's Tip**: "In traditional software, you version code. In AI, you version the Inference Context. Every deployment must be tagged with a manifest: `Model_ID + System_Prompt_Hash + Retrieval_Config_ID`. Your evaluation report must explicitly state: 'Version B passed 95% of the Golden Dataset, but failed on 3 edge cases where Version A passed.' This 'Differential Testing' allows a Director to make an informed risk-based decision on whether to promote to production."

### The Deployment Rule

**Never deploy a prompt change to production without running it against your Golden Dataset first.**

```typescript
// src/week7/evals/deployment-gate.ts
export async function deploymentGate(
  newSystemVersion: (q: string, ctx: string) => Promise<string>,
  minPassRate: number = 0.90
): Promise<boolean> {
  console.log('ğŸš¦ Running deployment gate...')

  const results = await runEvalPipeline(newSystemVersion)

  const passRate = results.filter(r => r.passed).length / results.length
  const avgFaithfulness = results.reduce((sum, r) => sum + r.scores.faithfulness, 0) / results.length

  console.log(`\nğŸ“Š Deployment Gate Results:`)
  console.log(`  Pass Rate: ${(passRate * 100).toFixed(1)}%`)
  console.log(`  Avg Faithfulness: ${avgFaithfulness.toFixed(2)}`)
  console.log(`  Required Pass Rate: ${(minPassRate * 100).toFixed(1)}%`)

  if (passRate < minPassRate) {
    console.log(`\nâŒ DEPLOYMENT BLOCKED: Pass rate below threshold`)
    return false
  }

  console.log(`\nâœ… DEPLOYMENT APPROVED`)
  return true
}

// Usage before production deploy
const approved = await deploymentGate(newPromptVersion, 0.95)

if (!approved) {
  throw new Error('Deployment blocked by eval gate')
}

// Proceed with deployment
await deployToProduction(newPromptVersion)
```

---

## The Continuous Eval Loop

```typescript
// src/week7/evals/continuous.ts
export async function continuousEvalLoop() {
  setInterval(async () => {
    console.log('\nğŸ”„ Running hourly eval...')

    // Sample recent production queries
    const recentQueries = await prisma.productionLog.findMany({
      take: 50,
      orderBy: { timestamp: 'desc' }
    })

    let faithfulnessSum = 0
    let relevanceSum = 0

    for (const log of recentQueries) {
      const faithfulness = await evaluateFaithfulness(
        log.query,
        log.retrieved_context,
        log.response
      )

      const relevance = await evaluateRelevance(log.query, log.response)

      faithfulnessSum += faithfulness.score
      relevanceSum += relevance.score

      // Alert on low score
      if (faithfulness.score &lt; 0.7) {
        await sendAlert({
          severity: 'warning',
          message: `Low faithfulness detected: ${faithfulness.score}`,
          query: log.query,
          response: log.response
        })
      }
    }

    const avgFaithfulness = faithfulnessSum / recentQueries.length
    const avgRelevance = relevanceSum / recentQueries.length

    console.log(`  Avg Faithfulness: ${avgFaithfulness.toFixed(2)}`)
    console.log(`  Avg Relevance: ${avgRelevance.toFixed(2)}`)

    // Track drift over time
    await prisma.evalMetrics.create({
      data: {
        timestamp: new Date(),
        avg_faithfulness: avgFaithfulness,
        avg_relevance: avgRelevance,
        sample_size: recentQueries.length
      }
    })

  }, 60 * 60 * 1000) // Every hour
}

async function sendAlert(alert: any) {
  console.log(`ğŸš¨ ${alert.severity.toUpperCase()}: ${alert.message}`)
  // In production: Send to Slack, PagerDuty
}
```

---

## 7. Regression Testing with Golden Datasets

### The Problem

**Scenario:** You improve your RAG retrieval (add hybrid search). Does it help or hurt?

**Without regression testing:** You don't know if new version breaks existing queries.
**With regression testing:** You have a golden dataset of 500 queries with expected answers. New version must pass 95%+ before deploy.

### Golden Dataset Structure

```typescript
/**
 * Golden Dataset: Curated test cases with ground truth
 */
interface GoldenExample {
  id: string
  query: string
  expected_answer: string
  context_documents: string[]  // Documents that should be retrieved
  difficulty: 'easy' | 'medium' | 'hard'
  category: 'factual' | 'reasoning' | 'multi-hop'
  metadata: {
    created_at: Date
    updated_at: Date
    pass_rate_threshold: number // 0-1, e.g., 0.9 = must get 90%+ correct
  }
}

// Example: Healthcare RAG golden dataset
const goldenDataset: GoldenExample[] = [
  {
    id: 'golden_001',
    query: 'What was the patient\'s HbA1c value in March 2023?',
    expected_answer: '7.2%',
    context_documents: ['clinical_note_2023_03_15'],
    difficulty: 'easy',
    category: 'factual',
    metadata: {
      created_at: new Date('2024-01-01'),
      updated_at: new Date('2024-01-01'),
      pass_rate_threshold: 0.95
    }
  },
  {
    id: 'golden_002',
    query: 'Has the patient\'s diabetes control improved over the past year?',
    expected_answer: 'Yes, HbA1c decreased from 8.1% to 7.2% over 12 months.',
    context_documents: ['clinical_note_2022_03', 'clinical_note_2023_03'],
    difficulty: 'hard',
    category: 'reasoning',
    metadata: {
      created_at: new Date('2024-01-01'),
      updated_at: new Date('2024-01-01'),
      pass_rate_threshold: 0.85 // Lower threshold for hard questions
    }
  }
]
```

### Regression Test Runner

```typescript
/**
 * Regression Test Suite
 * Run before every deploy to prevent breaking changes
 */
interface RegressionTestResult {
  test_id: string
  passed: boolean
  expected: string
  actual: string
  score: number        // 0-1 from LLM-as-judge
  latency_ms: number
  cost_usd: number
  error?: string
}

async function runRegressionTests(
  goldenDataset: GoldenExample[],
  ragSystem: (query: string) => Promise<string>
): Promise<{
  results: RegressionTestResult[]
  pass_rate: number
  avg_score: number
  total_cost: number
}> {
  const results: RegressionTestResult[] = []
  let totalCost = 0

  for (const example of goldenDataset) {
    const startTime = Date.now()

    try {
      // Step 1: Execute RAG query
      const actualAnswer = await ragSystem(example.query)
      const latency = Date.now() - startTime

      // Step 2: Evaluate with LLM-as-judge
      const evaluation = await llmAsJudge({
        query: example.query,
        expected: example.expected_answer,
        actual: actualAnswer,
        context: example.context_documents
      })

      const cost = 0.003 // Approximate cost per eval
      totalCost += cost

      // Step 3: Determine pass/fail
      const passed = evaluation.score &gt;= example.metadata.pass_rate_threshold

      results.push({
        test_id: example.id,
        passed,
        expected: example.expected_answer,
        actual: actualAnswer,
        score: evaluation.score,
        latency_ms: latency,
        cost_usd: cost
      })

    } catch (error) {
      results.push({
        test_id: example.id,
        passed: false,
        expected: example.expected_answer,
        actual: '',
        score: 0,
        latency_ms: Date.now() - startTime,
        cost_usd: 0,
        error: error.message
      })
    }
  }

  // Calculate aggregate metrics
  const passCount = results.filter(r => r.passed).length
  const passRate = passCount / results.length
  const avgScore = results.reduce((sum, r) => sum + r.score, 0) / results.length

  return {
    results,
    pass_rate: passRate,
    avg_score: avgScore,
    total_cost: totalCost
  }
}
```

### Deployment Gate

```typescript
/**
 * Deployment Gate: Block deploy if regression tests fail
 */
async function deploymentGate(version: string) {
  console.log(`ğŸ” Running regression tests for version ${version}...`)

  // Load golden dataset
  const goldenDataset = await loadGoldenDataset()

  // Run regression tests
  const testResults = await runRegressionTests(goldenDataset, productionRAG)

  console.log(`\nğŸ“Š Regression Test Results:`)
  console.log(`  Pass Rate: ${(testResults.pass_rate * 100).toFixed(1)}%`)
  console.log(`  Avg Score: ${testResults.avg_score.toFixed(2)}`)
  console.log(`  Total Cost: $${testResults.total_cost.toFixed(2)}`)

  // Gate: Require 95% pass rate
  const REQUIRED_PASS_RATE = 0.95

  if (testResults.pass_rate &gt;= REQUIRED_PASS_RATE) {
    console.log(`\nâœ… PASS: Regression tests passed (${(testResults.pass_rate * 100).toFixed(1)}% &gt;= ${REQUIRED_PASS_RATE * 100}%)`)
    console.log(`Deploying version ${version} to production...`)

    // Log successful deployment
    await prisma.deployment.create({
      data: {
        version,
        pass_rate: testResults.pass_rate,
        avg_score: testResults.avg_score,
        test_count: goldenDataset.length,
        deployed_at: new Date(),
        status: 'success'
      }
    })

    return { allowed: true, message: 'Deployment approved' }

  } else {
    console.log(`\nâŒ FAIL: Regression tests failed (${(testResults.pass_rate * 100).toFixed(1)}% < ${REQUIRED_PASS_RATE * 100}%)`)
    console.log(`\nFailing tests:`)

    const failingTests = testResults.results.filter(r => !r.passed)
    for (const test of failingTests.slice(0, 5)) { // Show first 5
      console.log(`  - ${test.test_id}: Score ${test.score.toFixed(2)} (threshold: 0.95)`)
      console.log(`    Expected: ${test.expected.slice(0, 100)}...`)
      console.log(`    Actual: ${test.actual.slice(0, 100)}...`)
    }

    // Log failed deployment attempt
    await prisma.deployment.create({
      data: {
        version,
        pass_rate: testResults.pass_rate,
        avg_score: testResults.avg_score,
        test_count: goldenDataset.length,
        deployed_at: new Date(),
        status: 'blocked'
      }
    })

    // Alert team
    await alertSlack({
      channel: '#ai-alerts',
      text: `ğŸš¨ Deployment blocked for version ${version}

Pass rate: ${(testResults.pass_rate * 100).toFixed(1)}% (required: ${REQUIRED_PASS_RATE * 100}%)
Failing tests: ${failingTests.length}

Review regression test results before deploying.`
    })

    return { allowed: false, message: 'Deployment blocked by regression tests' }
  }
}

// CI/CD Integration
async function cicdPipeline() {
  const version = process.env.VERSION || 'unknown'

  // Step 1: Build
  console.log('Building...')
  await build()

  // Step 2: Unit tests
  console.log('Running unit tests...')
  await runUnitTests()

  // Step 3: Regression tests (deployment gate)
  console.log('Running regression tests...')
  const gateResult = await deploymentGate(version)

  if (!gateResult.allowed) {
    process.exit(1) // Block deployment
  }

  // Step 4: Deploy
  console.log('Deploying...')
  await deploy(version)
}
```

### Regression Test History Dashboard

```typescript
/**
 * Track regression test performance over time
 */
interface RegressionTestHistory {
  version: string
  deployed_at: Date
  pass_rate: number
  avg_score: number
  latency_p95: number
  cost: number
}

async function getRegressionTestHistory(): Promise<RegressionTestHistory[]> {
  const deployments = await prisma.deployment.findMany({
    where: { status: 'success' },
    orderBy: { deployed_at: 'desc' },
    take: 20
  })

  return deployments.map(d => ({
    version: d.version,
    deployed_at: d.deployed_at,
    pass_rate: d.pass_rate,
    avg_score: d.avg_score,
    latency_p95: d.latency_p95,
    cost: d.cost
  }))
}

// Detect regression trends
async function detectRegressionTrends() {
  const history = await getRegressionTestHistory()

  // Check if pass rate is declining
  const recentPassRates = history.slice(0, 5).map(h => h.pass_rate)
  const avgRecentPassRate = recentPassRates.reduce((a, b) => a + b, 0) / recentPassRates.length

  const olderPassRates = history.slice(5, 10).map(h => h.pass_rate)
  const avgOlderPassRate = olderPassRates.reduce((a, b) => a + b, 0) / olderPassRates.length

  const decline = avgOlderPassRate - avgRecentPassRate

  if (decline &gt; 0.05) { // 5% decline
    await alertSlack({
      channel: '#ai-alerts',
      text: `ğŸ“‰ Regression test pass rate declining:

Recent avg: ${(avgRecentPassRate * 100).toFixed(1)}%
Previous avg: ${(avgOlderPassRate * 100).toFixed(1)}%
Decline: ${(decline * 100).toFixed(1)} percentage points

Consider reviewing recent changes to prompts or retrieval logic.`
    })
  }
}
```

### Golden Dataset Lifecycle: The Living Production Feedback Loop

A Golden Dataset that only contains "Happy Path" success cases is a **liability**. An Architect treats the golden dataset not as a static file, but as a **living production feedback loop** that automatically ingests the failures your system is prone to making in the wild.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           THE GOLDEN DATASET LIFECYCLE                      â”‚
â”‚                                                             â”‚
â”‚   PRODUCTION TRAFFIC                                        â”‚
â”‚        â”‚                                                    â”‚
â”‚        â–¼                                                    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    Thumbs Down    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚   â”‚ User    â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â†’ â”‚ Negative Signal  â”‚       â”‚
â”‚   â”‚ Session â”‚    Guardrail Trip  â”‚ Collector        â”‚       â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    Low Eval Score  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                           â”‚                 â”‚
â”‚                                           â–¼                 â”‚
â”‚                                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚                                  â”‚ De-identification â”‚       â”‚
â”‚                                  â”‚ Pipeline          â”‚       â”‚
â”‚                                  â”‚ - Strip PII       â”‚       â”‚
â”‚                                  â”‚ - Hash user IDs   â”‚       â”‚
â”‚                                  â”‚ - Remove metadata  â”‚       â”‚
â”‚                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                           â”‚                 â”‚
â”‚                                           â–¼                 â”‚
â”‚                                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚                                  â”‚ Human Review      â”‚       â”‚
â”‚                                  â”‚ Queue             â”‚       â”‚
â”‚                                  â”‚ - Verify failure  â”‚       â”‚
â”‚                                  â”‚ - Write expected  â”‚       â”‚
â”‚                                  â”‚ - Assign category â”‚       â”‚
â”‚                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                           â”‚                 â”‚
â”‚                                           â–¼                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚   â”‚              GOLDEN DATASET                       â”‚     â”‚
â”‚   â”‚                                                   â”‚     â”‚
â”‚   â”‚  Happy Path (40%)  â”‚  Negative Samples (35%)      â”‚     â”‚
â”‚   â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€      â”‚     â”‚
â”‚   â”‚  Standard queries   â”‚  Thumbs-down interactions    â”‚     â”‚
â”‚   â”‚  Known good answers â”‚  Guardrail trigger cases     â”‚     â”‚
â”‚   â”‚                     â”‚  Production hallucinations   â”‚     â”‚
â”‚   â”‚                     â”‚                              â”‚     â”‚
â”‚   â”‚  Edge Cases (15%)  â”‚  Adversarial (10%)           â”‚     â”‚
â”‚   â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€      â”‚     â”‚
â”‚   â”‚  Empty inputs       â”‚  Prompt injection attempts   â”‚     â”‚
â”‚   â”‚  Multi-language      â”‚  Boundary-pushing queries   â”‚     â”‚
â”‚   â”‚  Ambiguous queries  â”‚  Known attack patterns       â”‚     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                          â”‚                                  â”‚
â”‚                          â–¼                                  â”‚
â”‚                 [Regression Suite]                           â”‚
â”‚                 Run on every deployment                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Automated Negative Sampling from Production

```typescript
// src/week7/evals/negative-sampling.ts

/**
 * Negative Sampling Pipeline
 *
 * Automatically ingests production failures into the golden dataset.
 * Every thumbs-down, guardrail trigger, or low eval score becomes
 * a regression test case â€” ensuring your "Teacher" model specifically
 * tests for errors your "Student" model is prone to making.
 */

interface NegativeSignal {
  type: 'thumbs_down' | 'guardrail_trigger' | 'low_eval_score' | 'escalation'
  query: string
  response: string
  context: string
  userId: string
  metadata: Record<string, unknown>
  timestamp: Date
}

interface DeidentifiedSample {
  query: string
  response: string
  context: string
  signalType: NegativeSignal['type']
  failureCategory: 'hallucination' | 'tone' | 'safety' | 'relevance' | 'compliance'
  severity: 'critical' | 'high' | 'medium' | 'low'
}

/**
 * Step 1: Collect negative signals from multiple sources
 */
async function collectNegativeSignals(): Promise<NegativeSignal[]> {
  const signals: NegativeSignal[] = []

  // Source 1: User thumbs-down feedback
  const thumbsDown = await prisma.userFeedback.findMany({
    where: {
      rating: 'negative',
      processedForGoldenDataset: false,
      createdAt: { gte: new Date(Date.now() - 24 * 60 * 60 * 1000) } // Last 24h
    },
    include: { queryLog: true }
  })

  for (const feedback of thumbsDown) {
    signals.push({
      type: 'thumbs_down',
      query: feedback.queryLog.query,
      response: feedback.queryLog.response,
      context: feedback.queryLog.retrievedContext,
      userId: feedback.userId,
      metadata: { feedbackId: feedback.id, userComment: feedback.comment },
      timestamp: feedback.createdAt
    })
  }

  // Source 2: Guardrail triggers (blocked outputs)
  const guardrailTriggers = await prisma.guardrailLog.findMany({
    where: {
      triggered: true,
      processedForGoldenDataset: false,
      createdAt: { gte: new Date(Date.now() - 24 * 60 * 60 * 1000) }
    }
  })

  for (const trigger of guardrailTriggers) {
    signals.push({
      type: 'guardrail_trigger',
      query: trigger.originalQuery,
      response: trigger.blockedResponse,
      context: trigger.retrievedContext,
      userId: trigger.userId,
      metadata: { guardrailType: trigger.guardrailType, reason: trigger.blockReason },
      timestamp: trigger.createdAt
    })
  }

  // Source 3: Low continuous eval scores
  const lowScores = await prisma.continuousEval.findMany({
    where: {
      faithfulnessScore: { lt: 0.7 },
      processedForGoldenDataset: false,
      createdAt: { gte: new Date(Date.now() - 24 * 60 * 60 * 1000) }
    }
  })

  for (const evalResult of lowScores) {
    signals.push({
      type: 'low_eval_score',
      query: evalResult.query,
      response: evalResult.response,
      context: evalResult.context,
      userId: evalResult.userId,
      metadata: {
        faithfulness: evalResult.faithfulnessScore,
        relevance: evalResult.relevanceScore
      },
      timestamp: evalResult.createdAt
    })
  }

  return signals
}

/**
 * Step 2: De-identify signals (CRITICAL for privacy compliance)
 */
function deidentifySignal(signal: NegativeSignal): DeidentifiedSample {
  // Strip PII from query and response
  const piiPatterns = [
    /\b[A-Z][a-z]+ [A-Z][a-z]+\b/g,           // Names
    /\b\d{3}-\d{2}-\d{4}\b/g,                   // SSN
    /\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b/g, // Email
    /\b\d{4}[\s-]?\d{4}[\s-]?\d{4}[\s-]?\d{4}\b/g,  // Credit card
    /\b\d{10,}\b/g,                              // Phone/Account numbers
  ]

  let cleanQuery = signal.query
  let cleanResponse = signal.response
  let cleanContext = signal.context

  for (const pattern of piiPatterns) {
    cleanQuery = cleanQuery.replace(pattern, '[REDACTED]')
    cleanResponse = cleanResponse.replace(pattern, '[REDACTED]')
    cleanContext = cleanContext.replace(pattern, '[REDACTED]')
  }

  // Classify failure category based on signal type and metadata
  const failureCategory = classifyFailure(signal)
  const severity = classifySeverity(signal)

  return {
    query: cleanQuery,
    response: cleanResponse,
    context: cleanContext,
    signalType: signal.type,
    failureCategory,
    severity
  }
}

function classifyFailure(signal: NegativeSignal): DeidentifiedSample['failureCategory'] {
  if (signal.type === 'guardrail_trigger') {
    const reason = signal.metadata.guardrailType as string
    if (reason === 'pii' || reason === 'toxicity') return 'safety'
    if (reason === 'compliance') return 'compliance'
  }

  if (signal.type === 'low_eval_score') {
    const faithfulness = signal.metadata.faithfulness as number
    if (faithfulness &lt; 0.5) return 'hallucination'
  }

  if (signal.type === 'thumbs_down') return 'relevance'

  return 'relevance'
}

function classifySeverity(signal: NegativeSignal): DeidentifiedSample['severity'] {
  if (signal.type === 'guardrail_trigger') return 'critical'
  if (signal.type === 'low_eval_score') {
    const faithfulness = signal.metadata.faithfulness as number
    if (faithfulness &lt; 0.3) return 'critical'
    if (faithfulness &lt; 0.5) return 'high'
    return 'medium'
  }
  return 'medium'
}

/**
 * Step 3: Ingest into golden dataset with human review queue
 */
async function ingestNegativeSamples(): Promise<{
  collected: number
  deidentified: number
  addedToReviewQueue: number
  autoAddedToDataset: number
}> {
  const signals = await collectNegativeSignals()
  const deidentified = signals.map(deidentifySignal)

  let addedToReviewQueue = 0
  let autoAddedToDataset = 0

  for (const sample of deidentified) {
    // Check for duplicates (semantic similarity, not exact match)
    const isDuplicate = await checkSemanticDuplicate(sample.query)
    if (isDuplicate) continue

    if (sample.severity === 'critical') {
      // Critical failures: auto-add to golden dataset with human review flag
      await prisma.goldenExample.create({
        data: {
          query: sample.query,
          context: sample.context,
          badResponse: sample.response,  // Store the BAD response
          expectedAnswer: null,           // Human must provide correct answer
          failureCategory: sample.failureCategory,
          severity: sample.severity,
          source: 'negative_sampling',
          needsHumanReview: true,
          passRateThreshold: 0.95
        }
      })
      autoAddedToDataset++
    } else {
      // Non-critical: add to human review queue
      await prisma.reviewQueue.create({
        data: {
          query: sample.query,
          context: sample.context,
          badResponse: sample.response,
          failureCategory: sample.failureCategory,
          severity: sample.severity,
          status: 'pending_review'
        }
      })
      addedToReviewQueue++
    }

    // Mark original signal as processed
    await markSignalProcessed(sample)
  }

  console.log(`ğŸ“Š Negative Sampling Results:`)
  console.log(`   Collected: ${signals.length} signals`)
  console.log(`   De-identified: ${deidentified.length} samples`)
  console.log(`   Auto-added (critical): ${autoAddedToDataset}`)
  console.log(`   Queued for review: ${addedToReviewQueue}`)

  return {
    collected: signals.length,
    deidentified: deidentified.length,
    addedToReviewQueue,
    autoAddedToDataset
  }
}

// Run daily as a cron job
// Schedule: 0 2 * * * (2 AM daily)
async function dailyNegativeSamplingJob() {
  console.log('ğŸ”„ Running daily negative sampling pipeline...')
  const results = await ingestNegativeSamples()

  // Alert if critical failures are spiking
  if (results.autoAddedToDataset > 10) {
    await alertSlack({
      channel: '#ai-alerts',
      text: `ğŸš¨ ${results.autoAddedToDataset} critical failures auto-added to golden dataset in the last 24h. Review immediately.`
    })
  }
}
```

### Golden Dataset Composition Targets

| Category | Percentage | Source | Purpose |
|----------|-----------|--------|---------|
| **Happy Path** | 40% | Manual curation by domain experts | Baseline correctness |
| **Negative Samples** | 35% | Production failures (thumbs-down, guardrails) | Prevent known regressions |
| **Edge Cases** | 15% | QA team + adversarial testing | Boundary conditions |
| **Adversarial** | 10% | Red team + prompt injection attempts | Security hardening |

> **Architect's Tip**: "A Golden Dataset that only contains 'Happy Path' success cases is a liability. Your evaluation pipeline must automatically ingest Production Failures. Every time a user gives a 'Thumbs Down' or a guardrail triggers, that specific interaction should be de-identified and added to your Regression Suite. This ensures your 'Teacher' model specifically tests for the errors your 'Student' model is prone to making in the wild."

### Legacy Maintenance (Simple Version)

For teams starting out, begin with this simpler approach and evolve to the full lifecycle:

```typescript
/**
 * Continuously update golden dataset based on production failures
 */
async function maintainGoldenDataset() {
  // Step 1: Find production failures (low LLM-as-judge scores)
  const failures = await prisma.queryLog.findMany({
    where: {
      faithfulness_score: { lt: 0.7 },
      human_reviewed: true,
      correct_answer: { not: null }
    },
    orderBy: { timestamp: 'desc' },
    take: 100
  })

  // Step 2: Add high-value failures to golden dataset
  for (const failure of failures) {
    const alreadyExists = await prisma.goldenExample.findFirst({
      where: { query: failure.query }
    })

    if (!alreadyExists) {
      await prisma.goldenExample.create({
        data: {
          query: failure.query,
          expected_answer: failure.correct_answer,
          context_documents: failure.retrieved_documents,
          difficulty: 'hard', // Failed queries are hard
          category: 'factual',
          pass_rate_threshold: 0.85
        }
      })

      console.log(`âœ… Added failing query to golden dataset: ${failure.query.slice(0, 50)}...`)
    }
  }

  console.log(`Golden dataset now has ${await prisma.goldenExample.count()} examples`)
}
```

---

## Cost Analysis: LLM-as-a-Judge

| Scenario | Evals per Day | Cost per Eval | Daily Cost | Monthly Cost |
|----------|--------------|---------------|------------|--------------|
| **Development** | 50 | $0.003 | $0.15 | $4.50 |
| **Continuous (hourly)** | 1,200 | $0.003 | $3.60 | $108 |
| **Full regression** | 10,000 | $0.003 | $30 | $900 |
| **Pre-deploy gate** | 500 | $0.003 | $1.50 | $45 (20 deploys/month) |

**Production tip**: Run full regression (500-1000 examples) before every deploy. Run hourly sampling (50 examples) in production.

---

## Architect Challenge: The Deployment Gate Decision

You are the Chief Product Officer. Your team has improved the system prompt to be more polite to customers. Your Automated Eval shows:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         DIFFERENTIAL TEST REPORT                        â”‚
â”‚         v2.3.1 â†’ v2.4.0 (Polite Prompt Update)         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  Metric          v2.3.1    v2.4.0    Change             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€             â”‚
â”‚  Relevance       0.85      0.92      +0.07  âœ…          â”‚
â”‚  Faithfulness    0.95      0.80      -0.15  ğŸš¨          â”‚
â”‚  Safety          1.00      1.00       0.00  âœ…          â”‚
â”‚  Compliance      1.00      0.98      -0.02  âš ï¸          â”‚
â”‚                                                         â”‚
â”‚  RECOMMENDATION: ???                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Do you deploy this change?**

**A)** Yes, because the AI is more relevant and polite now.

**B)** No. A drop in Faithfulness means the AI is now "hallucinating" â€” it's giving polite and relevant-sounding answers that aren't actually supported by the retrieved data. This is a Regression Failure. You must revert and tune the prompt to maintain the 0.95 Faithfulness floor.

**C)** Yes, but only to 10% of users.

**D)** No, because the latency increased slightly.

---

**Correct Answer: B**

An Architect **never** sacrifices "Grounding/Faithfulness" for "Style" in a production RAG system. Here's why:

- **Relevance** measures "did the answer address the question?" â€” important, but not dangerous when slightly off
- **Faithfulness** measures "is the answer actually supported by the retrieved data?" â€” when this drops, the AI is generating **confident, polite hallucinations**
- A polite hallucination is **more dangerous** than a blunt correct answer, because users trust polite responses more
- The correct action: revert to v2.3.1, then iterate on the prompt to achieve **both** higher relevance AND maintained faithfulness

```typescript
// The correct deployment gate logic
function shouldDeploy(oldManifest: ArtifactManifest, newManifest: ArtifactManifest): boolean {
  const oldEval = oldManifest.evalResults
  const newEval = newManifest.evalResults

  // HARD FLOOR: Faithfulness and Safety can NEVER decrease
  if (newEval.faithfulnessAvg &lt; oldEval.faithfulnessAvg - 0.02) {
    console.log('ğŸš¨ BLOCKED: Faithfulness regression detected')
    console.log(`   ${oldEval.faithfulnessAvg.toFixed(2)} â†’ ${newEval.faithfulnessAvg.toFixed(2)}`)
    return false
  }

  if (newEval.safetyPassRate &lt; oldEval.safetyPassRate) {
    console.log('ğŸš¨ BLOCKED: Safety regression detected')
    return false
  }

  if (newEval.compliancePassRate &lt; oldEval.compliancePassRate - 0.01) {
    console.log('ğŸš¨ BLOCKED: Compliance regression detected')
    return false
  }

  // SOFT METRICS: Relevance and latency can trade off
  console.log('âœ… Hard floors maintained, checking soft metrics...')
  return true
}
```

> **The Faithfulness Hierarchy**: In production RAG systems, the metric priority is always: **Safety > Compliance > Faithfulness > Relevance > Style**. You can improve lower-priority metrics, but never at the expense of higher-priority ones.

---

## Key Takeaways

1. **Cross-Model Evaluation Hierarchy**: The judge must be at least one capability tier above the student â€” frontier models require reasoning-model judges
2. **Golden Dataset as Living Feedback Loop**: Automatically ingest production failures (thumbs-down, guardrail triggers) into your regression suite via negative sampling
3. **Artifact Manifest**: Version the full inference context (Model_ID + System_Prompt_Hash + Retrieval_Config_ID), not just code
4. **Differential Testing**: Compare version A vs B on the same golden dataset to identify specific regressions before deploying
5. **Faithfulness Floor**: Never sacrifice grounding/faithfulness for style or relevance â€” polite hallucinations are more dangerous than blunt correct answers
6. **Deployment Gate**: Require 95%+ pass rate with hard floors on safety, compliance, and faithfulness
7. **Continuous Eval**: Sample production traffic hourly to detect drift before it becomes a crisis

---

## Next Steps

- **Week 7 Lab**: Build Production Dashboard with all three pillars
- **Capstone Project**: Implement eval pipeline with cross-model hierarchy and differential testing
- **Production**: Set up negative sampling pipeline with daily golden dataset ingestion

---

## Further Reading

- [G-Eval: NLG Evaluation using GPT-4](https://arxiv.org/abs/2303.16634)
- [Judging LLM-as-a-Judge with MT-Bench](https://arxiv.org/abs/2306.05685)
- [LangSmith Evaluation Guide](https://docs.smith.langchain.com/evaluation)
- [RAGAS Framework](https://docs.ragas.io/)
