---
title: "System Architecture for AI Apps"
description: "Design scalable architecture for AI applications"
estimatedMinutes: 45
---

# System Architecture for AI Apps

## Why AI Apps Are Different

AI applications have unique challenges that traditional architectures don't address:

1. **External API Dependencies**: LLM APIs are the core logic (not just a feature)
2. **Unpredictable Costs**: Token usage varies wildly per request
3. **High Latency**: LLM calls take 1-5 seconds (vs. <100ms for databases)
4. **Non-Deterministic**: Same input can produce different outputs
5. **Context Management**: Need to manage conversation history and state
6. **Rate Limits**: Provider APIs have strict rate limits

## Core Components Explained

```
┌──────────────────────────────────────────────────┐
│                    Client                         │
│  (React, Next.js, Mobile App)                    │
│  - User Interface                                │
│  - Real-time Updates (streaming)                 │
│  - Error Handling                                │
└────────────────┬─────────────────────────────────┘
                 │ HTTPS / WebSocket
┌────────────────▼─────────────────────────────────┐
│                API Layer                          │
│  (Next.js API Routes, Express, FastAPI)          │
│  - Authentication & Authorization                │
│  - Request Validation                            │
│  - Rate Limiting                                 │
│  - Request Routing                               │
└──┬───────┬──────────┬─────────────┬──────────────┘
   │       │          │             │
   ▼       ▼          ▼             ▼
┌──────┐ ┌──────┐ ┌────────┐ ┌──────────┐
│ LLM  │ │Vector│ │ Cache  │ │ Database │
│ API  │ │  DB  │ │(Redis) │ │(Postgres)│
│      │ │      │ │        │ │          │
│Claude│ │Pinec-│ │Semantic│ │User data │
│OpenAI│ │ one  │ │Cache   │ │Convers.  │
└──────┘ └──────┘ └────────┘ └──────────┘
```

### Component Deep Dive

**1. Client Layer**
- **Purpose**: User interface and real-time feedback
- **Technical**: Streaming responses, optimistic updates, error recovery
- **Why Important**: LLM latency is high, users need immediate feedback

**2. API Layer**
- **Purpose**: Business logic, orchestration, security
- **Technical**: Rate limiting, authentication, prompt templating
- **Why Important**: Protects expensive LLM calls, enforces usage limits

**3. LLM API (Claude, OpenAI)**
- **Purpose**: Text generation and understanding
- **Technical**: REST API, token-based pricing, rate limits
- **Cost**: $0.003-$0.015 per 1K tokens
- **Latency**: 1-5 seconds per request

**4. Vector Database (Pinecone, Weaviate)**
- **Purpose**: Semantic search for RAG
- **Technical**: Vector embeddings, cosine similarity, ANN search
- **Cost**: ~$70/month for 1M vectors
- **Latency**: 50-200ms per query

**5. Cache (Redis)**
- **Purpose**: Reduce duplicate LLM calls
- **Technical**: Key-value store, semantic caching
- **Benefit**: 80%+ cost reduction for common queries
- **Latency**: <10ms

**6. Database (PostgreSQL)**
- **Purpose**: Store user data, conversations, metadata
- **Technical**: Relational DB, ACID compliance
- **Why**: Structured data, transactions, relationships

## Architecture Patterns

### Pattern 1: Simple Chatbot

**When to use:**
- Simple Q&A chatbot
- No external data needed
- Stateless conversations
- Low traffic (<1000 users)

**Architecture:**
```
User → API → LLM → Response
```

**Complete Implementation:**

```typescript
// app/api/chat/route.ts
import { Anthropic } from '@anthropic-ai/sdk'
import { NextResponse } from 'next/server'

const anthropic = new Anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY!
})

export async function POST(req: Request) {
  try {
    // 1. Parse and validate input
    const { message } = await req.json()

    if (!message || message.length > 1000) {
      return NextResponse.json(
        { error: 'Invalid message' },
        { status: 400 }
      )
    }

    // 2. Call LLM
    const startTime = Date.now()
    const response = await anthropic.messages.create({
      model: 'claude-3-sonnet-20240229',
      max_tokens: 1024,
      messages: [{ role: 'user', content: message }]
    })

    const latency = Date.now() - startTime

    // 3. Track usage (important for cost monitoring!)
    await logUsage({
      inputTokens: response.usage.input_tokens,
      outputTokens: response.usage.output_tokens,
      latency,
      cost: calculateCost(response.usage)
    })

    // 4. Return response
    return NextResponse.json({
      reply: response.content[0].text,
      usage: response.usage
    })

  } catch (error) {
    console.error('Chat error:', error)

    // Handle rate limits specifically
    if (error.status === 429) {
      return NextResponse.json(
        { error: 'Too many requests, please try again later' },
        { status: 429 }
      )
    }

    return NextResponse.json(
      { error: 'Failed to generate response' },
      { status: 500 }
    )
  }
}

function calculateCost(usage: { input_tokens: number; output_tokens: number }) {
  // Claude 3 Sonnet pricing (as of 2024)
  const INPUT_COST = 0.003 / 1000  // $0.003 per 1K tokens
  const OUTPUT_COST = 0.015 / 1000 // $0.015 per 1K tokens

  return (
    usage.input_tokens * INPUT_COST +
    usage.output_tokens * OUTPUT_COST
  )
}
```

**Pros:**
- Simple to build and deploy
- Low complexity
- Easy to debug

**Cons:**
- No conversation memory
- No external knowledge
- High cost (no caching)
- Can't handle complex queries

**Cost Example:**
- 1000 queries/day
- Average 500 tokens in + 200 tokens out
- Cost: ~$3/day = $90/month

### Pattern 2: RAG Application

**When to use:**
- Need to answer from specific documents
- Company knowledge base
- Customer support with docs
- Medium traffic (1K-100K users)

**Architecture:**
```
User → API → [Embed Query] → Vector DB → [Retrieve Docs]
              ↓
         LLM (with context) → Response
```

**Complete Implementation:**

```typescript
// app/api/rag/route.ts
import { embed } from '@/lib/embeddings'
import { pinecone } from '@/lib/vector-db'
import { anthropic } from '@/lib/llm'
import { redis } from '@/lib/cache'

export async function POST(req: Request) {
  const { question, userId } = await req.json()

  try {
    // 1. Check semantic cache first
    const cacheKey = await semanticCacheKey(question)
    const cached = await redis.get(cacheKey)

    if (cached) {
      return NextResponse.json({
        answer: cached,
        cached: true,
        cost: 0
      })
    }

    // 2. Embed the question (convert to vector)
    // Why: Vector search requires vectors, not text
    const queryEmbedding = await embed(question)
    // Cost: ~$0.0001, Latency: ~100ms

    // 3. Retrieve relevant documents
    // Why: LLM needs context to answer accurately
    const results = await pinecone.index('knowledge-base').query({
      vector: queryEmbedding,
      topK: 5, // Get top 5 most relevant chunks
      includeMetadata: true,
      filter: {
        // Optional: Filter by user permissions
        accessible_by: userId
      }
    })
    // Cost: ~$0.0001, Latency: ~50-200ms

    // 4. Format context from retrieved docs
    const context = results.matches
      .map((match, i) => `[${i + 1}] ${match.metadata.text}`)
      .join('\n\n')

    // 5. Generate answer with context
    // Why: Provide relevant context so LLM gives accurate answer
    const response = await anthropic.messages.create({
      model: 'claude-3-sonnet-20240229',
      max_tokens: 1024,
      messages: [{
        role: 'user',
        content: `Answer this question using ONLY the context below.
Include citations like [1], [2] for sources.
If the answer isn't in the context, say "I don't have enough information."

Context:
${context}

Question: ${question}`
      }]
    })
    // Cost: ~$0.01-0.03, Latency: ~2-4s

    const answer = response.content[0].text

    // 6. Cache the result
    await redis.setex(cacheKey, 3600, answer) // Cache for 1 hour

    // 7. Track usage and costs
    await logRAGUsage({
      question,
      documentsRetrieved: results.matches.length,
      tokens: response.usage,
      cost: calculateCost(response.usage),
      cacheHit: false
    })

    return NextResponse.json({
      answer,
      sources: results.matches.map(m => ({
        text: m.metadata.text,
        score: m.score,
        source: m.metadata.source
      })),
      cached: false,
      cost: calculateCost(response.usage)
    })

  } catch (error) {
    // Error handling...
  }
}

// Semantic cache: Similar questions get same cached answer
async function semanticCacheKey(question: string): Promise<string> {
  const embedding = await embed(question)

  // Search cache for similar questions
  const similar = await redis.ft.search('cache-index', {
    vector: embedding,
    threshold: 0.95 // 95% similarity required
  })

  if (similar.length > 0) {
    return similar[0].key
  }

  // New question, create new cache key
  return `cache:${hashString(question)}`
}
```

**Pros:**
- Answers from your own data
- More accurate than base LLM
- Can cite sources
- Scales to large knowledge bases

**Cons:**
- More complex setup
- Need to maintain vector DB
- Higher latency (embedding + retrieval + generation)
- More expensive (multiple API calls)

**Cost Example:**
- 5000 queries/day
- 60% cache hit rate
- Vector DB: $70/month
- LLM calls (40% of queries): ~$150/month
- **Total: ~$220/month**

**Performance Characteristics:**
```
Latency breakdown:
- Embedding: 100ms
- Vector search: 150ms
- LLM generation: 2500ms
- Total: ~2750ms (cached: <10ms)
```

### Pattern 3: Agent System

**When to use:**
- Complex multi-step tasks
- Need to use external tools/APIs
- Autonomous decision-making
- Variable workflows

**Architecture:**
```
User → Agent Orchestrator → [Loop]:
                              ↓
                         1. Analyze task
                              ↓
                         2. Choose tool
                              ↓
                         3. Execute tool
                              ↓
                         4. Evaluate result
                              ↓
                    [Repeat or Return Result]
```

**Complete Implementation:**

```typescript
// lib/agent.ts
interface Tool {
  name: string
  description: string
  execute: (input: string) => Promise<string>
}

class Agent {
  private tools: Tool[]
  private llm: LLM
  private maxIterations: number

  constructor(config: AgentConfig) {
    this.tools = config.tools
    this.llm = config.llm
    this.maxIterations = config.maxIterations || 10
  }

  async run(task: string): Promise<AgentResult> {
    const history: Step[] = []
    let iteration = 0

    while (iteration < this.maxIterations) {
      iteration++

      // 1. Agent decides what to do next
      const decision = await this.decide(task, history)

      // 2. If agent is done, return result
      if (decision.action === 'FINISH') {
        return {
          result: decision.output,
          steps: history,
          iterations: iteration
        }
      }

      // 3. Execute chosen tool
      const tool = this.tools.find(t => t.name === decision.tool)
      if (!tool) {
        throw new Error(`Tool ${decision.tool} not found`)
      }

      const toolResult = await tool.execute(decision.input)

      // 4. Record step
      history.push({
        iteration,
        thought: decision.reasoning,
        action: decision.tool,
        input: decision.input,
        output: toolResult
      })

      // 5. Check cost limits
      const totalCost = history.reduce((sum, step) => sum + step.cost, 0)
      if (totalCost > MAX_COST_PER_TASK) {
        throw new Error('Cost limit exceeded')
      }
    }

    throw new Error('Max iterations reached without completion')
  }

  private async decide(
    task: string,
    history: Step[]
  ): Promise<Decision> {
    const prompt = `You are an AI agent that can use tools to complete tasks.

Task: ${task}

Available Tools:
${this.tools.map(t => `- ${t.name}: ${t.description}`).join('\n')}

Previous Steps:
${history.map(s => `Step ${s.iteration}: Used ${s.action} with input "${s.input}", got: ${s.output}`).join('\n')}

Decide what to do next. Respond in JSON:
{
  "reasoning": "why you're doing this",
  "action": "FINISH" or tool name,
  "tool": tool name if action is not FINISH,
  "input": input for the tool,
  "output": final answer if action is FINISH
}`

    const response = await this.llm.complete(prompt)
    return JSON.parse(response)
  }
}

// Usage example
export async function POST(req: Request) {
  const { task } = await req.json()

  const agent = new Agent({
    tools: [
      {
        name: 'search_web',
        description: 'Search the web for information',
        execute: async (query) => {
          const results = await searchAPI(query)
          return results.join('\n')
        }
      },
      {
        name: 'calculator',
        description: 'Perform mathematical calculations',
        execute: async (expression) => {
          return eval(expression).toString()
        }
      },
      {
        name: 'database_query',
        description: 'Query the database',
        execute: async (sql) => {
          const results = await db.query(sql)
          return JSON.stringify(results)
        }
      }
    ],
    llm: claude,
    maxIterations: 10
  })

  const result = await agent.run(task)

  return NextResponse.json({
    result: result.result,
    steps: result.steps,
    iterations: result.iterations,
    cost: result.steps.reduce((sum, s) => sum + s.cost, 0)
  })
}
```

**Example Execution:**
```
Task: "What's the weather in the city where the Eiffel Tower is located?"

Step 1: Agent thinks: "I need to find where the Eiffel Tower is"
  → Uses: search_web("Eiffel Tower location")
  → Result: "Paris, France"

Step 2: Agent thinks: "Now I need the weather in Paris"
  → Uses: search_web("weather Paris France")
  → Result: "Currently 18°C, partly cloudy"

Step 3: Agent thinks: "I have the answer"
  → Action: FINISH
  → Output: "The weather in Paris (where the Eiffel Tower is located) is currently 18°C and partly cloudy"
```

**Pros:**
- Can solve complex tasks
- Autonomous reasoning
- Flexible and adaptive
- Can use any tools/APIs

**Cons:**
- Unpredictable cost (varies by task complexity)
- Can loop infinitely
- Slower (multiple LLM calls)
- Requires careful prompt engineering

**Cost Example:**
- 100 tasks/day
- Average 5 iterations per task
- ~$0.15 per task
- **Total: ~$15/day = $450/month**

## Scaling Strategies

### Strategy 1: Horizontal Scaling

**What it is:** Run multiple instances of your API behind a load balancer

**Why:** Single instance can't handle high traffic or provide redundancy

**Technical Implementation:**

```yaml
# docker-compose.yml
version: '3.8'

services:
  # Load Balancer
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - api

  # Multiple API instances
  api:
    image: ai-app:latest
    deploy:
      replicas: 3  # Run 3 copies
      resources:
        limits:
          cpus: '1'
          memory: 512M
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}

  # Shared Redis cache
  redis:
    image: redis:alpine
    ports:
      - "6379:6379"

  # Shared database
  postgres:
    image: postgres:14
    environment:
      - POSTGRES_DB=aiapp
      - POSTGRES_PASSWORD=${DB_PASSWORD}
```

**nginx.conf for load balancing:**
```nginx
upstream api_servers {
  # Round-robin load balancing
  server api:3000;
  server api:3001;
  server api:3002;
}

server {
  listen 80;

  location /api/ {
    proxy_pass http://api_servers;
    proxy_set_header Host $host;
    proxy_set_header X-Real-IP $remote_addr;

    # Important for streaming LLM responses
    proxy_buffering off;
    proxy_http_version 1.1;
  }
}
```

**When to scale horizontally:**
- API response time > 500ms
- CPU usage > 70%
- Traffic spikes expected
- Need high availability

**Pros:**
- Handles more traffic
- Zero downtime deployments
- Fault tolerance

**Cons:**
- More complex
- Shared state needs Redis
- Higher infrastructure cost

### Strategy 2: Caching Layer (Critical for AI Apps!)

**What it is:** Store LLM responses to avoid re-generating identical answers

**Why:** LLM calls are expensive ($0.01-0.03 per request) and slow (2-5s)

**Impact:** 80%+ cost reduction for common queries

**Multi-Level Caching Strategy:**

```typescript
// lib/cache.ts
class CacheHierarchy {
  private memoryCache: LRUCache  // L1: In-memory (fastest)
  private redisCache: Redis       // L2: Shared cache (fast)
  private semanticCache: VectorDB // L3: Semantic (smart)

  async get(query: string): Promise<CachedResponse | null> {
    // Level 1: Memory Cache (< 1ms)
    // Why: Fastest, but limited to single instance
    const memKey = hashQuery(query)
    let cached = this.memoryCache.get(memKey)
    if (cached) {
      console.log('L1 Cache HIT (memory)')
      return cached
    }

    // Level 2: Redis Cache (~5ms)
    // Why: Shared across instances, persistent
    cached = await this.redisCache.get(memKey)
    if (cached) {
      console.log('L2 Cache HIT (redis)')
      // Backfill L1
      this.memoryCache.set(memKey, cached)
      return JSON.parse(cached)
    }

    // Level 3: Semantic Cache (~50ms)
    // Why: Catches paraphrased/similar questions
    const similar = await this.findSimilarQuery(query)
    if (similar && similar.similarity > 0.95) {
      console.log('L3 Cache HIT (semantic)')
      const response = similar.response

      // Backfill L2 and L1
      await this.redisCache.setex(memKey, 3600, JSON.stringify(response))
      this.memoryCache.set(memKey, response)

      return response
    }

    console.log('Cache MISS - will call LLM')
    return null
  }

  async set(
    query: string,
    response: CachedResponse,
    ttl: number = 3600
  ): Promise<void> {
    const memKey = hashQuery(query)

    // Store in all levels
    this.memoryCache.set(memKey, response)
    await this.redisCache.setex(memKey, ttl, JSON.stringify(response))

    // Store in semantic cache with embedding
    const embedding = await embed(query)
    await this.semanticCache.upsert({
      query,
      embedding,
      response,
      timestamp: Date.now()
    })
  }

  private async findSimilarQuery(query: string): Promise<{
    query: string
    response: CachedResponse
    similarity: number
  } | null> {
    const embedding = await embed(query)

    const results = await this.semanticCache.query({
      vector: embedding,
      topK: 1,
      includeMetadata: true
    })

    if (results.length === 0) return null

    return {
      query: results[0].metadata.query,
      response: results[0].metadata.response,
      similarity: results[0].score
    }
  }
}

// Usage in API route
export async function POST(req: Request) {
  const { question } = await req.json()

  // Try cache first
  const cached = await cache.get(question)
  if (cached) {
    return NextResponse.json({
      ...cached,
      cached: true,
      cost: 0
    })
  }

  // Cache miss - call LLM
  const response = await generateResponse(question)

  // Store for future requests
  await cache.set(question, response)

  return NextResponse.json({
    ...response,
    cached: false
  })
}
```

**Cache Hit Rates in Production:**
```
Day 1:  20% hit rate (mostly exact matches)
Day 7:  45% hit rate (semantic cache kicking in)
Day 30: 75% hit rate (stable pattern)
```

**Cost Impact Example:**
```
Without caching:
- 10,000 requests/day
- $0.02 per request
- Cost: $200/day = $6,000/month

With 75% cache hit rate:
- 2,500 LLM calls/day (75% cached)
- $0.02 per LLM call
- Cost: $50/day = $1,500/month
- Savings: $4,500/month (75%)
```

### Strategy 3: Queue-Based Processing

**What it is:** Put long-running tasks in a queue, process asynchronously

**Why:** Don't block API responses waiting for slow LLM calls

**Use cases:**
- Batch document processing
- Email summarization
- Report generation
- Bulk analysis

**Implementation with BullMQ:**

```typescript
// lib/queue.ts
import { Queue, Worker } from 'bullmq'
import { redis } from './redis'

// Create queue
export const documentQueue = new Queue('document-processing', {
  connection: redis
})

// Add job to queue
export async function processDocument(documentId: string, userId: string) {
  const job = await documentQueue.add('analyze-document', {
    documentId,
    userId
  }, {
    attempts: 3,  // Retry 3 times on failure
    backoff: {
      type: 'exponential',
      delay: 5000  // 5s, 10s, 20s
    }
  })

  return job.id
}

// Worker - processes jobs in background
const worker = new Worker('document-processing', async (job) => {
  const { documentId, userId } = job.data

  // Update progress
  await job.updateProgress(0)

  // 1. Load document
  const document = await loadDocument(documentId)
  await job.updateProgress(20)

  // 2. Chunk document
  const chunks = chunkDocument(document)
  await job.updateProgress(40)

  // 3. Embed chunks (slow)
  const embeddings = await embedBatch(chunks)
  await job.updateProgress(70)

  // 4. Store in vector DB
  await vectorDB.upsert(embeddings)
  await job.updateProgress(90)

  // 5. Notify user
  await notifyUser(userId, {
    type: 'document_processed',
    documentId
  })
  await job.updateProgress(100)

  return { success: true, chunks: chunks.length }
}, {
  connection: redis,
  concurrency: 5  // Process 5 documents at once
})

// API route - just enqueues job
export async function POST(req: Request) {
  const { documentId } = await req.json()
  const userId = req.user.id

  const jobId = await processDocument(documentId, userId)

  return NextResponse.json({
    message: 'Processing started',
    jobId,
    status: 'queued'
  })
}

// Status check endpoint
export async function GET(req: Request) {
  const jobId = req.nextUrl.searchParams.get('jobId')
  const job = await documentQueue.getJob(jobId)

  if (!job) {
    return NextResponse.json({ error: 'Job not found' }, { status: 404 })
  }

  return NextResponse.json({
    id: job.id,
    progress: await job.progress(),
    state: await job.getState(),
    result: job.returnvalue
  })
}
```

**Benefits:**
- API responds instantly (< 100ms)
- Can process tasks in parallel
- Automatic retries on failure
- Better resource utilization
- Can scale workers independently

### Strategy 4: Database Optimization

**Why it matters:** Every API call queries database for user/session data

**Key Optimizations:**

```typescript
// 1. Connection Pooling
// Don't create new DB connection per request!
import { Pool } from 'pg'

const pool = new Pool({
  max: 20,  // Max 20 concurrent connections
  idleTimeoutMillis: 30000,
  connectionTimeoutMillis: 2000
})

// 2. Indexes on Common Queries
// Add indexes on frequently queried fields
CREATE INDEX idx_user_email ON users(email);
CREATE INDEX idx_conversation_user_date ON conversations(user_id, created_at DESC);
CREATE INDEX idx_messages_conversation ON messages(conversation_id);

// 3. Query Optimization
// Bad: N+1 query problem
const conversations = await prisma.conversation.findMany()
for (const conv of conversations) {
  const messages = await prisma.message.findMany({
    where: { conversationId: conv.id }
  })
}
// Makes 1 + N queries!

// Good: Join query
const conversations = await prisma.conversation.findMany({
  include: { messages: true }
})
// Makes 1 query

// 4. Pagination
// Don't load all messages at once
const messages = await prisma.message.findMany({
  where: { conversationId },
  orderBy: { createdAt: 'desc' },
  take: 50,  // Limit to 50 messages
  skip: page * 50
})
```

## Best Practices Summary

### 1. Separate Concerns (Clean Architecture)

```
┌─────────────────────────────────────┐
│         Presentation Layer           │
│  (API Routes, Controllers)           │
└──────────────┬──────────────────────┘
               │
┌──────────────▼──────────────────────┐
│         Business Logic Layer         │
│  (Services, Use Cases)               │
└──────────────┬──────────────────────┘
               │
┌──────────────▼──────────────────────┐
│         Data Access Layer            │
│  (Repositories, Database)            │
└─────────────────────────────────────┘
```

**Why:** Easier to test, maintain, and swap components

### 2. Cache Aggressively

**Cache these:**
- LLM responses (semantic cache)
- Database queries (Redis)
- Embeddings (don't re-embed same text)
- User sessions

**Don't cache:**
- User-specific sensitive data
- Real-time data
- Rapidly changing content

### 3. Queue Long Tasks

**Queue if task takes > 5 seconds:**
- Document processing
- Bulk operations
- Report generation
- Model fine-tuning

### 4. Monitor Everything

```typescript
// Track key metrics
interface Metrics {
  // Performance
  avgLatency: number
  p95Latency: number
  p99Latency: number

  // Costs
  dailyCost: number
  costPerRequest: number
  tokenUsage: number

  // Quality
  errorRate: number
  cacheHitRate: number
  userSatisfaction: number
}
```

### 5. Graceful Degradation

**Handle failures gracefully:**

```typescript
async function robustLLMCall(prompt: string): Promise<string> {
  try {
    return await claude.complete(prompt)
  } catch (error) {
    if (error.status === 429) {
      // Rate limited - try alternative provider
      return await openai.complete(prompt)
    }

    if (error.status === 500) {
      // Provider error - return cached or default response
      return await getCachedResponse() || DEFAULT_RESPONSE
    }

    throw error
  }
}
```

## Common Pitfalls to Avoid

1. **No cost tracking** → Surprise $10K bill
2. **No caching** → 10x higher costs
3. **Blocking API calls** → Slow UX
4. **No rate limiting** → API ban or cost explosion
5. **No error handling** → Poor user experience
6. **Storing LLM responses in memory** → Memory leaks
7. **No request validation** → Security issues
8. **Synchronous file processing** → Timeouts

## Resources

- [Next.js Architecture](https://nextjs.org/docs/architecture)
- [System Design Primer](https://github.com/donnemartin/system-design-primer)
- [AWS Well-Architected Framework](https://aws.amazon.com/architecture/well-architected/)
- [Patterns of Distributed Systems](https://martinfowler.com/articles/patterns-of-distributed-systems/)
