---
title: "The Three Pillars of AI Observability"
week: 7
concept: 1
description: "Monitor traces, evaluations, and unit economics to prevent silent failures in production AI systems"
estimatedMinutes: 50
objectives:
  - Implement OpenTelemetry tracing for RAG and agent flows
  - Track the three critical observability layers
  - Set up dashboards for latency, accuracy, and cost
  - Detect and prevent silent failures
---

# The Three Pillars of AI Observability

Hardening AI systems for the real world.

## The Core Problem

**Traditional software**: Errors are obvious (crashes, 500 errors, stack traces)
**AI systems**: Failures are silent (hallucinations, drift, quality degradation)

**Production reality**: An AI system can be "working" (no errors) while being completely broken (wrong answers).

**Solution**: Monitor three distinct layers to ensure the system doesn't drift or fail silently.

---

## The Three Pillars

| Pillar | Focus | Key Metrics | Tools |
|--------|-------|-------------|-------|
| **Traces** | Every step of the RAG/Agent flow | Retrieval Latency, Tool Execution Time, Token Usage | OpenTelemetry, LangSmith, Arize Phoenix |
| **Evaluations** | Semantic accuracy & safety | Faithfulness, Answer Relevance, PII Detection | RAGAS, LangSmith Evals, LLM-as-a-Judge |
| **Unit Economics** | Cost and efficiency | Tokens per User, Cost per Query, Cache Hit Rate | Custom dashboards, LangSmith |

---

## Pillar 1: Traces (The Flow)

**What it tracks**: Every step of your RAG or agent execution.

### Why Traces Matter

```typescript
// ‚ùå Without tracing, you see:
User query ‚Üí [BLACK BOX] ‚Üí Response (5 seconds)

// ‚úÖ With tracing, you see:
User query
  ‚îú‚îÄ Query embedding (200ms, 500 tokens, $0.0015)
  ‚îú‚îÄ Vector search (1200ms, 10 results)
  ‚îú‚îÄ Reranking (800ms, top 3 results)
  ‚îú‚îÄ LLM call (2500ms, 2000 tokens, $0.045)
  ‚îî‚îÄ Response (5000ms total, $0.0465 total)
```

**Production impact**: You can now see that reranking is slow and vector search is the bottleneck.

### Implementing OpenTelemetry Traces

```typescript
// src/week7/tracing.ts
import { trace, context } from '@opentelemetry/api'
import { NodeTracerProvider } from '@opentelemetry/sdk-trace-node'
import { SimpleSpanProcessor } from '@opentelemetry/sdk-trace-base'
import { OTLPTraceExporter } from '@opentelemetry/exporter-trace-otlp-http'

// Initialize OpenTelemetry
const provider = new NodeTracerProvider()
const exporter = new OTLPTraceExporter({
  url: process.env.OTEL_EXPORTER_OTLP_ENDPOINT || 'http://localhost:4318/v1/traces'
})

provider.addSpanProcessor(new SimpleSpanProcessor(exporter))
provider.register()

const tracer = trace.getTracer('ai-system', '1.0.0')

// Example: Tracing a RAG query
export async function tracedRAGQuery(query: string): Promise<string> {
  return await tracer.startActiveSpan('rag_query', async (span) => {
    span.setAttribute('query', query)
    span.setAttribute('user_id', 'user-123')

    try {
      // Step 1: Embed query
      const embedding = await tracer.startActiveSpan('embed_query', async (embedSpan) => {
        const start = Date.now()
        const result = await embedQuery(query)

        embedSpan.setAttribute('tokens', 50)
        embedSpan.setAttribute('latency_ms', Date.now() - start)
        embedSpan.setAttribute('cost_usd', 0.0015)
        embedSpan.end()

        return result
      })

      // Step 2: Vector search
      const documents = await tracer.startActiveSpan('vector_search', async (searchSpan) => {
        const start = Date.now()
        const results = await vectorSearch(embedding, { limit: 10 })

        searchSpan.setAttribute('results_count', results.length)
        searchSpan.setAttribute('latency_ms', Date.now() - start)
        searchSpan.end()

        return results
      })

      // Step 3: Rerank
      const reranked = await tracer.startActiveSpan('rerank', async (rerankSpan) => {
        const start = Date.now()
        const topDocs = await rerank(query, documents, { topK: 3 })

        rerankSpan.setAttribute('input_count', documents.length)
        rerankSpan.setAttribute('output_count', topDocs.length)
        rerankSpan.setAttribute('latency_ms', Date.now() - start)
        rerankSpan.end()

        return topDocs
      })

      // Step 4: LLM call
      const response = await tracer.startActiveSpan('llm_call', async (llmSpan) => {
        const start = Date.now()
        const completion = await callLLM(query, reranked)

        llmSpan.setAttribute('model', 'claude-3-5-sonnet-20240620')
        llmSpan.setAttribute('input_tokens', completion.usage.input_tokens)
        llmSpan.setAttribute('output_tokens', completion.usage.output_tokens)
        llmSpan.setAttribute('latency_ms', Date.now() - start)
        llmSpan.setAttribute('cost_usd', calculateCost(completion.usage))
        llmSpan.end()

        return completion.content[0].text
      })

      span.setStatus({ code: 1 }) // SUCCESS
      span.end()

      return response

    } catch (error) {
      span.recordException(error)
      span.setStatus({ code: 2, message: error.message }) // ERROR
      span.end()
      throw error
    }
  })
}

// Helper functions
async function embedQuery(query: string): Promise<number[]> {
  // Mock embedding
  return new Array(1536).fill(0.1)
}

async function vectorSearch(embedding: number[], options: { limit: number }) {
  // Mock vector search
  return Array(options.limit).fill({ id: '1', content: 'Sample document', score: 0.9 })
}

async function rerank(query: string, docs: any[], options: { topK: number }) {
  // Mock reranking
  return docs.slice(0, options.topK)
}

async function callLLM(query: string, docs: any[]) {
  // Mock LLM call
  return {
    content: [{ text: 'Sample response' }],
    usage: { input_tokens: 1500, output_tokens: 500 }
  }
}

function calculateCost(usage: { input_tokens: number; output_tokens: number }): number {
  return (usage.input_tokens * 0.000003) + (usage.output_tokens * 0.000015)
}
```

### Visualizing Traces with LangSmith

```typescript
// src/week7/langsmith-tracing.ts
import { Client } from 'langsmith'
import { traceable } from 'langsmith/traceable'

const langsmith = new Client({ apiKey: process.env.LANGSMITH_API_KEY })

// Automatic tracing with LangSmith decorators
export const ragQuery = traceable(
  async (query: string): Promise<string> => {
    const embedding = await embedQuery(query)
    const documents = await vectorSearch(embedding)
    const reranked = await rerank(query, documents)
    const response = await callLLM(query, reranked)
    return response
  },
  { name: 'rag_query', project: 'production-rag' }
)

// LangSmith automatically captures:
// - All inputs and outputs
// - Nested function calls
// - Token usage and costs
// - Latency at each step
// - Errors and stack traces
```

### Trace Analysis: What to Look For

| Metric | Good | Warning | Critical |
|--------|------|---------|----------|
| **Total Latency** | &lt; 2s | 2-5s | &gt; 5s |
| **Retrieval** | &lt; 500ms | 500-1000ms | &gt; 1000ms |
| **LLM Call** | &lt; 2s | 2-3s | &gt; 3s |
| **Cost per Query** | &lt; $0.05 | $0.05-$0.10 | &gt; $0.10 |

**Production alert**: If retrieval latency &gt; 1s, investigate database indexes or vector store performance.

### The Semantic Telemetry Standard

Traditional APM metrics like "HTTP 200" and "response time" are **useless for AI**. A RAG system can return HTTP 200 in 800ms while delivering a completely hallucinated answer. Your traces must include **Domain-Specific Semantic Spans** that capture retrieval quality, not just retrieval speed.

**The Refinement**: Standardize on **OpenTelemetry Semantic Conventions for GenAI**.

```typescript
// ‚ùå Traditional APM span (insufficient for AI)
span.setAttribute('http.status_code', 200)
span.setAttribute('http.response_time_ms', 800)
// Tells you NOTHING about answer quality

// ‚úÖ Semantic Telemetry span (AI-aware)
span.setAttribute('gen_ai.retrieval.top_1_similarity', 0.92)
span.setAttribute('gen_ai.retrieval.similarity_delta', 0.14)  // Gap between #1 and #2
span.setAttribute('gen_ai.retrieval.confidence_score', 0.87)
span.setAttribute('gen_ai.retrieval.chunks_above_threshold', 3)
span.setAttribute('gen_ai.generation.faithfulness_estimate', 0.94)
// Tells you the AI is retrieving relevant context AND generating grounded answers
```

**Why Similarity Delta Matters**:

The **Top-1 Similarity Delta** (gap between the #1 and #2 ranked results) is one of the most powerful early-warning signals in RAG:

| Similarity Delta | Interpretation | Action |
|---|---|---|
| &gt; 0.15 | Strong signal ‚Äî top result is clearly the best match | Serve with high confidence |
| 0.05 - 0.15 | Ambiguous ‚Äî multiple documents are nearly equal | Flag for re-ranking or consider returning multiple answers |
| &lt; 0.05 | Weak signal ‚Äî no document is clearly relevant | Trigger fallback (ask for clarification, refuse to answer) |

```typescript
// Enriched vector search span with semantic telemetry
const documents = await tracer.startActiveSpan('vector_search', async (searchSpan) => {
  const start = Date.now()
  const results = await vectorSearch(embedding, { limit: 10 })

  // Standard APM attributes
  searchSpan.setAttribute('results_count', results.length)
  searchSpan.setAttribute('latency_ms', Date.now() - start)

  // Semantic Telemetry attributes (the AI-specific layer)
  if (results.length >= 2) {
    const top1Score = results[0].score
    const top2Score = results[1].score
    const similarityDelta = top1Score - top2Score

    searchSpan.setAttribute('gen_ai.retrieval.top_1_similarity', top1Score)
    searchSpan.setAttribute('gen_ai.retrieval.top_2_similarity', top2Score)
    searchSpan.setAttribute('gen_ai.retrieval.similarity_delta', similarityDelta)
    searchSpan.setAttribute('gen_ai.retrieval.confidence_score',
      top1Score > 0.85 && similarityDelta > 0.10 ? 'high' : 'low'
    )

    // Count how many chunks exceed the relevance threshold
    const threshold = 0.75
    const aboveThreshold = results.filter(r => r.score > threshold).length
    searchSpan.setAttribute('gen_ai.retrieval.chunks_above_threshold', aboveThreshold)
  }

  searchSpan.end()
  return results
})
```

**Architect's Tip**: *"If your similarity scores drop while your latency stays stable, you have a data-layer failure that traditional monitoring will never catch. Maybe your embedding model was updated, your vector index drifted, or new documents diluted the index. Semantic telemetry is the only way to detect this class of silent regression."*

### Detecting Silent Quality Collapse with Semantic Spans

```typescript
// Alert rule: Detect data-layer regression via semantic telemetry
async function detectSemanticRegression(windowHours: number = 24) {
  const recentSpans = await traceStore.querySpans({
    name: 'vector_search',
    timeRange: { hours: windowHours },
    attributes: ['gen_ai.retrieval.top_1_similarity', 'gen_ai.retrieval.similarity_delta']
  })

  const avgSimilarity = mean(recentSpans.map(s => s.attributes['gen_ai.retrieval.top_1_similarity']))
  const avgDelta = mean(recentSpans.map(s => s.attributes['gen_ai.retrieval.similarity_delta']))

  // Compare against 7-day baseline
  const baseline = await getSemanticBaseline(7)

  // Alert: Similarity dropped but latency is stable (silent regression)
  if (avgSimilarity < baseline.avgSimilarity * 0.92) {
    await alertOpsTeam({
      severity: 'critical',
      title: 'Silent Quality Collapse Detected',
      message: `Average retrieval similarity dropped from ${baseline.avgSimilarity.toFixed(3)} ` +
        `to ${avgSimilarity.toFixed(3)} (-${((1 - avgSimilarity / baseline.avgSimilarity) * 100).toFixed(1)}%) ` +
        `while latency remains stable. Investigate: embedding model change, index drift, or data ingestion issue.`,
      dashboard: 'semantic-telemetry'
    })
  }

  // Alert: Similarity delta collapsed (ambiguous retrievals increasing)
  if (avgDelta < baseline.avgDelta * 0.70) {
    await alertOpsTeam({
      severity: 'warning',
      title: 'Retrieval Ambiguity Increasing',
      message: `Average similarity delta dropped from ${baseline.avgDelta.toFixed(3)} ` +
        `to ${avgDelta.toFixed(3)}. The system is less confident in top-1 selections. ` +
        `Consider re-ranking pipeline or index optimization.`,
      dashboard: 'semantic-telemetry'
    })
  }
}
```

---

## Pillar 2: Evaluations (The Quality)

**What it tracks**: Semantic accuracy and safety of AI outputs.

### The Silent Failure Problem

```typescript
// ‚ùå System is "working" but broken
Query: "What's our refund policy for enterprise customers?"

Response: "Our refund policy allows 30-day returns with receipt."
// ‚úÖ HTTP 200, no errors
// ‚ùå WRONG - enterprise customers have 90-day returns!
```

**Why traditional testing fails**: Unit tests check code logic, not semantic correctness.

### The Five Critical Evals

```typescript
export interface EvaluationMetrics {
  faithfulness: number        // 0-1: Does answer match retrieved context?
  answer_relevance: number    // 0-1: Does answer address the question?
  context_precision: number   // 0-1: Are retrieved docs relevant?
  toxicity: number           // 0-1: Contains harmful content?
  pii_detected: boolean      // True if PII found in output
}
```

### Implementing Evaluations

```typescript
// src/week7/evaluations.ts
import Anthropic from '@anthropic-ai/sdk'

const anthropic = new Anthropic({ apiKey: process.env.ANTHROPIC_API_KEY })

// Eval 1: Faithfulness (Answer is grounded in context)
export async function evaluateFaithfulness(
  question: string,
  context: string,
  answer: string
): Promise<number> {
  const evalPrompt = `You are an evaluator. Check if the ANSWER is faithful to the CONTEXT.

QUESTION: ${question}

CONTEXT:
${context}

ANSWER:
${answer}

Output JSON with score 0.0 (completely unfaithful) to 1.0 (perfectly faithful):
{
  "score": 0.0-1.0,
  "reasoning": "Why this score?"
}`

  const response = await anthropic.messages.create({
    model: 'claude-3-5-sonnet-20240620',
    max_tokens: 512,
    messages: [{ role: 'user', content: evalPrompt }]
  })

  const text = response.content[0].text
  const result = JSON.parse(text.match(/\{[\s\S]*\}/)?.[0] || '{"score": 0}')

  return result.score
}

// Eval 2: Answer Relevance (Answer addresses the question)
export async function evaluateAnswerRelevance(
  question: string,
  answer: string
): Promise<number> {
  const evalPrompt = `Does this ANSWER properly address the QUESTION?

QUESTION: ${question}
ANSWER: ${answer}

Score 0.0 (completely irrelevant) to 1.0 (perfectly relevant):
{ "score": 0.0-1.0 }`

  const response = await anthropic.messages.create({
    model: 'claude-3-5-sonnet-20240620',
    max_tokens: 256,
    messages: [{ role: 'user', content: evalPrompt }]
  })

  const result = JSON.parse(response.content[0].text.match(/\{[\s\S]*\}/)?.[0] || '{"score": 0}')
  return result.score
}

// Eval 3: PII Detection (Personal Identifiable Information)
export async function detectPII(text: string): Promise<boolean> {
  const patterns = [
    /\b\d{3}-\d{2}-\d{4}\b/,           // SSN: 123-45-6789
    /\b\d{16}\b/,                       // Credit card: 1234567812345678
    /\b[A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]{2,}\b/i, // Email
    /\b\d{3}-\d{3}-\d{4}\b/,           // Phone: 123-456-7890
  ]

  return patterns.some(pattern => pattern.test(text))
}

// Eval 4: Toxicity Detection
export async function evaluateToxicity(text: string): Promise<number> {
  // Using Perspective API or similar
  // For demo, simple keyword check
  const toxicKeywords = ['hate', 'kill', 'stupid', 'idiot']
  const matches = toxicKeywords.filter(word => text.toLowerCase().includes(word))

  return matches.length &gt; 0 ? 0.8 : 0.0
}

// Eval 5: Context Precision (Retrieved docs are relevant)
export async function evaluateContextPrecision(
  question: string,
  documents: string[]
): Promise<number> {
  const evalPrompt = `Rate how relevant these DOCUMENTS are to the QUESTION.

QUESTION: ${question}

DOCUMENTS:
${documents.map((d, i) => `${i + 1}. ${d}`).join('\n\n')}

Score each document 0 (irrelevant) or 1 (relevant), then average:
{ "scores": [0, 1, 1, 0], "precision": 0.5 }`

  const response = await anthropic.messages.create({
    model: 'claude-3-5-sonnet-20240620',
    max_tokens: 512,
    messages: [{ role: 'user', content: evalPrompt }]
  })

  const result = JSON.parse(response.content[0].text.match(/\{[\s\S]*\}/)?.[0] || '{"precision": 0}')
  return result.precision
}
```

### Running Evaluations in Production

```typescript
// src/week7/production-eval.ts
import { traceable } from 'langsmith/traceable'

export const evaluateRAGResponse = traceable(
  async (
    question: string,
    context: string,
    answer: string
  ): Promise<EvaluationMetrics> => {
    const [faithfulness, relevance, toxicity, pii] = await Promise.all([
      evaluateFaithfulness(question, context, answer),
      evaluateAnswerRelevance(question, answer),
      evaluateToxicity(answer),
      detectPII(answer)
    ])

    return {
      faithfulness,
      answer_relevance: relevance,
      context_precision: 1.0, // Would calculate from retrieved docs
      toxicity,
      pii_detected: pii
    }
  },
  { name: 'evaluate_rag_response' }
)

// Example usage
const metrics = await evaluateRAGResponse(
  'What is our enterprise refund policy?',
  'Enterprise customers have 90-day money-back guarantee...',
  'Enterprise customers can request refunds within 90 days.'
)

console.log('Evaluation Results:', metrics)
// {
//   faithfulness: 1.0,
//   answer_relevance: 1.0,
//   context_precision: 1.0,
//   toxicity: 0.0,
//   pii_detected: false
// }
```

### The Shadow Evaluation Proxy (Evaluation-as-a-Service)

Running LLM-as-a-Judge on **every request** in the hot path can double your latency and cost. In production, you need **Probabilistic Evaluation Sampling** ‚Äî a pattern where 100% of requests are served immediately, but a configurable percentage are **asynchronously** evaluated by a "Teacher Model" in the background.

**Architect's Tip**: *"Don't evaluate everything in the hot path. Build a Shadow Evaluation Proxy. 100% of your requests go to the user, but 10% are sampled and sent to a background worker where a 'Teacher Model' (like GPT-4o) evaluates the 'Student Model' (like Haiku) for Faithfulness and Bias. This gives you statistically significant quality metrics without impacting the user's P99 latency."*

```typescript
// The Shadow Evaluation Proxy pattern
interface ShadowEvalConfig {
  sampleRate: number              // 0.0 - 1.0 (e.g., 0.10 = 10%)
  teacherModel: string            // The stronger model that judges quality
  studentModel: string            // The production model being monitored
  evalQueue: string               // Background job queue name
  metricsToEvaluate: string[]     // Which RAGAS metrics to run
}

const PRODUCTION_CONFIG: ShadowEvalConfig = {
  sampleRate: 0.10,               // Evaluate 10% of requests
  teacherModel: 'gpt-4o',         // Teacher judges the student
  studentModel: 'claude-haiku-4', // Student serves production traffic
  evalQueue: 'shadow-eval-queue',
  metricsToEvaluate: ['faithfulness', 'answer_relevance', 'toxicity']
}

class ShadowEvaluationProxy {
  private config: ShadowEvalConfig

  constructor(config: ShadowEvalConfig) {
    this.config = config
  }

  // Called on EVERY request ‚Äî decides whether to sample
  async afterResponse(request: {
    query: string
    context: string[]
    response: string
    traceId: string
    latencyMs: number
  }): Promise<void> {
    // Deterministic sampling based on trace ID (reproducible)
    const shouldSample = this.shouldSample(request.traceId)

    if (!shouldSample) return  // 90% of requests: do nothing

    // 10% of requests: enqueue for background evaluation
    await this.enqueueForEvaluation({
      query: request.query,
      context: request.context,
      response: request.response,
      traceId: request.traceId,
      studentModel: this.config.studentModel,
      timestamp: new Date()
    })
  }

  private shouldSample(traceId: string): boolean {
    // Hash-based sampling for determinism (same trace always sampled or not)
    const hash = simpleHash(traceId)
    return (hash % 100) < (this.config.sampleRate * 100)
  }

  private async enqueueForEvaluation(payload: EvalPayload): Promise<void> {
    // Send to background worker queue (Redis, SQS, etc.)
    await jobQueue.add(this.config.evalQueue, payload, {
      priority: 'low',         // Don't compete with production traffic
      attempts: 3,
      backoff: { type: 'exponential', delay: 5000 }
    })
  }
}
```

**Background Worker ‚Äî The Teacher Model Evaluates**:

```typescript
// Background worker: processes the shadow evaluation queue
async function processShadowEvaluation(payload: EvalPayload): Promise<void> {
  // Teacher Model evaluates the Student Model's response
  const [faithfulness, relevance, toxicity] = await Promise.all([
    evaluateWithTeacher(payload, 'faithfulness'),
    evaluateWithTeacher(payload, 'answer_relevance'),
    evaluateWithTeacher(payload, 'toxicity')
  ])

  // Store evaluation results linked to the original trace
  await prisma.shadowEvaluation.create({
    data: {
      traceId: payload.traceId,
      studentModel: payload.studentModel,
      teacherModel: 'gpt-4o',
      faithfulness,
      answerRelevance: relevance,
      toxicity,
      evaluatedAt: new Date()
    }
  })

  // Alert if quality drops below threshold
  if (faithfulness < 0.70) {
    await alertOpsTeam({
      severity: 'warning',
      title: 'Low Faithfulness Detected (Shadow Eval)',
      message: `Trace ${payload.traceId}: faithfulness=${faithfulness.toFixed(2)}. ` +
        `Query: "${payload.query.substring(0, 100)}..."`,
      dashboard: 'shadow-evaluations'
    })
  }
}
```

**Why Shadow Evaluation Works**:

| Approach | Latency Impact | Cost | Quality Signal |
|---|---|---|---|
| Inline eval (every request) | +2,000ms (doubles P99) | 2x production cost | 100% coverage |
| **Shadow eval (10% sample)** | **+0ms (async)** | **+10% cost** | **Statistically significant** |
| No eval | +0ms | +0% | Blind ‚Äî no quality signal |

With 10,000 daily requests and a 10% sample rate, you evaluate 1,000 requests per day ‚Äî more than enough for statistically significant quality monitoring with a ¬±3% confidence interval.

---

## Pillar 3: Unit Economics (The Business)

**What it tracks**: Cost and efficiency of your AI system.

### The Critical Metrics

| Metric | Formula | Target | Red Flag |
|--------|---------|--------|----------|
| **Tokens per User** | Avg tokens across all user sessions | &lt; 50K/month | &gt; 100K/month |
| **Cost per Query** | Total cost / Total queries | &lt; $0.05 | &gt; $0.10 |
| **Cache Hit Rate** | Cached responses / Total queries | &gt; 40% | &lt; 20% |
| **Queries per Dollar** | Total queries / Total cost | &gt; 20 | &lt; 10 |

### Tracking Unit Economics

```typescript
// src/week7/economics.ts
import { prisma } from '@/lib/db'

export interface QueryMetrics {
  query_id: string
  user_id: string
  tokens_used: number
  cost_usd: number
  cache_hit: boolean
  latency_ms: number
  timestamp: Date
}

export async function trackQueryMetrics(metrics: QueryMetrics) {
  await prisma.queryMetrics.create({
    data: metrics
  })
}

// Dashboard query: Cost per user
export async function getCostPerUser(userId: string, startDate: Date, endDate: Date) {
  const result = await prisma.queryMetrics.aggregate({
    where: {
      user_id: userId,
      timestamp: {
        gte: startDate,
        lte: endDate
      }
    },
    _sum: {
      cost_usd: true,
      tokens_used: true
    },
    _count: true
  })

  return {
    total_cost: result._sum.cost_usd || 0,
    total_tokens: result._sum.tokens_used || 0,
    query_count: result._count,
    cost_per_query: (result._sum.cost_usd || 0) / (result._count || 1)
  }
}

// Dashboard query: Cache hit rate
export async function getCacheHitRate(startDate: Date, endDate: Date) {
  const total = await prisma.queryMetrics.count({
    where: {
      timestamp: { gte: startDate, lte: endDate }
    }
  })

  const cacheHits = await prisma.queryMetrics.count({
    where: {
      timestamp: { gte: startDate, lte: endDate },
      cache_hit: true
    }
  })

  return {
    total_queries: total,
    cache_hits: cacheHits,
    hit_rate: cacheHits / total,
    cost_saved: cacheHits * 0.045 // Avg cost per query
  }
}
```

### Setting Up Alerts

```typescript
// src/week7/alerts.ts
export async function checkCostAlerts() {
  const today = new Date()
  const startOfMonth = new Date(today.getFullYear(), today.getMonth(), 1)

  const monthlyCost = await prisma.queryMetrics.aggregate({
    where: {
      timestamp: { gte: startOfMonth }
    },
    _sum: { cost_usd: true }
  })

  const totalCost = monthlyCost._sum.cost_usd || 0

  // Alert if monthly cost exceeds budget
  if (totalCost &gt; 500) {
    await sendAlert({
      severity: 'critical',
      message: `Monthly AI cost exceeded $500: $${totalCost.toFixed(2)}`,
      action: 'Review usage patterns and optimize'
    })
  }

  // Alert if cost per query is high
  const avgCostPerQuery = totalCost / (await prisma.queryMetrics.count())
  if (avgCostPerQuery &gt; 0.10) {
    await sendAlert({
      severity: 'warning',
      message: `Cost per query is high: $${avgCostPerQuery.toFixed(4)}`,
      action: 'Consider caching or cheaper models'
    })
  }
}

async function sendAlert(alert: { severity: string; message: string; action: string }) {
  console.log(`üö® ${alert.severity.toUpperCase()}: ${alert.message}`)
  console.log(`   Action: ${alert.action}`)

  // In production: Send to Slack, PagerDuty, etc.
}
```

---

## Production Dashboard Example

```typescript
// src/week7/dashboard.ts
export async function getProductionDashboard() {
  const last24Hours = new Date(Date.now() - 24 * 60 * 60 * 1000)

  const [traces, evals, economics] = await Promise.all([
    // Pillar 1: Traces
    prisma.queryMetrics.aggregate({
      where: { timestamp: { gte: last24Hours } },
      _avg: { latency_ms: true },
      _max: { latency_ms: true }
    }),

    // Pillar 2: Evaluations
    prisma.evaluationMetrics.aggregate({
      where: { timestamp: { gte: last24Hours } },
      _avg: {
        faithfulness: true,
        answer_relevance: true,
        toxicity: true
      }
    }),

    // Pillar 3: Unit Economics
    getCacheHitRate(last24Hours, new Date())
  ])

  return {
    traces: {
      avg_latency: traces._avg.latency_ms,
      p99_latency: traces._max.latency_ms
    },
    evaluations: {
      avg_faithfulness: evals._avg.faithfulness,
      avg_relevance: evals._avg.answer_relevance,
      avg_toxicity: evals._avg.toxicity
    },
    economics: {
      cache_hit_rate: economics.hit_rate,
      cost_saved: economics.cost_saved
    }
  }
}
```

---

## 7. Distributed Tracing: Multi-Agent Systems

### The Challenge

Multi-agent systems have complex execution flows:
- Agent A calls Agent B calls Agent C
- Parallel tool execution
- Cross-service communication
- Nested RAG queries

**Without distributed tracing:** You only see the total latency (8 seconds) but don't know which agent is slow.

**With distributed tracing:** You see the full execution tree and identify bottlenecks.

### Trace Context Propagation

```typescript
/**
 * Distributed Tracing Across Multiple Agents
 * Propagate trace context through agent calls
 */
import { trace, context, propagation, SpanStatusCode } from '@opentelemetry/api'

interface TraceContext {
  traceId: string
  spanId: string
  traceFlags: number
}

// Parent agent: Research Coordinator
async function researchCoordinator(query: string): Promise<string> {
  return await tracer.startActiveSpan('research_coordinator', async (parentSpan) => {
    parentSpan.setAttribute('query', query)
    parentSpan.setAttribute('agent_type', 'coordinator')

    // Step 1: Decompose query into sub-tasks
    const subTasks = await tracer.startActiveSpan('decompose_query', async (span) => {
      const tasks = await decomposeQuery(query)
      span.setAttribute('task_count', tasks.length)
      span.end()
      return tasks
    })

    // Step 2: Execute sub-tasks in parallel with trace context
    const results = await Promise.all(
      subTasks.map(async (task, index) => {
        // Each sub-task gets its own child span
        return await tracer.startActiveSpan(`sub_task_${index}`, async (taskSpan) => {
          taskSpan.setAttribute('task_type', task.type)
          taskSpan.setAttribute('task_query', task.query)

          // Route to specialized agent based on task type
          let result: string
          if (task.type === 'technical') {
            result = await technicalAgent(task.query, { parentSpan: taskSpan })
          } else if (task.type === 'medical') {
            result = await medicalAgent(task.query, { parentSpan: taskSpan })
          } else {
            result = await generalAgent(task.query, { parentSpan: taskSpan })
          }

          taskSpan.setAttribute('result_length', result.length)
          taskSpan.end()
          return result
        })
      })
    )

    // Step 3: Synthesize results
    const synthesis = await tracer.startActiveSpan('synthesize', async (synthSpan) => {
      const final = await synthesizeResults(query, results)
      synthSpan.setAttribute('synthesis_length', final.length)
      synthSpan.end()
      return final
    })

    parentSpan.setStatus({ code: SpanStatusCode.OK })
    parentSpan.end()

    return synthesis
  })
}

// Child agent: Technical Agent
async function technicalAgent(
  query: string,
  options: { parentSpan?: any }
): Promise<string> {
  return await tracer.startActiveSpan(
    'technical_agent',
    { links: options.parentSpan ? [{ context: options.parentSpan.spanContext() }] : [] },
    async (agentSpan) => {
      agentSpan.setAttribute('agent_type', 'technical')
      agentSpan.setAttribute('query', query)

      // This agent's RAG flow
      const ragResult = await tracer.startActiveSpan('technical_rag', async (ragSpan) => {
        // Embed ‚Üí Search ‚Üí Rerank ‚Üí LLM
        const embedding = await embed(query)
        ragSpan.setAttribute('embedding_done', true)

        const docs = await vectorDb.search(embedding, { limit: 10 })
        ragSpan.setAttribute('docs_retrieved', docs.length)

        const reranked = await rerank(query, docs, { topK: 3 })
        ragSpan.setAttribute('docs_reranked', reranked.length)

        const llmResponse = await callLLM(query, reranked, { model: 'claude-3-5-sonnet-20241022' })
        ragSpan.setAttribute('llm_tokens', llmResponse.usage.total_tokens)

        ragSpan.end()
        return llmResponse.content[0].text
      })

      agentSpan.setStatus({ code: SpanStatusCode.OK })
      agentSpan.end()

      return ragResult
    }
  )
}
```

### Visualizing Distributed Traces

**Trace output in Jaeger/Zipkin:**

```
research_coordinator (8.2s)
‚îú‚îÄ decompose_query (0.5s)
‚îú‚îÄ sub_task_0 (2.8s)
‚îÇ  ‚îî‚îÄ technical_agent (2.7s)
‚îÇ     ‚îî‚îÄ technical_rag (2.5s)
‚îÇ        ‚îú‚îÄ embed_query (0.2s)
‚îÇ        ‚îú‚îÄ vector_search (0.8s)
‚îÇ        ‚îú‚îÄ rerank (0.5s)
‚îÇ        ‚îî‚îÄ llm_call (1.0s)
‚îú‚îÄ sub_task_1 (3.5s) ‚Üê BOTTLENECK!
‚îÇ  ‚îî‚îÄ medical_agent (3.4s)
‚îÇ     ‚îî‚îÄ medical_rag (3.2s)
‚îÇ        ‚îú‚îÄ embed_query (0.2s)
‚îÇ        ‚îú‚îÄ vector_search (2.5s) ‚Üê SLOW!
‚îÇ        ‚îú‚îÄ rerank (0.3s)
‚îÇ        ‚îî‚îÄ llm_call (0.2s)
‚îî‚îÄ synthesize (1.4s)
```

**Insight:** Medical agent's vector search is 3x slower (2.5s vs 0.8s). Investigate index size or query optimization.

### Production Metrics from Distributed Traces

```typescript
/**
 * Aggregate metrics across distributed traces
 */
interface DistributedTraceMetrics {
  total_latency_ms: number
  agent_latencies: Record<string, number>
  bottleneck_agent: string
  parallel_efficiency: number // 0-1, how well parallel tasks are balanced
}

async function analyzeDistributedTrace(traceId: string): Promise<DistributedTraceMetrics> {
  // Fetch all spans for this trace
  const spans = await traceStore.getSpansByTraceId(traceId)

  // Find root span (research_coordinator)
  const rootSpan = spans.find(s => !s.parentSpanId)
  const totalLatency = rootSpan.endTime - rootSpan.startTime

  // Calculate per-agent latencies
  const agentSpans = spans.filter(s => s.name.includes('_agent'))
  const agentLatencies: Record<string, number> = {}

  for (const agentSpan of agentSpans) {
    const agentType = agentSpan.attributes.agent_type
    const latency = agentSpan.endTime - agentSpan.startTime
    agentLatencies[agentType] = latency
  }

  // Find bottleneck
  const bottleneck = Object.entries(agentLatencies)
    .sort((a, b) => b[1] - a[1])[0]

  // Calculate parallel efficiency
  const parallelTasks = spans.filter(s => s.name.startsWith('sub_task_'))
  const maxParallelLatency = Math.max(...parallelTasks.map(s => s.endTime - s.startTime))
  const sumParallelLatency = parallelTasks.reduce((sum, s) => sum + (s.endTime - s.startTime), 0)
  const parallelEfficiency = maxParallelLatency / sumParallelLatency // Ideal: 1.0 (perfect load balancing)

  return {
    total_latency_ms: totalLatency,
    agent_latencies: agentLatencies,
    bottleneck_agent: bottleneck[0],
    parallel_efficiency: parallelEfficiency
  }
}
```

### Alert on Distributed Trace Anomalies

```typescript
/**
 * Alert when distributed traces show anomalies
 */
async function monitorDistributedTraces() {
  const recentTraces = await traceStore.getRecentTraces({ limit: 100 })

  for (const trace of recentTraces) {
    const metrics = await analyzeDistributedTrace(trace.traceId)

    // Alert 1: Total latency &gt; 10s
    if (metrics.total_latency_ms &gt; 10000) {
      await alertSlack({
        channel: '#ai-alerts',
        text: `‚ö†Ô∏è Slow distributed trace: ${metrics.total_latency_ms}ms
Bottleneck: ${metrics.bottleneck_agent} (${metrics.agent_latencies[metrics.bottleneck_agent]}ms)
Trace ID: ${trace.traceId}`
      })
    }

    // Alert 2: Poor parallel efficiency (&lt;0.5)
    if (metrics.parallel_efficiency &lt; 0.5) {
      await alertSlack({
        channel: '#ai-alerts',
        text: `‚ö†Ô∏è Poor parallel efficiency: ${(metrics.parallel_efficiency * 100).toFixed(1)}%
Load imbalance detected in parallel agent execution
Trace ID: ${trace.traceId}`
      })
    }

    // Alert 3: Agent consistently slow
    const agentAvgLatencies = await calculateAgentAverages()
    for (const [agent, latency] of Object.entries(metrics.agent_latencies)) {
      if (latency &gt; agentAvgLatencies[agent] * 2) {
        await alertSlack({
          channel: '#ai-alerts',
          text: `‚ö†Ô∏è Agent ${agent} is 2x slower than average: ${latency}ms vs ${agentAvgLatencies[agent]}ms
Trace ID: ${trace.traceId}`
        })
      }
    }
  }
}
```

---

## 8. Cost Attribution by User/Tenant

### The Multi-Tenant Problem

**Scenario:** Healthcare SaaS with 50 enterprise customers.

**Questions:**
- Which customer is driving the most cost?
- Is Customer A's $500/month cost justified by their plan?
- Should we throttle Customer B's usage?

**Solution:** Attribute costs to user/tenant in traces.

```typescript
/**
 * Cost Attribution in Traces
 * Track which user/tenant generated each trace
 */
async function tracedQueryWithCostAttribution(
  query: string,
  userId: string,
  tenantId: string
): Promise<string> {
  return await tracer.startActiveSpan('query', async (span) => {
    // Step 1: Add user/tenant to trace metadata
    span.setAttribute('user_id', userId)
    span.setAttribute('tenant_id', tenantId)
    span.setAttribute('timestamp', Date.now())

    // Step 2: Execute RAG query
    const embedding = await embed(query)
    const docs = await vectorDb.search(embedding, { limit: 10 })
    const response = await callLLM(query, docs)

    // Step 3: Calculate cost
    const cost = calculateCost(response.usage)

    // Step 4: Attribute cost to user/tenant
    span.setAttribute('cost_usd', cost)
    span.setAttribute('input_tokens', response.usage.input_tokens)
    span.setAttribute('output_tokens', response.usage.output_tokens)

    // Step 5: Record cost attribution in database
    await prisma.costAttribution.create({
      data: {
        user_id: userId,
        tenant_id: tenantId,
        trace_id: span.spanContext().traceId,
        cost_usd: cost,
        tokens: response.usage.total_tokens,
        timestamp: new Date()
      }
    })

    span.end()
    return response.content[0].text
  })
}

// Query cost by tenant
async function getTenantCosts(tenantId: string, startDate: Date, endDate: Date) {
  const costs = await prisma.costAttribution.aggregate({
    where: {
      tenant_id: tenantId,
      timestamp: {
        gte: startDate,
        lte: endDate
      }
    },
    _sum: { cost_usd: true, tokens: true },
    _count: { _all: true }
  })

  return {
    total_cost: costs._sum.cost_usd,
    total_tokens: costs._sum.tokens,
    total_queries: costs._count._all,
    avg_cost_per_query: costs._sum.cost_usd / costs._count._all
  }
}

// Alert on tenant overspend
async function alertOnTenantOverspend() {
  const tenants = await prisma.tenant.findMany({
    include: { plan: true }
  })

  for (const tenant of tenants) {
    const monthlyCosts = await getTenantCosts(
      tenant.id,
      new Date(new Date().getFullYear(), new Date().getMonth(), 1),
      new Date()
    )

    // Alert if tenant exceeds plan budget
    const budgetLimit = tenant.plan.monthly_budget_usd

    if (monthlyCosts.total_cost &gt; budgetLimit * 1.2) { // 120% of budget
      await alertSlack({
        channel: '#billing-alerts',
        text: `‚ö†Ô∏è Tenant ${tenant.name} is over budget:
Cost this month: $${monthlyCosts.total_cost.toFixed(2)}
Budget limit: $${budgetLimit.toFixed(2)}
Overage: $${(monthlyCosts.total_cost - budgetLimit).toFixed(2)} (${((monthlyCosts.total_cost / budgetLimit - 1) * 100).toFixed(1)}%)

Consider:
- Throttling requests
- Upgrading tenant plan
- Reaching out for contract renewal`
      })
    }
  }
}
```

### Tenant-Aware Header Propagation: The "Toxic Tenant" Detector

An Architect doesn't just look at a total bill. You must link **Token Attribution to SaaS Tiers** to identify **Toxic Tenants** ‚Äî users whose complex queries cost more in tokens than they pay in their monthly subscription.

**Architect's Tip**: *"Every trace must carry a `tenant_id` and `plan_level` header. This allows you to calculate per-tenant gross margin in real time. If a tenant on the $99/month plan is generating $150/month in AI costs, that's a negative-margin customer. This data is the only way to drive an 'AI-First' pricing strategy."*

```typescript
/**
 * Tenant-Aware Header Propagation
 * Inject business context into every trace for unit economics
 */
interface TenantContext {
  tenantId: string
  planLevel: 'starter' | 'professional' | 'enterprise'
  monthlyBudget: number        // What they pay
  monthlyTokenCeiling: number  // What they're allowed to consume
}

// Middleware: Inject tenant context into every request's trace
async function tenantPropagationMiddleware(req: Request, next: Function) {
  const tenant = await getTenantFromAuth(req)

  return await tracer.startActiveSpan('request', async (span) => {
    // Propagate business context into the trace
    span.setAttribute('tenant.id', tenant.tenantId)
    span.setAttribute('tenant.plan_level', tenant.planLevel)
    span.setAttribute('tenant.monthly_budget_usd', tenant.monthlyBudget)

    const response = await next(req)

    // After response: calculate this request's cost and compare to budget
    const requestCost = parseFloat(span.attributes['cost_usd'] || '0')
    const monthToDateCost = await getMonthToDateCost(tenant.tenantId)

    span.setAttribute('tenant.mtd_cost_usd', monthToDateCost)
    span.setAttribute('tenant.budget_utilization',
      monthToDateCost / tenant.monthlyBudget
    )

    // Flag toxic tenants in the trace itself
    if (monthToDateCost > tenant.monthlyBudget) {
      span.setAttribute('tenant.is_over_budget', true)
      span.setAttribute('tenant.overage_usd', monthToDateCost - tenant.monthlyBudget)
    }

    span.end()
    return response
  })
}

// Detect toxic tenants: cost exceeds revenue
async function detectToxicTenants(): Promise<ToxicTenantReport[]> {
  const tenants = await prisma.tenant.findMany({ include: { plan: true } })
  const startOfMonth = new Date(new Date().getFullYear(), new Date().getMonth(), 1)

  const toxicTenants: ToxicTenantReport[] = []

  for (const tenant of tenants) {
    const costs = await getTenantCosts(tenant.id, startOfMonth, new Date())
    const monthlyRevenue = tenant.plan.monthly_price_usd
    const grossMargin = monthlyRevenue - costs.total_cost

    if (grossMargin < 0) {
      toxicTenants.push({
        tenantId: tenant.id,
        tenantName: tenant.name,
        planLevel: tenant.plan.name,
        monthlyRevenue,
        monthlyAICost: costs.total_cost,
        grossMargin,
        marginPercent: (grossMargin / monthlyRevenue) * 100,
        avgCostPerQuery: costs.total_cost / costs.total_queries,
        totalQueries: costs.total_queries,
        recommendation: grossMargin < -100
          ? 'URGENT: Upgrade plan or implement query throttling'
          : 'MONITOR: Approaching negative margin territory'
      })
    }
  }

  return toxicTenants.sort((a, b) => a.grossMargin - b.grossMargin)
}
```

**Toxic Tenant Dashboard Output**:

| Tenant | Plan ($) | AI Cost ($) | Gross Margin | Queries | Recommendation |
|---|---|---|---|---|---|
| Acme Corp | $99/mo | $247/mo | **-$148** | 12,400 | URGENT: Upgrade or throttle |
| Beta Inc | $299/mo | $312/mo | **-$13** | 8,200 | MONITOR: Near breakeven |
| Gamma LLC | $299/mo | $89/mo | +$210 | 3,100 | Healthy |
| Delta Co | $99/mo | $41/mo | +$58 | 1,800 | Healthy |

This data drives **AI-First pricing decisions**: should you add a query cap to the Starter plan? Should enterprise plans include a token budget? Should you charge overage fees? Without tenant-aware header propagation, these questions are unanswerable.

### Cost Dashboard by Tenant

```typescript
/**
 * Cost attribution dashboard for SaaS products
 */
async function getCostDashboard(startDate: Date, endDate: Date) {
  // Top 10 most expensive tenants
  const topTenants = await prisma.costAttribution.groupBy({
    by: ['tenant_id'],
    where: {
      timestamp: { gte: startDate, lte: endDate }
    },
    _sum: { cost_usd: true, tokens: true },
    _count: { _all: true },
    orderBy: { _sum: { cost_usd: 'desc' } },
    take: 10
  })

  // Cost breakdown by model
  const costsByModel = await prisma.trace.groupBy({
    by: ['model'],
    where: {
      timestamp: { gte: startDate, lte: endDate }
    },
    _sum: { cost_usd: true },
    orderBy: { _sum: { cost_usd: 'desc' } }
  })

  return {
    top_tenants: topTenants.map(t => ({
      tenant_id: t.tenant_id,
      total_cost: t._sum.cost_usd,
      total_tokens: t._sum.tokens,
      query_count: t._count._all
    })),
    costs_by_model: costsByModel.map(m => ({
      model: m.model,
      total_cost: m._sum.cost_usd
    }))
  }
}
```

---

## Architect Challenge: The "Silent Failure" CTO Escalation

**Scenario**: Your system dashboard shows **100% Uptime** and **150ms P50 Latency**. The CEO is happy. But customer churn is increasing by 12% month-over-month, and users are complaining that the AI is *"getting dumber."* The support team escalates to you.

**Question**: Which pillar of observability failed you, and how do you fix it?

**A)** Traces ‚Äî You need to check if the database is slow. Latency spikes are probably causing bad user experience.

**B)** Unit Economics ‚Äî You need to check if you're spending enough on tokens. Maybe the model was downgraded to save costs and it's producing worse answers.

**C)** Evaluations ‚Äî You were monitoring "System Health" but not "Semantic Health." You need to implement LLM-as-a-Judge to track Answer Relevance scores over time to detect "Model Drift" or "Context Degradation" that doesn't trigger standard errors.

**D)** Logs ‚Äî You need to read more user chat histories to manually identify bad responses.

<details>
<summary>Correct Answer</summary>

**C ‚Äî Evaluations (Semantic Health Monitoring)**

In AI systems, **"Up" does not mean "Correct."** Your traces confirmed the system is fast (150ms). Your infrastructure is healthy (100% uptime). But **none of that measures whether the answers are actually right**.

This is the **Silent Quality Collapse** ‚Äî the most dangerous failure mode in AI:

- The embedding model was silently updated by your provider, shifting vector representations
- New documents ingested into the index diluted retrieval precision
- A prompt template was changed, subtly degrading faithfulness
- The LLM's behavior drifted after a provider-side model update

**None of these trigger errors.** None of them affect latency. None of them appear in traditional APM dashboards. The only way to detect them is through **continuous semantic evaluation** ‚Äî monitoring Faithfulness, Answer Relevance, and Context Precision over time using the Shadow Evaluation Proxy pattern.

**Why the other answers are wrong**:
- **A (Traces)**: Latency is 150ms ‚Äî traces would confirm the system is fast. Speed is not the problem.
- **B (Unit Economics)**: Cost optimization is important but doesn't explain *why* answers are degrading. You could spend more tokens and still hallucinate.
- **D (Logs)**: Manual log review doesn't scale and is reactive. By the time you read chat histories, you've already lost the customers.

</details>

---

## Key Takeaways

1. **Three Pillars**: Traces (flow), Evaluations (quality), Unit Economics (cost)
2. **Silent Failures**: AI systems fail without errors ‚Äî "Up" does not mean "Correct"
3. **Semantic Telemetry**: Enrich OpenTelemetry spans with domain-specific attributes (similarity scores, confidence deltas) to detect data-layer regressions that traditional APM misses
4. **Shadow Evaluation Proxy**: Sample 10% of production traffic for async Teacher Model evaluation ‚Äî statistically significant quality monitoring without impacting P99 latency
5. **LLM-as-a-Judge**: Use a stronger model to evaluate the production model's faithfulness and relevance
6. **Toxic Tenant Detection**: Propagate `tenant_id` and `plan_level` headers in every trace to identify negative-margin customers and drive AI-first pricing
7. **Distributed Tracing**: Trace context propagation across multi-agent systems reveals bottlenecks invisible to single-service monitoring
8. **Cost Attribution**: Track costs by user/tenant for SaaS billing, throttling, and gross margin analysis

---

## Next Steps

- **Week 7 Concept 2**: Implement guardrails for input/output validation
- **Week 7 Concept 3**: Build LLM-as-a-Judge automated evaluation pipelines
- **Week 7 Lab**: Production Dashboard with OpenTelemetry, Automated Evals, Cost Alerts

---

## Further Reading

- [OpenTelemetry for LLMs](https://opentelemetry.io/)
- [LangSmith Tracing Guide](https://docs.smith.langchain.com/)
- [RAGAS Evaluation Framework](https://docs.ragas.io/)
- [Arize Phoenix](https://phoenix.arize.com/)
