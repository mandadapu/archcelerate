---
title: "The Three Pillars of AI Observability"
week: 7
concept: 1
description: "Monitor traces, evaluations, and unit economics to prevent silent failures in production AI systems"
estimatedMinutes: 50
objectives:
  - Implement OpenTelemetry tracing for RAG and agent flows
  - Track the three critical observability layers
  - Set up dashboards for latency, accuracy, and cost
  - Detect and prevent silent failures
---

# The Three Pillars of AI Observability

Hardening AI systems for the real world.

## The Core Problem

**Traditional software**: Errors are obvious (crashes, 500 errors, stack traces)
**AI systems**: Failures are silent (hallucinations, drift, quality degradation)

**Production reality**: An AI system can be "working" (no errors) while being completely broken (wrong answers).

**Solution**: Monitor three distinct layers to ensure the system doesn't drift or fail silently.

---

## The Three Pillars

| Pillar | Focus | Key Metrics | Tools |
|--------|-------|-------------|-------|
| **Traces** | Every step of the RAG/Agent flow | Retrieval Latency, Tool Execution Time, Token Usage | OpenTelemetry, LangSmith, Arize Phoenix |
| **Evaluations** | Semantic accuracy & safety | Faithfulness, Answer Relevance, PII Detection | RAGAS, LangSmith Evals, LLM-as-a-Judge |
| **Unit Economics** | Cost and efficiency | Tokens per User, Cost per Query, Cache Hit Rate | Custom dashboards, LangSmith |

---

## Pillar 1: Traces (The Flow)

**What it tracks**: Every step of your RAG or agent execution.

### Why Traces Matter

```typescript
// ‚ùå Without tracing, you see:
User query ‚Üí [BLACK BOX] ‚Üí Response (5 seconds)

// ‚úÖ With tracing, you see:
User query
  ‚îú‚îÄ Query embedding (200ms, 500 tokens, $0.0015)
  ‚îú‚îÄ Vector search (1200ms, 10 results)
  ‚îú‚îÄ Reranking (800ms, top 3 results)
  ‚îú‚îÄ LLM call (2500ms, 2000 tokens, $0.045)
  ‚îî‚îÄ Response (5000ms total, $0.0465 total)
```

**Production impact**: You can now see that reranking is slow and vector search is the bottleneck.

### Implementing OpenTelemetry Traces

```typescript
// src/week7/tracing.ts
import { trace, context } from '@opentelemetry/api'
import { NodeTracerProvider } from '@opentelemetry/sdk-trace-node'
import { SimpleSpanProcessor } from '@opentelemetry/sdk-trace-base'
import { OTLPTraceExporter } from '@opentelemetry/exporter-trace-otlp-http'

// Initialize OpenTelemetry
const provider = new NodeTracerProvider()
const exporter = new OTLPTraceExporter({
  url: process.env.OTEL_EXPORTER_OTLP_ENDPOINT || 'http://localhost:4318/v1/traces'
})

provider.addSpanProcessor(new SimpleSpanProcessor(exporter))
provider.register()

const tracer = trace.getTracer('ai-system', '1.0.0')

// Example: Tracing a RAG query
export async function tracedRAGQuery(query: string): Promise<string> {
  return await tracer.startActiveSpan('rag_query', async (span) => {
    span.setAttribute('query', query)
    span.setAttribute('user_id', 'user-123')

    try {
      // Step 1: Embed query
      const embedding = await tracer.startActiveSpan('embed_query', async (embedSpan) => {
        const start = Date.now()
        const result = await embedQuery(query)

        embedSpan.setAttribute('tokens', 50)
        embedSpan.setAttribute('latency_ms', Date.now() - start)
        embedSpan.setAttribute('cost_usd', 0.0015)
        embedSpan.end()

        return result
      })

      // Step 2: Vector search
      const documents = await tracer.startActiveSpan('vector_search', async (searchSpan) => {
        const start = Date.now()
        const results = await vectorSearch(embedding, { limit: 10 })

        searchSpan.setAttribute('results_count', results.length)
        searchSpan.setAttribute('latency_ms', Date.now() - start)
        searchSpan.end()

        return results
      })

      // Step 3: Rerank
      const reranked = await tracer.startActiveSpan('rerank', async (rerankSpan) => {
        const start = Date.now()
        const topDocs = await rerank(query, documents, { topK: 3 })

        rerankSpan.setAttribute('input_count', documents.length)
        rerankSpan.setAttribute('output_count', topDocs.length)
        rerankSpan.setAttribute('latency_ms', Date.now() - start)
        rerankSpan.end()

        return topDocs
      })

      // Step 4: LLM call
      const response = await tracer.startActiveSpan('llm_call', async (llmSpan) => {
        const start = Date.now()
        const completion = await callLLM(query, reranked)

        llmSpan.setAttribute('model', 'claude-3-5-sonnet-20240620')
        llmSpan.setAttribute('input_tokens', completion.usage.input_tokens)
        llmSpan.setAttribute('output_tokens', completion.usage.output_tokens)
        llmSpan.setAttribute('latency_ms', Date.now() - start)
        llmSpan.setAttribute('cost_usd', calculateCost(completion.usage))
        llmSpan.end()

        return completion.content[0].text
      })

      span.setStatus({ code: 1 }) // SUCCESS
      span.end()

      return response

    } catch (error) {
      span.recordException(error)
      span.setStatus({ code: 2, message: error.message }) // ERROR
      span.end()
      throw error
    }
  })
}

// Helper functions
async function embedQuery(query: string): Promise<number[]> {
  // Mock embedding
  return new Array(1536).fill(0.1)
}

async function vectorSearch(embedding: number[], options: { limit: number }) {
  // Mock vector search
  return Array(options.limit).fill({ id: '1', content: 'Sample document', score: 0.9 })
}

async function rerank(query: string, docs: any[], options: { topK: number }) {
  // Mock reranking
  return docs.slice(0, options.topK)
}

async function callLLM(query: string, docs: any[]) {
  // Mock LLM call
  return {
    content: [{ text: 'Sample response' }],
    usage: { input_tokens: 1500, output_tokens: 500 }
  }
}

function calculateCost(usage: { input_tokens: number; output_tokens: number }): number {
  return (usage.input_tokens * 0.000003) + (usage.output_tokens * 0.000015)
}
```

### Visualizing Traces with LangSmith

```typescript
// src/week7/langsmith-tracing.ts
import { Client } from 'langsmith'
import { traceable } from 'langsmith/traceable'

const langsmith = new Client({ apiKey: process.env.LANGSMITH_API_KEY })

// Automatic tracing with LangSmith decorators
export const ragQuery = traceable(
  async (query: string): Promise<string> => {
    const embedding = await embedQuery(query)
    const documents = await vectorSearch(embedding)
    const reranked = await rerank(query, documents)
    const response = await callLLM(query, reranked)
    return response
  },
  { name: 'rag_query', project: 'production-rag' }
)

// LangSmith automatically captures:
// - All inputs and outputs
// - Nested function calls
// - Token usage and costs
// - Latency at each step
// - Errors and stack traces
```

### Trace Analysis: What to Look For

| Metric | Good | Warning | Critical |
|--------|------|---------|----------|
| **Total Latency** | < 2s | 2-5s | &gt; 5s |
| **Retrieval** | < 500ms | 500-1000ms | &gt; 1000ms |
| **LLM Call** | < 2s | 2-3s | &gt; 3s |
| **Cost per Query** | < $0.05 | $0.05-$0.10 | > $0.10 |

**Production alert**: If retrieval latency &gt; 1s, investigate database indexes or vector store performance.

---

## Pillar 2: Evaluations (The Quality)

**What it tracks**: Semantic accuracy and safety of AI outputs.

### The Silent Failure Problem

```typescript
// ‚ùå System is "working" but broken
Query: "What's our refund policy for enterprise customers?"

Response: "Our refund policy allows 30-day returns with receipt."
// ‚úÖ HTTP 200, no errors
// ‚ùå WRONG - enterprise customers have 90-day returns!
```

**Why traditional testing fails**: Unit tests check code logic, not semantic correctness.

### The Five Critical Evals

```typescript
export interface EvaluationMetrics {
  faithfulness: number        // 0-1: Does answer match retrieved context?
  answer_relevance: number    // 0-1: Does answer address the question?
  context_precision: number   // 0-1: Are retrieved docs relevant?
  toxicity: number           // 0-1: Contains harmful content?
  pii_detected: boolean      // True if PII found in output
}
```

### Implementing Evaluations

```typescript
// src/week7/evaluations.ts
import Anthropic from '@anthropic-ai/sdk'

const anthropic = new Anthropic({ apiKey: process.env.ANTHROPIC_API_KEY })

// Eval 1: Faithfulness (Answer is grounded in context)
export async function evaluateFaithfulness(
  question: string,
  context: string,
  answer: string
): Promise<number> {
  const evalPrompt = `You are an evaluator. Check if the ANSWER is faithful to the CONTEXT.

QUESTION: ${question}

CONTEXT:
${context}

ANSWER:
${answer}

Output JSON with score 0.0 (completely unfaithful) to 1.0 (perfectly faithful):
{
  "score": 0.0-1.0,
  "reasoning": "Why this score?"
}`

  const response = await anthropic.messages.create({
    model: 'claude-3-5-sonnet-20240620',
    max_tokens: 512,
    messages: [{ role: 'user', content: evalPrompt }]
  })

  const text = response.content[0].text
  const result = JSON.parse(text.match(/\{[\s\S]*\}/)?.[0] || '{"score": 0}')

  return result.score
}

// Eval 2: Answer Relevance (Answer addresses the question)
export async function evaluateAnswerRelevance(
  question: string,
  answer: string
): Promise<number> {
  const evalPrompt = `Does this ANSWER properly address the QUESTION?

QUESTION: ${question}
ANSWER: ${answer}

Score 0.0 (completely irrelevant) to 1.0 (perfectly relevant):
{ "score": 0.0-1.0 }`

  const response = await anthropic.messages.create({
    model: 'claude-3-5-sonnet-20240620',
    max_tokens: 256,
    messages: [{ role: 'user', content: evalPrompt }]
  })

  const result = JSON.parse(response.content[0].text.match(/\{[\s\S]*\}/)?.[0] || '{"score": 0}')
  return result.score
}

// Eval 3: PII Detection (Personal Identifiable Information)
export async function detectPII(text: string): Promise<boolean> {
  const patterns = [
    /\b\d{3}-\d{2}-\d{4}\b/,           // SSN: 123-45-6789
    /\b\d{16}\b/,                       // Credit card: 1234567812345678
    /\b[A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]{2,}\b/i, // Email
    /\b\d{3}-\d{3}-\d{4}\b/,           // Phone: 123-456-7890
  ]

  return patterns.some(pattern => pattern.test(text))
}

// Eval 4: Toxicity Detection
export async function evaluateToxicity(text: string): Promise<number> {
  // Using Perspective API or similar
  // For demo, simple keyword check
  const toxicKeywords = ['hate', 'kill', 'stupid', 'idiot']
  const matches = toxicKeywords.filter(word => text.toLowerCase().includes(word))

  return matches.length &gt; 0 ? 0.8 : 0.0
}

// Eval 5: Context Precision (Retrieved docs are relevant)
export async function evaluateContextPrecision(
  question: string,
  documents: string[]
): Promise<number> {
  const evalPrompt = `Rate how relevant these DOCUMENTS are to the QUESTION.

QUESTION: ${question}

DOCUMENTS:
${documents.map((d, i) => `${i + 1}. ${d}`).join('\n\n')}

Score each document 0 (irrelevant) or 1 (relevant), then average:
{ "scores": [0, 1, 1, 0], "precision": 0.5 }`

  const response = await anthropic.messages.create({
    model: 'claude-3-5-sonnet-20240620',
    max_tokens: 512,
    messages: [{ role: 'user', content: evalPrompt }]
  })

  const result = JSON.parse(response.content[0].text.match(/\{[\s\S]*\}/)?.[0] || '{"precision": 0}')
  return result.precision
}
```

### Running Evaluations in Production

```typescript
// src/week7/production-eval.ts
import { traceable } from 'langsmith/traceable'

export const evaluateRAGResponse = traceable(
  async (
    question: string,
    context: string,
    answer: string
  ): Promise<EvaluationMetrics> => {
    const [faithfulness, relevance, toxicity, pii] = await Promise.all([
      evaluateFaithfulness(question, context, answer),
      evaluateAnswerRelevance(question, answer),
      evaluateToxicity(answer),
      detectPII(answer)
    ])

    return {
      faithfulness,
      answer_relevance: relevance,
      context_precision: 1.0, // Would calculate from retrieved docs
      toxicity,
      pii_detected: pii
    }
  },
  { name: 'evaluate_rag_response' }
)

// Example usage
const metrics = await evaluateRAGResponse(
  'What is our enterprise refund policy?',
  'Enterprise customers have 90-day money-back guarantee...',
  'Enterprise customers can request refunds within 90 days.'
)

console.log('Evaluation Results:', metrics)
// {
//   faithfulness: 1.0,
//   answer_relevance: 1.0,
//   context_precision: 1.0,
//   toxicity: 0.0,
//   pii_detected: false
// }
```

---

## Pillar 3: Unit Economics (The Business)

**What it tracks**: Cost and efficiency of your AI system.

### The Critical Metrics

| Metric | Formula | Target | Red Flag |
|--------|---------|--------|----------|
| **Tokens per User** | Avg tokens across all user sessions | < 50K/month | &gt; 100K/month |
| **Cost per Query** | Total cost / Total queries | < $0.05 | > $0.10 |
| **Cache Hit Rate** | Cached responses / Total queries | &gt; 40% | < 20% |
| **Queries per Dollar** | Total queries / Total cost | &gt; 20 | < 10 |

### Tracking Unit Economics

```typescript
// src/week7/economics.ts
import { prisma } from '@/lib/db'

export interface QueryMetrics {
  query_id: string
  user_id: string
  tokens_used: number
  cost_usd: number
  cache_hit: boolean
  latency_ms: number
  timestamp: Date
}

export async function trackQueryMetrics(metrics: QueryMetrics) {
  await prisma.queryMetrics.create({
    data: metrics
  })
}

// Dashboard query: Cost per user
export async function getCostPerUser(userId: string, startDate: Date, endDate: Date) {
  const result = await prisma.queryMetrics.aggregate({
    where: {
      user_id: userId,
      timestamp: {
        gte: startDate,
        lte: endDate
      }
    },
    _sum: {
      cost_usd: true,
      tokens_used: true
    },
    _count: true
  })

  return {
    total_cost: result._sum.cost_usd || 0,
    total_tokens: result._sum.tokens_used || 0,
    query_count: result._count,
    cost_per_query: (result._sum.cost_usd || 0) / (result._count || 1)
  }
}

// Dashboard query: Cache hit rate
export async function getCacheHitRate(startDate: Date, endDate: Date) {
  const total = await prisma.queryMetrics.count({
    where: {
      timestamp: { gte: startDate, lte: endDate }
    }
  })

  const cacheHits = await prisma.queryMetrics.count({
    where: {
      timestamp: { gte: startDate, lte: endDate },
      cache_hit: true
    }
  })

  return {
    total_queries: total,
    cache_hits: cacheHits,
    hit_rate: cacheHits / total,
    cost_saved: cacheHits * 0.045 // Avg cost per query
  }
}
```

### Setting Up Alerts

```typescript
// src/week7/alerts.ts
export async function checkCostAlerts() {
  const today = new Date()
  const startOfMonth = new Date(today.getFullYear(), today.getMonth(), 1)

  const monthlyCost = await prisma.queryMetrics.aggregate({
    where: {
      timestamp: { gte: startOfMonth }
    },
    _sum: { cost_usd: true }
  })

  const totalCost = monthlyCost._sum.cost_usd || 0

  // Alert if monthly cost exceeds budget
  if (totalCost &gt; 500) {
    await sendAlert({
      severity: 'critical',
      message: `Monthly AI cost exceeded $500: $${totalCost.toFixed(2)}`,
      action: 'Review usage patterns and optimize'
    })
  }

  // Alert if cost per query is high
  const avgCostPerQuery = totalCost / (await prisma.queryMetrics.count())
  if (avgCostPerQuery &gt; 0.10) {
    await sendAlert({
      severity: 'warning',
      message: `Cost per query is high: $${avgCostPerQuery.toFixed(4)}`,
      action: 'Consider caching or cheaper models'
    })
  }
}

async function sendAlert(alert: { severity: string; message: string; action: string }) {
  console.log(`üö® ${alert.severity.toUpperCase()}: ${alert.message}`)
  console.log(`   Action: ${alert.action}`)

  // In production: Send to Slack, PagerDuty, etc.
}
```

---

## Production Dashboard Example

```typescript
// src/week7/dashboard.ts
export async function getProductionDashboard() {
  const last24Hours = new Date(Date.now() - 24 * 60 * 60 * 1000)

  const [traces, evals, economics] = await Promise.all([
    // Pillar 1: Traces
    prisma.queryMetrics.aggregate({
      where: { timestamp: { gte: last24Hours } },
      _avg: { latency_ms: true },
      _max: { latency_ms: true }
    }),

    // Pillar 2: Evaluations
    prisma.evaluationMetrics.aggregate({
      where: { timestamp: { gte: last24Hours } },
      _avg: {
        faithfulness: true,
        answer_relevance: true,
        toxicity: true
      }
    }),

    // Pillar 3: Unit Economics
    getCacheHitRate(last24Hours, new Date())
  ])

  return {
    traces: {
      avg_latency: traces._avg.latency_ms,
      p99_latency: traces._max.latency_ms
    },
    evaluations: {
      avg_faithfulness: evals._avg.faithfulness,
      avg_relevance: evals._avg.answer_relevance,
      avg_toxicity: evals._avg.toxicity
    },
    economics: {
      cache_hit_rate: economics.hit_rate,
      cost_saved: economics.cost_saved
    }
  }
}
```

---

## 7. Distributed Tracing: Multi-Agent Systems

### The Challenge

Multi-agent systems have complex execution flows:
- Agent A calls Agent B calls Agent C
- Parallel tool execution
- Cross-service communication
- Nested RAG queries

**Without distributed tracing:** You only see the total latency (8 seconds) but don't know which agent is slow.

**With distributed tracing:** You see the full execution tree and identify bottlenecks.

### Trace Context Propagation

```typescript
/**
 * Distributed Tracing Across Multiple Agents
 * Propagate trace context through agent calls
 */
import { trace, context, propagation, SpanStatusCode } from '@opentelemetry/api'

interface TraceContext {
  traceId: string
  spanId: string
  traceFlags: number
}

// Parent agent: Research Coordinator
async function researchCoordinator(query: string): Promise<string> {
  return await tracer.startActiveSpan('research_coordinator', async (parentSpan) => {
    parentSpan.setAttribute('query', query)
    parentSpan.setAttribute('agent_type', 'coordinator')

    // Step 1: Decompose query into sub-tasks
    const subTasks = await tracer.startActiveSpan('decompose_query', async (span) => {
      const tasks = await decomposeQuery(query)
      span.setAttribute('task_count', tasks.length)
      span.end()
      return tasks
    })

    // Step 2: Execute sub-tasks in parallel with trace context
    const results = await Promise.all(
      subTasks.map(async (task, index) => {
        // Each sub-task gets its own child span
        return await tracer.startActiveSpan(`sub_task_${index}`, async (taskSpan) => {
          taskSpan.setAttribute('task_type', task.type)
          taskSpan.setAttribute('task_query', task.query)

          // Route to specialized agent based on task type
          let result: string
          if (task.type === 'technical') {
            result = await technicalAgent(task.query, { parentSpan: taskSpan })
          } else if (task.type === 'medical') {
            result = await medicalAgent(task.query, { parentSpan: taskSpan })
          } else {
            result = await generalAgent(task.query, { parentSpan: taskSpan })
          }

          taskSpan.setAttribute('result_length', result.length)
          taskSpan.end()
          return result
        })
      })
    )

    // Step 3: Synthesize results
    const synthesis = await tracer.startActiveSpan('synthesize', async (synthSpan) => {
      const final = await synthesizeResults(query, results)
      synthSpan.setAttribute('synthesis_length', final.length)
      synthSpan.end()
      return final
    })

    parentSpan.setStatus({ code: SpanStatusCode.OK })
    parentSpan.end()

    return synthesis
  })
}

// Child agent: Technical Agent
async function technicalAgent(
  query: string,
  options: { parentSpan?: any }
): Promise<string> {
  return await tracer.startActiveSpan(
    'technical_agent',
    { links: options.parentSpan ? [{ context: options.parentSpan.spanContext() }] : [] },
    async (agentSpan) => {
      agentSpan.setAttribute('agent_type', 'technical')
      agentSpan.setAttribute('query', query)

      // This agent's RAG flow
      const ragResult = await tracer.startActiveSpan('technical_rag', async (ragSpan) => {
        // Embed ‚Üí Search ‚Üí Rerank ‚Üí LLM
        const embedding = await embed(query)
        ragSpan.setAttribute('embedding_done', true)

        const docs = await vectorDb.search(embedding, { limit: 10 })
        ragSpan.setAttribute('docs_retrieved', docs.length)

        const reranked = await rerank(query, docs, { topK: 3 })
        ragSpan.setAttribute('docs_reranked', reranked.length)

        const llmResponse = await callLLM(query, reranked, { model: 'claude-3-5-sonnet-20241022' })
        ragSpan.setAttribute('llm_tokens', llmResponse.usage.total_tokens)

        ragSpan.end()
        return llmResponse.content[0].text
      })

      agentSpan.setStatus({ code: SpanStatusCode.OK })
      agentSpan.end()

      return ragResult
    }
  )
}
```

### Visualizing Distributed Traces

**Trace output in Jaeger/Zipkin:**

```
research_coordinator (8.2s)
‚îú‚îÄ decompose_query (0.5s)
‚îú‚îÄ sub_task_0 (2.8s)
‚îÇ  ‚îî‚îÄ technical_agent (2.7s)
‚îÇ     ‚îî‚îÄ technical_rag (2.5s)
‚îÇ        ‚îú‚îÄ embed_query (0.2s)
‚îÇ        ‚îú‚îÄ vector_search (0.8s)
‚îÇ        ‚îú‚îÄ rerank (0.5s)
‚îÇ        ‚îî‚îÄ llm_call (1.0s)
‚îú‚îÄ sub_task_1 (3.5s) ‚Üê BOTTLENECK!
‚îÇ  ‚îî‚îÄ medical_agent (3.4s)
‚îÇ     ‚îî‚îÄ medical_rag (3.2s)
‚îÇ        ‚îú‚îÄ embed_query (0.2s)
‚îÇ        ‚îú‚îÄ vector_search (2.5s) ‚Üê SLOW!
‚îÇ        ‚îú‚îÄ rerank (0.3s)
‚îÇ        ‚îî‚îÄ llm_call (0.2s)
‚îî‚îÄ synthesize (1.4s)
```

**Insight:** Medical agent's vector search is 3x slower (2.5s vs 0.8s). Investigate index size or query optimization.

### Production Metrics from Distributed Traces

```typescript
/**
 * Aggregate metrics across distributed traces
 */
interface DistributedTraceMetrics {
  total_latency_ms: number
  agent_latencies: Record<string, number>
  bottleneck_agent: string
  parallel_efficiency: number // 0-1, how well parallel tasks are balanced
}

async function analyzeDistributedTrace(traceId: string): Promise<DistributedTraceMetrics> {
  // Fetch all spans for this trace
  const spans = await traceStore.getSpansByTraceId(traceId)

  // Find root span (research_coordinator)
  const rootSpan = spans.find(s => !s.parentSpanId)
  const totalLatency = rootSpan.endTime - rootSpan.startTime

  // Calculate per-agent latencies
  const agentSpans = spans.filter(s => s.name.includes('_agent'))
  const agentLatencies: Record<string, number> = {}

  for (const agentSpan of agentSpans) {
    const agentType = agentSpan.attributes.agent_type
    const latency = agentSpan.endTime - agentSpan.startTime
    agentLatencies[agentType] = latency
  }

  // Find bottleneck
  const bottleneck = Object.entries(agentLatencies)
    .sort((a, b) => b[1] - a[1])[0]

  // Calculate parallel efficiency
  const parallelTasks = spans.filter(s => s.name.startsWith('sub_task_'))
  const maxParallelLatency = Math.max(...parallelTasks.map(s => s.endTime - s.startTime))
  const sumParallelLatency = parallelTasks.reduce((sum, s) => sum + (s.endTime - s.startTime), 0)
  const parallelEfficiency = maxParallelLatency / sumParallelLatency // Ideal: 1.0 (perfect load balancing)

  return {
    total_latency_ms: totalLatency,
    agent_latencies: agentLatencies,
    bottleneck_agent: bottleneck[0],
    parallel_efficiency: parallelEfficiency
  }
}
```

### Alert on Distributed Trace Anomalies

```typescript
/**
 * Alert when distributed traces show anomalies
 */
async function monitorDistributedTraces() {
  const recentTraces = await traceStore.getRecentTraces({ limit: 100 })

  for (const trace of recentTraces) {
    const metrics = await analyzeDistributedTrace(trace.traceId)

    // Alert 1: Total latency &gt; 10s
    if (metrics.total_latency_ms &gt; 10000) {
      await alertSlack({
        channel: '#ai-alerts',
        text: `‚ö†Ô∏è Slow distributed trace: ${metrics.total_latency_ms}ms
Bottleneck: ${metrics.bottleneck_agent} (${metrics.agent_latencies[metrics.bottleneck_agent]}ms)
Trace ID: ${trace.traceId}`
      })
    }

    // Alert 2: Poor parallel efficiency (&lt;0.5)
    if (metrics.parallel_efficiency < 0.5) {
      await alertSlack({
        channel: '#ai-alerts',
        text: `‚ö†Ô∏è Poor parallel efficiency: ${(metrics.parallel_efficiency * 100).toFixed(1)}%
Load imbalance detected in parallel agent execution
Trace ID: ${trace.traceId}`
      })
    }

    // Alert 3: Agent consistently slow
    const agentAvgLatencies = await calculateAgentAverages()
    for (const [agent, latency] of Object.entries(metrics.agent_latencies)) {
      if (latency &gt; agentAvgLatencies[agent] * 2) {
        await alertSlack({
          channel: '#ai-alerts',
          text: `‚ö†Ô∏è Agent ${agent} is 2x slower than average: ${latency}ms vs ${agentAvgLatencies[agent]}ms
Trace ID: ${trace.traceId}`
        })
      }
    }
  }
}
```

---

## 8. Cost Attribution by User/Tenant

### The Multi-Tenant Problem

**Scenario:** Healthcare SaaS with 50 enterprise customers.

**Questions:**
- Which customer is driving the most cost?
- Is Customer A's $500/month cost justified by their plan?
- Should we throttle Customer B's usage?

**Solution:** Attribute costs to user/tenant in traces.

```typescript
/**
 * Cost Attribution in Traces
 * Track which user/tenant generated each trace
 */
async function tracedQueryWithCostAttribution(
  query: string,
  userId: string,
  tenantId: string
): Promise<string> {
  return await tracer.startActiveSpan('query', async (span) => {
    // Step 1: Add user/tenant to trace metadata
    span.setAttribute('user_id', userId)
    span.setAttribute('tenant_id', tenantId)
    span.setAttribute('timestamp', Date.now())

    // Step 2: Execute RAG query
    const embedding = await embed(query)
    const docs = await vectorDb.search(embedding, { limit: 10 })
    const response = await callLLM(query, docs)

    // Step 3: Calculate cost
    const cost = calculateCost(response.usage)

    // Step 4: Attribute cost to user/tenant
    span.setAttribute('cost_usd', cost)
    span.setAttribute('input_tokens', response.usage.input_tokens)
    span.setAttribute('output_tokens', response.usage.output_tokens)

    // Step 5: Record cost attribution in database
    await prisma.costAttribution.create({
      data: {
        user_id: userId,
        tenant_id: tenantId,
        trace_id: span.spanContext().traceId,
        cost_usd: cost,
        tokens: response.usage.total_tokens,
        timestamp: new Date()
      }
    })

    span.end()
    return response.content[0].text
  })
}

// Query cost by tenant
async function getTenantCosts(tenantId: string, startDate: Date, endDate: Date) {
  const costs = await prisma.costAttribution.aggregate({
    where: {
      tenant_id: tenantId,
      timestamp: {
        gte: startDate,
        lte: endDate
      }
    },
    _sum: { cost_usd: true, tokens: true },
    _count: { _all: true }
  })

  return {
    total_cost: costs._sum.cost_usd,
    total_tokens: costs._sum.tokens,
    total_queries: costs._count._all,
    avg_cost_per_query: costs._sum.cost_usd / costs._count._all
  }
}

// Alert on tenant overspend
async function alertOnTenantOverspend() {
  const tenants = await prisma.tenant.findMany({
    include: { plan: true }
  })

  for (const tenant of tenants) {
    const monthlyCosts = await getTenantCosts(
      tenant.id,
      new Date(new Date().getFullYear(), new Date().getMonth(), 1),
      new Date()
    )

    // Alert if tenant exceeds plan budget
    const budgetLimit = tenant.plan.monthly_budget_usd

    if (monthlyCosts.total_cost &gt; budgetLimit * 1.2) { // 120% of budget
      await alertSlack({
        channel: '#billing-alerts',
        text: `‚ö†Ô∏è Tenant ${tenant.name} is over budget:
Cost this month: $${monthlyCosts.total_cost.toFixed(2)}
Budget limit: $${budgetLimit.toFixed(2)}
Overage: $${(monthlyCosts.total_cost - budgetLimit).toFixed(2)} (${((monthlyCosts.total_cost / budgetLimit - 1) * 100).toFixed(1)}%)

Consider:
- Throttling requests
- Upgrading tenant plan
- Reaching out for contract renewal`
      })
    }
  }
}
```

### Cost Dashboard by Tenant

```typescript
/**
 * Cost attribution dashboard for SaaS products
 */
async function getCostDashboard(startDate: Date, endDate: Date) {
  // Top 10 most expensive tenants
  const topTenants = await prisma.costAttribution.groupBy({
    by: ['tenant_id'],
    where: {
      timestamp: { gte: startDate, lte: endDate }
    },
    _sum: { cost_usd: true, tokens: true },
    _count: { _all: true },
    orderBy: { _sum: { cost_usd: 'desc' } },
    take: 10
  })

  // Cost breakdown by model
  const costsByModel = await prisma.trace.groupBy({
    by: ['model'],
    where: {
      timestamp: { gte: startDate, lte: endDate }
    },
    _sum: { cost_usd: true },
    orderBy: { _sum: { cost_usd: 'desc' } }
  })

  return {
    top_tenants: topTenants.map(t => ({
      tenant_id: t.tenant_id,
      total_cost: t._sum.cost_usd,
      total_tokens: t._sum.tokens,
      query_count: t._count._all
    })),
    costs_by_model: costsByModel.map(m => ({
      model: m.model,
      total_cost: m._sum.cost_usd
    }))
  }
}
```

---

## Key Takeaways

1. **Three Pillars**: Traces (flow), Evaluations (quality), Unit Economics (cost)
2. **Silent Failures**: AI systems fail without errors - must monitor semantics
3. **OpenTelemetry**: Industry standard for distributed tracing
4. **LLM-as-a-Judge**: Use stronger model to evaluate weaker model
5. **Unit Economics**: Track tokens, cost, and cache hit rate
6. **Production Alerts**: Set thresholds for cost, latency, and quality
7. **Distributed Tracing**: Trace context propagation across multi-agent systems reveals bottlenecks
8. **Cost Attribution**: Track costs by user/tenant for SaaS billing and throttling

---

## Next Steps

- **Week 7 Concept 2**: Implement guardrails for input/output validation
- **Week 7 Concept 3**: Build LLM-as-a-Judge automated evaluation pipelines
- **Week 7 Lab**: Production Dashboard with OpenTelemetry, Automated Evals, Cost Alerts

---

## Further Reading

- [OpenTelemetry for LLMs](https://opentelemetry.io/)
- [LangSmith Tracing Guide](https://docs.smith.langchain.com/)
- [RAGAS Evaluation Framework](https://docs.ragas.io/)
- [Arize Phoenix](https://phoenix.arize.com/)
