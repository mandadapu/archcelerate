---
title: "LLM-as-a-Judge: Automated Evaluation Pipelines"
week: 7
concept: 3
description: "Build automated evaluation systems using stronger models to grade weaker models at scale"
estimatedMinutes: 45
objectives:
  - Understand the LLM-as-a-Judge pattern
  - Build eval pipelines with Claude 3.5 Sonnet
  - Implement semantic versioning for AI systems
  - Create golden datasets for regression testing
---

# LLM-as-a-Judge (Automated Eval)

You can't manually check every chat. Architects build Eval Pipelines that use a "Stronger" model to grade the "Junior" model's answers.

## The Core Problem

**Manual evaluation doesn't scale**:
- âŒ Can't review 10,000 queries/day manually
- âŒ Human evaluation is expensive ($20-50/hour)
- âŒ Subjective ("Is this answer good?")
- âŒ Slow feedback loop (hours to days)

**Automated evaluation with LLM-as-a-Judge**:
- âœ… Evaluate thousands of responses in minutes
- âœ… Consistent scoring criteria
- âœ… Immediate feedback (seconds)
- âœ… Cost: $0.001-0.01 per eval

---

## The LLM-as-a-Judge Pattern

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PRODUCTION MODEL (Being Evaluated)    â”‚
â”‚  - GPT-4o mini, Claude 3 Haiku         â”‚
â”‚  - Fast, cheap                         â”‚
â”‚  - Generates responses                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
         [Response to evaluate]
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  JUDGE MODEL (Evaluator)               â”‚
â”‚  - Claude 3.5 Sonnet, GPT-4o           â”‚
â”‚  - Strong reasoning                    â”‚
â”‚  - Scores the response                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
         [Evaluation Score 0-1]
```

**Key insight**: Use an expensive, powerful model to evaluate a cheaper production model.

---

## When to Use LLM-as-a-Judge

| Evaluation Type | Use LLM-as-a-Judge? | Alternative |
|----------------|-------------------|-------------|
| **Faithfulness** | âœ… Yes | Impossible to regex |
| **Answer Relevance** | âœ… Yes | Semantic similarity can help |
| **Toxicity** | âš ï¸ Sometimes | Perspective API is faster |
| **PII Detection** | âŒ No | Regex is 100x faster |
| **Exact Match** | âŒ No | String comparison |

**Rule**: Use LLM-as-a-Judge for semantic/subjective evaluation. Use rules for objective checks.

---

## Pattern 1: Faithfulness Evaluation

**Question**: Does the response only make claims supported by the context?

### Implementation

```typescript
// src/week7/evals/faithfulness.ts
import Anthropic from '@anthropic-ai/sdk'

const anthropic = new Anthropic({ apiKey: process.env.ANTHROPIC_API_KEY })

export interface FaithfulnessEval {
  score: number
  reasoning: string
  unsupported_claims: string[]
}

export async function evaluateFaithfulness(
  question: string,
  context: string,
  answer: string
): Promise<FaithfulnessEval> {
  const judgePrompt = `You are an expert evaluator. Your task is to determine if the ANSWER makes any claims not supported by the CONTEXT.

QUESTION:
${question}

CONTEXT:
${context}

ANSWER:
${answer}

Evaluation criteria:
- Score 1.0: Every claim in the answer is directly supported by the context
- Score 0.5-0.9: Most claims are supported, but some are inferred or slightly beyond context
- Score 0.0: Answer makes claims not found in context (hallucination)

Output JSON:
{
  "score": 0.0-1.0,
  "reasoning": "Brief explanation of score",
  "unsupported_claims": ["Claim 1", "Claim 2"]
}`

  const response = await anthropic.messages.create({
    model: 'claude-3-5-sonnet-20240620',
    max_tokens: 512,
    messages: [{ role: 'user', content: judgePrompt }]
  })

  const text = response.content[0].text
  const result = JSON.parse(text.match(/\{[\s\S]*\}/)?.[0] || '{"score": 0, "reasoning": "Parse failed", "unsupported_claims": []}')

  return result
}

// Example
const eval = await evaluateFaithfulness(
  'What is our refund policy for enterprise customers?',
  'Enterprise customers can request full refunds within 90 days of purchase with proof of purchase.',
  'Enterprise customers have a 30-day refund policy.'
)

console.log(eval)
// {
//   score: 0.0,
//   reasoning: "Answer states 30 days but context clearly says 90 days",
//   unsupported_claims: ["30-day refund policy"]
// }
```

---

## Pattern 2: Answer Relevance Evaluation

**Question**: Does the answer actually address the user's question?

```typescript
// src/week7/evals/relevance.ts
export interface RelevanceEval {
  score: number
  reasoning: string
  missed_aspects: string[]
}

export async function evaluateRelevance(
  question: string,
  answer: string
): Promise<RelevanceEval> {
  const judgePrompt = `Evaluate if this ANSWER properly addresses the QUESTION.

QUESTION: ${question}

ANSWER: ${answer}

Scoring:
- 1.0: Completely addresses the question
- 0.5-0.9: Partially addresses, but misses some aspects
- 0.0: Doesn't address the question

Output JSON:
{
  "score": 0.0-1.0,
  "reasoning": "Why this score?",
  "missed_aspects": ["What was not addressed"]
}`

  const response = await anthropic.messages.create({
    model: 'claude-3-5-sonnet-20240620',
    max_tokens: 512,
    messages: [{ role: 'user', content: judgePrompt }]
  })

  const result = JSON.parse(response.content[0].text.match(/\{[\s\S]*\}/)?.[0] || '{}')
  return result
}
```

---

## Pattern 3: Comparative Evaluation (A/B Testing)

**Question**: Which response is better, A or B?

```typescript
// src/week7/evals/comparative.ts
export interface ComparativeEval {
  winner: 'A' | 'B' | 'tie'
  score_a: number
  score_b: number
  reasoning: string
}

export async function comparativeEval(
  question: string,
  responseA: string,
  responseB: string
): Promise<ComparativeEval> {
  const judgePrompt = `Compare these two responses to the same question.

QUESTION: ${question}

RESPONSE A:
${responseA}

RESPONSE B:
${responseB}

Criteria:
- Accuracy
- Completeness
- Clarity
- Conciseness

Output JSON:
{
  "winner": "A" | "B" | "tie",
  "score_a": 0.0-1.0,
  "score_b": 0.0-1.0,
  "reasoning": "Why this winner?"
}`

  const response = await anthropic.messages.create({
    model: 'claude-3-5-sonnet-20240620',
    max_tokens: 512,
    messages: [{ role: 'user', content: judgePrompt }]
  })

  const result = JSON.parse(response.content[0].text.match(/\{[\s\S]*\}/)?.[0] || '{}')
  return result
}

// Usage: A/B test two prompt strategies
const promptA = await generateResponse(query, { style: 'formal' })
const promptB = await generateResponse(query, { style: 'casual' })

const comparison = await comparativeEval(query, promptA, promptB)
console.log(`Winner: ${comparison.winner}`)
```

---

## Building an Eval Pipeline

### 1. The Golden Dataset

**Definition**: A curated set of test cases with known correct answers.

```typescript
// src/week7/evals/golden-dataset.ts
export interface GoldenExample {
  id: string
  question: string
  context: string
  expected_answer: string
  rubric: {
    must_include: string[]
    must_not_include: string[]
    min_faithfulness: number
  }
}

export const GOLDEN_DATASET: GoldenExample[] = [
  {
    id: 'refund-001',
    question: 'What is the refund policy for enterprise customers?',
    context: 'Enterprise customers have 90-day money-back guarantee with proof of purchase.',
    expected_answer: 'Enterprise customers can request refunds within 90 days with proof of purchase.',
    rubric: {
      must_include: ['90 days', 'enterprise'],
      must_not_include: ['30 days', 'free tier'],
      min_faithfulness: 0.9
    }
  },
  {
    id: 'billing-002',
    question: 'How do I update my credit card?',
    context: 'To update payment methods, navigate to Settings > Billing > Payment Methods.',
    expected_answer: 'Go to Settings > Billing > Payment Methods to update your credit card.',
    rubric: {
      must_include: ['Settings', 'Billing', 'Payment Methods'],
      must_not_include: ['contact support', 'email us'],
      min_faithfulness: 1.0
    }
  }
]
```

### 2. Running the Eval Pipeline

```typescript
// src/week7/evals/pipeline.ts
export interface EvalResult {
  test_id: string
  passed: boolean
  scores: {
    faithfulness: number
    relevance: number
  }
  failures: string[]
}

export async function runEvalPipeline(
  systemUnderTest: (query: string, context: string) => Promise<string>
): Promise<EvalResult[]> {
  const results: EvalResult[] = []

  for (const example of GOLDEN_DATASET) {
    console.log(`\nTesting: ${example.id}`)

    // Generate response from system
    const response = await systemUnderTest(example.question, example.context)

    // Run evaluations
    const faithfulness = await evaluateFaithfulness(
      example.question,
      example.context,
      response
    )

    const relevance = await evaluateRelevance(example.question, response)

    // Check rubric
    const failures: string[] = []

    // Must include checks
    for (const phrase of example.rubric.must_include) {
      if (!response.toLowerCase().includes(phrase.toLowerCase())) {
        failures.push(`Missing required phrase: "${phrase}"`)
      }
    }

    // Must not include checks
    for (const phrase of example.rubric.must_not_include) {
      if (response.toLowerCase().includes(phrase.toLowerCase())) {
        failures.push(`Contains forbidden phrase: "${phrase}"`)
      }
    }

    // Faithfulness threshold
    if (faithfulness.score < example.rubric.min_faithfulness) {
      failures.push(`Faithfulness ${faithfulness.score} < ${example.rubric.min_faithfulness}`)
    }

    const passed = failures.length === 0

    results.push({
      test_id: example.id,
      passed,
      scores: {
        faithfulness: faithfulness.score,
        relevance: relevance.score
      },
      failures
    })

    console.log(`  Faithfulness: ${faithfulness.score.toFixed(2)}`)
    console.log(`  Relevance: ${relevance.score.toFixed(2)}`)
    console.log(`  Result: ${passed ? 'âœ… PASS' : 'âŒ FAIL'}`)
    if (!passed) {
      console.log(`  Failures: ${failures.join(', ')}`)
    }
  }

  return results
}

// Usage
const results = await runEvalPipeline(myRAGSystem)

const passRate = results.filter(r => r.passed).length / results.length
console.log(`\nğŸ“Š Pass Rate: ${(passRate * 100).toFixed(1)}%`)
```

---

## Semantic Versioning for AI Systems

**The Problem**: In traditional software, you version code. In AI, you must version **Prompt + Model + Data**.

### What to Version

```typescript
export interface AISystemVersion {
  version: string         // "1.2.3"
  model: string          // "claude-3-5-sonnet-20240620"
  prompt_template_hash: string  // SHA-256 of system prompt
  rag_strategy: string   // "hybrid-search-v2"
  eval_results: {
    faithfulness_avg: number
    relevance_avg: number
    cost_per_query: number
  }
  deployed_at: Date
}

// Example versions
const versions: AISystemVersion[] = [
  {
    version: '1.0.0',
    model: 'claude-3-haiku-20240307',
    prompt_template_hash: 'a1b2c3...',
    rag_strategy: 'vector-only',
    eval_results: {
      faithfulness_avg: 0.75,
      relevance_avg: 0.82,
      cost_per_query: 0.012
    },
    deployed_at: new Date('2025-01-01')
  },
  {
    version: '1.1.0',
    model: 'claude-3-haiku-20240307',
    prompt_template_hash: 'd4e5f6...',  // â† Changed prompt
    rag_strategy: 'hybrid-search',       // â† Changed strategy
    eval_results: {
      faithfulness_avg: 0.88,  // âœ… Improved
      relevance_avg: 0.90,     // âœ… Improved
      cost_per_query: 0.018    // âš ï¸ Increased
    },
    deployed_at: new Date('2025-02-01')
  }
]
```

### The Deployment Rule

**Never deploy a prompt change to production without running it against your Golden Dataset first.**

```typescript
// src/week7/evals/deployment-gate.ts
export async function deploymentGate(
  newSystemVersion: (q: string, ctx: string) => Promise<string>,
  minPassRate: number = 0.90
): Promise<boolean> {
  console.log('ğŸš¦ Running deployment gate...')

  const results = await runEvalPipeline(newSystemVersion)

  const passRate = results.filter(r => r.passed).length / results.length
  const avgFaithfulness = results.reduce((sum, r) => sum + r.scores.faithfulness, 0) / results.length

  console.log(`\nğŸ“Š Deployment Gate Results:`)
  console.log(`  Pass Rate: ${(passRate * 100).toFixed(1)}%`)
  console.log(`  Avg Faithfulness: ${avgFaithfulness.toFixed(2)}`)
  console.log(`  Required Pass Rate: ${(minPassRate * 100).toFixed(1)}%`)

  if (passRate < minPassRate) {
    console.log(`\nâŒ DEPLOYMENT BLOCKED: Pass rate below threshold`)
    return false
  }

  console.log(`\nâœ… DEPLOYMENT APPROVED`)
  return true
}

// Usage before production deploy
const approved = await deploymentGate(newPromptVersion, 0.95)

if (!approved) {
  throw new Error('Deployment blocked by eval gate')
}

// Proceed with deployment
await deployToProduction(newPromptVersion)
```

---

## The Continuous Eval Loop

```typescript
// src/week7/evals/continuous.ts
export async function continuousEvalLoop() {
  setInterval(async () => {
    console.log('\nğŸ”„ Running hourly eval...')

    // Sample recent production queries
    const recentQueries = await prisma.productionLog.findMany({
      take: 50,
      orderBy: { timestamp: 'desc' }
    })

    let faithfulnessSum = 0
    let relevanceSum = 0

    for (const log of recentQueries) {
      const faithfulness = await evaluateFaithfulness(
        log.query,
        log.retrieved_context,
        log.response
      )

      const relevance = await evaluateRelevance(log.query, log.response)

      faithfulnessSum += faithfulness.score
      relevanceSum += relevance.score

      // Alert on low score
      if (faithfulness.score < 0.7) {
        await sendAlert({
          severity: 'warning',
          message: `Low faithfulness detected: ${faithfulness.score}`,
          query: log.query,
          response: log.response
        })
      }
    }

    const avgFaithfulness = faithfulnessSum / recentQueries.length
    const avgRelevance = relevanceSum / recentQueries.length

    console.log(`  Avg Faithfulness: ${avgFaithfulness.toFixed(2)}`)
    console.log(`  Avg Relevance: ${avgRelevance.toFixed(2)}`)

    // Track drift over time
    await prisma.evalMetrics.create({
      data: {
        timestamp: new Date(),
        avg_faithfulness: avgFaithfulness,
        avg_relevance: avgRelevance,
        sample_size: recentQueries.length
      }
    })

  }, 60 * 60 * 1000) // Every hour
}

async function sendAlert(alert: any) {
  console.log(`ğŸš¨ ${alert.severity.toUpperCase()}: ${alert.message}`)
  // In production: Send to Slack, PagerDuty
}
```

---

## 7. Regression Testing with Golden Datasets

### The Problem

**Scenario:** You improve your RAG retrieval (add hybrid search). Does it help or hurt?

**Without regression testing:** You don't know if new version breaks existing queries.
**With regression testing:** You have a golden dataset of 500 queries with expected answers. New version must pass 95%+ before deploy.

### Golden Dataset Structure

```typescript
/**
 * Golden Dataset: Curated test cases with ground truth
 */
interface GoldenExample {
  id: string
  query: string
  expected_answer: string
  context_documents: string[]  // Documents that should be retrieved
  difficulty: 'easy' | 'medium' | 'hard'
  category: 'factual' | 'reasoning' | 'multi-hop'
  metadata: {
    created_at: Date
    updated_at: Date
    pass_rate_threshold: number // 0-1, e.g., 0.9 = must get 90%+ correct
  }
}

// Example: Healthcare RAG golden dataset
const goldenDataset: GoldenExample[] = [
  {
    id: 'golden_001',
    query: 'What was the patient\'s HbA1c value in March 2023?',
    expected_answer: '7.2%',
    context_documents: ['clinical_note_2023_03_15'],
    difficulty: 'easy',
    category: 'factual',
    metadata: {
      created_at: new Date('2024-01-01'),
      updated_at: new Date('2024-01-01'),
      pass_rate_threshold: 0.95
    }
  },
  {
    id: 'golden_002',
    query: 'Has the patient\'s diabetes control improved over the past year?',
    expected_answer: 'Yes, HbA1c decreased from 8.1% to 7.2% over 12 months.',
    context_documents: ['clinical_note_2022_03', 'clinical_note_2023_03'],
    difficulty: 'hard',
    category: 'reasoning',
    metadata: {
      created_at: new Date('2024-01-01'),
      updated_at: new Date('2024-01-01'),
      pass_rate_threshold: 0.85 // Lower threshold for hard questions
    }
  }
]
```

### Regression Test Runner

```typescript
/**
 * Regression Test Suite
 * Run before every deploy to prevent breaking changes
 */
interface RegressionTestResult {
  test_id: string
  passed: boolean
  expected: string
  actual: string
  score: number        // 0-1 from LLM-as-judge
  latency_ms: number
  cost_usd: number
  error?: string
}

async function runRegressionTests(
  goldenDataset: GoldenExample[],
  ragSystem: (query: string) => Promise<string>
): Promise<{
  results: RegressionTestResult[]
  pass_rate: number
  avg_score: number
  total_cost: number
}> {
  const results: RegressionTestResult[] = []
  let totalCost = 0

  for (const example of goldenDataset) {
    const startTime = Date.now()

    try {
      // Step 1: Execute RAG query
      const actualAnswer = await ragSystem(example.query)
      const latency = Date.now() - startTime

      // Step 2: Evaluate with LLM-as-judge
      const evaluation = await llmAsJudge({
        query: example.query,
        expected: example.expected_answer,
        actual: actualAnswer,
        context: example.context_documents
      })

      const cost = 0.003 // Approximate cost per eval
      totalCost += cost

      // Step 3: Determine pass/fail
      const passed = evaluation.score >= example.metadata.pass_rate_threshold

      results.push({
        test_id: example.id,
        passed,
        expected: example.expected_answer,
        actual: actualAnswer,
        score: evaluation.score,
        latency_ms: latency,
        cost_usd: cost
      })

    } catch (error) {
      results.push({
        test_id: example.id,
        passed: false,
        expected: example.expected_answer,
        actual: '',
        score: 0,
        latency_ms: Date.now() - startTime,
        cost_usd: 0,
        error: error.message
      })
    }
  }

  // Calculate aggregate metrics
  const passCount = results.filter(r => r.passed).length
  const passRate = passCount / results.length
  const avgScore = results.reduce((sum, r) => sum + r.score, 0) / results.length

  return {
    results,
    pass_rate: passRate,
    avg_score: avgScore,
    total_cost: totalCost
  }
}
```

### Deployment Gate

```typescript
/**
 * Deployment Gate: Block deploy if regression tests fail
 */
async function deploymentGate(version: string) {
  console.log(`ğŸ” Running regression tests for version ${version}...`)

  // Load golden dataset
  const goldenDataset = await loadGoldenDataset()

  // Run regression tests
  const testResults = await runRegressionTests(goldenDataset, productionRAG)

  console.log(`\nğŸ“Š Regression Test Results:`)
  console.log(`  Pass Rate: ${(testResults.pass_rate * 100).toFixed(1)}%`)
  console.log(`  Avg Score: ${testResults.avg_score.toFixed(2)}`)
  console.log(`  Total Cost: $${testResults.total_cost.toFixed(2)}`)

  // Gate: Require 95% pass rate
  const REQUIRED_PASS_RATE = 0.95

  if (testResults.pass_rate >= REQUIRED_PASS_RATE) {
    console.log(`\nâœ… PASS: Regression tests passed (${(testResults.pass_rate * 100).toFixed(1)}% >= ${REQUIRED_PASS_RATE * 100}%)`)
    console.log(`Deploying version ${version} to production...`)

    // Log successful deployment
    await prisma.deployment.create({
      data: {
        version,
        pass_rate: testResults.pass_rate,
        avg_score: testResults.avg_score,
        test_count: goldenDataset.length,
        deployed_at: new Date(),
        status: 'success'
      }
    })

    return { allowed: true, message: 'Deployment approved' }

  } else {
    console.log(`\nâŒ FAIL: Regression tests failed (${(testResults.pass_rate * 100).toFixed(1)}% < ${REQUIRED_PASS_RATE * 100}%)`)
    console.log(`\nFailing tests:`)

    const failingTests = testResults.results.filter(r => !r.passed)
    for (const test of failingTests.slice(0, 5)) { // Show first 5
      console.log(`  - ${test.test_id}: Score ${test.score.toFixed(2)} (threshold: 0.95)`)
      console.log(`    Expected: ${test.expected.slice(0, 100)}...`)
      console.log(`    Actual: ${test.actual.slice(0, 100)}...`)
    }

    // Log failed deployment attempt
    await prisma.deployment.create({
      data: {
        version,
        pass_rate: testResults.pass_rate,
        avg_score: testResults.avg_score,
        test_count: goldenDataset.length,
        deployed_at: new Date(),
        status: 'blocked'
      }
    })

    // Alert team
    await alertSlack({
      channel: '#ai-alerts',
      text: `ğŸš¨ Deployment blocked for version ${version}

Pass rate: ${(testResults.pass_rate * 100).toFixed(1)}% (required: ${REQUIRED_PASS_RATE * 100}%)
Failing tests: ${failingTests.length}

Review regression test results before deploying.`
    })

    return { allowed: false, message: 'Deployment blocked by regression tests' }
  }
}

// CI/CD Integration
async function cicdPipeline() {
  const version = process.env.VERSION || 'unknown'

  // Step 1: Build
  console.log('Building...')
  await build()

  // Step 2: Unit tests
  console.log('Running unit tests...')
  await runUnitTests()

  // Step 3: Regression tests (deployment gate)
  console.log('Running regression tests...')
  const gateResult = await deploymentGate(version)

  if (!gateResult.allowed) {
    process.exit(1) // Block deployment
  }

  // Step 4: Deploy
  console.log('Deploying...')
  await deploy(version)
}
```

### Regression Test History Dashboard

```typescript
/**
 * Track regression test performance over time
 */
interface RegressionTestHistory {
  version: string
  deployed_at: Date
  pass_rate: number
  avg_score: number
  latency_p95: number
  cost: number
}

async function getRegressionTestHistory(): Promise<RegressionTestHistory[]> {
  const deployments = await prisma.deployment.findMany({
    where: { status: 'success' },
    orderBy: { deployed_at: 'desc' },
    take: 20
  })

  return deployments.map(d => ({
    version: d.version,
    deployed_at: d.deployed_at,
    pass_rate: d.pass_rate,
    avg_score: d.avg_score,
    latency_p95: d.latency_p95,
    cost: d.cost
  }))
}

// Detect regression trends
async function detectRegressionTrends() {
  const history = await getRegressionTestHistory()

  // Check if pass rate is declining
  const recentPassRates = history.slice(0, 5).map(h => h.pass_rate)
  const avgRecentPassRate = recentPassRates.reduce((a, b) => a + b, 0) / recentPassRates.length

  const olderPassRates = history.slice(5, 10).map(h => h.pass_rate)
  const avgOlderPassRate = olderPassRates.reduce((a, b) => a + b, 0) / olderPassRates.length

  const decline = avgOlderPassRate - avgRecentPassRate

  if (decline > 0.05) { // 5% decline
    await alertSlack({
      channel: '#ai-alerts',
      text: `ğŸ“‰ Regression test pass rate declining:

Recent avg: ${(avgRecentPassRate * 100).toFixed(1)}%
Previous avg: ${(avgOlderPassRate * 100).toFixed(1)}%
Decline: ${(decline * 100).toFixed(1)} percentage points

Consider reviewing recent changes to prompts or retrieval logic.`
    })
  }
}
```

### Golden Dataset Maintenance

```typescript
/**
 * Continuously update golden dataset based on production failures
 */
async function maintainGoldenDataset() {
  // Step 1: Find production failures (low LLM-as-judge scores)
  const failures = await prisma.queryLog.findMany({
    where: {
      faithfulness_score: { lt: 0.7 },
      human_reviewed: true,
      correct_answer: { not: null }
    },
    orderBy: { timestamp: 'desc' },
    take: 100
  })

  // Step 2: Add high-value failures to golden dataset
  for (const failure of failures) {
    const alreadyExists = await prisma.goldenExample.findFirst({
      where: { query: failure.query }
    })

    if (!alreadyExists) {
      await prisma.goldenExample.create({
        data: {
          query: failure.query,
          expected_answer: failure.correct_answer,
          context_documents: failure.retrieved_documents,
          difficulty: 'hard', // Failed queries are hard
          category: 'factual',
          pass_rate_threshold: 0.85
        }
      })

      console.log(`âœ… Added failing query to golden dataset: ${failure.query.slice(0, 50)}...`)
    }
  }

  console.log(`Golden dataset now has ${await prisma.goldenExample.count()} examples`)
}
```

---

## Cost Analysis: LLM-as-a-Judge

| Scenario | Evals per Day | Cost per Eval | Daily Cost | Monthly Cost |
|----------|--------------|---------------|------------|--------------|
| **Development** | 50 | $0.003 | $0.15 | $4.50 |
| **Continuous (hourly)** | 1,200 | $0.003 | $3.60 | $108 |
| **Full regression** | 10,000 | $0.003 | $30 | $900 |
| **Pre-deploy gate** | 500 | $0.003 | $1.50 | $45 (20 deploys/month) |

**Production tip**: Run full regression (500-1000 examples) before every deploy. Run hourly sampling (50 examples) in production.

---

## Key Takeaways

1. **LLM-as-a-Judge**: Use stronger model to evaluate production model
2. **Golden Dataset**: Curated test cases with known correct answers
3. **Eval Pipeline**: Automated testing before every deployment
4. **Semantic Versioning**: Version prompt + model + data, not just code
5. **Deployment Gate**: Require 90%+ pass rate before production deploy
6. **Continuous Eval**: Sample production traffic hourly to detect drift

---

## Next Steps

- **Week 7 Lab**: Build Production Dashboard with all three pillars
- **Capstone Project**: Implement eval pipeline with 100% faithfulness requirement
- **Production**: Set up continuous eval loop with alerting

---

## Further Reading

- [G-Eval: NLG Evaluation using GPT-4](https://arxiv.org/abs/2303.16634)
- [Judging LLM-as-a-Judge with MT-Bench](https://arxiv.org/abs/2306.05685)
- [LangSmith Evaluation Guide](https://docs.smith.langchain.com/evaluation)
- [RAGAS Framework](https://docs.ragas.io/)
