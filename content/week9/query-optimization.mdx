---
title: "Query Optimization & Expansion"
description: "Transform vague user queries into optimized retrieval strategies with decomposition, multi-query generation, and cost-aware expansion"
estimatedMinutes: 45
week: 9
concept: 5
difficulty: intermediate
objectives:
  - Decompose complex queries into sub-queries for better retrieval
  - Generate multiple query variations for comprehensive coverage
  - Expand queries with semantic embeddings for recall improvement
  - Optimize costs with caching and query routing strategies
---

import { CodePlayground } from '@/components/curriculum/CodePlayground'

# Query Optimization & Expansion

Transform ambiguous user queries into precise retrieval strategies that maximize answer quality while minimizing cost and latency.

> **Note**: Query optimization is the highest-ROI technique in RAG systems - improving both accuracy (15-25%) and cost efficiency (30-50%) with minimal infrastructure changes.

## What is Query Optimization?

**Simple Explanation**: Query optimization transforms vague or poorly-worded user questions into better search queries that retrieval systems can understand. It's like having a librarian who understands what you really mean and knows exactly which books to pull.

**The Problem**:
```
User query: "how to make it faster"

❌ Poor retrieval:
- Returns generic "speed optimization" articles
- Misses specific context (What needs to be faster? Code? Database? API?)
- Low precision: 45% relevant results

✅ Optimized query:
→ Decompose: "What needs optimization?"
→ Expand: "code performance optimization", "reduce execution time", "improve latency"
→ Retrieve: 85% relevant results
```

## Why Query Optimization Matters

### Real-World Impact

**Example 1: Vague Technical Query**
```
Original: "error with login"
Problems:
- Which error? (401, 500, timeout?)
- Which login? (web, mobile, API?)
- What context? (after deployment, specific user?)

Optimized:
→ "authentication error 401 unauthorized web login"
→ "login timeout connection issues mobile app"
→ "OAuth token expiration login failure"

Result: 3x more relevant results
```

**Example 2: Medical Query**
```
Original: "treatment for diabetes"
Problems:
- Type 1 or Type 2?
- What stage?
- Drug therapy or lifestyle?

Optimized:
→ "Type 2 diabetes medication metformin dosage"
→ "prediabetes lifestyle interventions diet exercise"
→ "insulin therapy Type 1 diabetes management"

Result: Precision 65% → 91%
```

**Cost Savings**:
- **Without optimization**: Retrieve 50 documents, 80% irrelevant = wasted tokens
- **With optimization**: Retrieve 20 documents, 85% relevant = 60% cost reduction

**Industry Benchmarks**:
- Query rewriting: +15-20% accuracy improvement
- Multi-query generation: +25-30% coverage (recall)
- Combined optimization: +35-40% overall performance

## Query Decomposition

**Strategy 1**: Break complex queries into simpler sub-queries.

<CodePlayground
  title="Query Decomposition"
  description="Decompose complex multi-part questions into sub-queries. Try different complex queries to see the breakdown!"
  exerciseType="query-decomposition"
  code={`import Anthropic from '@anthropic-ai/sdk'

const anthropic = new Anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY
})

interface SubQuery {
  query: string
  reasoning: string
  priority: number
}

interface DecompositionResult {
  originalQuery: string
  subQueries: SubQuery[]
  strategy: string
}

async function decomposeQuery(query: string): Promise<DecompositionResult> {
  console.log(\`Decomposing query: "\${query}"\\n\`)

  const response = await anthropic.messages.create({
    model: 'claude-sonnet-4-5-20251101',
    max_tokens: 1000,
    messages: [{
      role: 'user',
      content: \`Decompose this complex query into simpler sub-queries that can be answered independently.

RULES:
- Each sub-query should be self-contained
- Order by priority (most important first)
- Include reasoning for why each sub-query is needed
- Return JSON only

Query: "\${query}"

Return JSON:
{
  "strategy": "parallel | sequential | hybrid",
  "subQueries": [
    {"query": "...", "reasoning": "...", "priority": 1}
  ]
}\`
    }]
  })

  const result = JSON.parse(response.content[0].text)

  console.log('Decomposition Strategy:', result.strategy)
  console.log(\`Generated \${result.subQueries.length} sub-queries:\\n\`)

  result.subQueries.forEach((sq: SubQuery, i: number) => {
    console.log(\`\${i + 1}. [Priority \${sq.priority}] \${sq.query}\`)
    console.log(\`   Reasoning: \${sq.reasoning}\`)
    console.log()
  })

  return {
    originalQuery: query,
    subQueries: result.subQueries,
    strategy: result.strategy
  }
}

// Execute sub-queries and merge results
async function executeDecomposedQuery(
  decomposition: DecompositionResult
): Promise<any[]> {
  const { subQueries, strategy } = decomposition

  if (strategy === 'parallel') {
    // Execute all sub-queries simultaneously
    console.log('Executing sub-queries in parallel...\\n')

    const results = await Promise.all(
      subQueries.map(sq => retrieveForSubQuery(sq.query))
    )

    // Merge and deduplicate results
    const merged = mergeResults(results)
    console.log(\`Merged to \${merged.length} unique results\`)

    return merged
  } else if (strategy === 'sequential') {
    // Execute sub-queries in order, using previous results as context
    console.log('Executing sub-queries sequentially...\\n')

    const allResults: any[] = []

    for (const sq of subQueries) {
      const results = await retrieveForSubQuery(sq.query, allResults)
      allResults.push(...results)
      console.log(\`  Completed: \${sq.query} (\${results.length} results)\`)
    }

    return allResults
  } else {
    // Hybrid: critical queries first, then parallel
    console.log('Executing hybrid strategy...\\n')

    const criticalQueries = subQueries.filter(sq => sq.priority === 1)
    const otherQueries = subQueries.filter(sq => sq.priority &gt; 1)

    // Execute critical queries sequentially
    const criticalResults: any[] = []
    for (const sq of criticalQueries) {
      const results = await retrieveForSubQuery(sq.query)
      criticalResults.push(...results)
    }

    // Execute remaining queries in parallel
    const otherResults = await Promise.all(
      otherQueries.map(sq => retrieveForSubQuery(sq.query))
    )

    return [...criticalResults, ...otherResults.flat()]
  }
}

async function retrieveForSubQuery(query: string, context?: any[]): Promise<any[]> {
  // Simulated retrieval (replace with actual vector search)
  return [
    { id: \`doc-\${Math.random()}\`, text: \`Result for: \${query}\`, score: 0.85 }
  ]
}

function mergeResults(resultSets: any[][]): any[] {
  const seen = new Set<string>()
  const merged: any[] = []

  for (const results of resultSets) {
    for (const result of results) {
      if (!seen.has(result.id)) {
        seen.add(result.id)
        merged.push(result)
      }
    }
  }

  return merged
}

// Example usage
async function runDecomposition() {
  const complexQueries = [
    "What are the HIPAA requirements for data encryption and how do they compare to GDPR?",
    "How to implement authentication and authorization for a REST API with rate limiting?",
    "What caused the performance regression in v2.3 and how was it fixed in v2.4?"
  ]

  for (const query of complexQueries) {
    const decomposition = await decomposeQuery(query)
    const results = await executeDecomposedQuery(decomposition)

    console.log(\`Final result count: \${results.length}\\n\`)
    console.log('─'.repeat(80))
    console.log()
  }
}

runDecomposition()`}
/>

**Decomposition Strategies**:

| Strategy | When to Use | Example | Latency | Accuracy |
|----------|-------------|---------|---------|----------|
| **Parallel** | Independent sub-queries | "HIPAA vs GDPR" | Low (200ms) | Good |
| **Sequential** | Dependent sub-queries | "What → Why → How" | High (600ms) | Excellent |
| **Hybrid** | Mix of both | Critical first, rest parallel | Medium (350ms) | Very Good |

## Multi-Query Generation

**Strategy 2**: Generate multiple variations of the same query for better coverage.

<CodePlayground
  title="Multi-Query Generation"
  description="Generate diverse query variations to cover different phrasings. Watch recall improvement!"
  exerciseType="multi-query-generation"
  code={`import Anthropic from '@anthropic-ai/sdk'

const anthropic = new Anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY
})

interface QueryVariation {
  query: string
  perspective: string
  targetDocType: string
}

async function generateMultipleQueries(
  originalQuery: string,
  numVariations: number = 3
): Promise<QueryVariation[]> {
  console.log(\`Generating \${numVariations} query variations for: "\${originalQuery}"\\n\`)

  const response = await anthropic.messages.create({
    model: 'claude-haiku-4-5-20250929', // Haiku for cost efficiency
    max_tokens: 500,
    messages: [{
      role: 'user',
      content: \`Generate \${numVariations} different search queries that capture different aspects of this question.

Each variation should:
- Use different phrasing and keywords
- Target different types of documents (technical, conceptual, practical)
- Cover different perspectives on the topic

Original query: "\${originalQuery}"

Return JSON array:
[
  {"query": "...", "perspective": "...", "targetDocType": "..."}
]\`
    }]
  })

  const variations = JSON.parse(response.content[0].text)

  console.log('Generated variations:\\n')
  variations.forEach((v: QueryVariation, i: number) => {
    console.log(\`\${i + 1}. \${v.query}\`)
    console.log(\`   Perspective: \${v.perspective}\`)
    console.log(\`   Targets: \${v.targetDocType}\`)
    console.log()
  })

  return variations
}

// Retrieve with multiple queries and fuse results
async function multiQueryRetrieval(
  originalQuery: string,
  topK: number = 5
): Promise<any[]> {
  // Generate query variations
  const variations = await generateMultipleQueries(originalQuery)

  console.log('Retrieving with all query variations...\\n')

  // Retrieve with each variation
  const allResults = await Promise.all(
    variations.map(async v => {
      const results = await vectorSearch(v.query, topK * 2) // Get more to account for overlap
      return results.map(r => ({
        ...r,
        queryVariation: v.query,
        perspective: v.perspective
      }))
    })
  )

  // Flatten and deduplicate
  const flat = allResults.flat()
  const deduplicated = deduplicateById(flat)

  console.log(\`Retrieved \${flat.length} total results\`)
  console.log(\`After deduplication: \${deduplicated.length} unique results\\n\`)

  // Reciprocal Rank Fusion to combine rankings
  const fused = reciprocalRankFusion(allResults)

  console.log(\`Top \${topK} after RRF fusion:\\n\`)
  fused.slice(0, topK).forEach((result, i) => {
    console.log(\`\${i + 1}. \${result.id} (score: \${result.score.toFixed(4)})\`)
    console.log(\`   From query: "\${result.queryVariation}"\`)
  })

  return fused.slice(0, topK)
}

async function vectorSearch(query: string, topK: number): Promise<any[]> {
  // Simulated vector search
  return Array.from({ length: topK }, (_, i) => ({
    id: \`doc-\${Math.random().toString(36).substr(2, 9)}\`,
    text: \`Content for "\${query}"\`,
    score: 0.9 - i * 0.05
  }))
}

function deduplicateById(results: any[]): any[] {
  const seen = new Map<string, any>()

  for (const result of results) {
    if (!seen.has(result.id) || result.score > seen.get(result.id).score) {
      seen.set(result.id, result)
    }
  }

  return Array.from(seen.values())
}

function reciprocalRankFusion(resultSets: any[][], k: number = 60): any[] {
  const scores = new Map<string, number>()
  const docs = new Map<string, any>()

  resultSets.forEach(results => {
    results.forEach((doc, rank) => {
      const rrfScore = 1 / (k + rank + 1)
      scores.set(doc.id, (scores.get(doc.id) || 0) + rrfScore)
      if (!docs.has(doc.id)) docs.set(doc.id, doc)
    })
  })

  return Array.from(scores.entries())
    .sort((a, b) => b[1] - a[1])
    .map(([id, score]) => ({
      ...docs.get(id)!,
      score
    }))
}

// Example usage
async function runMultiQuery() {
  const queries = [
    "How to optimize database performance?",
    "What are the differences between OAuth and JWT?",
    "Best practices for error handling in microservices"
  ]

  for (const query of queries) {
    await multiQueryRetrieval(query, 5)
    console.log('\\n' + '─'.repeat(80) + '\\n')
  }
}

runMultiQuery()`}
/>

**Multi-Query Benefits**:
- **Recall improvement**: +25-30% more relevant documents found
- **Phrasing robustness**: Handles different ways users ask the same question
- **Document diversity**: Retrieves from different doc types (technical, conceptual, how-to)

**Cost Consideration**:
```typescript
// Without multi-query: 1 embedding call
const embedding = await getEmbedding(query) // $0.00001

// With multi-query (3 variations): 3 embedding calls
const embeddings = await Promise.all([
  getEmbedding(query1), // $0.00001
  getEmbedding(query2), // $0.00001
  getEmbedding(query3)  // $0.00001
]) // Total: $0.00003 (3x cost, but 30% better recall)

// ROI: Worth it for high-value queries, not for high-volume
```

## Query Expansion with Embeddings

**Strategy 3**: Expand queries semantically using embedding similarity.

<CodePlayground
  title="Semantic Query Expansion"
  description="Expand queries with semantically similar terms using embeddings. Watch vocabulary coverage improve!"
  exerciseType="semantic-expansion"
  code={`import Anthropic from '@anthropic-ai/sdk'

const anthropic = new Anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY
})

interface ExpansionTerm {
  term: string
  similarity: number
  category: string
}

async function expandQuerySemanticall(query: string): Promise<ExpansionTerm[]> {
  console.log(\`Expanding query: "\${query}"\\n\`)

  // Method 1: LLM-based expansion (fast, good quality)
  const response = await anthropic.messages.create({
    model: 'claude-haiku-4-5-20250929',
    max_tokens: 300,
    messages: [{
      role: 'user',
      content: \`Generate synonyms, related terms, and technical variations for this query.

Query: "\${query}"

Categories:
- Synonyms: Different words with same meaning
- Technical: Industry-specific terminology
- Related: Closely related concepts
- Hyponyms: More specific terms

Return JSON:
[
  {"term": "...", "category": "synonym|technical|related|hyponym", "similarity": 0.0-1.0}
]\`
    }]
  })

  const expansions = JSON.parse(response.content[0].text)

  console.log('Expansion terms:\\n')
  const categories = ['synonym', 'technical', 'related', 'hyponym']

  categories.forEach(category => {
    const terms = expansions.filter((e: ExpansionTerm) => e.category === category)
    if (terms.length &gt; 0) {
      console.log(\`\${category.toUpperCase()}:\`)
      terms.forEach((t: ExpansionTerm) => {
        console.log(\`  - \${t.term} (similarity: \${t.similarity.toFixed(2)})\`)
      })
      console.log()
    }
  })

  return expansions
}

// Method 2: Embedding-based expansion using document corpus
async function expandWithCorpusEmbeddings(
  query: string,
  corpus: string[]
): Promise<string[]> {
  console.log('Expanding with corpus embeddings...\\n')

  // Get query embedding
  const queryEmbedding = await getEmbedding(query)

  // Get embeddings for all terms in corpus (typically pre-computed)
  const corpusTerms = extractTerms(corpus)
  const termEmbeddings = await Promise.all(
    corpusTerms.slice(0, 100).map(term => getEmbedding(term)) // Limit for demo
  )

  // Find terms with high similarity to query
  const similarities = termEmbeddings.map((embedding, i) => ({
    term: corpusTerms[i],
    similarity: cosineSimilarity(queryEmbedding, embedding)
  }))

  // Keep terms with similarity &gt; 0.7
  const expansionTerms = similarities
    .filter(s => s.similarity &gt; 0.7 && s.similarity < 0.95) // Not too similar (likely duplicates)
    .sort((a, b) => b.similarity - a.similarity)
    .slice(0, 5)

  console.log('Top expansion terms from corpus:')
  expansionTerms.forEach(t => {
    console.log(\`  - \${t.term} (similarity: \${t.similarity.toFixed(3)})\`)
  })
  console.log()

  return expansionTerms.map(t => t.term)
}

function extractTerms(corpus: string[]): string[] {
  // Extract noun phrases, technical terms
  // Simplified: just extract 2-3 word phrases
  const terms = new Set<string>()

  corpus.forEach(doc => {
    const words = doc.toLowerCase().split(/\\s+/)
    for (let i = 0; i < words.length - 2; i++) {
      terms.add(\`\${words[i]} \${words[i + 1]}\`)
      terms.add(\`\${words[i]} \${words[i + 1]} \${words[i + 2]}\`)
    }
  })

  return Array.from(terms)
}

async function getEmbedding(text: string): Promise<number[]> {
  const response = await fetch('https://api.openai.com/v1/embeddings', {
    method: 'POST',
    headers: {
      'Authorization': \`Bearer \${process.env.OPENAI_API_KEY}\`,
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({
      input: text,
      model: 'text-embedding-3-small'
    })
  })
  const data = await response.json()
  return data.data[0].embedding
}

function cosineSimilarity(a: number[], b: number[]): number {
  let dotProduct = 0
  let normA = 0
  let normB = 0

  for (let i = 0; i < a.length; i++) {
    dotProduct += a[i] * b[i]
    normA += a[i] * a[i]
    normB += b[i] * b[i]
  }

  return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB))
}

// Example usage
async function runExpansion() {
  const query = "optimize database queries"

  // Method 1: LLM expansion
  const llmExpansions = await expandQuerySemanticall(query)

  // Method 2: Corpus-based expansion
  const corpus = [
    "database performance tuning and query optimization",
    "SQL query execution plan analysis",
    "index optimization for faster queries",
    "database connection pooling strategies"
  ]

  const corpusExpansions = await expandWithCorpusEmbeddings(query, corpus)

  console.log('Combined expansion strategy:')
  console.log('  Original:', query)
  console.log('  + LLM expansions:', llmExpansions.map(e => e.term).join(', '))
  console.log('  + Corpus expansions:', corpusExpansions.join(', '))
}

runExpansion()`}
/>

**Expansion ROI**:
```typescript
interface ExpansionMetrics {
  method: string
  recallImprovement: number
  cost: number
  latency: number
}

const expansionMethods: ExpansionMetrics[] = [
  {
    method: 'No expansion (baseline)',
    recallImprovement: 0,
    cost: 0,
    latency: 0
  },
  {
    method: 'LLM expansion (Haiku)',
    recallImprovement: 0.22, // +22% recall
    cost: 0.00003,
    latency: 100
  },
  {
    method: 'Corpus embedding expansion',
    recallImprovement: 0.18, // +18% recall
    cost: 0.00001, // Pre-computed embeddings
    latency: 50
  },
  {
    method: 'Combined (LLM + Corpus)',
    recallImprovement: 0.28, // +28% recall
    cost: 0.00004,
    latency: 120
  }
]

console.log('Expansion Method Comparison:')
expansionMethods.forEach(m => {
  console.log(\`\${m.method}:\`)
  console.log(\`  Recall improvement: +\${(m.recallImprovement * 100).toFixed(0)}%\`)
  console.log(\`  Cost per query: $\${m.cost.toFixed(5)}\`)
  console.log(\`  Latency: \${m.latency}ms\`)
  console.log()
})
```

## Cost Optimization Strategies

**Strategy 4**: Reduce costs with caching and smart routing.

<CodePlayground
  title="Cost-Optimized Query Routing"
  description="Route queries to appropriate optimization strategies based on cost/benefit analysis. Watch savings!"
  exerciseType="cost-optimization"
  code={`interface QueryProfile {
  complexity: 'simple' | 'medium' | 'complex'
  ambiguity: 'clear' | 'ambiguous'
  frequency: 'common' | 'rare'
}

interface OptimizationStrategy {
  name: string
  cost: number
  accuracyBoost: number
  latency: number
}

function routeQuery(query: string, profile: QueryProfile): OptimizationStrategy {
  // Route based on query characteristics

  if (profile.frequency === 'common') {
    // Common queries: Use cache (free, instant)
    return {
      name: 'Cache lookup',
      cost: 0,
      accuracyBoost: 0,
      latency: 5
    }
  }

  if (profile.complexity === 'simple' && profile.ambiguity === 'clear') {
    // Simple, clear queries: No optimization needed
    return {
      name: 'Direct retrieval',
      cost: 0.0001,
      accuracyBoost: 0,
      latency: 100
    }
  }

  if (profile.complexity === 'complex') {
    // Complex queries: Full decomposition
    return {
      name: 'Query decomposition + multi-query',
      cost: 0.0015,
      accuracyBoost: 0.35,
      latency: 400
    }
  }

  if (profile.ambiguity === 'ambiguous') {
    // Ambiguous queries: Rewrite + expand
    return {
      name: 'Rewrite + expansion',
      cost: 0.0008,
      accuracyBoost: 0.25,
      latency: 250
    }
  }

  // Default: Basic multi-query
  return {
    name: 'Multi-query generation',
    cost: 0.0003,
    accuracyBoost: 0.15,
    latency: 150
  }
}

function profileQuery(query: string): QueryProfile {
  // Classify query characteristics

  const wordCount = query.split(/\\s+/).length

  const complexity: 'simple' | 'medium' | 'complex' =
    wordCount < 5 ? 'simple' :
    wordCount < 15 ? 'medium' : 'complex'

  const hasVagueTerms = /\\b(it|that|this|stuff|thing)\\b/i.test(query)
  const ambiguity: 'clear' | 'ambiguous' = hasVagueTerms ? 'ambiguous' : 'clear'

  // In production, check cache hit rate
  const frequency: 'common' | 'rare' = 'rare' // Simplified

  return { complexity, ambiguity, frequency }
}

// Example queries with cost analysis
async function analyzeCostOptimization() {
  const testQueries = [
    {
      query: "HIPAA compliance",
      expectedHits: 1000 // Common query
    },
    {
      query: "how to make it faster",
      expectedHits: 10 // Ambiguous
    },
    {
      query: "What are the HIPAA encryption requirements and how do they compare to GDPR data protection rules for healthcare?",
      expectedHits: 5 // Complex
    },
    {
      query: "API rate limiting RFC 6585",
      expectedHits: 50 // Clear, simple
    }
  ]

  console.log('Cost Optimization Analysis:\\n')

  let totalCost = 0
  let totalLatency = 0

  testQueries.forEach(({ query, expectedHits }) => {
    const profile = profileQuery(query)
    const strategy = routeQuery(query, profile)

    const monthlyCost = strategy.cost * expectedHits

    console.log(\`Query: "\${query.substring(0, 60)}..."\`)
    console.log(\`  Profile: \${profile.complexity}, \${profile.ambiguity}, \${profile.frequency}\`)
    console.log(\`  Strategy: \${strategy.name}\`)
    console.log(\`  Cost per query: $\${strategy.cost.toFixed(5)}\`)
    console.log(\`  Monthly cost (\${expectedHits} queries): $\${monthlyCost.toFixed(2)}\`)
    console.log(\`  Accuracy boost: +\${(strategy.accuracyBoost * 100).toFixed(0)}%\`)
    console.log(\`  Latency: \${strategy.latency}ms\`)
    console.log()

    totalCost += monthlyCost
    totalLatency += strategy.latency
  })

  console.log(\`Total monthly cost: $\${totalCost.toFixed(2)}\`)
  console.log(\`Average latency: \${(totalLatency / testQueries.length).toFixed(0)}ms\`)
}

analyzeCostOptimization()`}
/>

**Cost Optimization Techniques**:

1. **Query caching** (80% cost savings for common queries)
```typescript
const cacheKey = `query:${hash(query)}`
const cached = await redis.get(cacheKey)
if (cached) return JSON.parse(cached) // $0 cost, 5ms latency

const result = await optimizeAndRetrieve(query)
await redis.set(cacheKey, JSON.stringify(result), { EX: 3600 })
```

2. **Adaptive optimization** (50% cost savings overall)
```typescript
if (isSimpleQuery(query)) {
  return directRetrieval(query) // $0.0001
} else {
  return fullOptimization(query) // $0.0015
}
```

3. **Batch processing** (40% latency savings)
```typescript
// Process multiple queries in single LLM call
const optimized = await batchOptimize([query1, query2, query3])
// 1 LLM call instead of 3
```

## Production Metrics

### Cost Comparison (10,000 queries/month)

```
┌─────────────────────────┬──────────┬─────────────┐
│ Strategy                │ Cost/Q   │ Monthly     │
├─────────────────────────┼──────────┼─────────────┤
│ No optimization         │ $0.0001  │ $1.00       │
│ Query rewriting         │ $0.0003  │ $3.00       │
│ Multi-query generation  │ $0.0005  │ $5.00       │
│ Full optimization       │ $0.0015  │ $15.00      │
│ Adaptive (recommended)  │ $0.0004  │ $4.00       │
└─────────────────────────┴──────────┴─────────────┘

ROI: +35% accuracy for +$3/month = Excellent
```

## Common Pitfalls

### 1. Over-Optimization
**Problem**: Optimizing every query wastes money
```typescript
// ❌ Bad: Full optimization for simple queries
const optimized = await decompose(await expand(await rewrite("API docs")))
// Cost: $0.0015, benefit: +2% accuracy

// ✅ Good: Route based on complexity
const result = isSimple(query) ? direct(query) : optimize(query)
```

### 2. Ignoring Cache Misses
**Problem**: Cache lookups that always miss add latency
```typescript
// ✅ Good: Monitor cache hit rate, disable if &lt;20%
if (cacheHitRate < 0.2) {
  // Skip cache, go direct to optimization
}
```

### 3. Not Deduplicating Multi-Query Results
**Problem**: Duplicate documents inflate costs
```typescript
// ❌ Bad: 3 queries × 10 results = 30 documents to process
const results = queries.map(q => retrieve(q)) // No dedup

// ✅ Good: Deduplicate before context fusion
const results = deduplicate(queries.map(q => retrieve(q))) // 15 unique docs
```

## Key Takeaways

### When to Use Query Optimization
- ✅ User queries are vague or ambiguous
- ✅ Complex multi-part questions
- ✅ Accuracy is critical (legal, medical)
- ❌ Simple lookups with clear intent
- ❌ Ultra-high volume (&gt;1M queries/day) without caching

### Architecture Decisions
- **Routing**: Adaptive based on query complexity
- **Decomposition**: Complex queries only
- **Multi-query**: Ambiguous queries or high-recall scenarios
- **Expansion**: Technical domains with specific jargon
- **Caching**: Top 20% of queries (80% of traffic)

### Production Metrics
- **Accuracy improvement**: +15-35% depending on strategy
- **Cost**: $0.0004 per query (adaptive routing)
- **Latency**: 150-400ms (parallelizable)
- **Cache hit rate**: 60-80% for production systems

### Best Practices
- Profile queries before optimizing (simple vs complex)
- Cache optimized queries for 1 hour
- Use Haiku for query generation (3x cheaper than Sonnet)
- Parallelize multi-query retrieval
- Monitor ROI: accuracy improvement vs cost increase

## Further Reading

- [Query Understanding at Scale](https://arxiv.org/abs/2104.08663) - Google research
- [Multi-Query Retrieval](https://arxiv.org/abs/2305.14283) - RAGatouille paper
- [Pinecone Query Understanding](https://www.pinecone.io/learn/query-understanding/) - Production guide
- [Anthropic Prompt Engineering](https://docs.anthropic.com/claude/docs/prompt-engineering) - Query optimization with Claude
- [Semantic Query Expansion](https://aclanthology.org/2022.naacl-main.349/) - NAACL 2022 paper
