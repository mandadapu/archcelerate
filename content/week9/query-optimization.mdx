---
title: "Query Optimization & Expansion"
description: "Transform vague user queries into optimized retrieval strategies with decomposition, multi-query generation, and cost-aware expansion"
estimatedMinutes: 45
week: 9
concept: 5
difficulty: intermediate
objectives:
  - Decompose complex queries into sub-queries for better retrieval
  - Generate multiple query variations for comprehensive coverage
  - Expand queries with semantic embeddings for recall improvement
  - Optimize costs with caching and query routing strategies
---

import { CodePlayground } from '@/components/curriculum/CodePlayground'

# Query Optimization & Expansion

Transform ambiguous user queries into precise retrieval strategies that maximize answer quality while minimizing cost and latency.

> **Note**: Query optimization is the highest-ROI technique in RAG systems - improving both accuracy (15-25%) and cost efficiency (30-50%) with minimal infrastructure changes.

## What is Query Optimization?

**Simple Explanation**: Query optimization transforms vague or poorly-worded user questions into better search queries that retrieval systems can understand. It's like having a librarian who understands what you really mean and knows exactly which books to pull.

**The Problem**:
```
User query: "how to make it faster"

❌ Poor retrieval:
- Returns generic "speed optimization" articles
- Misses specific context (What needs to be faster? Code? Database? API?)
- Low precision: 45% relevant results

✅ Optimized query:
→ Decompose: "What needs optimization?"
→ Expand: "code performance optimization", "reduce execution time", "improve latency"
→ Retrieve: 85% relevant results
```

## Why Query Optimization Matters

### Real-World Impact

**Example 1: Vague Technical Query**
```
Original: "error with login"
Problems:
- Which error? (401, 500, timeout?)
- Which login? (web, mobile, API?)
- What context? (after deployment, specific user?)

Optimized:
→ "authentication error 401 unauthorized web login"
→ "login timeout connection issues mobile app"
→ "OAuth token expiration login failure"

Result: 3x more relevant results
```

**Example 2: Medical Query**
```
Original: "treatment for diabetes"
Problems:
- Type 1 or Type 2?
- What stage?
- Drug therapy or lifestyle?

Optimized:
→ "Type 2 diabetes medication metformin dosage"
→ "prediabetes lifestyle interventions diet exercise"
→ "insulin therapy Type 1 diabetes management"

Result: Precision 65% → 91%
```

**Cost Savings**:
- **Without optimization**: Retrieve 50 documents, 80% irrelevant = wasted tokens
- **With optimization**: Retrieve 20 documents, 85% relevant = 60% cost reduction

**Industry Benchmarks**:
- Query rewriting: +15-20% accuracy improvement
- Multi-query generation: +25-30% coverage (recall)
- Combined optimization: +35-40% overall performance

## Query Decomposition

**Strategy 1**: Break complex queries into simpler sub-queries.

<CodePlayground
  title="Query Decomposition"
  description="Decompose complex multi-part questions into sub-queries. Try different complex queries to see the breakdown!"
  exerciseType="query-decomposition"
  code={`import Anthropic from '@anthropic-ai/sdk'

const anthropic = new Anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY
})

interface SubQuery {
  query: string
  reasoning: string
  priority: number
}

interface DecompositionResult {
  originalQuery: string
  subQueries: SubQuery[]
  strategy: string
}

async function decomposeQuery(query: string): Promise<DecompositionResult> {
  console.log(\`Decomposing query: "\${query}"\\n\`)

  const response = await anthropic.messages.create({
    model: 'claude-sonnet-4-5-20251101',
    max_tokens: 1000,
    messages: [{
      role: 'user',
      content: \`Decompose this complex query into simpler sub-queries that can be answered independently.

RULES:
- Each sub-query should be self-contained
- Order by priority (most important first)
- Include reasoning for why each sub-query is needed
- Return JSON only

Query: "\${query}"

Return JSON:
{
  "strategy": "parallel | sequential | hybrid",
  "subQueries": [
    {"query": "...", "reasoning": "...", "priority": 1}
  ]
}\`
    }]
  })

  const result = JSON.parse(response.content[0].text)

  console.log('Decomposition Strategy:', result.strategy)
  console.log(\`Generated \${result.subQueries.length} sub-queries:\\n\`)

  result.subQueries.forEach((sq: SubQuery, i: number) => {
    console.log(\`\${i + 1}. [Priority \${sq.priority}] \${sq.query}\`)
    console.log(\`   Reasoning: \${sq.reasoning}\`)
    console.log()
  })

  return {
    originalQuery: query,
    subQueries: result.subQueries,
    strategy: result.strategy
  }
}

// Execute sub-queries and merge results
async function executeDecomposedQuery(
  decomposition: DecompositionResult
): Promise<any[]> {
  const { subQueries, strategy } = decomposition

  if (strategy === 'parallel') {
    // Execute all sub-queries simultaneously
    console.log('Executing sub-queries in parallel...\\n')

    const results = await Promise.all(
      subQueries.map(sq => retrieveForSubQuery(sq.query))
    )

    // Merge and deduplicate results
    const merged = mergeResults(results)
    console.log(\`Merged to \${merged.length} unique results\`)

    return merged
  } else if (strategy === 'sequential') {
    // Execute sub-queries in order, using previous results as context
    console.log('Executing sub-queries sequentially...\\n')

    const allResults: any[] = []

    for (const sq of subQueries) {
      const results = await retrieveForSubQuery(sq.query, allResults)
      allResults.push(...results)
      console.log(\`  Completed: \${sq.query} (\${results.length} results)\`)
    }

    return allResults
  } else {
    // Hybrid: critical queries first, then parallel
    console.log('Executing hybrid strategy...\\n')

    const criticalQueries = subQueries.filter(sq => sq.priority === 1)
    const otherQueries = subQueries.filter(sq => sq.priority &gt; 1)

    // Execute critical queries sequentially
    const criticalResults: any[] = []
    for (const sq of criticalQueries) {
      const results = await retrieveForSubQuery(sq.query)
      criticalResults.push(...results)
    }

    // Execute remaining queries in parallel
    const otherResults = await Promise.all(
      otherQueries.map(sq => retrieveForSubQuery(sq.query))
    )

    return [...criticalResults, ...otherResults.flat()]
  }
}

async function retrieveForSubQuery(query: string, context?: any[]): Promise<any[]> {
  // Simulated retrieval (replace with actual vector search)
  return [
    { id: \`doc-\${Math.random()}\`, text: \`Result for: \${query}\`, score: 0.85 }
  ]
}

function mergeResults(resultSets: any[][]): any[] {
  const seen = new Set<string>()
  const merged: any[] = []

  for (const results of resultSets) {
    for (const result of results) {
      if (!seen.has(result.id)) {
        seen.add(result.id)
        merged.push(result)
      }
    }
  }

  return merged
}

// Example usage
async function runDecomposition() {
  const complexQueries = [
    "What are the HIPAA requirements for data encryption and how do they compare to GDPR?",
    "How to implement authentication and authorization for a REST API with rate limiting?",
    "What caused the performance regression in v2.3 and how was it fixed in v2.4?"
  ]

  for (const query of complexQueries) {
    const decomposition = await decomposeQuery(query)
    const results = await executeDecomposedQuery(decomposition)

    console.log(\`Final result count: \${results.length}\\n\`)
    console.log('─'.repeat(80))
    console.log()
  }
}

runDecomposition()`}
/>

**Decomposition Strategies**:

| Strategy | When to Use | Example | Latency | Accuracy |
|----------|-------------|---------|---------|----------|
| **Parallel** | Independent sub-queries | "HIPAA vs GDPR" | Low (200ms) | Good |
| **Sequential** | Dependent sub-queries | "What → Why → How" | High (600ms) | Excellent |
| **Hybrid** | Mix of both | Critical first, rest parallel | Medium (350ms) | Very Good |

## Multi-Query Generation

**Strategy 2**: Generate multiple variations of the same query for better coverage.

<CodePlayground
  title="Multi-Query Generation"
  description="Generate diverse query variations to cover different phrasings. Watch recall improvement!"
  exerciseType="multi-query-generation"
  code={`import Anthropic from '@anthropic-ai/sdk'

const anthropic = new Anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY
})

interface QueryVariation {
  query: string
  perspective: string
  targetDocType: string
}

async function generateMultipleQueries(
  originalQuery: string,
  numVariations: number = 3
): Promise<QueryVariation[]> {
  console.log(\`Generating \${numVariations} query variations for: "\${originalQuery}"\\n\`)

  const response = await anthropic.messages.create({
    model: 'claude-haiku-4-5-20250929', // Haiku for cost efficiency
    max_tokens: 500,
    messages: [{
      role: 'user',
      content: \`Generate \${numVariations} different search queries that capture different aspects of this question.

Each variation should:
- Use different phrasing and keywords
- Target different types of documents (technical, conceptual, practical)
- Cover different perspectives on the topic

Original query: "\${originalQuery}"

Return JSON array:
[
  {"query": "...", "perspective": "...", "targetDocType": "..."}
]\`
    }]
  })

  const variations = JSON.parse(response.content[0].text)

  console.log('Generated variations:\\n')
  variations.forEach((v: QueryVariation, i: number) => {
    console.log(\`\${i + 1}. \${v.query}\`)
    console.log(\`   Perspective: \${v.perspective}\`)
    console.log(\`   Targets: \${v.targetDocType}\`)
    console.log()
  })

  return variations
}

// Retrieve with multiple queries and fuse results
async function multiQueryRetrieval(
  originalQuery: string,
  topK: number = 5
): Promise<any[]> {
  // Generate query variations
  const variations = await generateMultipleQueries(originalQuery)

  console.log('Retrieving with all query variations...\\n')

  // Retrieve with each variation
  const allResults = await Promise.all(
    variations.map(async v => {
      const results = await vectorSearch(v.query, topK * 2) // Get more to account for overlap
      return results.map(r => ({
        ...r,
        queryVariation: v.query,
        perspective: v.perspective
      }))
    })
  )

  // Flatten and deduplicate
  const flat = allResults.flat()
  const deduplicated = deduplicateById(flat)

  console.log(\`Retrieved \${flat.length} total results\`)
  console.log(\`After deduplication: \${deduplicated.length} unique results\\n\`)

  // Reciprocal Rank Fusion to combine rankings
  const fused = reciprocalRankFusion(allResults)

  console.log(\`Top \${topK} after RRF fusion:\\n\`)
  fused.slice(0, topK).forEach((result, i) => {
    console.log(\`\${i + 1}. \${result.id} (score: \${result.score.toFixed(4)})\`)
    console.log(\`   From query: "\${result.queryVariation}"\`)
  })

  return fused.slice(0, topK)
}

async function vectorSearch(query: string, topK: number): Promise<any[]> {
  // Simulated vector search
  return Array.from({ length: topK }, (_, i) => ({
    id: \`doc-\${Math.random().toString(36).substr(2, 9)}\`,
    text: \`Content for "\${query}"\`,
    score: 0.9 - i * 0.05
  }))
}

function deduplicateById(results: any[]): any[] {
  const seen = new Map<string, any>()

  for (const result of results) {
    if (!seen.has(result.id) || result.score > seen.get(result.id).score) {
      seen.set(result.id, result)
    }
  }

  return Array.from(seen.values())
}

function reciprocalRankFusion(resultSets: any[][], k: number = 60): any[] {
  const scores = new Map<string, number>()
  const docs = new Map<string, any>()

  resultSets.forEach(results => {
    results.forEach((doc, rank) => {
      const rrfScore = 1 / (k + rank + 1)
      scores.set(doc.id, (scores.get(doc.id) || 0) + rrfScore)
      if (!docs.has(doc.id)) docs.set(doc.id, doc)
    })
  })

  return Array.from(scores.entries())
    .sort((a, b) => b[1] - a[1])
    .map(([id, score]) => ({
      ...docs.get(id)!,
      score
    }))
}

// Example usage
async function runMultiQuery() {
  const queries = [
    "How to optimize database performance?",
    "What are the differences between OAuth and JWT?",
    "Best practices for error handling in microservices"
  ]

  for (const query of queries) {
    await multiQueryRetrieval(query, 5)
    console.log('\\n' + '─'.repeat(80) + '\\n')
  }
}

runMultiQuery()`}
/>

**Multi-Query Benefits**:
- **Recall improvement**: +25-30% more relevant documents found
- **Phrasing robustness**: Handles different ways users ask the same question
- **Document diversity**: Retrieves from different doc types (technical, conceptual, how-to)

**Cost Consideration**:
```typescript
// Without multi-query: 1 embedding call
const embedding = await getEmbedding(query) // $0.00001

// With multi-query (3 variations): 3 embedding calls
const embeddings = await Promise.all([
  getEmbedding(query1), // $0.00001
  getEmbedding(query2), // $0.00001
  getEmbedding(query3)  // $0.00001
]) // Total: $0.00003 (3x cost, but 30% better recall)

// ROI: Worth it for high-value queries, not for high-volume
```

**Architect's Tip — Relevance-Weighted Fusion**: "When you generate 5 queries from 1 user question, you risk retrieving 'noise' that dilutes the 'signal' of the original intent — this is **Context Poisoning**. An Architect doesn't just deduplicate; they **rank-aggregate with intent weighting**. Assign a higher weight to results from the **Original Query** and use expanded query results only as supporting context. This ensures Multi-Query Expansion improves recall without sacrificing the precision of the primary intent."

```typescript
/**
 * Relevance-Weighted Fusion
 *
 * Problem: Standard RRF treats all query variations equally. But the
 * original query IS the user's intent — expanded queries are guesses.
 * Equal weighting can dilute precision with tangential results.
 *
 * Solution: Weight the original query's results 2-3x higher than
 * expanded queries. Use expanded results as "supporting evidence"
 * that boosts scores, not replaces the primary signal.
 *
 * Interview Defense: "We weight the original query at 2.5x because
 * it represents verified user intent. Expanded queries improve recall
 * but shouldn't override what the user actually asked."
 */

interface WeightedQuerySource {
  query: string
  results: Array<{ id: string; score: number; text: string }>
  weight: number  // 1.0 = baseline, 2.5 = original query boost
  source: 'original' | 'expanded' | 'decomposed'
}

function relevanceWeightedFusion(
  sources: WeightedQuerySource[],
  k: number = 60
): Array<{ id: string; score: number; text: string; contributingSources: string[] }> {
  const scores = new Map<string, number>()
  const docs = new Map<string, any>()
  const contributors = new Map<string, string[]>()

  for (const source of sources) {
    for (let rank = 0; rank < source.results.length; rank++) {
      const doc = source.results[rank]

      // RRF with source weighting: original query results score higher
      const rrfScore = source.weight * (1 / (k + rank + 1))

      scores.set(doc.id, (scores.get(doc.id) || 0) + rrfScore)
      if (!docs.has(doc.id)) docs.set(doc.id, doc)

      const existing = contributors.get(doc.id) || []
      existing.push(source.source)
      contributors.set(doc.id, existing)
    }
  }

  return Array.from(scores.entries())
    .sort((a, b) => b[1] - a[1])
    .map(([id, score]) => ({
      ...docs.get(id),
      score,
      contributingSources: contributors.get(id) || []
    }))
}

// Usage: Original query gets 2.5x weight
//
// const sources: WeightedQuerySource[] = [
//   { query: originalQuery, results: originalResults, weight: 2.5, source: 'original' },
//   { query: expanded1,     results: expanded1Results, weight: 1.0, source: 'expanded' },
//   { query: expanded2,     results: expanded2Results, weight: 1.0, source: 'expanded' },
// ]
//
// Production Impact:
// | Fusion Method     | Precision | Recall | F1    |
// |-------------------|-----------|--------|-------|
// | No fusion         | 72%       | 45%    | 0.55  |
// | Equal-weight RRF  | 65%       | 78%    | 0.71  |
// | Weighted RRF (2.5x) | 74%    | 76%    | 0.75  |
//
// Weighted fusion preserves precision (+9% vs equal RRF)
// while maintaining nearly the same recall (-2%)
```

## Query Expansion with Embeddings

**Strategy 3**: Expand queries semantically using embedding similarity.

<CodePlayground
  title="Semantic Query Expansion"
  description="Expand queries with semantically similar terms using embeddings. Watch vocabulary coverage improve!"
  exerciseType="semantic-expansion"
  code={`import Anthropic from '@anthropic-ai/sdk'

const anthropic = new Anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY
})

interface ExpansionTerm {
  term: string
  similarity: number
  category: string
}

async function expandQuerySemanticall(query: string): Promise<ExpansionTerm[]> {
  console.log(\`Expanding query: "\${query}"\\n\`)

  // Method 1: LLM-based expansion (fast, good quality)
  const response = await anthropic.messages.create({
    model: 'claude-haiku-4-5-20250929',
    max_tokens: 300,
    messages: [{
      role: 'user',
      content: \`Generate synonyms, related terms, and technical variations for this query.

Query: "\${query}"

Categories:
- Synonyms: Different words with same meaning
- Technical: Industry-specific terminology
- Related: Closely related concepts
- Hyponyms: More specific terms

Return JSON:
[
  {"term": "...", "category": "synonym|technical|related|hyponym", "similarity": 0.0-1.0}
]\`
    }]
  })

  const expansions = JSON.parse(response.content[0].text)

  console.log('Expansion terms:\\n')
  const categories = ['synonym', 'technical', 'related', 'hyponym']

  categories.forEach(category => {
    const terms = expansions.filter((e: ExpansionTerm) => e.category === category)
    if (terms.length &gt; 0) {
      console.log(\`\${category.toUpperCase()}:\`)
      terms.forEach((t: ExpansionTerm) => {
        console.log(\`  - \${t.term} (similarity: \${t.similarity.toFixed(2)})\`)
      })
      console.log()
    }
  })

  return expansions
}

// Method 2: Embedding-based expansion using document corpus
async function expandWithCorpusEmbeddings(
  query: string,
  corpus: string[]
): Promise<string[]> {
  console.log('Expanding with corpus embeddings...\\n')

  // Get query embedding
  const queryEmbedding = await getEmbedding(query)

  // Get embeddings for all terms in corpus (typically pre-computed)
  const corpusTerms = extractTerms(corpus)
  const termEmbeddings = await Promise.all(
    corpusTerms.slice(0, 100).map(term => getEmbedding(term)) // Limit for demo
  )

  // Find terms with high similarity to query
  const similarities = termEmbeddings.map((embedding, i) => ({
    term: corpusTerms[i],
    similarity: cosineSimilarity(queryEmbedding, embedding)
  }))

  // Keep terms with similarity &gt; 0.7
  const expansionTerms = similarities
    .filter(s => s.similarity &gt; 0.7 && s.similarity &lt; 0.95) // Not too similar (likely duplicates)
    .sort((a, b) => b.similarity - a.similarity)
    .slice(0, 5)

  console.log('Top expansion terms from corpus:')
  expansionTerms.forEach(t => {
    console.log(\`  - \${t.term} (similarity: \${t.similarity.toFixed(3)})\`)
  })
  console.log()

  return expansionTerms.map(t => t.term)
}

function extractTerms(corpus: string[]): string[] {
  // Extract noun phrases, technical terms
  // Simplified: just extract 2-3 word phrases
  const terms = new Set<string>()

  corpus.forEach(doc => {
    const words = doc.toLowerCase().split(/\\s+/)
    for (let i = 0; i < words.length - 2; i++) {
      terms.add(\`\${words[i]} \${words[i + 1]}\`)
      terms.add(\`\${words[i]} \${words[i + 1]} \${words[i + 2]}\`)
    }
  })

  return Array.from(terms)
}

async function getEmbedding(text: string): Promise<number[]> {
  const response = await fetch('https://api.openai.com/v1/embeddings', {
    method: 'POST',
    headers: {
      'Authorization': \`Bearer \${process.env.OPENAI_API_KEY}\`,
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({
      input: text,
      model: 'text-embedding-3-small'
    })
  })
  const data = await response.json()
  return data.data[0].embedding
}

function cosineSimilarity(a: number[], b: number[]): number {
  let dotProduct = 0
  let normA = 0
  let normB = 0

  for (let i = 0; i < a.length; i++) {
    dotProduct += a[i] * b[i]
    normA += a[i] * a[i]
    normB += b[i] * b[i]
  }

  return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB))
}

// Example usage
async function runExpansion() {
  const query = "optimize database queries"

  // Method 1: LLM expansion
  const llmExpansions = await expandQuerySemanticall(query)

  // Method 2: Corpus-based expansion
  const corpus = [
    "database performance tuning and query optimization",
    "SQL query execution plan analysis",
    "index optimization for faster queries",
    "database connection pooling strategies"
  ]

  const corpusExpansions = await expandWithCorpusEmbeddings(query, corpus)

  console.log('Combined expansion strategy:')
  console.log('  Original:', query)
  console.log('  + LLM expansions:', llmExpansions.map(e => e.term).join(', '))
  console.log('  + Corpus expansions:', corpusExpansions.join(', '))
}

runExpansion()`}
/>

**Expansion ROI**:
```typescript
interface ExpansionMetrics {
  method: string
  recallImprovement: number
  cost: number
  latency: number
}

const expansionMethods: ExpansionMetrics[] = [
  {
    method: 'No expansion (baseline)',
    recallImprovement: 0,
    cost: 0,
    latency: 0
  },
  {
    method: 'LLM expansion (Haiku)',
    recallImprovement: 0.22, // +22% recall
    cost: 0.00003,
    latency: 100
  },
  {
    method: 'Corpus embedding expansion',
    recallImprovement: 0.18, // +18% recall
    cost: 0.00001, // Pre-computed embeddings
    latency: 50
  },
  {
    method: 'Combined (LLM + Corpus)',
    recallImprovement: 0.28, // +28% recall
    cost: 0.00004,
    latency: 120
  }
]

console.log('Expansion Method Comparison:')
expansionMethods.forEach(m => {
  console.log(\`\${m.method}:\`)
  console.log(\`  Recall improvement: +\${(m.recallImprovement * 100).toFixed(0)}%\`)
  console.log(\`  Cost per query: $\${m.cost.toFixed(5)}\`)
  console.log(\`  Latency: \${m.latency}ms\`)
  console.log()
})
```

### HyDE (Hypothetical Document Embeddings)

**Strategy 4**: Generate a hypothetical answer to the user's query, then use that answer's embedding to search — bridging the vocabulary gap between user language and corpus terminology.

**Architect's Tip — Similarity-to-Ground-Truth Verification**: "HyDE is a **Translation layer** between user-speak and expert-speak. When a lawyer asks 'Can my landlord kick me out?', HyDE generates a hypothetical statute using terms like 'unlawful detainer' and 'notice to quit', which matches real statutes far better than the original query. But HyDE is a **double-edged sword**: if the generated hypothetical document has very low similarity to your actual corpus, the 'Translator' has **hallucinated** and will lead the search into a ghost chase. An Architect implements a **Similarity Guardrail**: if the hypothetical document's similarity to the top 3 real documents falls below a threshold, discard the HyDE results and fall back to the original query or Query Decomposition."

```typescript
/**
 * HyDE with Similarity Guardrail
 *
 * Problem: HyDE generates a "fake" answer to use as a search query.
 * If the LLM hallucinate a wrong answer, the embedding will search
 * for documents that match the hallucination, not the truth.
 *
 * Solution: After HyDE retrieval, compare the hypothetical document's
 * embedding to the top retrieved documents. If similarity is below
 * threshold, the hypothetical was wrong — fall back to original query.
 *
 * Interview Defense: "HyDE improved our legal search precision from
 * 38% to 89% by translating plain language into legal terminology.
 * But we added a similarity guardrail: if the hypothetical document
 * scores below 0.65 against corpus results, we fall back to
 * decomposition. This prevents 'hallucinated retrieval' from
 * poisoning the context window."
 */

interface HyDEConfig {
  similarityThreshold: number  // Minimum similarity to corpus (0.0-1.0)
  fallbackStrategy: 'original' | 'decomposition'
  model: string                // Model for hypothesis generation
}

const HYDE_DOMAIN_THRESHOLDS: Record<string, number> = {
  legal: 0.60,      // Legal jargon varies widely, lower threshold
  medical: 0.65,    // Medical terminology is precise
  technical: 0.70,  // Code/API docs are very specific
  general: 0.55     // General knowledge, most lenient
}

async function hydeWithGuardrail(
  query: string,
  config: HyDEConfig = {
    similarityThreshold: 0.65,
    fallbackStrategy: 'decomposition',
    model: 'claude-haiku-4-5-20250929'
  }
): Promise<{ results: any[]; method: 'hyde' | 'fallback'; similarity: number }> {

  // Step 1: Generate hypothetical document
  const hydeResponse = await anthropic.messages.create({
    model: config.model,
    max_tokens: 300,
    messages: [{
      role: 'user',
      content: `Write a short, factual paragraph that would perfectly answer this question.
Write as if you are quoting from an authoritative source document.

Question: "${query}"

Paragraph:`
    }]
  })

  const hypotheticalDoc = hydeResponse.content[0].text

  // Step 2: Search using hypothetical document's embedding
  const hydeEmbedding = await getEmbedding(hypotheticalDoc)
  const hydeResults = await vectorSearch(hydeEmbedding, 10)

  // Step 3: GUARDRAIL — Check similarity between hypothesis and real results
  const topResults = hydeResults.slice(0, 3)
  const avgSimilarity = topResults.reduce((sum, r) => sum + r.score, 0) / topResults.length

  if (avgSimilarity < config.similarityThreshold) {
    // ❌ HyDE hallucinated — hypothesis doesn't match real corpus
    console.warn(
      `⚠️ HyDE guardrail triggered: similarity ${avgSimilarity.toFixed(3)} < threshold ${config.similarityThreshold}`
    )
    console.warn(`   Falling back to: ${config.fallbackStrategy}`)

    // Fall back to original query or decomposition
    if (config.fallbackStrategy === 'decomposition') {
      const decomposed = await decomposeQuery(query)
      const fallbackResults = await executeDecomposedQuery(decomposed)
      return { results: fallbackResults, method: 'fallback', similarity: avgSimilarity }
    }

    const originalEmbedding = await getEmbedding(query)
    const originalResults = await vectorSearch(originalEmbedding, 10)
    return { results: originalResults, method: 'fallback', similarity: avgSimilarity }
  }

  // ✅ HyDE hypothesis matches corpus — use HyDE results
  console.log(`✅ HyDE passed guardrail: similarity ${avgSimilarity.toFixed(3)} >= ${config.similarityThreshold}`)
  return { results: hydeResults, method: 'hyde', similarity: avgSimilarity }
}

// Domain-specific threshold calibration:
//
// | Domain    | Threshold | Rationale                          |
// |-----------|-----------|-------------------------------------|
// | Legal     | 0.60      | Legal language varies by jurisdiction |
// | Medical   | 0.65      | Medical terminology is standardized  |
// | Technical | 0.70      | Code/API docs have exact terminology |
// | General   | 0.55      | Broad topics, flexible matching      |
//
// Production Impact (Legal Search):
// | Method            | Precision | Recall | Hallucinated Results |
// |-------------------|-----------|--------|----------------------|
// | Original query    | 38%       | 52%    | 0%                   |
// | HyDE (no guard)   | 89%       | 85%    | 12% (dangerous)      |
// | HyDE + guardrail  | 87%       | 83%    | 0.3% (safe)          |
//
// The guardrail reduces HyDE precision by only 2% while eliminating
// 97.5% of hallucinated retrieval results
```

## Cost Optimization Strategies

**Strategy 4**: Reduce costs with caching and smart routing.

<CodePlayground
  title="Cost-Optimized Query Routing"
  description="Route queries to appropriate optimization strategies based on cost/benefit analysis. Watch savings!"
  exerciseType="cost-optimization"
  code={`interface QueryProfile {
  complexity: 'simple' | 'medium' | 'complex'
  ambiguity: 'clear' | 'ambiguous'
  frequency: 'common' | 'rare'
}

interface OptimizationStrategy {
  name: string
  cost: number
  accuracyBoost: number
  latency: number
}

function routeQuery(query: string, profile: QueryProfile): OptimizationStrategy {
  // Route based on query characteristics

  if (profile.frequency === 'common') {
    // Common queries: Use cache (free, instant)
    return {
      name: 'Cache lookup',
      cost: 0,
      accuracyBoost: 0,
      latency: 5
    }
  }

  if (profile.complexity === 'simple' && profile.ambiguity === 'clear') {
    // Simple, clear queries: No optimization needed
    return {
      name: 'Direct retrieval',
      cost: 0.0001,
      accuracyBoost: 0,
      latency: 100
    }
  }

  if (profile.complexity === 'complex') {
    // Complex queries: Full decomposition
    return {
      name: 'Query decomposition + multi-query',
      cost: 0.0015,
      accuracyBoost: 0.35,
      latency: 400
    }
  }

  if (profile.ambiguity === 'ambiguous') {
    // Ambiguous queries: Rewrite + expand
    return {
      name: 'Rewrite + expansion',
      cost: 0.0008,
      accuracyBoost: 0.25,
      latency: 250
    }
  }

  // Default: Basic multi-query
  return {
    name: 'Multi-query generation',
    cost: 0.0003,
    accuracyBoost: 0.15,
    latency: 150
  }
}

function profileQuery(query: string): QueryProfile {
  // Classify query characteristics

  const wordCount = query.split(/\\s+/).length

  const complexity: 'simple' | 'medium' | 'complex' =
    wordCount &lt; 5 ? 'simple' :
    wordCount &lt; 15 ? 'medium' : 'complex'

  const hasVagueTerms = /\\b(it|that|this|stuff|thing)\\b/i.test(query)
  const ambiguity: 'clear' | 'ambiguous' = hasVagueTerms ? 'ambiguous' : 'clear'

  // In production, check cache hit rate
  const frequency: 'common' | 'rare' = 'rare' // Simplified

  return { complexity, ambiguity, frequency }
}

// Example queries with cost analysis
async function analyzeCostOptimization() {
  const testQueries = [
    {
      query: "HIPAA compliance",
      expectedHits: 1000 // Common query
    },
    {
      query: "how to make it faster",
      expectedHits: 10 // Ambiguous
    },
    {
      query: "What are the HIPAA encryption requirements and how do they compare to GDPR data protection rules for healthcare?",
      expectedHits: 5 // Complex
    },
    {
      query: "API rate limiting RFC 6585",
      expectedHits: 50 // Clear, simple
    }
  ]

  console.log('Cost Optimization Analysis:\\n')

  let totalCost = 0
  let totalLatency = 0

  testQueries.forEach(({ query, expectedHits }) => {
    const profile = profileQuery(query)
    const strategy = routeQuery(query, profile)

    const monthlyCost = strategy.cost * expectedHits

    console.log(\`Query: "\${query.substring(0, 60)}..."\`)
    console.log(\`  Profile: \${profile.complexity}, \${profile.ambiguity}, \${profile.frequency}\`)
    console.log(\`  Strategy: \${strategy.name}\`)
    console.log(\`  Cost per query: $\${strategy.cost.toFixed(5)}\`)
    console.log(\`  Monthly cost (\${expectedHits} queries): $\${monthlyCost.toFixed(2)}\`)
    console.log(\`  Accuracy boost: +\${(strategy.accuracyBoost * 100).toFixed(0)}%\`)
    console.log(\`  Latency: \${strategy.latency}ms\`)
    console.log()

    totalCost += monthlyCost
    totalLatency += strategy.latency
  })

  console.log(\`Total monthly cost: $\${totalCost.toFixed(2)}\`)
  console.log(\`Average latency: \${(totalLatency / testQueries.length).toFixed(0)}ms\`)
}

analyzeCostOptimization()`}
/>

**Cost Optimization Techniques**:

1. **Query caching** (80% cost savings for common queries)
```typescript
const cacheKey = `query:${hash(query)}`
const cached = await redis.get(cacheKey)
if (cached) return JSON.parse(cached) // $0 cost, 5ms latency

const result = await optimizeAndRetrieve(query)
await redis.set(cacheKey, JSON.stringify(result), { EX: 3600 })
```

2. **Adaptive optimization** (50% cost savings overall)
```typescript
if (isSimpleQuery(query)) {
  return directRetrieval(query) // $0.0001
} else {
  return fullOptimization(query) // $0.0015
}
```

3. **Batch processing** (40% latency savings)
```typescript
// Process multiple queries in single LLM call
const optimized = await batchOptimize([query1, query2, query3])
// 1 LLM call instead of 3
```

**Architect's Tip — Complexity Classifier**: "Don't run expensive Query Expansion on every request. Use a high-speed, small model (like Haiku) to classify incoming queries into three buckets: **Direct** (ID/exact match lookup), **Semantic** (conceptual search), and **Complex** (multi-part, ambiguous). Only trigger the Query Rewriting pipeline for the 'Complex' and 'Semantic' buckets. This preserves your P95 latency for simple lookups while applying intelligence only where it's needed."

```typescript
/**
 * Query Complexity Classifier
 *
 * Problem: Running full query expansion on every request wastes
 * 300-500ms and $0.001+ on queries like "RFC 6585" or "user ID 12345"
 * that need zero transformation.
 *
 * Solution: Use Haiku (~15ms, $0.000003) to classify query complexity
 * BEFORE deciding which optimization pipeline to invoke.
 *
 * Interview Defense: "We classify first, optimize second. 60% of our
 * queries are Direct lookups that bypass the expansion pipeline entirely.
 * This cut our average query cost from $0.0015 to $0.0004 — a 73%
 * reduction — while maintaining the same accuracy for complex queries."
 */

type QueryBucket = 'direct' | 'semantic' | 'complex'

interface ClassificationResult {
  bucket: QueryBucket
  confidence: number
  reasoning: string
  recommendedPipeline: string
}

async function classifyQueryComplexity(
  query: string
): Promise<ClassificationResult> {
  const response = await anthropic.messages.create({
    model: 'claude-haiku-4-5-20250929',  // Fast + cheap (~15ms)
    max_tokens: 150,
    messages: [{
      role: 'user',
      content: `Classify this search query into exactly one bucket.

BUCKETS:
- "direct": Exact match lookups — IDs, error codes, specific names, patent numbers
- "semantic": Conceptual queries — "how does X work", "what is Y", single-topic questions
- "complex": Multi-part, ambiguous, or cross-domain — comparisons, multi-step, vague terms

Query: "${query}"

Return JSON: {"bucket": "direct|semantic|complex", "confidence": 0.0-1.0, "reasoning": "..."}`
    }]
  })

  const result = JSON.parse(response.content[0].text)

  const pipelines: Record<QueryBucket, string> = {
    direct: 'Direct retrieval (no expansion)',
    semantic: 'HyDE + single expansion',
    complex: 'Decomposition + Multi-Query + Weighted Fusion'
  }

  return {
    ...result,
    recommendedPipeline: pipelines[result.bucket as QueryBucket]
  }
}

// Route to appropriate pipeline based on classification
async function adaptiveQueryPipeline(query: string): Promise<any[]> {
  const classification = await classifyQueryComplexity(query)

  switch (classification.bucket) {
    case 'direct':
      // Skip all expansion — direct vector search
      // Cost: $0.0001, Latency: ~100ms
      return await vectorSearch(await getEmbedding(query), 10)

    case 'semantic':
      // Use HyDE with guardrail for vocabulary bridging
      // Cost: $0.0005, Latency: ~250ms
      const hyde = await hydeWithGuardrail(query)
      return hyde.results

    case 'complex':
      // Full pipeline: decompose → multi-query → weighted fusion
      // Cost: $0.0015, Latency: ~450ms
      const decomposed = await decomposeQuery(query)
      const subResults = await executeDecomposedQuery(decomposed)
      return subResults

    default:
      return await vectorSearch(await getEmbedding(query), 10)
  }
}

// Production impact (10,000 queries/month):
//
// Query distribution:
//   Direct:   60% (6,000 queries)  → $0.0001 each = $0.60
//   Semantic: 25% (2,500 queries)  → $0.0005 each = $1.25
//   Complex:  15% (1,500 queries)  → $0.0015 each = $2.25
//   Classifier overhead:            → $0.000003 each = $0.03
//   Total: $4.13/month
//
// Without classifier (full pipeline on everything):
//   10,000 × $0.0015 = $15.00/month
//
// Savings: $10.87/month (72% reduction)
// P95 latency improvement: 450ms → 180ms (60% of queries skip expansion)
```

## Production Metrics

### Cost Comparison (10,000 queries/month)

```
┌─────────────────────────┬──────────┬─────────────┐
│ Strategy                │ Cost/Q   │ Monthly     │
├─────────────────────────┼──────────┼─────────────┤
│ No optimization         │ $0.0001  │ $1.00       │
│ Query rewriting         │ $0.0003  │ $3.00       │
│ Multi-query generation  │ $0.0005  │ $5.00       │
│ Full optimization       │ $0.0015  │ $15.00      │
│ Adaptive (recommended)  │ $0.0004  │ $4.00       │
└─────────────────────────┴──────────┴─────────────┘

ROI: +35% accuracy for +$3/month = Excellent
```

## Common Pitfalls

### 1. Over-Optimization
**Problem**: Optimizing every query wastes money
```typescript
// ❌ Bad: Full optimization for simple queries
const optimized = await decompose(await expand(await rewrite("API docs")))
// Cost: $0.0015, benefit: +2% accuracy

// ✅ Good: Route based on complexity
const result = isSimple(query) ? direct(query) : optimize(query)
```

### 2. Ignoring Cache Misses
**Problem**: Cache lookups that always miss add latency
```typescript
// ✅ Good: Monitor cache hit rate, disable if &lt;20%
if (cacheHitRate &lt; 0.2) {
  // Skip cache, go direct to optimization
}
```

### 3. Not Deduplicating Multi-Query Results
**Problem**: Duplicate documents inflate costs
```typescript
// ❌ Bad: 3 queries × 10 results = 30 documents to process
const results = queries.map(q => retrieve(q)) // No dedup

// ✅ Good: Deduplicate before context fusion
const results = deduplicate(queries.map(q => retrieve(q))) // 15 unique docs
```

## Key Takeaways

- **Classify first, optimize second**: Use a Complexity Classifier to route Direct/Semantic/Complex queries to appropriate pipelines (72% cost reduction)
- **Weight the original intent**: Relevance-Weighted Fusion preserves precision while Multi-Query improves recall
- **Guard HyDE against hallucination**: Similarity threshold prevents "ghost chasing" when the hypothetical document misses the corpus
- **Decompose complex queries**: Break multi-part questions into parallel sub-queries for better coverage
- **Cache aggressively**: Top 20% of queries handle 80% of traffic at $0 incremental cost
- **Production target**: +35% accuracy, $0.0004/query (adaptive), 150-400ms latency

---

## Architect Challenge: The Retrieval ROI Quiz

**You are the Director of Retrieval Engineering at a patent search company.**

**The Problem:**

A patent lawyer searches for **"Method for high-speed packet switching in 5G."** Your system returns 12 results, but misses several critical patents because they use the term **"Ultra-reliable low-latency communication (URLLC)"** — a technical synonym the lawyer doesn't know. The lawyer escalates to your VP, saying the search tool is "useless for real patent work."

**Your VP asks: "How do we fix this without making every search take 2 seconds?"**

**A)** Run Multi-Query Expansion on every request — generate 5 variations for all queries to maximize recall.

**B)** Implement **Adaptive Query Expansion**. Use an LLM to identify "URLLC" as a technical synonym for the user's intent, but **only trigger this Expansion** after a Complexity Classifier determines the query is in the "Technical Jargon" domain. This ensures high recall for lawyers without adding 500ms of latency to simple "Patent Number" lookups.

**C)** Use a larger vector embedding model with more dimensions to capture more semantic nuance.

**D)** Ask the user to provide their own synonyms in an "Advanced Search" box.

<details>
<summary><strong>Click to reveal the correct answer</strong></summary>

### Correct Answer: B — Adaptive Query Expansion

An Architect balances intelligence with **Economic and Latency Guardrails**.

**Why B is correct:**

```typescript
// Step 1: Classify the query (~15ms, $0.000003)
const classification = await classifyQueryComplexity(
  "Method for high-speed packet switching in 5G"
)
// Result: { bucket: 'semantic', confidence: 0.92, reasoning: 'Technical domain query' }

// Step 2: Because bucket is 'semantic', trigger HyDE + expansion
const hydeResult = await hydeWithGuardrail(query)
// HyDE generates: "...Ultra-Reliable Low-Latency Communication (URLLC)
// protocols for packet switching in 5G NR architectures..."
// → Similarity to corpus: 0.78 (passes 0.70 technical threshold)

// Step 3: Expanded search finds patents using "URLLC" terminology
// Recall: 45% → 89% (+44% improvement)
// Latency: +250ms (only for semantic/complex queries)

// Meanwhile, simple queries like "US Patent 10,234,567":
// → Classified as 'direct' → bypass expansion entirely
// → Latency: 100ms (no degradation)
```

**Why other answers fail:**

- **A) Multi-Query on every request** — Adds 300-500ms to ALL queries, including simple patent number lookups that represent 60% of traffic. At 100K queries/day, this wastes $150/day on unnecessary expansion. Your P95 latency goes from 200ms to 700ms, violating your SLA.

- **C) Larger embedding model** — A larger model captures more semantic nuance but still won't bridge "packet switching" → "URLLC" because these are domain-specific synonyms, not semantic neighbors. The vocabulary gap requires a **Translation layer** (HyDE), not a wider net.

- **D) User-provided synonyms** — Shifts the burden to the user. Patent lawyers don't know all possible technical synonyms — that's the entire reason they're using a search tool. This also fails for new terminology that enters the corpus after the user's last training.

**The Architect's Principle:** "Intelligence should be applied proportionally. A Complexity Classifier acts as a **cost-aware gateway** — expensive optimization runs only where it generates ROI, while simple queries flow through at minimum latency. This is how you scale RAG to millions of queries without breaking the bank or the SLA."

</details>

---

## Further Reading

- [Query Understanding at Scale](https://arxiv.org/abs/2104.08663) - Google research
- [Multi-Query Retrieval](https://arxiv.org/abs/2305.14283) - RAGatouille paper
- [Pinecone Query Understanding](https://www.pinecone.io/learn/query-understanding/) - Production guide
- [Anthropic Prompt Engineering](https://docs.anthropic.com/claude/docs/prompt-engineering) - Query optimization with Claude
- [Semantic Query Expansion](https://aclanthology.org/2022.naacl-main.349/) - NAACL 2022 paper
