---
title: "RAG + Memory Fundamentals"
description: "Introduction to Retrieval-Augmented Generation and vector-based memory systems"
estimatedMinutes: 40
---

import { CodePlayground } from '@/components/curriculum/CodePlayground'

# RAG + Memory Fundamentals

Build AI systems with external memory using Retrieval-Augmented Generation (RAG) and vector databases.

## Why RAG Matters

**Without RAG**:
- ğŸ¤· LLMs limited to training data cutoff
- ğŸ“‰ Hallucinations when facts are uncertain
- ğŸš« Can't access private/proprietary data
- ğŸ’¸ Expensive to retrain for new information

**With RAG**:
- âœ… Ground responses in real, up-to-date data
- âœ… Access private documents and knowledge bases
- âœ… Reduce hallucinations with factual context
- âœ… Update knowledge without retraining

## The Three-Phase RAG Architecture

RAG systems operate in three distinct phases: **Ingestion**, **Retrieval**, and **Generation**.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Phase 1: INGESTION                        â”‚
â”‚  Documents â†’ Chunks â†’ Embeddings â†’ Vector Database           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Phase 2: RETRIEVAL                        â”‚
â”‚  Query â†’ Embedding â†’ Similarity Search â†’ Top-K Results       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Phase 3: GENERATION                       â”‚
â”‚  Context + Query â†’ LLM â†’ Grounded Response                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Semantic Search vs. Lexical Search: The Architect's Decision

Before diving into implementation, architects must understand when to use **semantic search** (vector-based) versus **lexical search** (keyword-based) and how to justify the added latency.

### The Trade-Off: Meaning vs. Speed

| Dimension | Lexical Search (BM25, Elasticsearch) | Semantic Search (Vector Embeddings) |
|-----------|-------------------------------------|-------------------------------------|
| **Match Type** | Exact keywords | Semantic meaning |
| **Query**: "reset password" | Matches only "reset password" | Matches "password recovery", "change credentials", "forgot login" |
| **Latency** | 10-50ms | 200-500ms (embedding + vector search) |
| **Cost** | $0.001/query | $0.005-0.015/query (embedding + LLM) |
| **Use Case** | IDs, codes, exact terms | Natural language, paraphrasing |
| **Accuracy (paraphrased)** | Low (30-40%) | High (80-90%) |
| **Accuracy (exact match)** | Very High (95%+) | Medium (70-80%) |

### Decision Framework: When to Use Which

```typescript
interface SearchDecision {
  queryType: 'exact' | 'semantic' | 'hybrid'
  reasoning: string
  expectedAccuracy: number
  latencyBudget: number
}

/**
 * Decide search strategy based on query characteristics
 */
function chooseSearchStrategy(query: string, context: {
  userExpectsInstantResults: boolean
  queryHasIDs: boolean
  budgetConstraint: 'tight' | 'normal' | 'flexible'
}): SearchDecision {
  // Rule 1: Exact matches for IDs, part numbers, error codes
  if (/\b[A-Z0-9-]{5,}\b/.test(query) || context.queryHasIDs) {
    return {
      queryType: 'exact',
      reasoning: 'Query contains ID-like patterns - lexical search will be faster and more accurate',
      expectedAccuracy: 95,
      latencyBudget: 50 // ms
    }
  }

  // Rule 2: Tight budget + speed requirement = lexical only
  if (context.userExpectsInstantResults && context.budgetConstraint === 'tight') {
    return {
      queryType: 'exact',
      reasoning: 'Latency and cost constraints favor keyword search',
      expectedAccuracy: 60,
      latencyBudget: 50
    }
  }

  // Rule 3: Natural language queries = semantic
  if (query.split(' ').length &gt; 5 && !context.queryHasIDs) {
    return {
      queryType: 'semantic',
      reasoning: 'Natural language query benefits from semantic understanding',
      expectedAccuracy: 85,
      latencyBudget: 500
    }
  }

  // Rule 4: Default = hybrid (best of both worlds)
  return {
    queryType: 'hybrid',
    reasoning: 'Hybrid search combines exact match precision with semantic recall',
    expectedAccuracy: 90,
    latencyBudget: 400
  }
}
```

### Justifying Latency to Stakeholders

**The Business Case for Semantic Search** (present this to your PM/CTO):

1. **Accuracy Improvement**: 80-90% vs 30-40% for paraphrased queries
   - **Impact**: 50% fewer "no results found" experiences
   - **Revenue impact**: 15-25% increase in conversion for search-dependent features

2. **Support Cost Reduction**: Users find answers faster
   - **Zendesk case study**: 30% reduction in support tickets after implementing semantic search
   - **ROI**: $50K/year saved (50 tickets/day Ã— $2.74/ticket Ã— 30% reduction Ã— 365 days)

3. **Latency is Acceptable**:
   - **Google research**: Users tolerate 500ms for "better results"
   - **Mitigation**: Semantic caching reduces repeat queries to 50ms
   - **Perceived latency**: Show skeleton loaders, keep under 1s total

**The Anti-Pattern**: Don't use semantic search everywhere
- âŒ Product ID lookup ("Part #9921") â†’ Use lexical search (50ms)
- âŒ Autocomplete dropdowns â†’ Use lexical search (instant feedback required)
- âœ… "How do I reset my password?" â†’ Use semantic search (paraphrasing common)

### Production Pattern: Hybrid Search with Intelligent Routing

```typescript
interface HybridSearchConfig {
  vectorWeight: number    // 0.7 for semantic-heavy queries
  keywordWeight: number   // 0.3 for semantic-heavy queries
  useCache: boolean
  maxLatency: number      // Circuit breaker threshold
}

/**
 * Production-grade search router
 * Automatically chooses best strategy based on query
 */
class IntelligentSearchRouter {
  async search(
    query: string,
    context: { userId: string; urgency: 'high' | 'normal' }
  ): Promise<SearchResult[]> {
    const startTime = Date.now()

    // Step 1: Classify query
    const strategy = chooseSearchStrategy(query, {
      userExpectsInstantResults: context.urgency === 'high',
      queryHasIDs: /\b[A-Z0-9-]{5,}\b/.test(query),
      budgetConstraint: 'normal'
    })

    // Step 2: Check semantic cache (if applicable)
    if (strategy.queryType !== 'exact') {
      const cached = await semanticCache.get(query)
      if (cached) {
        console.log(`Cache hit: ${Date.now() - startTime}ms`)
        return cached
      }
    }

    // Step 3: Execute search based on strategy
    let results: SearchResult[]

    switch (strategy.queryType) {
      case 'exact':
        results = await this.lexicalSearch(query)
        break

      case 'semantic':
        results = await this.vectorSearch(query)
        break

      case 'hybrid':
        // Parallel execution for speed
        const [vectorResults, keywordResults] = await Promise.all([
          this.vectorSearch(query),
          this.lexicalSearch(query)
        ])
        results = this.fuseResults(vectorResults, keywordResults)
        break
    }

    // Step 4: Latency circuit breaker
    const latency = Date.now() - startTime
    if (latency &gt; strategy.latencyBudget) {
      console.warn(`Search exceeded latency budget: ${latency}ms > ${strategy.latencyBudget}ms`)
      // Metric for alerting
      await metrics.recordLatencyViolation(strategy.queryType, latency)
    }

    // Step 5: Cache result if semantic
    if (strategy.queryType !== 'exact') {
      await semanticCache.set(query, results)
    }

    return results
  }

  private async lexicalSearch(query: string): Promise<SearchResult[]> {
    // BM25 keyword search (50ms)
    return await bm25Index.search(query, 10)
  }

  private async vectorSearch(query: string): Promise<SearchResult[]> {
    // Semantic vector search (200-500ms)
    const embedding = await embed(query)
    return await vectorDB.query({ vector: embedding, topK: 10 })
  }

  private fuseResults(
    vectorResults: SearchResult[],
    keywordResults: SearchResult[]
  ): SearchResult[] {
    // Reciprocal Rank Fusion (see Week 9: Hybrid Search for details)
    return reciprocalRankFusion([vectorResults, keywordResults])
  }
}
```

### Key Architectural Insights

1. **Semantic search is NOT always better** - Use hybrid or lexical for IDs/codes
2. **Latency justification = business value** - 30% support reduction pays for 400ms added latency
3. **Semantic caching mitigates cost** - 50-70% of queries hit cache â†’ 50ms response
4. **Circuit breakers prevent degradation** - Fall back to lexical if vector DB slow
5. **Measure everything** - Track accuracy, latency, cost per query type

**ROI Example** (for stakeholder presentations):
- **Baseline**: Lexical search, 50ms, $0.001/query, 60% accuracy
- **Upgraded**: Hybrid search, 400ms, $0.008/query, 90% accuracy
- **Business impact**: 25% conversion increase = $100K/year revenue
- **Cost increase**: 10M queries Ã— $0.007 difference = $70K/year
- **Net ROI**: $30K/year profit + better UX

---

## Phase 1: Ingestion/Extraction

Convert your documents into searchable vectors and store them in a vector database.

### Step 1: Data Collection & Chunking

Break documents into semantically coherent chunks for better retrieval.

```typescript
interface Document {
  id: string
  content: string
  metadata: {
    source: string
    title?: string
    author?: string
    createdAt: Date
  }
}

interface Chunk {
  id: string
  documentId: string
  content: string
  metadata: Record<string, any>
  startIndex: number
  endIndex: number
}

/**
 * Chunk documents using sliding window approach
 * Overlap ensures context isn't lost at chunk boundaries
 */
function chunkDocument(
  document: Document,
  chunkSize: number = 500,    // tokens
  overlapSize: number = 50    // tokens
): Chunk[] {
  const chunks: Chunk[] = []
  const text = document.content

  // Simple approximation: 1 token â‰ˆ 4 characters
  const chunkChars = chunkSize * 4
  const overlapChars = overlapSize * 4

  let startIndex = 0
  let chunkIndex = 0

  while (startIndex < text.length) {
    const endIndex = Math.min(startIndex + chunkChars, text.length)

    // Try to break at sentence boundary
    let actualEndIndex = endIndex
    if (endIndex < text.length) {
      const sentenceEnd = text.lastIndexOf('.', endIndex)
      if (sentenceEnd > startIndex + chunkChars / 2) {
        actualEndIndex = sentenceEnd + 1
      }
    }

    chunks.push({
      id: `${document.id}-chunk-${chunkIndex}`,
      documentId: document.id,
      content: text.slice(startIndex, actualEndIndex).trim(),
      metadata: {
        ...document.metadata,
        chunkIndex,
        totalChunks: -1  // Will update after
      },
      startIndex,
      endIndex: actualEndIndex
    })

    startIndex = actualEndIndex - overlapChars
    chunkIndex++
  }

  // Update total chunks count
  chunks.forEach(chunk => {
    chunk.metadata.totalChunks = chunks.length
  })

  return chunks
}
```

**Chunking Strategies**:

| Strategy | Best For | Pros | Cons |
|----------|----------|------|------|
| **Fixed-size** | General purpose | Simple, predictable | May split sentences |
| **Sentence-based** | Q&A systems | Semantic coherence | Variable size |
| **Paragraph-based** | Long-form content | Natural boundaries | Large variance |
| **Semantic** | Complex documents | Context-aware | Computationally expensive |

### Step 2: Generate Embeddings

Convert text chunks into high-dimensional vectors that capture semantic meaning.

```typescript
import Anthropic from '@anthropic-ai/sdk'
import OpenAI from 'openai'

const anthropic = new Anthropic({ apiKey: process.env.ANTHROPIC_API_KEY })
const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY })

interface Embedding {
  chunkId: string
  vector: number[]
  model: string
  dimensions: number
}

/**
 * Generate embeddings using different providers
 */
async function generateEmbeddings(
  chunks: Chunk[],
  provider: 'openai' | 'anthropic' = 'openai'
): Promise<Embedding[]> {
  const embeddings: Embedding[] = []

  if (provider === 'openai') {
    // OpenAI embeddings: text-embedding-3-large (3072 dims)
    const response = await openai.embeddings.create({
      model: 'text-embedding-3-large',
      input: chunks.map(c => c.content),
      encoding_format: 'float'
    })

    response.data.forEach((embedding, i) => {
      embeddings.push({
        chunkId: chunks[i].id,
        vector: embedding.embedding,
        model: 'text-embedding-3-large',
        dimensions: embedding.embedding.length
      })
    })
  } else {
    // Anthropic: Use Claude to generate semantic embeddings
    // (Note: As of 2026, use dedicated embedding models in production)
    for (const chunk of chunks) {
      const response = await anthropic.messages.create({
        model: 'claude-3-haiku-20240307',
        max_tokens: 100,
        messages: [{
          role: 'user',
          content: `Generate a semantic summary vector for: "${chunk.content}"`
        }]
      })

      // In practice, use proper embedding models
      // This is simplified for demonstration
      const vector = new Array(1536).fill(0).map(() => Math.random())

      embeddings.push({
        chunkId: chunk.id,
        vector,
        model: 'claude-semantic',
        dimensions: vector.length
      })
    }
  }

  return embeddings
}
```

**Embedding Model Comparison** (2026):

| Model | Provider | Dimensions | Cost (per 1M tokens) | Best For |
|-------|----------|------------|---------------------|----------|
| text-embedding-3-large | OpenAI | 3072 | $0.13 | High accuracy, semantic search |
| text-embedding-3-small | OpenAI | 1536 | $0.02 | Cost-effective, general purpose |
| voyage-large-2 | Voyage AI | 1536 | $0.12 | Code search, technical docs |
| cohere-embed-v3 | Cohere | 1024 | $0.10 | Multilingual, clustering |

### Step 3: Index & Store in Vector Database

Store embeddings in a vector database for efficient similarity search.

```typescript
import { pgvector } from 'pgvector/pg'
import { Pool } from 'pg'

const pool = new Pool({
  connectionString: process.env.DATABASE_URL
})

/**
 * Setup pgvector extension and create tables
 */
async function setupVectorDatabase() {
  const client = await pool.connect()

  try {
    // Enable pgvector extension
    await client.query('CREATE EXTENSION IF NOT EXISTS vector')

    // Create chunks table with vector column
    await client.query(`
      CREATE TABLE IF NOT EXISTS document_chunks (
        id TEXT PRIMARY KEY,
        document_id TEXT NOT NULL,
        content TEXT NOT NULL,
        metadata JSONB,
        embedding vector(1536),  -- Adjust dimensions to match your model
        created_at TIMESTAMP DEFAULT NOW()
      )
    `)

    // Create index for similarity search (HNSW algorithm)
    await client.query(`
      CREATE INDEX IF NOT EXISTS chunks_embedding_idx
      ON document_chunks
      USING hnsw (embedding vector_cosine_ops)
    `)

    // Create index for metadata filtering
    await client.query(`
      CREATE INDEX IF NOT EXISTS chunks_metadata_idx
      ON document_chunks
      USING gin (metadata)
    `)

  } finally {
    client.release()
  }
}

/**
 * Insert chunks with embeddings into vector database
 */
async function insertChunks(
  chunks: Chunk[],
  embeddings: Embedding[]
): Promise<void> {
  const client = await pool.connect()

  try {
    await client.query('BEGIN')

    for (let i = 0; i < chunks.length; i++) {
      const chunk = chunks[i]
      const embedding = embeddings[i]

      await client.query(
        `INSERT INTO document_chunks
         (id, document_id, content, metadata, embedding)
         VALUES ($1, $2, $3, $4, $5)
         ON CONFLICT (id) DO UPDATE SET
           content = EXCLUDED.content,
           metadata = EXCLUDED.metadata,
           embedding = EXCLUDED.embedding`,
        [
          chunk.id,
          chunk.documentId,
          chunk.content,
          JSON.stringify(chunk.metadata),
          pgvector.toSql(embedding.vector)
        ]
      )
    }

    await client.query('COMMIT')
  } catch (error) {
    await client.query('ROLLBACK')
    throw error
  } finally {
    client.release()
  }
}
```

**Vector Database Options**:

| Database | Type | Best For | Pros | Cons |
|----------|------|----------|------|------|
| **pgvector** | Postgres extension | Existing Postgres apps | SQL integration, ACID | Slower than specialized |
| **Pinecone** | Managed service | Rapid prototyping | Serverless, easy setup | Vendor lock-in |
| **Milvus** | Open source | Self-hosted production | High performance, flexible | Complex setup |
| **Qdrant** | Open source | Privacy-sensitive | On-premise, fast | Self-managed |
| **Weaviate** | Open source | Semantic search | Built-in ML, GraphQL | Learning curve |

---

## Phase 2: Retrieval

Find the most relevant chunks for a user's query using semantic similarity.

### Step 1: Query Embedding

Convert the user's query into a vector using the same embedding model.

```typescript
async function embedQuery(query: string): Promise<number[]> {
  const response = await openai.embeddings.create({
    model: 'text-embedding-3-large',
    input: [query],
    encoding_format: 'float'
  })

  return response.data[0].embedding
}
```

### Step 2: Similarity Search

Perform vector similarity search to find the most relevant chunks.

```typescript
interface SearchResult {
  chunkId: string
  content: string
  metadata: Record<string, any>
  similarity: number
}

/**
 * Search for similar chunks using cosine similarity
 */
async function searchSimilarChunks(
  queryEmbedding: number[],
  topK: number = 5,
  minSimilarity: number = 0.7,
  filters?: Record<string, any>
): Promise<SearchResult[]> {
  const client = await pool.connect()

  try {
    let query = `
      SELECT
        id as chunk_id,
        content,
        metadata,
        1 - (embedding &lt;=&gt; $1::vector) as similarity
      FROM document_chunks
    `

    const params: any[] = [pgvector.toSql(queryEmbedding)]

    // Add metadata filters if provided
    if (filters) {
      const filterConditions = Object.entries(filters).map(([key, value], i) => {
        params.push(JSON.stringify(value))
        return `metadata->>'${key}' = $${i + 2}`
      })

      if (filterConditions.length &gt; 0) {
        query += ` WHERE ${filterConditions.join(' AND ')}`
      }
    }

    query += `
      ORDER BY embedding &lt;=&gt; $1::vector
      LIMIT $${params.length + 1}
    `
    params.push(topK)

    const result = await client.query(query, params)

    return result.rows
      .filter(row => row.similarity &gt;= minSimilarity)
      .map(row => ({
        chunkId: row.chunk_id,
        content: row.content,
        metadata: row.metadata,
        similarity: row.similarity
      }))

  } finally {
    client.release()
  }
}
```

**Similarity Metrics**:

| Metric | Formula | Range | Best For |
|--------|---------|-------|----------|
| **Cosine** | `1 - (aÂ·b)/(â€–aâ€–â€–bâ€–)` | 0-2 | Most common, normalized |
| **Euclidean** | `âˆšÎ£(ai-bi)Â²` | 0-âˆ | Absolute distance |
| **Dot Product** | `aÂ·b` | -âˆ to âˆ | When magnitude matters |

### Step 3: Re-ranking (Optional)

Use a re-ranking model to refine results and improve relevance.

```typescript
import Anthropic from '@anthropic-ai/sdk'

/**
 * Re-rank retrieved chunks using LLM-based relevance scoring
 */
async function rerankResults(
  query: string,
  results: SearchResult[],
  topK: number = 3
): Promise<SearchResult[]> {
  const anthropic = new Anthropic({ apiKey: process.env.ANTHROPIC_API_KEY })

  // Ask Claude to score relevance
  const prompt = `
Given this query: "${query}"

Score the relevance of each passage (0-10, where 10 is most relevant):

${results.map((r, i) => `
Passage ${i + 1}:
${r.content}
`).join('\n')}

Return only JSON: { "scores": [score1, score2, ...] }
`

  const response = await anthropic.messages.create({
    model: 'claude-3-haiku-20240307',
    max_tokens: 200,
    messages: [{ role: 'user', content: prompt }]
  })

  const text = response.content[0].type === 'text' ? response.content[0].text : '{}'
  const { scores } = JSON.parse(text)

  // Add LLM-based relevance scores
  const reranked = results.map((result, i) => ({
    ...result,
    rerankScore: scores[i] || 0
  }))

  // Sort by rerank score and return top-K
  return reranked
    .sort((a, b) => b.rerankScore - a.rerankScore)
    .slice(0, topK)
}
```

---

## Phase 3: Generation

Augment the LLM prompt with retrieved context to generate grounded responses.

### Step 1: Context Augmentation

Build an augmented prompt with retrieved chunks.

```typescript
interface RAGPrompt {
  systemPrompt: string
  userPrompt: string
  context: string
}

/**
 * Build augmented prompt with retrieved context
 */
function buildRAGPrompt(
  query: string,
  results: SearchResult[]
): RAGPrompt {
  // Format retrieved chunks as context
  const context = results.map((result, i) => `
[Source ${i + 1}] ${result.metadata.source || 'Unknown'}
${result.content}
`).join('\n\n')

  const systemPrompt = `You are a helpful AI assistant. Answer questions based on the provided context.

IMPORTANT:
- Only use information from the provided context
- If the context doesn't contain relevant information, say "I don't have enough information to answer that"
- Cite sources using [Source N] notation
- Be concise and factual`

  const userPrompt = `Context:
${context}

Question: ${query}

Answer:`

  return {
    systemPrompt,
    userPrompt,
    context
  }
}
```

### Step 2: Generate Response

Send the augmented prompt to the LLM.

```typescript
interface RAGResponse {
  answer: string
  sources: SearchResult[]
  confidence: 'high' | 'medium' | 'low'
}

/**
 * Generate response using RAG pipeline
 */
async function generateRAGResponse(
  query: string,
  results: SearchResult[]
): Promise<RAGResponse> {
  const anthropic = new Anthropic({ apiKey: process.env.ANTHROPIC_API_KEY })

  const { systemPrompt, userPrompt } = buildRAGPrompt(query, results)

  const response = await anthropic.messages.create({
    model: 'claude-3-5-sonnet-20240620',
    max_tokens: 1024,
    system: systemPrompt,
    messages: [{ role: 'user', content: userPrompt }]
  })

  const answer = response.content[0].type === 'text' ? response.content[0].text : ''

  // Determine confidence based on similarity scores
  const avgSimilarity = results.reduce((sum, r) => sum + r.similarity, 0) / results.length
  const confidence = avgSimilarity &gt; 0.85 ? 'high' : avgSimilarity &gt; 0.7 ? 'medium' : 'low'

  return {
    answer,
    sources: results,
    confidence
  }
}
```

### Complete RAG Pipeline

Putting it all together:

```typescript
/**
 * Full RAG pipeline: Query â†’ Retrieval â†’ Generation
 */
async function queryRAG(
  query: string,
  options?: {
    topK?: number
    minSimilarity?: number
    useReranking?: boolean
    filters?: Record<string, any>
  }
): Promise<RAGResponse> {
  // Phase 2: Retrieval
  // Step 1: Embed query
  const queryEmbedding = await embedQuery(query)

  // Step 2: Search for similar chunks
  let results = await searchSimilarChunks(
    queryEmbedding,
    options?.topK || 5,
    options?.minSimilarity || 0.7,
    options?.filters
  )

  if (results.length === 0) {
    return {
      answer: "I don't have any relevant information to answer that question.",
      sources: [],
      confidence: 'low'
    }
  }

  // Step 3: Optional re-ranking
  if (options?.useReranking) {
    results = await rerankResults(query, results, 3)
  }

  // Phase 3: Generation
  const response = await generateRAGResponse(query, results)

  return response
}
```

---

## Key Takeaways

### When to Use RAG

âœ… **Use RAG when:**
- You need up-to-date information beyond LLM training data
- Working with private/proprietary documents
- Facts and accuracy are critical (legal, medical, financial)
- You want to reduce hallucinations
- Knowledge base changes frequently

âŒ **Don't use RAG when:**
- General knowledge questions LLM can answer
- Low-latency requirements (retrieval adds ~200-500ms)
- Simple tasks not requiring external knowledge
- Budget is extremely constrained

### Architecture Best Practices

1. **Chunking**: Use 500-1000 tokens with 10-20% overlap
2. **Embeddings**: text-embedding-3-large for quality, text-embedding-3-small for cost
3. **Vector DB**: pgvector for Postgres users, Pinecone for rapid prototyping
4. **Retrieval**: Start with top-5, adjust based on quality
5. **Re-ranking**: Use for critical applications where precision matters
6. **Generation**: Claude Sonnet 4.5 for balanced cost/quality

### Cost Considerations

For 1000 queries/day with 5 retrieved chunks each:

| Component | Cost/Month |
|-----------|------------|
| Embeddings (query) | $0.13 Ã— 30M tokens = $4 |
| Vector DB (pgvector) | Included in Postgres hosting |
| LLM Generation | 1000 Ã— $0.015 Ã— 30 = $450 |
| **Total** | **~$454/month** |

### Performance Metrics

| Metric | Target | How to Measure |
|--------|--------|----------------|
| **Retrieval Latency** | &lt;200ms | Vector search time |
| **End-to-End Latency** | &lt;2s | Full pipeline time |
| **Relevance** | &gt;0.8 similarity | Average similarity score |
| **Accuracy** | &gt;90% | Human eval on sample queries |

---

## Further Reading

- **Week 3**: Deep dive into RAG pipelines, memory systems, vector embeddings
- **Pinecone Docs**: [Vector Database Fundamentals](https://www.pinecone.io/learn/vector-database/)
- **OpenAI Embeddings**: [Embeddings Guide](https://platform.openai.com/docs/guides/embeddings)
- **pgvector**: [GitHub Repository](https://github.com/pgvector/pgvector)
- **LangChain**: [RAG Tutorial](https://python.langchain.com/docs/use_cases/question_answering/)

## Next Steps

- **Week 3**: Advanced RAG pipelines, hybrid search, memory systems
- **Week 6**: Monitoring RAG quality with observability
- **Week 12**: Enterprise RAG with compliance and governance
