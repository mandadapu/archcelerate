---
title: "Vector Embeddings & Similarity Search"
description: "Understand vector embeddings and how to use them for semantic search"
estimatedMinutes: 35
---

# Vector Embeddings & Similarity Search

## Introduction

Vector embeddings are the foundation of modern semantic search and RAG systems. They transform text into high-dimensional numerical representations that capture meaning.

## What Are Vector Embeddings?

An **embedding** is a dense vector (array of numbers) that represents the semantic meaning of text.

```python
# Example embedding (simplified - real embeddings are 1536+ dimensions)
text = "The cat sat on the mat"
embedding = [0.23, -0.45, 0.67, 0.12, ...]  # 1536 numbers
```

**Key Properties:**
- **Semantic similarity**: Similar meanings → similar vectors
- **Fixed size**: All text maps to same dimension (e.g., 1536)
- **Dense**: Every dimension has a meaningful value
- **Learned**: Trained on massive text corpora

## How Embeddings Work

```typescript
import OpenAI from 'openai'

const openai = new OpenAI()

async function embed(text: string): Promise<number[]> {
  const response = await openai.embeddings.create({
    model: 'text-embedding-3-small',
    input: text
  })

  return response.data[0].embedding
}

// Usage
const query = "What is machine learning?"
const embedding = await embed(query)
// Returns: [0.023, -0.045, 0.067, ...] (1536 numbers)
```

## Similarity Search

Find similar texts by comparing their embeddings using **cosine similarity**.

### Cosine Similarity

Measures the angle between two vectors (range: -1 to 1, where 1 = identical).

```typescript
function cosineSimilarity(a: number[], b: number[]): number {
  const dotProduct = a.reduce((sum, val, i) => sum + val * b[i], 0)
  const magnitudeA = Math.sqrt(a.reduce((sum, val) => sum + val * val, 0))
  const magnitudeB = Math.sqrt(b.reduce((sum, val) => sum + val * val, 0))

  return dotProduct / (magnitudeA * magnitudeB)
}

// Example
const query = await embed("machine learning algorithms")
const doc1 = await embed("neural networks and deep learning")
const doc2 = await embed("pizza recipes and cooking tips")

console.log(cosineSimilarity(query, doc1))  // 0.87 (high similarity)
console.log(cosineSimilarity(query, doc2))  // 0.12 (low similarity)
```

## Vector Databases

Store and search millions of embeddings efficiently.

### Popular Vector Databases

| Database | Best For | Hosting |
|----------|----------|---------|
| **Pinecone** | Production, managed | Cloud-only |
| **Chroma** | Local development | Self-hosted |
| **Weaviate** | Enterprise, open-source | Both |
| **Qdrant** | Performance, filtering | Both |
| **pgvector** | Existing PostgreSQL | Self-hosted |

### Using Pinecone

```typescript
import { Pinecone } from '@pinecone-database/pinecone'

const pinecone = new Pinecone({
  apiKey: process.env.PINECONE_API_KEY!
})

const index = pinecone.index('my-index')

// Insert vectors
await index.upsert([
  {
    id: 'doc-1',
    values: embedding1,  // [0.23, -0.45, ...]
    metadata: { text: 'Original document text', source: 'page-1.pdf' }
  },
  {
    id: 'doc-2',
    values: embedding2,
    metadata: { text: 'Another document', source: 'page-2.pdf' }
  }
])

// Search
const queryEmbedding = await embed("machine learning")

const results = await index.query({
  vector: queryEmbedding,
  topK: 5,
  includeMetadata: true
})

results.matches.forEach(match => {
  console.log(`Score: ${match.score}`)
  console.log(`Text: ${match.metadata?.text}`)
})
```

### Using Chroma (Local)

```typescript
import { ChromaClient } from 'chromadb'

const client = new ChromaClient()
const collection = await client.createCollection({
  name: 'my-documents'
})

// Add documents (Chroma handles embedding automatically)
await collection.add({
  ids: ['doc1', 'doc2'],
  documents: [
    'Neural networks are inspired by biological neurons',
    'Pizza is a popular Italian dish'
  ],
  metadatas: [
    { source: 'ml-book' },
    { source: 'cooking-blog' }
  ]
})

// Search
const results = await collection.query({
  queryTexts: ['machine learning'],
  nResults: 5
})
```

---

## Embedding Model Selection: The Architect's Guide

Choosing the right embedding model is a critical architectural decision that impacts accuracy, cost, and latency. This section provides a production-grade comparison framework.

### Comprehensive Model Comparison (2026)

| Model | Provider | Dimensions | Cost/1M Tokens | Latency (P95) | Quality Score | Best For |
|-------|----------|------------|----------------|---------------|---------------|----------|
| **text-embedding-3-large** | OpenAI | 3072 | $0.13 | 150ms | 95/100 | Highest accuracy RAG |
| **text-embedding-3-small** | OpenAI | 1536 | $0.02 | 80ms | 88/100 | Cost-effective general use |
| **text-embedding-ada-002** | OpenAI | 1536 | $0.10 | 100ms | 85/100 | Legacy (use v3-small instead) |
| **voyage-large-2-instruct** | Voyage AI | 1024 | $0.12 | 120ms | 92/100 | Code search, technical docs |
| **voyage-code-2** | Voyage AI | 1536 | $0.10 | 110ms | 91/100 | Codebase semantic search |
| **cohere-embed-v3-multilingual** | Cohere | 1024 | $0.10 | 100ms | 90/100 | Non-English languages |
| **bge-large-en-v1.5** | BAAI (open) | 1024 | Free (self-hosted) | 50ms (local) | 87/100 | Self-hosted, privacy-sensitive |
| **e5-large-v2** | Microsoft (open) | 1024 | Free (self-hosted) | 45ms (local) | 86/100 | Open-source, customizable |
| **all-MiniLM-L6-v2** | Sentence-BERT | 384 | Free (self-hosted) | 20ms (local) | 78/100 | Fast, low-resource environments |

**Quality Score Methodology**: Averaged across MTEB benchmark (retrieval, classification, clustering, semantic similarity).

### The Cost-Quality-Latency Triangle

```typescript
interface EmbeddingModelProfile {
  model: string
  dimensions: number
  costPer1MTokens: number
  p95Latency: number
  qualityScore: number
  throughput: number  // requests/second
}

const EMBEDDING_MODELS: Record<string, EmbeddingModelProfile> = {
  'premium': {
    model: 'text-embedding-3-large',
    dimensions: 3072,
    costPer1MTokens: 0.13,
    p95Latency: 150,
    qualityScore: 95,
    throughput: 3000
  },
  'balanced': {
    model: 'text-embedding-3-small',
    dimensions: 1536,
    costPer1MTokens: 0.02,
    p95Latency: 80,
    qualityScore: 88,
    throughput: 8000
  },
  'fast': {
    model: 'all-MiniLM-L6-v2',
    dimensions: 384,
    costPer1MTokens: 0.00,  // Self-hosted
    p95Latency: 20,
    qualityScore: 78,
    throughput: 50000  // Local GPU
  }
}

/**
 * Choose embedding model based on requirements
 */
function selectEmbeddingModel(requirements: {
  qualityThreshold: number
  latencyBudget: number
  monthlyBudget: number
  expectedQueriesPerMonth: number
}): EmbeddingModelProfile {
  const models = Object.values(EMBEDDING_MODELS)

  // Filter by quality requirement
  let candidates = models.filter(m => m.qualityScore &gt;= requirements.qualityThreshold)

  // Filter by latency requirement
  candidates = candidates.filter(m => m.p95Latency &lt;= requirements.latencyBudget)

  // Calculate monthly cost
  const costsWithinBudget = candidates.filter(m => {
    const monthlyCost = (requirements.expectedQueriesPerMonth / 1_000_000) * m.costPer1MTokens
    return monthlyCost &lt;= requirements.monthlyBudget
  })

  if (costsWithinBudget.length === 0) {
    throw new Error('No model meets all requirements - consider self-hosted options')
  }

  // Return highest quality model that meets constraints
  return costsWithinBudget.sort((a, b) => b.qualityScore - a.qualityScore)[0]
}

// Example usage
const model = selectEmbeddingModel({
  qualityThreshold: 85,
  latencyBudget: 100,  // ms
  monthlyBudget: 500,  // USD
  expectedQueriesPerMonth: 10_000_000
})
// Result: text-embedding-3-small (88 quality, 80ms, $200/month)
```

### Decision Framework: When to Use Which Model

| Use Case | Recommended Model | Reasoning |
|----------|------------------|-----------|
| **High-stakes RAG** (legal, medical, financial) | text-embedding-3-large | Accuracy > Cost. Errors are expensive. |
| **General-purpose RAG** (docs, support, internal tools) | text-embedding-3-small | Best balance of cost/quality for most use cases. |
| **Code search** (GitHub Copilot-style) | voyage-code-2 | Trained on code, understands syntax and semantics. |
| **Multi-language support** (global products) | cohere-embed-v3-multilingual | Handles 100+ languages with consistent quality. |
| **Privacy-sensitive** (HIPAA, PII, on-prem) | bge-large-en-v1.5 (self-hosted) | No data leaves your infrastructure. |
| **High-volume, cost-constrained** (startups, MVPs) | all-MiniLM-L6-v2 (self-hosted) | Free, fast, good enough for prototypes. |

### ROI Analysis: Premium vs. Balanced Embedding Models

**Scenario**: 10M queries/month RAG system for customer support

| Metric | Premium (v3-large) | Balanced (v3-small) | Difference |
|--------|-------------------|-------------------|------------|
| **Cost/month** | $1,300 | $200 | -$1,100/month |
| **Quality Score** | 95 | 88 | -7 points |
| **Retrieval Accuracy** | 92% | 86% | -6% |
| **Support Tickets Created** | 2,000/month | 2,600/month | +600 tickets |
| **Support Cost** (@$5/ticket) | $10,000 | $13,000 | +$3,000/month |
| **Net Cost** | $11,300 | $13,200 | **Premium CHEAPER by $1,900/month** |

**Key Insight**: For high-stakes use cases (support, sales, compliance), premium models are often cheaper due to downstream cost savings from better accuracy.

---

## The Embedding Performance Frontier: Dimension Density & Matryoshka Embeddings

**The Problem with "Just Look at Price"**: Architects must match the model's **Dimension Density** to the complexity of the data. For highly technical domains (legal, medical, code), "small" models often fail to capture nuanced differences between similar terms.

### Why Dimension Density Matters

```typescript
// Example: Legal document terminology
const documents = [
  "The defendant pleaded guilty to the charge",     // Criminal law
  "The defendant filed a plea in response to the motion",  // Civil procedure
  "The defendant entered a guilty plea agreement"   // Plea bargaining
]

// With text-embedding-3-small (1536 dimensions):
const similarities_small = {
  'doc1 vs doc2': 0.87,  // ← Too similar! Different legal contexts
  'doc1 vs doc3': 0.91,  // ← Conflated as nearly identical
}

// With text-embedding-3-large (3072 dimensions):
const similarities_large = {
  'doc1 vs doc2': 0.76,  // ← Correctly distinguishes criminal vs civil
  'doc1 vs doc3': 0.84,  // ← Captures similarity but maintains distinction
}
```

**The Architect's Rule**: Don't just look at the price per token. For highly technical domains, the "small" models collapse semantically distinct concepts into similar vectors. Use **text-embedding-3-large** and implement **Matryoshka Embeddings** to get the best of both worlds.

### Matryoshka Embeddings: Multi-Resolution Vectors

**The Innovation**: Store a 3072-dimension vector but **truncate** it to 256 dimensions for high-speed initial filtering, only using the full vector for the final Top-10 precision check.

**How It Works**:
```typescript
interface MatryoshkaEmbedding {
  full: number[]      // 3072 dimensions (high precision)
  coarse: number[]    // 256 dimensions (fast filtering)
  medium: number[]    // 1024 dimensions (optional middle tier)
}

/**
 * Generate Matryoshka embedding from text-embedding-3-large
 * OpenAI's v3 models are trained with Matryoshka structure
 */
async function generateMatryoshkaEmbedding(text: string): Promise<MatryoshkaEmbedding> {
  const response = await openai.embeddings.create({
    model: 'text-embedding-3-large',
    input: text,
    dimensions: 3072  // Request full dimensions
  })

  const fullEmbedding = response.data[0].embedding

  // Matryoshka property: First N dimensions are semantically coherent
  // You can truncate to any power of 2 and still get useful embeddings
  return {
    full: fullEmbedding,                     // 3072D (highest precision)
    coarse: fullEmbedding.slice(0, 256),     // 256D (12x faster search)
    medium: fullEmbedding.slice(0, 1024)     // 1024D (middle ground)
  }
}
```

**The Performance Trick**: Use coarse embeddings (256D) for initial filtering, then re-rank with full embeddings (3072D).

### Two-Stage Retrieval with Matryoshka

```typescript
interface MatryoshkaSearchConfig {
  stage1_k: number      // Candidates from coarse search
  stage2_k: number      // Final results from full precision
  coarse_dims: number   // 256 for speed
  full_dims: number     // 3072 for precision
}

class MatryoshkaVectorSearch {
  private config: MatryoshkaSearchConfig = {
    stage1_k: 100,      // Cast wide net with fast coarse search
    stage2_k: 10,       // Narrow down with precise full search
    coarse_dims: 256,
    full_dims: 3072
  }

  /**
   * Stage 1: Fast filtering with 256D embeddings
   * Stage 2: Precision ranking with 3072D embeddings
   */
  async search(queryText: string): Promise<SearchResult[]> {
    const queryEmbedding = await generateMatryoshkaEmbedding(queryText)

    // STAGE 1: Coarse search (256D) - FAST
    const stage1Start = Date.now()
    const coarseCandidates = await this.searchCoarse(
      queryEmbedding.coarse,
      this.config.stage1_k
    )
    const stage1Latency = Date.now() - stage1Start  // ~10ms for 1M vectors

    // STAGE 2: Precision re-ranking (3072D) - ACCURATE
    const stage2Start = Date.now()
    const preciseResults = await this.rerankWithFull(
      queryEmbedding.full,
      coarseCandidates,
      this.config.stage2_k
    )
    const stage2Latency = Date.now() - stage2Start  // ~5ms for 100 vectors

    console.log(`Stage 1 (coarse): ${stage1Latency}ms, Stage 2 (full): ${stage2Latency}ms`)
    // Total: ~15ms vs 150ms for full 3072D search on entire dataset

    return preciseResults
  }

  private async searchCoarse(
    coarseEmbedding: number[],
    topK: number
  ): Promise<CandidateResult[]> {
    // Query against 256D index (12x faster than 3072D)
    const results = await vectorDB.query({
      vector: coarseEmbedding,
      topK,
      indexName: 'coarse_256d'
    })

    return results
  }

  private async rerankWithFull(
    fullEmbedding: number[],
    candidates: CandidateResult[],
    topK: number
  ): Promise<SearchResult[]> {
    // Fetch full 3072D embeddings for just the top 100 candidates
    const candidateIds = candidates.map(c => c.id)
    const fullVectors = await vectorDB.fetchVectors(candidateIds, 'full_3072d')

    // Compute precise similarity with full embeddings
    const scored = candidates.map((candidate, i) => ({
      ...candidate,
      preciseSimilarity: cosineSimilarity(fullEmbedding, fullVectors[i])
    }))

    // Sort by precise similarity and return top K
    return scored
      .sort((a, b) => b.preciseSimilarity - a.preciseSimilarity)
      .slice(0, topK)
  }
}

/* Performance Comparison (1M vectors, legal documents):

Single-Stage (3072D full search):
- Latency: 150ms
- Accuracy: 94%
- Cost: $0.13/1M tokens

Two-Stage Matryoshka:
- Latency: 15ms (10ms coarse + 5ms rerank)
- Accuracy: 93.5% (negligible loss)
- Cost: $0.13/1M tokens (same - you need full embeddings anyway)

Winner: Matryoshka
- 10x faster (15ms vs 150ms)
- 0.5% accuracy trade-off (acceptable)
- Same cost (just a different indexing strategy)
*/
```

### Database Schema for Matryoshka Storage

```typescript
// Store both coarse and full embeddings in separate columns/tables
const setupMatryoshkaSchema = `
  CREATE TABLE document_embeddings (
    id TEXT PRIMARY KEY,
    document_id TEXT NOT NULL,
    content TEXT NOT NULL,

    -- Coarse embedding for Stage 1 filtering (fast)
    embedding_coarse vector(256),

    -- Full embedding for Stage 2 precision (accurate)
    embedding_full vector(3072),

    metadata JSONB,
    created_at TIMESTAMP DEFAULT NOW()
  );

  -- Index on coarse embeddings (Stage 1 - majority of compute)
  CREATE INDEX embedding_coarse_idx ON document_embeddings
  USING hnsw (embedding_coarse vector_cosine_ops)
  WITH (m = 16, ef_construction = 64);

  -- NO index on full embeddings (Stage 2 - only 100 vectors, brute force is fine)
`
```

### When to Use Matryoshka Embeddings

**Use Matryoshka when**:
- ✅ **Large-scale search** (&gt;500K vectors) where latency matters
- ✅ **High-precision domains** (legal, medical, code) where small models fail
- ✅ **Real-time applications** (customer-facing search) needing &lt;20ms response
- ✅ **Cost-conscious** but can't sacrifice accuracy for small models

**Don't use Matryoshka when**:
- ❌ **Small datasets** (&lt;100K vectors) where full search is already fast
- ❌ **General-purpose RAG** where text-embedding-3-small suffices
- ❌ **Simplicity preferred** and 150ms latency is acceptable

### The Dimension Density Decision Matrix

| Domain Complexity | Recommended Model | Dimensions | Strategy |
|-------------------|------------------|------------|----------|
| **Simple** (blog posts, FAQs) | text-embedding-3-small | 1536 | Single-stage |
| **Moderate** (technical docs, support) | text-embedding-3-small | 1536 | Single-stage + re-ranking |
| **Complex** (legal, medical, finance) | text-embedding-3-large | 3072 | Single-stage (full) |
| **Complex + Large-scale** (legal + 1M docs) | text-embedding-3-large | 256 + 3072 | **Matryoshka (two-stage)** |

**The Architect's Verdict**:

> "Don't just look at cost per token. For highly technical domains, 'small' models conflate semantically distinct concepts into similar vectors. Use **text-embedding-3-large** for precision, and implement **Matryoshka Embeddings** to maintain speed.
>
> The two-stage Matryoshka approach gives you **10x faster search** (15ms vs 150ms) with only **0.5% accuracy loss**. This is the production pattern for large-scale RAG in specialized domains."

**ROI Calculation**:
```typescript
const matryoshkaROI = {
  initialSetup: {
    engineeringTime: '3 days',
    infrastructureCost: '$0 (same storage, different indexing)',
    total: '$3,000'
  },

  performanceGains: {
    latencyImprovement: '10x faster (150ms → 15ms)',
    accuracyLoss: '0.5% (93.5% vs 94%)',
    userSatisfaction: '+20% (perceived as "instant" under 20ms)',
    throughputIncrease: '10x (more queries per second)'
  },

  businessImpact: {
    customerRetention: '+5% (fast search = better UX)',
    revenueIncrease: '$50K/year (5% × $1M revenue)',
    costReduction: '$0 (same embedding cost)',
    netBenefit: '$47K/year (after engineering cost)'
  }
}

// Break-even: 3 days engineering / $50K annual benefit = &lt;1 month ROI
```

---

## Distance Metrics: Choosing the Right Math

Vector similarity can be measured in multiple ways. Choosing the wrong metric can degrade retrieval quality by 10-20%.

### The Three Core Metrics

```typescript
/**
 * Cosine Similarity: Measures angle between vectors
 * Range: -1 (opposite) to 1 (identical)
 * Use: When magnitude doesn't matter (most common)
 */
function cosineSimilarity(a: number[], b: number[]): number {
  const dotProduct = a.reduce((sum, val, i) => sum + val * b[i], 0)
  const magnitudeA = Math.sqrt(a.reduce((sum, val) => sum + val * val, 0))
  const magnitudeB = Math.sqrt(b.reduce((sum, val) => sum + val * val, 0))

  return dotProduct / (magnitudeA * magnitudeB)
}

/**
 * Euclidean Distance: Straight-line distance in N-dimensional space
 * Range: 0 (identical) to ∞ (very different)
 * Use: When absolute magnitude matters
 */
function euclideanDistance(a: number[], b: number[]): number {
  const squaredDiffs = a.map((val, i) => Math.pow(val - b[i], 2))
  return Math.sqrt(squaredDiffs.reduce((sum, val) => sum + val, 0))
}

/**
 * Dot Product: Sum of element-wise products
 * Range: -∞ to ∞
 * Use: When vectors are pre-normalized (fastest)
 */
function dotProduct(a: number[], b: number[]): number {
  return a.reduce((sum, val, i) => sum + val * b[i], 0)
}
```

### Visual Comparison: When Metrics Disagree

Consider three document embeddings:
- **Query**: `[1.0, 0.0]`
- **Doc A**: `[0.9, 0.1]` (similar direction, similar magnitude)
- **Doc B**: `[2.0, 0.0]` (exact same direction, 2x magnitude)

| Metric | Doc A Score | Doc B Score | Winner |
|--------|-------------|-------------|--------|
| **Cosine** | 0.995 | 1.000 | Doc B (ignores magnitude) |
| **Euclidean** | 0.141 | 1.000 | Doc A (penalizes magnitude difference) |
| **Dot Product** | 0.900 | 2.000 | Doc B (rewards larger vectors) |

### Decision Matrix: Which Metric to Use

| Scenario | Recommended Metric | Reasoning |
|----------|-------------------|-----------|
| **General RAG** | Cosine Similarity | Most embedding models are NOT normalized - cosine handles varying magnitudes |
| **Pre-normalized embeddings** | Dot Product | 2-3x faster than cosine, equivalent results when normalized |
| **Image embeddings** | Euclidean Distance | Magnitude often encodes meaningful info (brightness, contrast) |
| **Recommendation systems** | Dot Product | Favors popular items (higher magnitude = more interactions) |
| **Anomaly detection** | Euclidean Distance | Absolute distance from cluster center matters |

### The Normalization Trick: Make Cosine = Dot Product

```typescript
/**
 * Normalize vector to unit length
 * After normalization, dot product = cosine similarity (but 3x faster!)
 */
function normalize(vector: number[]): number[] {
  const magnitude = Math.sqrt(vector.reduce((sum, val) => sum + val * val, 0))
  return vector.map(v => v / magnitude)
}

// Performance comparison
const query = await embed("machine learning")
const doc = await embed("artificial intelligence")

// Method 1: Cosine (slower)
const cosine = cosineSimilarity(query, doc)  // 150 µs

// Method 2: Dot product after normalization (faster)
const normQuery = normalize(query)
const normDoc = normalize(doc)
const dotProd = dotProduct(normQuery, normDoc)  // 50 µs

console.log(cosine === dotProd)  // true (within floating-point precision)
```

**Production Tip**: Normalize embeddings once during indexing, then use dot product for all searches. This is 3x faster than cosine and gives identical results.

### Pgvector Operator Reference

When using pgvector, choose the right operator:

```sql
-- Cosine Distance (most common)
SELECT * FROM chunks ORDER BY embedding &lt;=&gt; query_vector LIMIT 5;

-- Euclidean Distance (L2)
SELECT * FROM chunks ORDER BY embedding <-> query_vector LIMIT 5;

-- Dot Product (for normalized vectors)
SELECT * FROM chunks ORDER BY embedding <#> query_vector LIMIT 5;

-- Note: pgvector uses distance (lower = more similar), not similarity
-- To convert: similarity = 1 - distance
```

### Production Performance: Index Choice Matters

```typescript
interface VectorIndexConfig {
  type: 'hnsw' | 'ivfflat'
  distanceMetric: 'cosine' | 'euclidean' | 'dot'
  indexParams?: {
    m?: number             // HNSW: connections per layer
    efConstruction?: number // HNSW: build quality
    lists?: number         // IVFFlat: number of clusters
  }
}

/**
 * Production index configuration for pgvector
 */
const PRODUCTION_CONFIG: VectorIndexConfig = {
  type: 'hnsw',
  distanceMetric: 'cosine',
  indexParams: {
    m: 16,               // Good balance of speed/recall
    efConstruction: 64   // Higher = better quality, slower build
  }
}

// SQL to create production index
const createIndexSQL = `
  CREATE INDEX embedding_idx ON chunks
  USING hnsw (embedding vector_cosine_ops)
  WITH (m = 16, ef_construction = 64);
`
```

**Index Performance Comparison** (1M vectors, 1536 dimensions):

| Index Type | Build Time | Query Latency (P95) | Recall@10 | Memory |
|------------|-----------|-------------------|-----------|---------|
| **HNSW** | 45 min | 15ms | 98% | 12 GB |
| **IVFFlat** (100 lists) | 10 min | 25ms | 92% | 8 GB |
| **No Index (brute force)** | 0 min | 8,000ms | 100% | 6 GB |

**Key Takeaway**: HNSW is almost always the right choice for production. IVFFlat is only better if you have &lt;100K vectors or extreme memory constraints.

---

## Approximate Nearest Neighbor (ANN): The Infrastructure Trade-Off

**The Scalability Problem**: Exact K-Nearest Neighbor (kNN) search is **O(N)**—you must compare the query against EVERY vector in the database. This dies at scale.

### Why Brute-Force Search Fails at Scale

```typescript
// The O(N) problem with exact kNN
function exactKNN(
  queryVector: number[],
  allVectors: number[][],  // 1 million vectors
  k: number = 10
): SearchResult[] {
  // Must compute similarity for ALL vectors
  const similarities = allVectors.map((vec, i) => ({
    id: i,
    similarity: cosineSimilarity(queryVector, vec)
  }))

  // Sort and return top K
  return similarities
    .sort((a, b) => b.similarity - a.similarity)
    .slice(0, k)
}

/* Performance disaster:
- 1536 dimensions × 1M vectors = 1.5 billion comparisons
- Latency: 8+ seconds per query
- Throughput: &lt;1 query per second
- Cost: Requires massive CPU to handle concurrent queries
*/
```

**The Architect's Reality**:

| Dataset Size | Exact kNN Latency | User Expectation | Verdict |
|--------------|------------------|------------------|---------|
| 10K vectors | 80ms | &lt;200ms | ✅ Acceptable |
| 100K vectors | 800ms | &lt;200ms | ❌ Too slow |
| 1M vectors | 8,000ms | &lt;200ms | ❌❌ Catastrophic |
| 10M vectors | 80,000ms (80s!) | &lt;200ms | ❌❌❌ Unusable |

**The Solution**: **Approximate Nearest Neighbor (ANN) algorithms**—trade 1-2% recall for 100-1000x speed improvement.

### The ANN Paradigm Shift: From Comparison to Navigation

**Key Insight**: Instead of "comparing everything," ANN algorithms build a **searchable data structure** (graph or index) that lets you **navigate** to the nearest neighbors without checking every vector.

```typescript
// Exact kNN: Compare against every vector
// Complexity: O(N × D) where N = vectors, D = dimensions
const exactResults = compareAgainstAll(query, allVectors)

// ANN (HNSW): Navigate a graph to likely neighbors
// Complexity: O(log N × D) - exponentially faster!
const approxResults = navigateGraph(query, hnswGraph)
```

### HNSW: Hierarchical Navigable Small World

**The Concept**: Build a multi-layer graph where each vector is a node, connected to its nearest neighbors. Search starts at the top (coarse) layer and descends to bottom (precise) layer.

```typescript
/**
 * HNSW Index Structure
 *
 * Layer 2 (coarse):     A ←→ B ←→ C
 *                        ↓     ↓     ↓
 * Layer 1 (medium):   A₁ ←→ B₁ ←→ C₁ ←→ D₁
 *                      ↓     ↓     ↓     ↓
 * Layer 0 (precise): A₀←→B₀←→C₀←→D₀←→E₀←→F₀←→G₀
 *
 * Search Process:
 * 1. Start at Layer 2, find closest node (B)
 * 2. Descend to Layer 1, navigate to closer node (B₁ → C₁)
 * 3. Descend to Layer 0, refine to final neighbors (C₀, D₀, E₀)
 *
 * Result: Checked ~10 nodes instead of 1,000,000
 */

interface HNSWConfig {
  m: number               // Connections per layer (default: 16)
  efConstruction: number  // Build quality (default: 64)
  efSearch: number        // Search quality (default: 100)
}

class HNSWIndex {
  private config: HNSWConfig = {
    m: 16,              // Each node connects to 16 neighbors
    efConstruction: 64, // Consider 64 candidates during build
    efSearch: 100       // Consider 100 candidates during search
  }

  /**
   * Build HNSW index - expensive one-time operation
   */
  async build(vectors: EmbeddingVector[]): Promise<void> {
    console.log(`Building HNSW index for ${vectors.length} vectors...`)
    const startTime = Date.now()

    // Phase 1: Insert vectors layer-by-layer
    for (const vector of vectors) {
      await this.insertVector(vector)
    }

    // Phase 2: Optimize connections
    await this.pruneConnections()

    const buildTime = Date.now() - startTime
    console.log(`HNSW build complete: ${buildTime}ms (${buildTime / vectors.length}ms per vector)`)
  }

  /**
   * Search HNSW index - fast logarithmic lookup
   */
  async search(
    queryVector: number[],
    k: number = 10
  ): Promise<SearchResult[]> {
    const startTime = Date.now()

    // Start at top layer
    let currentLayer = this.maxLayer
    let currentNode = this.entryPoint

    // Descend through layers
    while (currentLayer > 0) {
      currentNode = this.findClosestInLayer(queryVector, currentNode, currentLayer)
      currentLayer--
    }

    // Final search at layer 0 (most precise)
    const candidates = this.searchLayer0(queryVector, currentNode, this.config.efSearch)

    // Return top K
    const results = candidates.slice(0, k)

    const searchTime = Date.now() - startTime
    console.log(`HNSW search: ${searchTime}ms`)

    return results
  }

  private findClosestInLayer(
    query: number[],
    startNode: Node,
    layer: number
  ): Node {
    // Greedy search: navigate to closest neighbor iteratively
    let current = startNode
    let improved = true

    while (improved) {
      improved = false
      const neighbors = current.connections[layer]

      for (const neighbor of neighbors) {
        const similarity = cosineSimilarity(query, neighbor.vector)
        if (similarity > cosineSimilarity(query, current.vector)) {
          current = neighbor
          improved = true
        }
      }
    }

    return current
  }
}
```

### HNSW Performance Characteristics

**Build Time**:
```typescript
// HNSW index build is expensive (one-time cost)
const buildPerformance = {
  '100K vectors': '5 minutes',
  '1M vectors': '45 minutes',
  '10M vectors': '7 hours'
}

// But it's a one-time cost - amortized over billions of queries
const amortizedCost = {
  buildTime: 45 * 60,        // 45 minutes = 2,700 seconds
  queriesServed: 1_000_000_000,  // 1 billion queries over lifetime
  costPerQuery: 2700 / 1_000_000_000  // = 0.0000027 seconds = negligible
}
```

**Search Time**:
```typescript
// HNSW search is logarithmic
const searchLatency = {
  '100K vectors': '5ms',
  '1M vectors': '15ms',    // ← Only 3x slower for 10x more data
  '10M vectors': '35ms',   // ← Only 7x slower for 100x more data
  '100M vectors': '80ms'   // ← Only 16x slower for 1000x more data
}

// Compare to exact kNN (linear):
const exactKNNLatency = {
  '100K vectors': '800ms',
  '1M vectors': '8,000ms',     // ← 10x slower for 10x more data
  '10M vectors': '80,000ms',   // ← 100x slower for 100x more data
  '100M vectors': '800,000ms'  // ← 1000x slower for 1000x more data
}

// HNSW is 100-1000x faster at scale
```

**Accuracy (Recall)**:
```typescript
// HNSW recall @ k=10 (how many of the true top-10 are found)
const hnswRecall = {
  'efSearch = 50': '92% recall',   // Fast but less accurate
  'efSearch = 100': '97% recall',  // ← Production sweet spot
  'efSearch = 200': '99% recall',  // Slower, near-perfect
  'Exact kNN': '100% recall'       // Perfect but 100x slower
}

// Trade-off: 97% recall at 15ms >> 100% recall at 8,000ms
```

### IVF (Inverted File Index): The Alternative

**The Concept**: Cluster vectors into N groups (cells), then only search within the most relevant cells.

```typescript
/**
 * IVF Index Structure
 *
 * Training phase: Cluster 1M vectors into 1000 cells
 *
 * Cell 1: [vec_a, vec_b, vec_c, ...]      (1000 vectors)
 * Cell 2: [vec_x, vec_y, vec_z, ...]      (1000 vectors)
 * ...
 * Cell 1000: [vec_j, vec_k, vec_l, ...]   (1000 vectors)
 *
 * Search process:
 * 1. Find 5 closest cell centroids (compare query to 1000 centroids)
 * 2. Search within those 5 cells (5 × 1000 = 5,000 vectors)
 * 3. Return top K
 *
 * Result: Checked 5,000 vectors instead of 1,000,000 (200x speedup)
 */

interface IVFConfig {
  nlist: number      // Number of cells/clusters
  nprobe: number     // Number of cells to search
}

class IVFIndex {
  private config: IVFConfig = {
    nlist: 1000,   // 1000 clusters
    nprobe: 5      // Search 5 closest clusters
  }

  async build(vectors: EmbeddingVector[]): Promise<void> {
    // Phase 1: Train centroids using k-means clustering
    this.centroids = await this.kMeansClustering(vectors, this.config.nlist)

    // Phase 2: Assign each vector to nearest centroid
    for (const vector of vectors) {
      const nearestCentroid = this.findNearestCentroid(vector)
      this.cells[nearestCentroid].push(vector)
    }
  }

  async search(queryVector: number[], k: number = 10): Promise<SearchResult[]> {
    // Step 1: Find nprobe closest centroids (fast: only 1000 comparisons)
    const closestCentroids = this.findClosestCentroids(queryVector, this.config.nprobe)

    // Step 2: Search within those cells only
    const candidates: SearchResult[] = []
    for (const centroidId of closestCentroids) {
      const cellVectors = this.cells[centroidId]
      for (const vec of cellVectors) {
        candidates.push({
          id: vec.id,
          similarity: cosineSimilarity(queryVector, vec.embedding)
        })
      }
    }

    // Step 3: Sort and return top K
    return candidates
      .sort((a, b) => b.similarity - a.similarity)
      .slice(0, k)
  }
}
```

### HNSW vs IVF: The Architect's Decision Matrix

| Factor | HNSW | IVF | Winner |
|--------|------|-----|--------|
| **Search Latency** | 15ms (1M vectors) | 25ms (1M vectors) | ✅ HNSW (faster) |
| **Build Time** | 45 minutes (1M vectors) | 10 minutes (1M vectors) | ✅ IVF (faster build) |
| **Memory Usage** | 12 GB (1M vectors, 1536D) | 8 GB (1M vectors, 1536D) | ✅ IVF (lower memory) |
| **Recall @ k=10** | 97% (efSearch=100) | 92% (nprobe=5) | ✅ HNSW (more accurate) |
| **Insert Performance** | Slow (rebuild graph) | Fast (assign to cell) | ✅ IVF (better for streaming) |
| **Complexity** | O(log N) | O(N/C) where C = nlist | ✅ HNSW (scales better) |

**Decision Guidelines**:

```typescript
function chooseANNAlgorithm(requirements: {
  datasetSize: number
  insertionFrequency: 'static' | 'batch' | 'streaming'
  latencyBudget: number
  accuracyThreshold: number
  memoryConstraint: number
}): 'HNSW' | 'IVF' {
  // Rule 1: Small datasets (&lt;100K) don't need ANN
  if (requirements.datasetSize < 100_000) {
    return 'HNSW'  // Overhead not worth it, but HNSW still wins
  }

  // Rule 2: Streaming inserts → IVF (easier to update)
  if (requirements.insertionFrequency === 'streaming') {
    return 'IVF'
  }

  // Rule 3: Tight memory constraint → IVF (30% less memory)
  if (requirements.memoryConstraint < 10_000) {  // MB
    return 'IVF'
  }

  // Rule 4: High accuracy requirement → HNSW (97% vs 92%)
  if (requirements.accuracyThreshold > 95) {
    return 'HNSW'
  }

  // Rule 5: Default → HNSW (best overall performance)
  return 'HNSW'
}

// Examples:
chooseANNAlgorithm({
  datasetSize: 1_000_000,
  insertionFrequency: 'static',
  latencyBudget: 20,
  accuracyThreshold: 97,
  memoryConstraint: 16_000
}) // → 'HNSW' (production standard)

chooseANNAlgorithm({
  datasetSize: 5_000_000,
  insertionFrequency: 'streaming',
  latencyBudget: 50,
  accuracyThreshold: 90,
  memoryConstraint: 8_000
}) // → 'IVF' (memory-constrained, streaming updates)
```

### Production Deployment: pgvector with HNSW

```typescript
// SQL to create HNSW index in production
const createHNSWIndex = `
  -- Create HNSW index with production parameters
  CREATE INDEX embedding_hnsw_idx ON document_chunks
  USING hnsw (embedding vector_cosine_ops)
  WITH (
    m = 16,                 -- Connections per layer (16 = balanced)
    ef_construction = 64    -- Build quality (64 = production default)
  );

  -- For search, set ef dynamically based on accuracy needs
  SET hnsw.ef_search = 100;  -- 97% recall (production sweet spot)
  -- SET hnsw.ef_search = 200;  -- 99% recall (high-accuracy queries)
  -- SET hnsw.ef_search = 50;   -- 92% recall (speed-optimized queries)
`

// Performance tuning
const hnswTuning = {
  'm = 16': {
    memory: '1x',
    buildTime: '1x',
    searchSpeed: '1x',
    recall: '97%',
    use: 'Default (recommended)'
  },

  'm = 8': {
    memory: '0.5x',
    buildTime: '0.6x',
    searchSpeed: '1.3x',
    recall: '94%',
    use: 'Memory-constrained environments'
  },

  'm = 32': {
    memory: '2x',
    buildTime: '1.8x',
    searchSpeed: '0.8x',
    recall: '99%',
    use: 'High-accuracy applications (legal, medical)'
  }
}
```

**The Architect's Rule**: For production RAG, **HNSW is almost always the right choice**. It shifts your search from "comparing everything" to "navigating a graph," reducing search time from **seconds to milliseconds** for million-scale datasets.

**ROI Calculation**:
```typescript
const annROI = {
  withoutANN: {
    latency: '8,000ms (1M vectors)',
    maxThroughput: '1 query/second',
    costPerQuery: '$0.05 (massive CPU)',
    userExperience: 'Unacceptable'
  },

  withHNSW: {
    latency: '15ms (1M vectors)',
    maxThroughput: '1,000 queries/second',
    costPerQuery: '$0.0001 (efficient)',
    userExperience: 'Instant',
    indexBuildCost: '45 minutes one-time'
  },

  improvement: {
    speedup: '533x faster (8,000ms → 15ms)',
    throughputIncrease: '1,000x more queries/second',
    costReduction: '99.8% per query',
    tradeOff: '3% recall loss (100% → 97%)'
  }
}

// Verdict: HNSW is mandatory for datasets &gt;100K vectors
// The 45-minute build time is negligible compared to billions of fast queries
```

---

## Hybrid Search: Vector + Keyword with Reciprocal Rank Fusion

> **Architect Perspective**: Vector search is great for "vibes" (semantic similarity), but it's terrible at finding specific part numbers, unique IDs, or exact terms. A Director-level architecture uses **Hybrid Search**: run a Keyword (BM25) search and a Vector search in parallel, then combine the results using **Reciprocal Rank Fusion (RRF)**. This ensures that if a user asks for "Part #XJ-900", they get that exact part, not just something that "feels" like a machine part.

### The Hybrid Search Problem

**Reality Check**: Vector search alone fails at precision tasks.

**Example Failures**:
```typescript
// User query: "Find invoice #INV-2024-1337"

// Vector search results (semantic similarity):
const vectorResults = [
  'Invoice #INV-2024-1338 - Similar date',           // ❌ Wrong invoice
  'Invoice template for 2024',                       // ❌ Not an invoice
  'Invoice #INV-2023-1337 - Same number, wrong year' // ❌ Close but wrong
]

// The EXACT invoice exists in the database, but vector search
// prioritizes "semantic similarity" over exact matches
```

**The Root Cause**: Vector embeddings compress meaning into 1536 dimensions. Specific tokens like "1337" or "XJ-900" get "averaged out" during embedding, losing their distinctiveness.

### BM25: The Keyword Search Engine

**BM25 (Best Match 25)** is a ranking function used by search engines like Elasticsearch. It excels at **exact term matching** and **keyword frequency**.

**How BM25 Works**:
```typescript
interface BM25Config {
  k1: number  // Term frequency saturation (default: 1.2)
  b: number   // Length normalization (default: 0.75)
}

/**
 * BM25 score = Σ IDF(term) × (TF × (k1 + 1)) / (TF + k1 × (1 - b + b × (docLength / avgDocLength)))
 *
 * Where:
 * - IDF = Inverse Document Frequency (rare words score higher)
 * - TF = Term Frequency (how often term appears in doc)
 * - docLength = Length of current document
 * - avgDocLength = Average length of all documents
 */
function bm25Score(
  query: string[],
  document: string[],
  config: BM25Config = { k1: 1.2, b: 0.75 }
): number {
  let score = 0

  for (const term of query) {
    const tf = document.filter(t => t === term).length  // Term frequency
    const idf = Math.log((totalDocs - docsWithTerm(term) + 0.5) / (docsWithTerm(term) + 0.5))

    const numerator = tf * (config.k1 + 1)
    const denominator = tf + config.k1 * (1 - config.b + config.b * (document.length / avgDocLength))

    score += idf * (numerator / denominator)
  }

  return score
}
```

**BM25 Strengths**:
- ✅ Perfect for exact matches (part numbers, IDs, emails)
- ✅ Fast (inverted index lookup)
- ✅ Rare terms score higher (good for technical jargon)

**BM25 Weaknesses**:
- ❌ No semantic understanding ("car" ≠ "automobile")
- ❌ Sensitive to typos ("invoce" won't match "invoice")
- ❌ No understanding of synonyms or paraphrases

### Parallel Execution: Vector + BM25

**The Pattern**: Run both searches concurrently, then merge results.

```typescript
import Anthropic from '@anthropic-ai/sdk'
import { Client } from '@elastic/elasticsearch'

const anthropic = new Anthropic({ apiKey: process.env.ANTHROPIC_API_KEY })
const elasticsearch = new Client({ node: 'http://localhost:9200' })

interface SearchResult {
  id: string
  content: string
  score: number
  source: 'vector' | 'bm25' | 'hybrid'
}

async function hybridSearch(
  query: string,
  k: number = 10
): Promise<SearchResult[]> {
  // Generate embedding for vector search
  const embeddingPromise = openai.embeddings.create({
    model: 'text-embedding-3-small',
    input: query
  })

  // Execute BM25 keyword search
  const bm25Promise = elasticsearch.search({
    index: 'documents',
    body: {
      query: {
        match: {
          content: {
            query: query,
            operator: 'or'
          }
        }
      },
      size: 100  // Fetch more candidates for RRF
    }
  })

  // Wait for both searches in parallel
  const [embeddingResponse, bm25Response] = await Promise.all([
    embeddingPromise,
    bm25Promise
  ])

  // Vector search (pgvector)
  const queryEmbedding = embeddingResponse.data[0].embedding
  const vectorResults = await prisma.$queryRaw<SearchResult[]>`
    SELECT id, content,
           1 - (embedding <=> ${queryEmbedding}::vector) AS score
    FROM document_chunks
    ORDER BY embedding <=> ${queryEmbedding}::vector
    LIMIT 100
  `

  // BM25 results
  const bm25Results: SearchResult[] = bm25Response.hits.hits.map(hit => ({
    id: hit._id,
    content: hit._source.content,
    score: hit._score,
    source: 'bm25'
  }))

  // Merge with Reciprocal Rank Fusion
  return reciprocalRankFusion(vectorResults, bm25Results, k)
}
```

**Latency Comparison**:
```typescript
const performance = {
  vectorOnly: {
    embeddingGeneration: '50ms',
    vectorSearch: '15ms (HNSW)',
    total: '65ms'
  },

  bm25Only: {
    keywordSearch: '20ms (Elasticsearch)',
    total: '20ms'
  },

  hybrid: {
    embeddingGeneration: '50ms (parallel)',
    vectorSearch: '15ms (parallel)',
    bm25Search: '20ms (parallel)',
    rrfMerge: '5ms',
    total: '70ms'  // Only 50ms + 20ms + 5ms (max of parallel + merge)
  }
}

// Verdict: Hybrid search adds only 5ms latency (RRF merge cost)
// You get both semantic AND exact matching for just 7% more latency
```

### Reciprocal Rank Fusion (RRF)

**The Algorithm**: Merge rankings by considering each result's position in both lists.

```typescript
/**
 * Reciprocal Rank Fusion
 *
 * RRF score = Σ 1 / (k + rank_i)
 *
 * Where:
 * - k = constant (default: 60) to prevent division by zero
 * - rank_i = position of result in ranking i (1-indexed)
 */
function reciprocalRankFusion(
  vectorResults: SearchResult[],
  bm25Results: SearchResult[],
  k: number = 10,
  rrfConstant: number = 60
): SearchResult[] {
  // Create a map to store combined scores
  const scoreMap = new Map<string, { result: SearchResult; rrfScore: number }>()

  // Add vector results
  vectorResults.forEach((result, index) => {
    const rank = index + 1  // 1-indexed
    const rrfScore = 1 / (rrfConstant + rank)

    scoreMap.set(result.id, {
      result: { ...result, source: 'vector' as const },
      rrfScore
    })
  })

  // Add BM25 results (merge if already exists)
  bm25Results.forEach((result, index) => {
    const rank = index + 1
    const rrfScore = 1 / (rrfConstant + rank)

    const existing = scoreMap.get(result.id)
    if (existing) {
      // Document appears in BOTH rankings - boost it!
      scoreMap.set(result.id, {
        result: { ...result, source: 'hybrid' as const },
        rrfScore: existing.rrfScore + rrfScore  // Sum RRF scores
      })
    } else {
      scoreMap.set(result.id, {
        result: { ...result, source: 'bm25' as const },
        rrfScore
      })
    }
  })

  // Sort by RRF score and return top K
  return Array.from(scoreMap.values())
    .sort((a, b) => b.rrfScore - a.rrfScore)
    .slice(0, k)
    .map(item => ({
      ...item.result,
      score: item.rrfScore
    }))
}
```

**Why RRF Works Better Than Score Averaging**:

```typescript
// Example: User searches for "red Tesla Model 3"

// Vector results (semantic similarity):
const vectorResults = [
  { id: '1', content: 'Red Tesla Model 3 review', rank: 1 },      // Perfect match
  { id: '2', content: 'Blue Tesla Model 3 specs', rank: 2 },      // Good match
  { id: '3', content: 'Red Ford Mustang', rank: 3 }               // Partial match
]

// BM25 results (keyword matching):
const bm25Results = [
  { id: '1', content: 'Red Tesla Model 3 review', rank: 1 },      // Exact terms
  { id: '4', content: 'Tesla Model 3 in red color', rank: 2 },    // All terms present
  { id: '5', content: 'Model 3 Tesla red interior', rank: 3 }     // Terms scattered
]

// RRF scores:
const rrfScores = {
  '1': 1/(60+1) + 1/(60+1) = 0.0328,  // ← Appears in BOTH (rank 1 + rank 1)
  '2': 1/(60+2) = 0.0161,              // Only in vector (rank 2)
  '4': 1/(60+2) = 0.0161,              // Only in BM25 (rank 2)
  '3': 1/(60+3) = 0.0159,              // Only in vector (rank 3)
  '5': 1/(60+3) = 0.0159               // Only in BM25 (rank 3)
}

// Final ranking:
// 1. ID=1 (0.0328) ← Boosted because it appears in BOTH rankings
// 2. ID=2 (0.0161)
// 3. ID=4 (0.0161)
// 4. ID=3 (0.0159)
// 5. ID=5 (0.0159)
```

**The Key Insight**: Documents that rank well in **both** vector and BM25 searches get exponentially higher RRF scores. This naturally surfaces results that are both **semantically relevant** AND **keyword-accurate**.

### Production Implementation

**Full System with Elasticsearch + pgvector**:

```typescript
import { Client } from '@elastic/elasticsearch'
import { PrismaClient } from '@prisma/client'
import OpenAI from 'openai'

const elasticsearch = new Client({ node: process.env.ELASTICSEARCH_URL })
const prisma = new PrismaClient()
const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY })

interface HybridSearchConfig {
  vectorWeight: number  // Default: 1.0
  bm25Weight: number    // Default: 1.0
  rrfConstant: number   // Default: 60
  candidateSize: number // Default: 100 (fetch more for better RRF)
}

class HybridSearchEngine {
  constructor(private config: HybridSearchConfig = {
    vectorWeight: 1.0,
    bm25Weight: 1.0,
    rrfConstant: 60,
    candidateSize: 100
  }) {}

  async search(
    query: string,
    k: number = 10,
    filters?: {
      dateRange?: { start: Date; end: Date }
      category?: string
      priceRange?: { min: number; max: number }
    }
  ): Promise<SearchResult[]> {
    const startTime = Date.now()

    // 1. Generate embedding for vector search
    const embeddingPromise = openai.embeddings.create({
      model: 'text-embedding-3-small',
      input: query
    })

    // 2. Build BM25 query with filters
    const bm25Query: any = {
      bool: {
        must: [
          { match: { content: { query, operator: 'or' } } }
        ],
        filter: []
      }
    }

    if (filters?.dateRange) {
      bm25Query.bool.filter.push({
        range: {
          created_at: {
            gte: filters.dateRange.start,
            lte: filters.dateRange.end
          }
        }
      })
    }

    if (filters?.category) {
      bm25Query.bool.filter.push({
        term: { category: filters.category }
      })
    }

    // 3. Execute both searches in parallel
    const [embeddingResponse, bm25Response] = await Promise.all([
      embeddingPromise,
      elasticsearch.search({
        index: 'documents',
        body: { query: bm25Query, size: this.config.candidateSize }
      })
    ])

    // 4. Vector search with same filters
    const queryEmbedding = embeddingResponse.data[0].embedding

    let vectorQuery = `
      SELECT id, content, metadata,
             1 - (embedding <=> $1::vector) AS score
      FROM document_chunks
      WHERE 1=1
    `
    const params: any[] = [queryEmbedding]
    let paramIndex = 2

    if (filters?.category) {
      vectorQuery += ` AND metadata->>'category' = $${paramIndex}`
      params.push(filters.category)
      paramIndex++
    }

    if (filters?.dateRange) {
      vectorQuery += ` AND created_at BETWEEN $${paramIndex} AND $${paramIndex + 1}`
      params.push(filters.dateRange.start, filters.dateRange.end)
      paramIndex += 2
    }

    vectorQuery += `
      ORDER BY embedding <=> $1::vector
      LIMIT ${this.config.candidateSize}
    `

    const vectorResults = await prisma.$queryRawUnsafe<SearchResult[]>(
      vectorQuery,
      ...params
    )

    // 5. Apply RRF
    const bm25Results: SearchResult[] = bm25Response.hits.hits.map(hit => ({
      id: hit._id,
      content: hit._source.content,
      score: hit._score,
      metadata: hit._source.metadata,
      source: 'bm25' as const
    }))

    const mergedResults = this.reciprocalRankFusion(
      vectorResults,
      bm25Results,
      k
    )

    const totalTime = Date.now() - startTime
    console.log(`Hybrid search completed in ${totalTime}ms`)
    console.log(`- Vector: ${vectorResults.length} candidates`)
    console.log(`- BM25: ${bm25Results.length} candidates`)
    console.log(`- RRF merged: ${mergedResults.length} final results`)

    return mergedResults
  }

  private reciprocalRankFusion(
    vectorResults: SearchResult[],
    bm25Results: SearchResult[],
    k: number
  ): SearchResult[] {
    const scoreMap = new Map<string, { result: SearchResult; rrfScore: number }>()

    // Vector results
    vectorResults.forEach((result, index) => {
      const rank = index + 1
      const rrfScore = this.config.vectorWeight / (this.config.rrfConstant + rank)

      scoreMap.set(result.id, {
        result: { ...result, source: 'vector' as const },
        rrfScore
      })
    })

    // BM25 results
    bm25Results.forEach((result, index) => {
      const rank = index + 1
      const rrfScore = this.config.bm25Weight / (this.config.rrfConstant + rank)

      const existing = scoreMap.get(result.id)
      if (existing) {
        scoreMap.set(result.id, {
          result: { ...result, source: 'hybrid' as const },
          rrfScore: existing.rrfScore + rrfScore
        })
      } else {
        scoreMap.set(result.id, {
          result: { ...result, source: 'bm25' as const },
          rrfScore
        })
      }
    })

    return Array.from(scoreMap.values())
      .sort((a, b) => b.rrfScore - a.rrfScore)
      .slice(0, k)
      .map(item => ({
        ...item.result,
        score: item.rrfScore
      }))
  }
}

// Usage
const searchEngine = new HybridSearchEngine()

const results = await searchEngine.search(
  'red Tesla Model 3',
  10,
  {
    dateRange: {
      start: new Date('2024-01-01'),
      end: new Date('2024-12-31')
    },
    category: 'vehicles'
  }
)
```

### When to Use Hybrid vs. Vector-Only

**Decision Matrix**:

| Use Case | Recommended Approach | Reason |
|----------|---------------------|--------|
| **General Q&A** | Vector-only | Semantic similarity is sufficient |
| **E-commerce search** | Hybrid (RRF) | Users mix semantic ("red dress") + exact (price, size) |
| **Technical docs** | Hybrid (RRF) | Function names, error codes need exact matching |
| **Legal/Medical** | Hybrid (RRF) | Precise terminology critical |
| **Chatbot memory** | Vector-only | Conversational context is semantic |
| **Code search** | Hybrid (RRF) | Variable names, function names are exact tokens |
| **Customer support** | Hybrid (RRF) | Ticket IDs, product SKUs need exact matches |

**Performance Trade-offs**:
```typescript
const searchPerformance = {
  vectorOnly: {
    latency: '65ms (embedding + search)',
    accuracy: '85% (misses exact terms)',
    cost: '$0.0001/query',
    complexity: 'Low'
  },

  hybridRRF: {
    latency: '70ms (+5ms for RRF merge)',
    accuracy: '95% (semantic + exact)',
    cost: '$0.00015/query (+50% for Elasticsearch)',
    complexity: 'Medium',
    infrastructure: 'Requires Elasticsearch + pgvector'
  },

  verdict: 'Hybrid is worth 7% latency penalty for 10% accuracy gain in production RAG'
}
```

### Advanced: Weighted RRF for Domain-Specific Tuning

**Problem**: Sometimes you want to prioritize vector or BM25 based on query type.

```typescript
/**
 * Auto-tune RRF weights based on query characteristics
 */
function detectQueryType(query: string): 'semantic' | 'exact' | 'mixed' {
  // Exact patterns: IDs, SKUs, part numbers, emails
  const exactPatterns = [
    /\b[A-Z]{2,}-\d{4,}\b/,        // Part numbers (XJ-900)
    /\b\d{3,}-\d{3,}-\d{4}\b/,     // Phone numbers
    /@[\w\.-]+\.\w{2,}/,           // Emails
    /\b[A-Z0-9]{8,}\b/             // Order IDs
  ]

  const hasExactPattern = exactPatterns.some(pattern => pattern.test(query))
  const hasNumbers = /\d+/.test(query)
  const hasQuotes = /["']/.test(query)

  if (hasExactPattern || hasQuotes) {
    return 'exact'  // Prioritize BM25
  } else if (hasNumbers) {
    return 'mixed'  // Balance both
  } else {
    return 'semantic'  // Prioritize vector
  }
}

async function adaptiveHybridSearch(
  query: string,
  k: number = 10
): Promise<SearchResult[]> {
  const queryType = detectQueryType(query)

  const weights = {
    semantic: { vectorWeight: 2.0, bm25Weight: 0.5 },  // Favor vector
    exact:    { vectorWeight: 0.5, bm25Weight: 2.0 },  // Favor BM25
    mixed:    { vectorWeight: 1.0, bm25Weight: 1.0 }   // Balanced
  }

  const config = {
    ...weights[queryType],
    rrfConstant: 60,
    candidateSize: 100
  }

  const searchEngine = new HybridSearchEngine(config)
  return searchEngine.search(query, k)
}

// Examples:
await adaptiveHybridSearch('red summer dress')        // → semantic (2.0 vector, 0.5 BM25)
await adaptiveHybridSearch('Part #XJ-900')            // → exact (0.5 vector, 2.0 BM25)
await adaptiveHybridSearch('invoice under $50')       // → mixed (1.0 vector, 1.0 BM25)
```

**The Architect's Rule**: For production RAG, **always start with hybrid search (RRF)**. The 5ms latency penalty is negligible compared to the 10% accuracy gain. Only fall back to vector-only if you're certain your use case never requires exact matching.

---

## Embedding Best Practices

### 1. Chunk Size Matters

```typescript
// ❌ Too large - loses specificity
const chunk = "entire 50-page document..."
const embedding = await embed(chunk)  // Mediocre results

// ✅ Right size - focused meaning
const chunks = [
  "Section 1: Introduction to neural networks...",
  "Section 2: Training deep learning models...",
  "Section 3: Evaluation metrics..."
]
const embeddings = await Promise.all(chunks.map(embed))
```

**Optimal chunk sizes:**
- Technical docs: 200-500 tokens
- Conversational text: 100-300 tokens
- Code: 50-200 lines

### 2. Add Context

```typescript
// ❌ No context
const embedding = await embed("he scored 3 goals")

// ✅ With context
const embedding = await embed(
  "Document: Soccer Match Report\n\n" +
  "he scored 3 goals"
)
```

### 3. Use Appropriate Models

```typescript
// OpenAI Embeddings
const models = {
  'text-embedding-3-small': {
    dimensions: 1536,
    cost: '$0.02 / 1M tokens',
    speed: 'Fast',
    quality: 'Good'
  },
  'text-embedding-3-large': {
    dimensions: 3072,
    cost: '$0.13 / 1M tokens',
    speed: 'Slower',
    quality: 'Excellent'
  }
}
```

## Metadata Filtering

Combine vector search with metadata filters for precision.

```typescript
// Search with filters
const results = await index.query({
  vector: queryEmbedding,
  topK: 10,
  filter: {
    source: { $eq: 'technical-docs' },
    date: { $gte: '2024-01-01' }
  }
})
```

## Common Pitfalls

1. **Embedding mismatch**: Query and documents use different embedding models
2. **No chunking strategy**: Embedding entire documents reduces quality
3. **Ignoring metadata**: Missing opportunities for hybrid search
4. **Not normalizing**: Some vector DBs require normalized vectors
5. **Over-fetching**: Retrieving too many results increases noise

## Performance Optimization

```typescript
// Batch embeddings for efficiency
async function embedBatch(texts: string[]): Promise<number[][]> {
  const response = await openai.embeddings.create({
    model: 'text-embedding-3-small',
    input: texts  // Up to 2048 texts in one call
  })

  return response.data.map(d => d.embedding)
}

// Instead of:
for (const text of texts) {
  await embed(text)  // Slow: 100 API calls
}

// Do:
const embeddings = await embedBatch(texts)  // Fast: 1 API call
```

## Architectural Optimization Challenge

> **Scenario**: You're the AI Architect for a large e-commerce platform with 500K products. Your RAG-powered search system is live in production, but you're seeing a **critical failure pattern** in user analytics.

### The Problem

**User Query**: `"red summer dress under $50"`

**Current System** (Vector-only search):
```typescript
// Current implementation
async function searchProducts(query: string, k: number = 10) {
  const embedding = await openai.embeddings.create({
    model: 'text-embedding-3-small',
    input: query
  })

  const results = await prisma.$queryRaw`
    SELECT id, name, description, price, color, category
    FROM products
    ORDER BY embedding <=> ${embedding.data[0].embedding}::vector
    LIMIT ${k}
  `

  return results
}
```

**What Users Get** (failure pattern):
```typescript
const actualResults = [
  { name: 'Elegant Red Evening Gown',    price: 249.99, color: 'red',    category: 'dresses' },   // ❌ Right color/category, WRONG price
  { name: 'Summer Floral Sundress',      price: 39.99,  color: 'yellow', category: 'dresses' },   // ❌ Right price/season, WRONG color
  { name: 'Red Summer Tank Top',         price: 19.99,  color: 'red',    category: 'tops' },      // ❌ Right color/price, WRONG category
  { name: 'Casual Red Dress',            price: 89.99,  color: 'red',    category: 'dresses' },   // ❌ Right color/category, WRONG price
  { name: 'Summer White Linen Dress',    price: 45.00,  color: 'white',  category: 'dresses' },   // ❌ Right price/category, WRONG color
  // ... rarely gets all 3 criteria correct
]
```

**The actual target product** (exists in the database):
```typescript
const targetProduct = {
  name: 'Red Floral Summer Dress',
  price: 42.99,
  color: 'red',
  category: 'dresses',
  season: 'summer'
}  // ← This should be rank #1, but vector search ranks it #47
```

**Business Impact**:
- **Conversion rate**: 2.3% (industry average: 5.8%)
- **Cart abandonment**: 73% (industry average: 58%)
- **Customer complaints**: 847 support tickets/month mentioning "search doesn't work"
- **Revenue loss**: $1.2M/month in lost sales

### Your Task

**The CEO asks**: "Our search is broken. Users are searching for specific products with clear price and color requirements, but they're getting the wrong results. What's causing this, and how do we fix it?"

You have **4 architectural options**. Choose the best solution and explain why the others fail.

### Option A: Upgrade to Larger Embedding Model

**Solution**: Use `text-embedding-3-large` (3072 dimensions) instead of `text-embedding-3-small` (1536 dimensions) for better semantic understanding.

```typescript
// Upgraded implementation
async function searchProducts(query: string, k: number = 10) {
  const embedding = await openai.embeddings.create({
    model: 'text-embedding-3-large',  // ← Upgraded from -small
    input: query
  })

  // Same vector-only search
  const results = await prisma.$queryRaw`
    SELECT id, name, description, price, color, category
    FROM products
    ORDER BY embedding <=> ${embedding.data[0].embedding}::vector
    LIMIT ${k}
  `

  return results
}
```

**Would this work?** ❌ **No**

**Why it fails**:
- Embeddings (even 3072D) compress "under $50" into a semantic vector
- The model learns that "$49.99" and "$89.99" are semantically similar (both are "prices")
- Vector search optimizes for **semantic similarity**, not **numeric constraints**
- You'd still get dresses in the $80-$120 range because they're "semantically similar" to $50 dresses
- **Cost impact**: 6.5x more expensive ($0.13/1M vs $0.02/1M tokens), no quality gain for structured data

**The Architect's Lesson**: You can't fix a **constraint problem** with a **similarity model**.

---

### Option B: Hybrid Search with Metadata Hard-Filters (✅ CORRECT)

**Solution**: Use vector search for semantic matching ("red summer dress") but apply **SQL-style hard filters** for structured constraints (price < $50).

```typescript
interface ProductSearchFilters {
  priceMax?: number
  priceMin?: number
  colors?: string[]
  categories?: string[]
  sizes?: string[]
}

async function hybridSearchProducts(
  query: string,
  filters: ProductSearchFilters,
  k: number = 10
): Promise<Product[]> {
  // 1. Generate embedding for semantic search
  const embedding = await openai.embeddings.create({
    model: 'text-embedding-3-small',
    input: query  // "red summer dress" (semantic)
  })

  const queryEmbedding = embedding.data[0].embedding

  // 2. Build SQL query with MANDATORY filters
  let sqlQuery = `
    SELECT id, name, description, price, color, category, size,
           1 - (embedding <=> $1::vector) AS similarity_score
    FROM products
    WHERE 1=1
  `
  const params: any[] = [queryEmbedding]
  let paramIndex = 2

  // Hard filter: Price constraint (MANDATORY)
  if (filters.priceMax !== undefined) {
    sqlQuery += ` AND price <= $${paramIndex}`
    params.push(filters.priceMax)
    paramIndex++
  }

  if (filters.priceMin !== undefined) {
    sqlQuery += ` AND price >= $${paramIndex}`
    params.push(filters.priceMin)
    paramIndex++
  }

  // Hard filter: Color constraint (MANDATORY)
  if (filters.colors && filters.colors.length > 0) {
    sqlQuery += ` AND color = ANY($${paramIndex}::text[])`
    params.push(filters.colors)
    paramIndex++
  }

  // Hard filter: Category constraint (MANDATORY)
  if (filters.categories && filters.categories.length > 0) {
    sqlQuery += ` AND category = ANY($${paramIndex}::text[])`
    params.push(filters.categories)
    paramIndex++
  }

  // 3. Vector search ONLY among filtered results
  sqlQuery += `
    ORDER BY embedding <=> $1::vector
    LIMIT ${k}
  `

  const results = await prisma.$queryRawUnsafe<Product[]>(sqlQuery, ...params)
  return results
}

// Usage
const results = await hybridSearchProducts(
  'red summer dress',        // Semantic: "dress", "summer" vibes
  {
    priceMax: 50,            // Hard constraint: MUST be <= $50
    colors: ['red'],         // Hard constraint: MUST be red
    categories: ['dresses']  // Hard constraint: MUST be a dress
  },
  10
)
```

**Why this works?** ✅ **Yes**

**Architecture breakdown**:
1. **Vector search handles semantic matching**: "summer dress" matches products with summer-related descriptions
2. **SQL filters handle structured constraints**: `price <= 50` is a database-level filter (instant, exact)
3. **Two-phase filtering**:
   - **Phase 1 (SQL)**: Filter down from 500K → 3,247 products (only red dresses under $50)
   - **Phase 2 (Vector)**: Rank those 3,247 by semantic similarity to "summer dress"

**Results after fix**:
```typescript
const fixedResults = [
  { name: 'Red Floral Summer Dress',     price: 42.99, color: 'red', category: 'dresses', score: 0.94 },  // ✅ Perfect
  { name: 'Red Casual Summer Midi',      price: 38.50, color: 'red', category: 'dresses', score: 0.91 },  // ✅ Perfect
  { name: 'Crimson Sundress',            price: 45.00, color: 'red', category: 'dresses', score: 0.89 },  // ✅ Perfect
  { name: 'Red A-Line Beach Dress',      price: 49.99, color: 'red', category: 'dresses', score: 0.87 },  // ✅ Perfect
  // All results now meet ALL 3 criteria: red + dress + under $50
]
```

**Business impact after fix**:
- **Conversion rate**: 5.2% (↑ 126% from 2.3%)
- **Cart abandonment**: 61% (↓ 16% from 73%)
- **Support tickets**: 89/month (↓ 89% from 847/month)
- **Revenue gain**: $1.1M/month recovered

**Performance**:
- **Latency**: 65ms (same as vector-only—filters are instant)
- **Cost**: $0.0001/query (same as vector-only)
- **Accuracy**: 94% (vs 31% with vector-only)

**The Architect's Win**: You preserved vector search for semantic understanding ("summer dress") while adding **zero-cost, zero-latency filters** for structured data (price, color). This is the canonical production pattern.

---

### Option C: Retrieve More Chunks (10 → 100)

**Solution**: Increase the number of retrieved results from 10 to 100, hoping the right products are "somewhere in there."

```typescript
async function searchProducts(query: string) {
  const embedding = await openai.embeddings.create({
    model: 'text-embedding-3-small',
    input: query
  })

  // Retrieve 100 results instead of 10
  const results = await prisma.$queryRaw`
    SELECT id, name, description, price, color, category
    FROM products
    ORDER BY embedding <=> ${embedding.data[0].embedding}::vector
    LIMIT 100  -- ← Increased from 10
  `

  // Return all 100 to the user (?)
  return results
}
```

**Would this work?** ❌ **No**

**Why it fails**:
- **UX disaster**: Users don't want to scroll through 100 results to find one product
- **Accuracy still low**: Even at K=100, the target product might rank #47 (still too low)
- **Doesn't solve root cause**: The problem isn't "not enough results," it's "wrong ranking logic"
- **Performance penalty**: 10x more data to transfer, render, and scroll through
- **Cost increase**: 10x more data to retrieve from database

**The Architect's Lesson**: Throwing more data at a problem rarely fixes it. You need better **ranking logic**, not more results.

---

### Option D: Fine-tune Embedding Model on E-commerce Data

**Solution**: Fine-tune the embedding model on e-commerce product descriptions to better understand product attributes.

```typescript
// 1. Collect training data
const trainingPairs = [
  { query: 'red summer dress under $50', targetProduct: 'Red Floral Summer Dress $42.99' },
  { query: 'black leather jacket', targetProduct: 'Black Genuine Leather Jacket $120' },
  // ... 10,000 more examples
]

// 2. Fine-tune embedding model (hypothetical API)
const fineTunedModel = await openai.fineTune({
  model: 'text-embedding-3-small',
  trainingData: trainingPairs,
  epochs: 3
})

// 3. Use fine-tuned model for search
async function searchProducts(query: string) {
  const embedding = await openai.embeddings.create({
    model: fineTunedModel.id,  // ← Fine-tuned model
    input: query
  })

  const results = await prisma.$queryRaw`
    SELECT id, name, description, price, color, category
    FROM products
    ORDER BY embedding <=> ${embedding.data[0].embedding}::vector
    LIMIT 10
  `

  return results
}
```

**Would this work?** ❌ **No** (not cost-effective)

**Why it fails**:
- **Embeddings can't learn math**: Even with fine-tuning, the model can't learn that "$42.99 < $50" is a hard constraint
- **Data requirements**: You'd need 100K+ labeled examples to teach the model e-commerce patterns
- **Cost**: Fine-tuning costs $5,000-$20,000 for training data + compute
- **Maintenance**: Must retrain every time product attributes change
- **Latency**: Fine-tuned models are often slower (custom infrastructure)
- **Accuracy gain**: Maybe 5-10% improvement, still won't solve structured constraint problem

**When fine-tuning DOES work**:
- Domain-specific language (medical, legal jargon)
- Custom similarity definitions (company-specific taxonomy)
- Rare concepts not in base model training data

**When fine-tuning DOESN'T work**:
- Structured constraints (price, date, numeric ranges)
- Exact matching (IDs, SKUs, part numbers)
- Boolean logic (must be red AND under $50)

**The Architect's Lesson**: Fine-tuning is a **last resort** after exhausting architectural solutions. In this case, metadata filters solve the problem at 1/1000th the cost.

---

### Summary: Why Option B Wins

| Approach | Accuracy | Latency | Cost | Maintenance | Production-Ready? |
|----------|----------|---------|------|-------------|-------------------|
| **A: Larger model** | 35% | 65ms | 6.5x | Low | ❌ No (still wrong results) |
| **B: Hybrid + filters** | 94% | 65ms | 1x | Low | ✅ Yes (solves root cause) |
| **C: More results** | 31% | 80ms | 1x | Low | ❌ No (UX disaster) |
| **D: Fine-tuning** | 40% | 90ms | 100x | High | ❌ No (not cost-effective) |

**The Correct Answer**: **Option B — Hybrid Search with Metadata Hard-Filters**

**The Architect's Rule**: When your data has **structured attributes** (price, date, category, size), always use **SQL-style filters** for those constraints. Reserve vector search for **semantic/fuzzy matching** (descriptions, intent, sentiment). This pattern applies to:
- E-commerce (price, size, color)
- Job boards (salary, location, experience)
- Real estate (price, bedrooms, sqft)
- Travel (dates, budget, destination)

**Production Implementation Checklist**:
- ✅ Extract structured filters from natural language queries (use LLM to parse "under $50" → `priceMax: 50`)
- ✅ Apply filters as SQL WHERE clauses (hard constraints)
- ✅ Use vector search only on filtered subset (semantic ranking)
- ✅ Log filter effectiveness (% of queries using filters, accuracy gain)
- ✅ A/B test vector-only vs. hybrid to measure conversion lift

## Resources

- [OpenAI Embeddings Guide](https://platform.openai.com/docs/guides/embeddings)
- [Pinecone Documentation](https://docs.pinecone.io/)
- [Understanding Vector Databases](https://www.pinecone.io/learn/vector-database/)
