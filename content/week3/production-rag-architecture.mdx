---
title: "Production RAG Architecture"
description: "Build enterprise-grade RAG systems with hybrid search, multi-tenancy, and data lifecycle management"
estimatedMinutes: 50
---

# Production RAG Architecture

Build enterprise-grade RAG systems that handle multi-tenancy, security, data lifecycle, and scale.

> **For Architects**: This guide covers production considerations beyond naive RAG (Query → Embed → Retrieve). Learn hybrid search, ACL-based filtering, incremental syncing, and performance optimization.

## The Production Gap

**Naive RAG** (used in tutorials):
```
Query → Embed → Vector Search → Top-K Results → LLM
```

**Production RAG** (what enterprises need):
```
┌─────────────┐
│ User Query  │ "Show me part #9921 specs"
└──────┬──────┘
       │
       ▼
┌──────────────────────────────────────────────────┐
│ Layer 1: QUERY TRANSFORMATION                    │
│ - Clean up query                                 │
│ - Expand acronyms ("specs" → "specifications")  │
│ - Add context from conversation history          │
└──────────────────┬───────────────────────────────┘
                   │
                   ▼
┌──────────────────────────────────────────────────┐
│ Layer 2: SEMANTIC CACHE CHECK                    │
│ - Check Redis for similar queries               │
│ - Return cached result if similarity &gt; 0.95      │
└──────────────────┬───────────────────────────────┘
                   │ (cache miss)
                   ▼
┌──────────────────────────────────────────────────┐
│ Layer 3: HYBRID RETRIEVAL                       │
│ ┌────────────────┐  ┌─────────────────┐        │
│ │ Vector Search  │  │ Keyword (BM25)  │        │
│ │ (semantic)     │  │ (exact matches) │        │
│ └────────┬───────┘  └────────┬────────┘        │
│          └──────────┬─────────┘                 │
│                     ▼                            │
│          Reciprocal Rank Fusion                 │
│          (Combine rankings)                      │
└──────────────────┬───────────────────────────────┘
                   │
                   ▼
┌──────────────────────────────────────────────────┐
│ Layer 4: ACL FILTERING                          │
│ - Filter results by user permissions            │
│ - Remove unauthorized documents                  │
└──────────────────┬───────────────────────────────┘
                   │
                   ▼
┌──────────────────────────────────────────────────┐
│ Layer 5: RERANKING                              │
│ - Cross-encoder scores query-doc relevance      │
│ - Keep top-3 high-quality results               │
│ - ⚠️ More context ≠ better (Lost in Middle)     │
└──────────────────┬───────────────────────────────┘
                   │
                   ▼
┌──────────────────────────────────────────────────┐
│ Layer 6: LLM GENERATION                         │
│ - Augment prompt with top-3 chunks              │
│ - Generate grounded response                     │
└──────────────────┬───────────────────────────────┘
                   │
                   ▼
┌──────────────────────────────────────────────────┐
│ Layer 7: EVALUATION & MONITORING                │
│ - RAGAS: Faithfulness, Answer Relevance         │
│ - Log metrics for continuous improvement         │
└──────────────────────────────────────────────────┘
```

**Key Differences**:
- **Naive**: 5 steps, no optimization
- **Production**: 7 layers with caching, security, quality control

---

## 0. The "Build vs. Buy" Database Decision

Before implementing any RAG system, architects must choose the right vector database. This decision impacts cost, latency, scalability, and operational burden for years.

### The Three Paths

| Path | Examples | Best For | Key Trade-Off |
|------|----------|----------|---------------|
| **Managed Cloud** | Pinecone, Weaviate Cloud | Rapid MVP, no ops team | Vendor lock-in, ongoing cost |
| **Self-Hosted Open Source** | Milvus, Qdrant, Weaviate | Privacy, cost at scale | Ops burden, expertise required |
| **SQL Extension** | pgvector, SQLite VSS | Existing SQL infrastructure | Performance ceiling, limited features |

### Comparative Analysis: The Big Three

#### 1. Pinecone (Managed Serverless)

**Architecture**: Fully managed, serverless vector database hosted on AWS/GCP/Azure.

```typescript
// Pinecone setup (zero infrastructure)
import { Pinecone } from '@pinecone-database/pinecone'

const pinecone = new Pinecone({ apiKey: process.env.PINECONE_API_KEY })

// Create index (managed for you)
await pinecone.createIndex({
  name: 'my-index',
  dimension: 1536,
  metric: 'cosine',
  spec: {
    serverless: {
      cloud: 'aws',
      region: 'us-east-1'
    }
  }
})

const index = pinecone.index('my-index')

// Upsert vectors
await index.upsert([
  {
    id: 'doc-1',
    values: embedding,  // 1536-dim vector
    metadata: { text: 'Document content', source: 'page-1.pdf' }
  }
])

// Search
const results = await index.query({
  vector: queryEmbedding,
  topK: 10,
  filter: { source: { $eq: 'page-1.pdf' } }
})
```

**Pros**:
- ✅ Zero infrastructure management (no servers, no scaling, no backups)
- ✅ Production-ready in 10 minutes
- ✅ Auto-scales to billions of vectors
- ✅ 99.9% uptime SLA
- ✅ Built-in monitoring and logging

**Cons**:
- ❌ Vendor lock-in (no self-hosted option)
- ❌ Cost scales linearly with data ($70/month per 100K vectors at 1536 dims)
- ❌ Limited control over infrastructure
- ❌ Metadata filtering less powerful than SQL

**Cost Analysis** (1M vectors, 1536 dims, 10M queries/month):
- **Storage**: $700/month
- **Queries**: $200/month (after free tier)
- **Total**: ~$900/month

**Decision**: Choose Pinecone if you value time-to-market > cost and don't have a dedicated ops team.

---

#### 2. pgvector (PostgreSQL Extension)

**Architecture**: Extension to existing PostgreSQL database, runs in your infrastructure.

```typescript
// pgvector setup (requires Postgres)
import { Pool } from 'pg'
import pgvector from 'pgvector/pg'

const pool = new Pool({ connectionString: process.env.DATABASE_URL })

// Enable extension
await pool.query('CREATE EXTENSION IF NOT EXISTS vector')

// Create table with vector column
await pool.query(`
  CREATE TABLE chunks (
    id TEXT PRIMARY KEY,
    content TEXT,
    metadata JSONB,
    embedding vector(1536)  -- pgvector column type
  )
`)

// Create HNSW index for fast search
await pool.query(`
  CREATE INDEX ON chunks
  USING hnsw (embedding vector_cosine_ops)
  WITH (m = 16, ef_construction = 64)
`)

// Insert vectors
await pool.query(
  'INSERT INTO chunks (id, content, metadata, embedding) VALUES ($1, $2, $3, $4)',
  ['doc-1', 'Document content', JSON.stringify({ source: 'page-1.pdf' }), pgvector.toSql(embedding)]
)

// Search with SQL (metadata filtering is native!)
const result = await pool.query(`
  SELECT id, content, metadata, embedding &lt;=&gt; $1 AS distance
  FROM chunks
  WHERE metadata->>'source' = 'page-1.pdf'
  ORDER BY embedding &lt;=&gt; $1
  LIMIT 10
`, [pgvector.toSql(queryEmbedding)])
```

**Pros**:
- ✅ Leverages existing Postgres infrastructure (no new database)
- ✅ SQL-native metadata filtering (JOIN with user tables, complex WHERE clauses)
- ✅ ACID transactions (consistency guarantees)
- ✅ Mature backup/replication tooling (pg_dump, logical replication)
- ✅ Low cost at scale (just compute/storage, no per-vector fees)

**Cons**:
- ❌ Slower than specialized vector DBs (50-200ms vs 15ms for Pinecone)
- ❌ Requires Postgres expertise (tuning, scaling)
- ❌ Limited to ~10M vectors per table before performance degrades
- ❌ Manual scaling (read replicas, sharding)

**Cost Analysis** (1M vectors, 1536 dims, 10M queries/month):
- **Compute**: RDS db.r5.2xlarge ($500/month) or $100/month self-hosted
- **Storage**: 10 GB × $0.10/GB = $1/month
- **Total**: $101-$501/month (5-9x cheaper than Pinecone)

**Decision**: Choose pgvector if you already use Postgres, need complex SQL queries, and have DB expertise.

---

#### 3. Chroma (Open Source, Local-First)

**Architecture**: Embedded database (like SQLite) that runs in-process or as a self-hosted server.

```typescript
// Chroma setup (local-first)
import { ChromaClient } from 'chromadb'

// Option 1: In-memory (for prototyping)
const client = new ChromaClient()

// Option 2: Persistent local storage
const client = new ChromaClient({
  path: './chroma-data'
})

// Option 3: Self-hosted server
const client = new ChromaClient({
  path: 'http://localhost:8000'
})

// Create collection
const collection = await client.createCollection({
  name: 'my-docs',
  metadata: { 'hnsw:space': 'cosine' }
})

// Add documents (Chroma handles embeddings if you provide an embedding function)
await collection.add({
  ids: ['doc-1'],
  documents: ['Document content'],
  metadatas: [{ source: 'page-1.pdf' }],
  embeddings: [embedding]  // Or let Chroma generate embeddings
})

// Query
const results = await collection.query({
  queryEmbeddings: [queryEmbedding],
  nResults: 10,
  where: { source: 'page-1.pdf' }
})
```

**Pros**:
- ✅ Fully open source (Apache 2.0 license)
- ✅ Runs locally (no API keys, no network calls)
- ✅ Perfect for prototyping and development
- ✅ Can self-host for production (Docker, Kubernetes)
- ✅ Zero cost for small-scale (&lt;1M vectors)

**Cons**:
- ❌ Less mature than Pinecone/pgvector
- ❌ Limited production battle-testing
- ❌ Smaller ecosystem and community
- ❌ Scaling to billions of vectors requires expert tuning

**Cost Analysis** (1M vectors, 1536 dims, 10M queries/month):
- **Compute**: Self-hosted VM ($50-200/month) or local dev (free)
- **Storage**: Included
- **Total**: $50-200/month (4-18x cheaper than Pinecone)

**Decision**: Choose Chroma for local development, privacy-sensitive applications, or if you want full control without vendor lock-in.

---

### Decision Framework: Which Database to Choose?

```typescript
interface ProjectRequirements {
  teamSize: number
  hasOpsTeam: boolean
  budgetPerMonth: number
  expectedVectors: number
  queriesPerSecond: number
  privacyConstraints: 'none' | 'moderate' | 'strict'
  timeToMarket: 'urgent' | 'normal' | 'flexible'
}

/**
 * Architect's decision framework for vector database selection
 */
function chooseVectorDatabase(req: ProjectRequirements): {
  recommendation: 'pinecone' | 'pgvector' | 'chroma' | 'milvus'
  reasoning: string
  tradeoffs: string[]
} {
  // Rule 1: Privacy constraints
  if (req.privacyConstraints === 'strict') {
    return {
      recommendation: 'chroma',
      reasoning: 'Strict privacy requires self-hosted, open-source solution',
      tradeoffs: ['Must manage infrastructure', 'Slower than managed options']
    }
  }

  // Rule 2: Already using Postgres
  if (req.teamSize &gt; 0 && usesPostgres()) {
    return {
      recommendation: 'pgvector',
      reasoning: 'Leverage existing Postgres infrastructure and expertise',
      tradeoffs: ['Slower than specialized DBs', 'Complex SQL queries required']
    }
  }

  // Rule 3: No ops team + urgent timeline
  if (!req.hasOpsTeam && req.timeToMarket === 'urgent') {
    return {
      recommendation: 'pinecone',
      reasoning: 'Zero infrastructure burden, production-ready immediately',
      tradeoffs: ['Ongoing cost', 'Vendor lock-in']
    }
  }

  // Rule 4: Cost-constrained + large scale
  if (req.expectedVectors &gt; 10_000_000 && req.budgetPerMonth &lt; 500) {
    return {
      recommendation: 'chroma',
      reasoning: 'Self-hosted open-source minimizes per-vector costs at scale',
      tradeoffs: ['Requires Kubernetes/ops expertise', 'Less mature ecosystem']
    }
  }

  // Rule 5: High QPS + budget available
  if (req.queriesPerSecond &gt; 1000 && req.budgetPerMonth &gt; 2000) {
    return {
      recommendation: 'pinecone',
      reasoning: 'Best performance and reliability for high-traffic production',
      tradeoffs: ['High cost at scale']
    }
  }

  // Default: Start with pgvector, migrate if needed
  return {
    recommendation: 'pgvector',
    reasoning: 'Best default for most teams - SQL-native, low cost, good enough performance',
    tradeoffs: ['May need to migrate to specialized DB at very large scale']
  }
}
```

### Real-World Architecture Decisions

**Case Study 1: Notion's AI Search**
- **Chose**: pgvector (Postgres extension)
- **Why**: Already running Postgres for user data; SQL JOINs essential for permission filtering
- **Scale**: 50M+ vectors, 100M+ users
- **Result**: Sub-200ms P95 latency with read replicas

**Case Study 2: Hugging Face Hub Search**
- **Chose**: Elasticsearch (keyword) + Custom vector search
- **Why**: Hybrid search critical; already had Elasticsearch infrastructure
- **Scale**: 500M+ vectors (model embeddings)
- **Result**: &lt;50ms P95 for hybrid search

**Case Study 3**: **Early-Stage SaaS Startup**
- **Chose**: Pinecone
- **Why**: 2-person team, needed production RAG in 2 weeks
- **Scale**: 100K vectors, 10K users
- **Result**: Shipped in 1 week; $100/month cost acceptable for MVP

### The Migration Path

**Start Simple, Migrate When Necessary**:

```
Phase 1 (MVP): Chroma local → Prototype in days
Phase 2 (Beta): Pinecone → Ship to users in weeks
Phase 3 (Scale): pgvector → Optimize costs at 1M+ vectors
Phase 4 (Hyper-scale): Custom solution → Sub-10ms latency at billions of vectors
```

**When to migrate**:
- Chroma → Pinecone: When you need production SLA (&gt;100K queries/day)
- Pinecone → pgvector: When cost &gt; $1K/month and you have Postgres expertise
- pgvector → Milvus/Qdrant: When latency &gt; 200ms or vectors &gt; 50M

### Key Takeaways

1. **No single "best" database** - It depends on team, scale, budget, and constraints
2. **pgvector is the best default** - SQL-native, low cost, good enough for most
3. **Pinecone for speed** - Best for MVP and teams without ops expertise
4. **Chroma for privacy** - Self-hosted, open-source, full control
5. **Plan for migration** - Start simple, optimize when you hit limits

---

## 1. The "Retrieval Gap": Hybrid Search

**Problem**: Vector search alone fails on exact matches (Part IDs, product codes, acronyms).

**Solution**: Combine Vector (semantic) + Keyword (BM25) search.

**Key Finding**: Hybrid search improves retrieval accuracy by 15-25% in enterprise settings.

### BM25 Keyword Search

```typescript
import { BM25 } from 'bm25-ts'

interface Document {
  id: string
  text: string
  metadata: Record<string, any>
}

class KeywordSearchIndex {
  private bm25: BM25
  private documents: Document[]

  constructor(documents: Document[]) {
    this.documents = documents

    // Tokenize documents for BM25
    const tokenizedDocs = documents.map(doc =>
      doc.text.toLowerCase().split(/\W+/)
    )

    this.bm25 = new BM25(tokenizedDocs)
  }

  search(query: string, topK: number = 10): Array<{ doc: Document; score: number }> {
    const tokenizedQuery = query.toLowerCase().split(/\W+/)
    const scores = this.bm25.search(tokenizedQuery)

    return scores
      .map((score, index) => ({
        doc: this.documents[index],
        score
      }))
      .sort((a, b) => b.score - a.score)
      .slice(0, topK)
  }
}
```

### Hybrid Search Implementation

```typescript
interface SearchConfig {
  vectorWeight: number    // 0.0 - 1.0
  keywordWeight: number   // 0.0 - 1.0
  topK: number
}

class HybridSearchEngine {
  constructor(
    private vectorDB: VectorDatabase,
    private keywordIndex: KeywordSearchIndex,
    private config: SearchConfig = {
      vectorWeight: 0.7,
      keywordWeight: 0.3,
      topK: 10
    }
  ) {}

  async search(query: string): Promise<Document[]> {
    // 1. Vector search (semantic)
    const queryEmbedding = await embed(query)
    const vectorResults = await this.vectorDB.query({
      vector: queryEmbedding,
      topK: 20  // Over-fetch for fusion
    })

    // 2. Keyword search (BM25)
    const keywordResults = this.keywordIndex.search(query, 20)

    // 3. Reciprocal Rank Fusion (RRF)
    const fused = this.reciprocalRankFusion([
      vectorResults.map(r => ({ doc: r, score: r.similarity })),
      keywordResults
    ])

    // 4. Return top-K
    return fused.slice(0, this.config.topK).map(r => r.doc)
  }

  /**
   * Reciprocal Rank Fusion (RRF)
   * Combines multiple ranked lists without needing to normalize scores
   */
  private reciprocalRankFusion(
    rankings: Array<Array<{ doc: Document; score: number }>>,
    k: number = 60
  ): Array<{ doc: Document; score: number }> {
    const scores = new Map<string, number>()
    const docs = new Map<string, Document>()

    for (const ranking of rankings) {
      ranking.forEach((result, index) => {
        const docId = result.doc.id
        const rrfScore = 1 / (k + index + 1)

        scores.set(docId, (scores.get(docId) || 0) + rrfScore)
        docs.set(docId, result.doc)
      })
    }

    return Array.from(scores.entries())
      .sort((a, b) => b[1] - a[1])
      .map(([docId, score]) => ({
        doc: docs.get(docId)!,
        score
      }))
  }
}
```

### Postgres-Native Hybrid Search with RRF

For teams using **pgvector**, you can implement RRF directly in SQL for better performance:

```sql
-- Hybrid search combining pgvector (semantic) + tsvector (full-text)
-- Using Reciprocal Rank Fusion (RRF) to combine rankings

WITH vector_search AS (
  -- Semantic search with pgvector
  SELECT
    id,
    content,
    metadata,
    1.0 / (60 + row_number() OVER (ORDER BY embedding &lt;=&gt; $1::vector)) AS rrf_score
  FROM document_chunks
  ORDER BY embedding &lt;=&gt; $1::vector
  LIMIT 20
),
keyword_search AS (
  -- Full-text search with tsvector
  SELECT
    id,
    content,
    metadata,
    1.0 / (60 + row_number() OVER (ORDER BY ts_rank(content_tsv, query) DESC)) AS rrf_score
  FROM document_chunks,
       to_tsquery('english', $2) query
  WHERE content_tsv @@ query
  ORDER BY ts_rank(content_tsv, query) DESC
  LIMIT 20
)
-- Combine both rankings with RRF
SELECT
  d.id,
  d.content,
  d.metadata,
  COALESCE(v.rrf_score, 0) + COALESCE(k.rrf_score, 0) AS combined_score
FROM document_chunks d
LEFT JOIN vector_search v ON d.id = v.id
LEFT JOIN keyword_search k ON d.id = k.id
WHERE v.id IS NOT NULL OR k.id IS NOT NULL
ORDER BY combined_score DESC
LIMIT 10;
```

**TypeScript wrapper**:

```typescript
interface HybridSearchParams {
  queryEmbedding: number[]
  queryText: string
  topK?: number
}

async function hybridSearchPostgres(
  params: HybridSearchParams
): Promise<Document[]> {
  const { queryEmbedding, queryText, topK = 10 } = params

  // Convert query text to tsquery format
  const tsQuery = queryText
    .split(/\s+/)
    .map(term => term + ':*')
    .join(' & ')

  const result = await pool.query(
    `
    WITH vector_search AS (
      SELECT
        id, content, metadata,
        1.0 / (60 + row_number() OVER (ORDER BY embedding &lt;=&gt; $1::vector)) AS rrf_score
      FROM document_chunks
      ORDER BY embedding &lt;=&gt; $1::vector
      LIMIT 20
    ),
    keyword_search AS (
      SELECT
        id, content, metadata,
        1.0 / (60 + row_number() OVER (ORDER BY ts_rank(content_tsv, query) DESC)) AS rrf_score
      FROM document_chunks, to_tsquery('english', $2) query
      WHERE content_tsv @@ query
      ORDER BY ts_rank(content_tsv, query) DESC
      LIMIT 20
    )
    SELECT
      d.id, d.content, d.metadata,
      COALESCE(v.rrf_score, 0) + COALESCE(k.rrf_score, 0) AS combined_score
    FROM document_chunks d
    LEFT JOIN vector_search v ON d.id = v.id
    LEFT JOIN keyword_search k ON d.id = k.id
    WHERE v.id IS NOT NULL OR k.id IS NOT NULL
    ORDER BY combined_score DESC
    LIMIT $3
    `,
    [pgvector.toSql(queryEmbedding), tsQuery, topK]
  )

  return result.rows
}
```

**Why RRF works**:
- **No score normalization needed**: Vector similarity and BM25 scores are on different scales. RRF uses ranks, not raw scores.
- **Robust**: Works even when one method returns no results
- **Simple**: `1/(k + rank)` formula is easy to implement and understand
- **k parameter** (typically 60): Controls how much weight to give lower-ranked results

**When to use what**:
- **Vector only**: Semantic queries, paraphrasing ("how to reset password" → "password recovery")
- **Keyword only**: Exact matches, IDs, codes ("Part #9921")
- **Hybrid**: Production systems (use both, 70/30 split)

> **Deep Dive**: See [Week 9: Hybrid Search](../../week9/hybrid-search.mdx) for BM25 implementation details and advanced fusion techniques.

---


---

### Multi-Tenant Isolation: The Security Architect's Rule

> **Architect Perspective**: Never rely on the LLM to "ignore" documents the user shouldn't see. Your vector database query must include a **Hard Filter** on the `tenant_id` or `user_group_id` metadata. This ensures the search engine **physically cannot "see" unauthorized data**, making accidental data leaks impossible.

**The Critical Security Principle**: **Filter at Query Time, Not Post-Retrieval**

**❌ DANGEROUS Anti-Pattern**:
```typescript
// NEVER DO THIS: LLM sees unauthorized data in memory
const allResults = await vectorDB.query({ vector: queryEmbedding, topK: 10 })
const filtered = allResults.filter(doc => doc.metadata.tenantId === user.tenantId)
const answer = await llm.generate({ context: filtered })
```

**Why this is a security vulnerability**:
1. Unauthorized data is retrieved into application memory
2. Logging systems may capture unauthorized content
3. Memory dumps could expose sensitive data
4. Risk of developer error accidentally passing unfiltered results to LLM
5. Violates principle of least privilege

**✅ SECURE Pattern**:
```typescript
// CORRECT: Database never retrieves unauthorized data
const results = await vectorDB.query({
  vector: queryEmbedding,
  topK: 10,
  filter: {
    tenant_id: { $eq: user.tenantId }  // MANDATORY at query level
  }
})
const answer = await llm.generate({ context: results })
```

**Why this is secure**:
1. Vector database only searches within authorized tenant's data
2. Unauthorized data never enters application memory
3. Zero risk of accidental exposure
4. Audit logs show only authorized access attempts
5. Physical data isolation at database level

### Production Multi-Tenant Architecture

```typescript
interface TenantContext {
  tenantId: string           // Primary tenant identifier
  userId: string             // User within tenant
  userRoles: string[]        // User's roles
  dataClassification: 'public' | 'internal' | 'confidential'
}

/**
 * Secure multi-tenant RAG query with mandatory filtering
 * SECURITY: Filter MUST be applied at database query level
 */
async function secureMultiTenantRAG(
  query: string,
  context: TenantContext
): Promise<string> {
  // 1. Build mandatory tenant filter
  const tenantFilter = {
    $and: [
      // MANDATORY: Tenant isolation
      { tenant_id: { $eq: context.tenantId } },
      
      // MANDATORY: Data classification check
      {
        $or: [
          { classification: { $eq: 'public' } },
          { 
            classification: { $eq: 'internal' },
            allowed_roles: { $in: context.userRoles }
          },
          {
            classification: { $eq: 'confidential' },
            owner_id: { $eq: context.userId }
          }
        ]
      }
    ]
  }

  // 2. Embed query
  const queryEmbedding = await embed(query)

  // 3. Retrieve with MANDATORY tenant filter
  const results = await vectorDB.query({
    vector: queryEmbedding,
    topK: 10,
    filter: tenantFilter,  // ← Physical isolation at DB level
    includeMetadata: true
  })

  // 4. Defense in depth: Verify all results pass tenant check
  const verified = results.filter(doc => 
    doc.metadata.tenant_id === context.tenantId
  )

  if (verified.length !== results.length) {
    // Alert: Database returned unauthorized data (DB config error)
    await alertSecurityIncident({
      type: 'tenant_isolation_breach',
      tenantId: context.tenantId,
      unauthorizedCount: results.length - verified.length
    })
  }

  // 5. Generate answer
  const contextStr = verified.map(d => d.content).join('\n\n')
  
  return await llm.generate({
    system: `Answer using ONLY the provided context.`,
    context: contextStr,
    query
  })
}
```

### Postgres Row-Level Security (RLS) for Physical Isolation

**The Gold Standard**: Use Postgres RLS to enforce tenant isolation at the database level, making it **impossible** for application bugs to leak data.

```sql
-- Enable Row-Level Security on document_chunks table
ALTER TABLE document_chunks ENABLE ROW LEVEL SECURITY;

-- Create policy: Users can only see their tenant's data
CREATE POLICY tenant_isolation_policy ON document_chunks
  FOR SELECT
  USING (
    -- Extract tenant_id from metadata and compare to session variable
    (metadata->>'tenant_id') = current_setting('app.tenant_id', true)
  );

-- Create policy: Queries must specify tenant_id filter
CREATE POLICY tenant_filter_required ON document_chunks
  FOR SELECT
  USING (
    -- Ensure all queries filter by tenant_id
    (metadata->>'tenant_id') IS NOT NULL
  );
```

**Application-side enforcement**:

```typescript
/**
 * Set tenant context for database session
 * Postgres RLS will automatically enforce tenant isolation
 */
async function executeWithTenantContext<T>(
  tenantId: string,
  operation: (client: PoolClient) => Promise<T>
): Promise<T> {
  const client = await pool.connect()
  
  try {
    // Set tenant_id session variable
    await client.query(`SET LOCAL app.tenant_id = '${tenantId}'`)
    
    // Execute operation - RLS automatically filters results
    const result = await operation(client)
    
    return result
  } finally {
    // Clear session variable and release connection
    await client.query(`RESET app.tenant_id`)
    client.release()
  }
}

/**
 * Secure vector search with Postgres RLS
 */
async function rlsVectorSearch(
  queryEmbedding: number[],
  tenantId: string,
  topK: number = 5
): Promise<Document[]> {
  return await executeWithTenantContext(tenantId, async (client) => {
    // RLS automatically filters to tenant's data
    const result = await client.query(`
      SELECT id, content, metadata,
             embedding <=> $1::vector AS distance
      FROM document_chunks
      WHERE (metadata->>'tenant_id') = $2
      ORDER BY embedding <=> $1::vector
      LIMIT $3
    `, [pgvector.toSql(queryEmbedding), tenantId, topK])
    
    return result.rows
  })
}
```

### Multi-Tenant Metadata Schema

```typescript
interface MultiTenantMetadata {
  // MANDATORY fields for tenant isolation
  tenant_id: string                    // Primary isolation boundary
  owner_id: string                     // User who created document
  
  // Access control
  classification: 'public' | 'internal' | 'confidential'
  allowed_roles: string[]              // Roles that can access
  allowed_users: string[]              // Specific users with access
  
  // Audit trail
  created_at: Date
  created_by: string
  last_accessed: Date
  access_count: number
  
  // Data lineage
  source_system: string                // Where data came from
  data_owner: string                   // Business unit that owns data
  retention_policy: string             // How long to keep
}

/**
 * Validate metadata before indexing
 * SECURITY: Reject documents without tenant_id
 */
function validateTenantMetadata(metadata: Partial<MultiTenantMetadata>): boolean {
  // tenant_id is MANDATORY
  if (!metadata.tenant_id) {
    throw new Error('SECURITY: tenant_id is required for all documents')
  }
  
  // owner_id is MANDATORY
  if (!metadata.owner_id) {
    throw new Error('SECURITY: owner_id is required for all documents')
  }
  
  // classification is MANDATORY
  if (!metadata.classification) {
    throw new Error('SECURITY: classification is required for all documents')
  }
  
  return true
}
```

### Tenant Isolation Audit Trail

```typescript
/**
 * Log all data access for security audits
 * COMPLIANCE: Required for SOC 2, ISO 27001, GDPR
 */
async function auditTenantAccess(
  tenantId: string,
  userId: string,
  query: string,
  retrievedDocIds: string[]
): Promise<void> {
  await prisma.tenantAccessLog.create({
    data: {
      timestamp: new Date(),
      tenantId,
      userId,
      query,
      documentIds: retrievedDocIds,
      documentCount: retrievedDocIds.length,
      ipAddress: req.ip,
      userAgent: req.headers['user-agent']
    }
  })
  
  // Alert on suspicious patterns
  const recentAccess = await prisma.tenantAccessLog.count({
    where: {
      userId,
      timestamp: {
        gte: new Date(Date.now() - 3600000)  // Last hour
      }
    }
  })
  
  if (recentAccess > 100) {
    // Unusual access pattern
    await alertSecurityTeam({
      type: 'suspicious_access_pattern',
      userId,
      tenantId,
      accessCount: recentAccess,
      timeWindow: '1 hour'
    })
  }
}
```

### The Architect's Checklist for Multi-Tenant Security

- ✅ **Filter at query time**: `tenant_id` in WHERE clause, not post-retrieval
- ✅ **Use database RLS**: Postgres Row-Level Security for physical isolation
- ✅ **Validate metadata**: Reject documents without `tenant_id`
- ✅ **Audit all access**: Log every query with tenant context
- ✅ **Defense in depth**: Application-level validation + database-level enforcement
- ✅ **Alert on violations**: Monitor for unauthorized access attempts
- ✅ **Test cross-tenant leaks**: Security tests that attempt to access other tenants' data

**The Gold Standard**: A security researcher should be able to prove that even with application bugs, it's **physically impossible** for Tenant A to access Tenant B's data due to database-level RLS enforcement.

## 2. Multi-Tenancy & Security: ACL-Based Filtering

**Problem**: In multi-tenant systems, users must only see documents they have permission to access.

**Solution**: Metadata filtering for Access Control Lists (ACLs) **before** the LLM sees results.

### The Architect's Key Finding: Pre-filtering vs Post-filtering

**❌ Post-filtering (Anti-pattern)**:
```typescript
// BAD: Retrieve first, filter later
const allResults = await vectorDB.query({ vector: queryEmbedding, topK: 10 })
const authorized = allResults.filter(doc => userHasAccess(user, doc))
// Problem: You might be left with only 2 results after filtering 8!
```

**Why this fails**:
- You retrieve top-10 most similar chunks across ALL documents
- Then filter out the 8 chunks the user can't see
- You're left with only 2 chunks, which might not be the most relevant authorized ones
- The vector DB never searched within the user's authorized document set

**✅ Pre-filtering (Best practice)**:
```typescript
// GOOD: Filter at query time
const results = await vectorDB.query({
  vector: queryEmbedding,
  topK: 10,
  filter: { teamId: user.teamId }  // Search ONLY authorized documents
})
// Result: Top-10 most similar chunks from documents user can access
```

**Why this works**:
- The vector DB only searches within documents the user is authorized to see
- You get the top-10 most relevant chunks from the authorized set
- No information leakage, no incomplete results
- Much better retrieval quality

**Performance comparison**:

| Approach | Search Space | Results Quality | Security Risk |
|----------|-------------|-----------------|---------------|
| Post-filtering | All documents (10M) | Poor (incomplete top-K) | High (leakage possible) |
| Pre-filtering | Authorized only (100K) | Good (true top-K) | Low (never sees unauthorized) |

### ACL-Based Retrieval

```typescript
interface DocumentMetadata {
  docId: string
  source: string
  createdAt: Date
  // Access control
  owner: string
  teamId: string
  accessLevel: 'public' | 'team' | 'private'
  allowedUsers: string[]
  allowedTeams: string[]
}

interface UserContext {
  userId: string
  teamId: string
  role: 'admin' | 'member' | 'viewer'
}

/**
 * Retrieve documents with ACL filtering
 * Security boundary: Filter BEFORE sending to LLM
 */
async function secureRetrieve(
  query: string,
  userContext: UserContext,
  topK: number = 5
): Promise<Document[]> {
  const queryEmbedding = await embed(query)

  // Build ACL filter
  const aclFilter = {
    $or: [
      // Public documents
      { accessLevel: { $eq: 'public' } },

      // Team documents where user is team member
      {
        accessLevel: { $eq: 'team' },
        teamId: { $eq: userContext.teamId }
      },

      // Private documents where user is explicitly allowed
      {
        accessLevel: { $eq: 'private' },
        allowedUsers: { $in: [userContext.userId] }
      },

      // Documents owned by user
      {
        owner: { $eq: userContext.userId }
      }
    ]
  }

  // Vector search WITH metadata filtering
  const results = await vectorDB.query({
    vector: queryEmbedding,
    topK: topK * 2,  // Over-fetch to account for filtering
    filter: aclFilter,
    includeMetadata: true
  })

  // Additional permission check (defense in depth)
  const authorized = results.filter(doc =>
    hasAccess(userContext, doc.metadata as DocumentMetadata)
  )

  return authorized.slice(0, topK)
}

/**
 * Check if user has access to document
 */
function hasAccess(
  userContext: UserContext,
  metadata: DocumentMetadata
): boolean {
  // Admin can see everything
  if (userContext.role === 'admin') {
    return true
  }

  // Public documents
  if (metadata.accessLevel === 'public') {
    return true
  }

  // Owner can always see their documents
  if (metadata.owner === userContext.userId) {
    return true
  }

  // Team documents
  if (metadata.accessLevel === 'team' && metadata.teamId === userContext.teamId) {
    return true
  }

  // Explicitly allowed users
  if (metadata.allowedUsers.includes(userContext.userId)) {
    return true
  }

  // Explicitly allowed teams
  if (metadata.allowedTeams.includes(userContext.teamId)) {
    return true
  }

  return false
}
```

### Postgres-Native ACL Filtering

If you're using **pgvector** with Postgres, you can push the filtering directly to the database:

```typescript
/**
 * Secure search using Postgres + pgvector with SQL-level filtering
 * Filter is applied at query time, not post-retrieval
 */
async function secureSearchPostgres(
  queryEmbedding: number[],
  userId: string,
  userRoles: string[],
  topK: number = 5
): Promise<Document[]> {
  const result = await pool.query(
    `
    SELECT
      id,
      content,
      metadata,
      embedding &lt;=&gt; $1::vector AS distance
    FROM document_chunks
    WHERE
      -- Pre-filter: Only search documents user can access
      (
        -- User is in allowed roles
        metadata->'allowed_roles' ?| $2
        OR
        -- User is the owner
        metadata->>'owner_id' = $3
        OR
        -- Document is public
        metadata->>'access_level' = 'public'
      )
    ORDER BY embedding &lt;=&gt; $1::vector
    LIMIT $4
    `,
    [pgvector.toSql(queryEmbedding), userRoles, userId, topK]
  )

  return result.rows
}
```

**Key advantages**:
- **Filter happens at query time**: Postgres only searches authorized documents
- **Leverages Postgres indexes**: Can use HNSW or IVFFlat indexes with filtered queries
- **No over-fetching**: Don't retrieve unauthorized documents at all
- **Audit-friendly**: Database logs capture access patterns

**Metadata schema for ACL**:

```sql
-- Example metadata structure for ACL
CREATE TABLE document_chunks (
  id UUID PRIMARY KEY,
  content TEXT NOT NULL,
  embedding vector(1536) NOT NULL,
  metadata JSONB NOT NULL,
  created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Example metadata:
-- {
--   "owner_id": "user-123",
--   "team_id": "team-456",
--   "access_level": "team",
--   "allowed_roles": ["engineer", "manager"],
--   "allowed_users": ["user-789", "user-012"]
-- }

-- Indexes for fast filtering
CREATE INDEX idx_owner ON document_chunks ((metadata->>'owner_id'));
CREATE INDEX idx_access_level ON document_chunks ((metadata->>'access_level'));
CREATE INDEX idx_allowed_roles ON document_chunks USING gin ((metadata->'allowed_roles'));

-- HNSW index for vector similarity
CREATE INDEX idx_embedding ON document_chunks USING hnsw (embedding vector_cosine_ops);
```

### Row-Level Security in Vector Database

```sql
-- pgvector with Row-Level Security (RLS)
CREATE POLICY user_documents_policy ON document_chunks
  FOR SELECT
  USING (
    -- Public documents
    (metadata->>'accessLevel' = 'public')
    OR
    -- User's own documents
    (metadata->>'owner' = current_user_id())
    OR
    -- Team documents
    (metadata->>'teamId' = current_user_team_id())
    OR
    -- Explicitly shared
    (metadata->'allowedUsers' ? current_user_id())
  );
```

**Security Best Practices**:
1. **Filter at vector DB level**: Don't retrieve unauthorized docs
2. **Double-check in application**: Defense in depth
3. **Audit access logs**: Track who accessed what
4. **Encrypt metadata**: Especially for sensitive fields

---

## 3. Data Lifecycle: The "Deletion Problem"

**Problem**: Documents deleted from S3 but not from vector DB → AI hallucinates deleted information.

**Solution**: Incremental syncing with Change Data Capture (CDC).

### Incremental Sync Pipeline

```typescript
interface SyncEvent {
  type: 'created' | 'updated' | 'deleted'
  documentId: string
  s3Key: string
  timestamp: Date
}

/**
 * Async ingestion pipeline with change tracking
 */
class IncrementalSyncPipeline {
  private queue: Queue<SyncEvent>

  constructor(
    private s3: S3Client,
    private vectorDB: VectorDatabase,
    private lastSyncTimestamp: Date
  ) {
    this.queue = new Queue('rag-sync-queue')
  }

  /**
   * Poll S3 for changes and enqueue sync events
   */
  async pollChanges(): Promise<void> {
    // Get all objects modified since last sync
    const objects = await this.s3.listObjects({
      bucket: 'documents',
      modifiedSince: this.lastSyncTimestamp
    })

    for (const obj of objects) {
      // Check if document exists in vector DB
      const exists = await this.vectorDB.documentExists(obj.key)

      if (obj.deleted) {
        // Document was deleted from S3
        await this.queue.add({
          type: 'deleted',
          documentId: obj.key,
          s3Key: obj.key,
          timestamp: obj.lastModified
        })
      } else if (exists) {
        // Document was updated
        await this.queue.add({
          type: 'updated',
          documentId: obj.key,
          s3Key: obj.key,
          timestamp: obj.lastModified
        })
      } else {
        // New document
        await this.queue.add({
          type: 'created',
          documentId: obj.key,
          s3Key: obj.key,
          timestamp: obj.lastModified
        })
      }
    }

    this.lastSyncTimestamp = new Date()
  }

  /**
   * Process sync events from queue
   */
  async processQueue(): Promise<void> {
    this.queue.process(async (event: SyncEvent) => {
      try {
        switch (event.type) {
          case 'created':
            await this.handleCreate(event)
            break
          case 'updated':
            await this.handleUpdate(event)
            break
          case 'deleted':
            await this.handleDelete(event)
            break
        }

        // Log successful sync
        await this.logSync(event, 'success')
      } catch (error) {
        await this.logSync(event, 'failed', error)
        throw error  // Retry via queue
      }
    })
  }

  private async handleCreate(event: SyncEvent): Promise<void> {
    // Download document from S3
    const content = await this.s3.getObject({
      bucket: 'documents',
      key: event.s3Key
    })

    // Extract text
    const text = await extractText(content)

    // Chunk document
    const chunks = chunkDocument(text, {
      chunkSize: 500,
      overlap: 50
    })

    // Generate embeddings
    const embeddings = await generateEmbeddings(chunks)

    // Insert into vector DB
    await this.vectorDB.upsert({
      id: event.documentId,
      embeddings,
      metadata: {
        source: event.s3Key,
        createdAt: event.timestamp,
        lastModified: event.timestamp
      }
    })
  }

  private async handleUpdate(event: SyncEvent): Promise<void> {
    // Delete old version
    await this.handleDelete(event)

    // Insert new version
    await this.handleCreate(event)
  }

  private async handleDelete(event: SyncEvent): Promise<void> {
    // Delete all chunks for this document
    await this.vectorDB.delete({
      filter: {
        documentId: { $eq: event.documentId }
      }
    })

    console.log(`Deleted document ${event.documentId} from vector DB`)
  }

  private async logSync(
    event: SyncEvent,
    status: 'success' | 'failed',
    error?: any
  ): Promise<void> {
    await prisma.syncLog.create({
      data: {
        documentId: event.documentId,
        type: event.type,
        status,
        error: error?.message,
        timestamp: new Date()
      }
    })
  }
}
```

### Change Data Capture (CDC) with PostgreSQL

```typescript
/**
 * Listen to PostgreSQL changes for real-time sync
 */
async function setupCDC() {
  const client = await pool.connect()

  // Create trigger function
  await client.query(`
    CREATE OR REPLACE FUNCTION notify_document_change()
    RETURNS trigger AS $$
    BEGIN
      PERFORM pg_notify(
        'document_changes',
        json_build_object(
          'type', TG_OP,
          'id', NEW.id,
          's3_key', NEW.s3_key,
          'timestamp', NEW.updated_at
        )::text
      );
      RETURN NEW;
    END;
    $$ LANGUAGE plpgsql;
  `)

  // Create trigger
  await client.query(`
    CREATE TRIGGER document_change_trigger
    AFTER INSERT OR UPDATE OR DELETE ON documents
    FOR EACH ROW
    EXECUTE FUNCTION notify_document_change();
  `)

  // Listen for notifications
  await client.query('LISTEN document_changes')

  client.on('notification', async (msg) => {
    const event = JSON.parse(msg.payload)

    // Enqueue sync event
    await syncQueue.add(event)
  })
}
```

**Best Practices**:
- **Schedule regular syncs**: Every 15 minutes or hourly
- **Use CDC for real-time**: For critical documents
- **Track sync status**: Log all operations for debugging
- **Handle failures gracefully**: Retry with exponential backoff

---

## 4. Performance Optimization


### Semantic Caching 2.0: Cache Invalidation & Relevance Thresholds

> **Architect Perspective**: A 0.95 similarity score in Redis doesn't always mean the answer is correct. Implement a **Cache TTL (Time to Live)** and a **"Verified" flag**. If the underlying source document is updated in the vector DB, your ingestion pipeline must automatically invalidate the related semantic cache entries to prevent the AI from giving "stale" advice.

**The Stale Cache Problem**:
```typescript
// DANGEROUS: Cached answer may be outdated
const cached = await redis.get(queryHash)
if (cached) {
  return cached  // ← What if source document was updated yesterday?
}
```

**Why this fails**:
- Source PDF updated with new refund policy (30 → 90 days)
- Vector DB gets new embeddings
- But Redis still has cached answer saying "30 days"
- Users get incorrect information

**The Solution**: Cache Invalidation + Verification

```typescript
interface CachedResponse {
  query: string
  queryEmbedding: number[]
  answer: string
  sourceChunkIds: string[]       // Which chunks were used
  sourceVersion: string           // Version/timestamp of source docs
  similarity: number              // How similar was cached query
  verified: boolean               // Has answer been verified correct?
  ttl: number                     // Time-to-live in seconds
  createdAt: Date
  lastVerified: Date
  hitCount: number                // How many times served
}

class SemanticCacheV2 {
  private redis: Redis
  private similarityThreshold: number = 0.95
  private relevanceThreshold: number = 0.85  // NEW: Confidence gate
  
  /**
   * Get cached response with staleness check
   */
  async get(query: string): Promise<string | null> {
    const queryEmbedding = await embed(query)
    
    // Search cache for similar queries
    const cachedQueries = await this.redis.keys('cache:*')
    
    for (const key of cachedQueries) {
      const cached: CachedResponse = JSON.parse(await this.redis.get(key))
      
      // Check similarity
      const similarity = cosineSimilarity(queryEmbedding, cached.queryEmbedding)
      
      if (similarity >= this.similarityThreshold) {
        // Found similar query - but is it still valid?
        
        // 1. Check if source documents have been updated
        const sourcesStale = await this.checkSourceFreshness(cached.sourceChunkIds)
        if (sourcesStale) {
          console.log(`Cache miss: Sources updated since cache creation`)
          await this.redis.del(key)  // Invalidate stale cache
          continue
        }
        
        // 2. Check relevance threshold
        if (similarity < this.relevanceThreshold) {
          console.log(`Cache miss: Similarity ${similarity} below threshold ${this.relevanceThreshold}`)
          continue
        }
        
        // 3. Check if answer has been verified
        if (!cached.verified) {
          // Unverified cache entry - verify now
          const isCorrect = await this.verifyAnswer(query, cached.answer, cached.sourceChunkIds)
          if (!isCorrect) {
            console.log(`Cache miss: Answer failed verification`)
            await this.redis.del(key)
            continue
          }
          
          // Mark as verified
          cached.verified = true
          cached.lastVerified = new Date()
          await this.redis.set(key, JSON.stringify(cached))
        }
        
        // Cache hit!
        console.log(`Cache HIT: similarity=${similarity}, verified=${cached.verified}`)
        
        // Update hit count
        cached.hitCount++
        await this.redis.set(key, JSON.stringify(cached))
        
        return cached.answer
      }
    }
    
    return null  // Cache miss
  }
  
  /**
   * Cache response with metadata for invalidation
   */
  async set(
    query: string,
    answer: string,
    sourceChunkIds: string[],
    ttl: number = 3600  // 1 hour default
  ): Promise<void> {
    const queryEmbedding = await embed(query)
    const key = `cache:${hashQuery(query)}`
    
    // Get source document versions
    const sourceVersion = await this.getSourceVersion(sourceChunkIds)
    
    const cached: CachedResponse = {
      query,
      queryEmbedding,
      answer,
      sourceChunkIds,
      sourceVersion,
      similarity: 1.0,
      verified: false,  // Verify on first use
      ttl,
      createdAt: new Date(),
      lastVerified: null,
      hitCount: 0
    }
    
    await this.redis.set(key, JSON.stringify(cached), 'EX', ttl)
    
    // Register cache entry in invalidation index
    await this.registerForInvalidation(sourceChunkIds, key)
  }
  
  /**
   * Check if source documents have been updated
   */
  private async checkSourceFreshness(chunkIds: string[]): Promise<boolean> {
    for (const chunkId of chunkIds) {
      const chunk = await vectorDB.get(chunkId)
      if (!chunk) {
        // Source deleted - cache is stale
        return true
      }
      
      // Check if source was modified after cache creation
      const cachedVersion = await this.getCachedSourceVersion(chunkId)
      if (chunk.metadata.version !== cachedVersion) {
        return true  // Source updated
      }
    }
    
    return false  // All sources are fresh
  }
  
  /**
   * Verify cached answer is still correct
   * Uses LLM-as-judge to check faithfulness
   */
  private async verifyAnswer(
    query: string,
    cachedAnswer: string,
    sourceChunkIds: string[]
  ): Promise<boolean> {
    // Retrieve latest source content
    const sources = await Promise.all(
      sourceChunkIds.map(id => vectorDB.get(id))
    )
    
    const contextStr = sources.map(s => s.content).join('\n\n')
    
    // Use fast model to verify faithfulness
    const verification = await anthropic.messages.create({
      model: 'claude-haiku-4',
      max_tokens: 100,
      messages: [{
        role: 'user',
        content: `Is this answer faithful to the context?

Context:
${contextStr}

Question: ${query}

Answer: ${cachedAnswer}

Return JSON: { "faithful": true/false, "reason": "explanation" }`
      }]
    })
    
    const result = JSON.parse(verification.content[0].text)
    return result.faithful
  }
  
  /**
   * Register cache entry for invalidation when sources change
   */
  private async registerForInvalidation(
    chunkIds: string[],
    cacheKey: string
  ): Promise<void> {
    for (const chunkId of chunkIds) {
      // Add cache key to invalidation set for this chunk
      await this.redis.sadd(`invalidate:${chunkId}`, cacheKey)
    }
  }
  
  /**
   * Invalidate all cache entries that reference updated chunk
   * Called by ingestion pipeline when source document is updated
   */
  async invalidateChunk(chunkId: string): Promise<void> {
    // Get all cache keys that reference this chunk
    const cacheKeys = await this.redis.smembers(`invalidate:${chunkId}`)
    
    console.log(`Invalidating ${cacheKeys.length} cache entries for chunk ${chunkId}`)
    
    // Delete all related cache entries
    if (cacheKeys.length > 0) {
      await this.redis.del(...cacheKeys)
    }
    
    // Clean up invalidation set
    await this.redis.del(`invalidate:${chunkId}`)
  }
  
  private async getSourceVersion(chunkIds: string[]): Promise<string> {
    const versions = await Promise.all(
      chunkIds.map(async id => {
        const chunk = await vectorDB.get(id)
        return chunk?.metadata.version || 'unknown'
      })
    )
    return versions.join('|')
  }
  
  private async getCachedSourceVersion(chunkId: string): Promise<string> {
    // Implementation depends on how versions are tracked
    return 'v1'
  }
}
```

### Integration with Ingestion Pipeline

```typescript
/**
 * When document is updated, invalidate related caches
 */
async function handleDocumentUpdate(documentId: string): Promise<void> {
  // 1. Get all chunks for this document
  const chunks = await vectorDB.query({
    filter: { documentId: { $eq: documentId } }
  })
  
  // 2. Invalidate cache for each chunk
  for (const chunk of chunks) {
    await semanticCache.invalidateChunk(chunk.id)
  }
  
  console.log(`Invalidated cache for ${chunks.length} chunks in document ${documentId}`)
}
```

### TTL Strategy by Content Type

```typescript
const TTL_STRATEGY = {
  'frequently-updated': 900,      // 15 minutes (product prices, stock)
  'daily-updated': 3600,          // 1 hour (news, blog posts)
  'weekly-updated': 86400,        // 1 day (documentation, guides)
  'static': 604800                // 1 week (legal, compliance)
}

/**
 * Determine appropriate TTL based on content metadata
 */
function getTTL(metadata: DocumentMetadata): number {
  const category = metadata.updateFrequency || 'daily-updated'
  return TTL_STRATEGY[category] || 3600
}
```

### Cache Performance Monitoring

```typescript
interface CacheMetrics {
  hitRate: number                 // % of queries served from cache
  missRate: number                // % of queries not in cache
  invalidationRate: number        // % of cache entries invalidated
  averageAge: number              // Avg age of cache hits (seconds)
  staleCachePrevented: number     // # of stale caches caught
}

async function getCacheMetrics(timeWindow: number = 3600000): Promise<CacheMetrics> {
  const logs = await prisma.cacheLog.findMany({
    where: {
      timestamp: {
        gte: new Date(Date.now() - timeWindow)
      }
    }
  })
  
  const hits = logs.filter(l => l.type === 'hit').length
  const misses = logs.filter(l => l.type === 'miss').length
  const invalidations = logs.filter(l => l.type === 'invalidation').length
  const stalePrevented = logs.filter(l => l.reason === 'stale_source').length
  
  const total = hits + misses
  
  return {
    hitRate: total > 0 ? hits / total : 0,
    missRate: total > 0 ? misses / total : 0,
    invalidationRate: hits > 0 ? invalidations / hits : 0,
    averageAge: calculateAverageAge(logs.filter(l => l.type === 'hit')),
    staleCachePrevented
  }
}
```

**The Architect's Rule**: Semantic caching is critical for cost reduction, but **stale caches are worse than no cache**. Always implement:
1. **TTL based on content update frequency**
2. **Automatic invalidation when sources change**
3. **Verification of cached answers before serving**
4. **Relevance threshold** (don't serve low-similarity matches)

### Semantic Caching

```typescript
import { Redis } from 'ioredis'

class SemanticCache {
  private redis: Redis
  private similarityThreshold: number = 0.95

  constructor(redisUrl: string) {
    this.redis = new Redis(redisUrl)
  }

  /**
   * Check cache for similar queries
   * Returns cached result if query is semantically similar
   */
  async get(query: string): Promise<string | null> {
    // Embed query
    const queryEmbedding = await embed(query)

    // Search cache for similar queries
    const cachedQueries = await this.redis.keys('cache:*')

    for (const key of cachedQueries) {
      const cached = await this.redis.hgetall(key)
      const cachedEmbedding = JSON.parse(cached.embedding)

      // Calculate similarity
      const similarity = cosineSimilarity(queryEmbedding, cachedEmbedding)

      if (similarity &gt;= this.similarityThreshold) {
        console.log(`Cache hit for query: "${query}" (similarity: ${similarity})`)
        return cached.result
      }
    }

    return null
  }

  /**
   * Cache query result with embedding for semantic lookup
   */
  async set(
    query: string,
    result: string,
    ttl: number = 3600  // 1 hour
  ): Promise<void> {
    const queryEmbedding = await embed(query)
    const key = `cache:${hashQuery(query)}`

    await this.redis.hset(key, {
      query,
      embedding: JSON.stringify(queryEmbedding),
      result,
      timestamp: Date.now()
    })

    await this.redis.expire(key, ttl)
  }
}

/**
 * Use semantic cache in RAG pipeline
 */
async function cachedRAGQuery(query: string): Promise<string> {
  // Check cache first
  const cached = await semanticCache.get(query)
  if (cached) {
    return cached
  }

  // Cache miss: perform RAG
  const result = await ragQuery(query)

  // Cache result
  await semanticCache.set(query, result)

  return result
}
```

**Cache Optimization**:
- **Similarity threshold**: 0.95 for exact, 0.85 for broader matches
- **TTL strategy**: Shorter for dynamic data (1 hour), longer for static (1 day)
- **Cost savings**: 50-70% reduction in LLM calls for common queries

### Async Ingestion Pipeline

```typescript
import Bull from 'bull'

interface IngestionJob {
  documentId: string
  s3Key: string
  priority: 'high' | 'normal' | 'low'
}

class AsyncIngestionPipeline {
  private queue: Bull.Queue<IngestionJob>

  constructor() {
    this.queue = new Bull('ingestion', {
      redis: {
        host: process.env.REDIS_HOST,
        port: 6379
      },
      defaultJobOptions: {
        attempts: 3,
        backoff: {
          type: 'exponential',
          delay: 2000
        }
      }
    })

    this.setupWorkers()
  }

  /**
   * Add document to ingestion queue
   */
  async enqueue(job: IngestionJob): Promise<void> {
    await this.queue.add(job, {
      priority: job.priority === 'high' ? 1 : job.priority === 'normal' ? 2 : 3
    })
  }

  /**
   * Setup queue workers for parallel processing
   */
  private setupWorkers(): void {
    // Process jobs with concurrency of 5
    this.queue.process(5, async (job) => {
      const { documentId, s3Key } = job.data

      try {
        // 1. Download from S3
        const content = await s3.getObject({ key: s3Key })

        // 2. Extract text
        const text = await extractText(content)

        // 3. Chunk
        const chunks = chunkDocument(text)

        // 4. Embed
        const embeddings = await generateEmbeddings(chunks)

        // 5. Upsert to vector DB
        await vectorDB.upsert({
          id: documentId,
          embeddings,
          metadata: { source: s3Key }
        })

        console.log(`Ingested document: ${documentId}`)
      } catch (error) {
        console.error(`Failed to ingest ${documentId}:`, error)
        throw error  // Will retry
      }
    })

    // Monitor queue
    this.queue.on('completed', (job) => {
      console.log(`Job ${job.id} completed`)
    })

    this.queue.on('failed', (job, err) => {
      console.error(`Job ${job.id} failed:`, err)
    })
  }
}
```

---

## Confidence Score Gate: The "No-Context" Fallback

> **Architect Perspective**: A common RAG failure is the LLM trying to be "helpful" when no relevant documents are found. Implement a **Confidence Score Gate**: if the Top-1 reranked score is below a threshold (e.g., 0.4), the system should **bypass the LLM** and return a standard: "I cannot find sufficient documentation to answer this accurately." This preserves the system's **"Truthfulness" NFR**.

**The "Helpful" Hallucination Problem**:
```typescript
// DANGEROUS: LLM tries to help even with poor retrieval
const chunks = await vectorSearch(query, 5)  // Top chunk score: 0.28 (poor!)
const answer = await llm.generate({ context: chunks, query })
// Result: LLM hallucinates answer from training data, not context
```

**Why this fails**:
- No relevant documents found (top score 0.28)
- LLM sees irrelevant chunks as "context"
- LLM defaults to training data knowledge
- User gets plausible-sounding but wrong answer
- **Violations**: Truthfulness, faithfulness, trustworthiness

**The Solution**: Confidence Score Gate

```typescript
interface ConfidenceConfig {
  minTopScore: number              // Minimum score for top-1 result (e.g., 0.4)
  minAverageScore: number          // Minimum average score for top-K (e.g., 0.3)
  minChunksAboveThreshold: number  // At least N chunks above threshold
  bypassThreshold: number          // Below this, ALWAYS bypass LLM (e.g., 0.2)
}

/**
 * Production RAG with confidence gate
 * QUALITY: Refuse to answer when confidence is low
 */
async function confidenceGatedRAG(
  query: string,
  config: ConfidenceConfig = {
    minTopScore: 0.4,
    minAverageScore: 0.3,
    minChunksAboveThreshold: 2,
    bypassThreshold: 0.2
  }
): Promise<{ answer: string; confidence: 'high' | 'medium' | 'low' | 'none' }> {
  
  // 1. Retrieve candidates
  const candidates = await vectorSearch(query, 20)
  
  // 2. Rerank for precision
  const reranked = await crossEncoderRerank(query, candidates)
  
  // 3. Calculate confidence metrics
  const topScore = reranked[0]?.score || 0
  const topKScores = reranked.slice(0, 5).map(r => r.score)
  const averageScore = average(topKScores)
  const aboveThreshold = topKScores.filter(s => s >= config.minTopScore).length
  
  // 4. Evaluate confidence level
  if (topScore < config.bypassThreshold) {
    // CRITICAL: No relevant information found
    return {
      answer: `I cannot find sufficient documentation to answer "${query}" accurately. This may be because:
- The information is not in my knowledge base
- The question is outside my documented domain
- The query needs to be rephrased

Please try:
- Rephrasing your question more specifically
- Checking if the information exists in the system
- Contacting support for topics outside documentation`,
      confidence: 'none'
    }
  }
  
  if (topScore < config.minTopScore || averageScore < config.minAverageScore) {
    // LOW CONFIDENCE: Warn user
    const limitedContext = reranked.slice(0, 2)  // Only top-2
    const contextStr = limitedContext.map(c => c.content).join('\n\n')
    
    const answer = await llm.generate({
      system: `Answer ONLY if confident. If uncertain, say "I'm not confident about this answer."`,
      context: contextStr,
      query
    })
    
    return {
      answer: `⚠️ LOW CONFIDENCE ANSWER ⚠️\n\n${answer}\n\n_Note: Limited relevant documentation found. This answer may be incomplete._`,
      confidence: 'low'
    }
  }
  
  if (aboveThreshold < config.minChunksAboveThreshold) {
    // MEDIUM CONFIDENCE: Only one good chunk
    const topChunks = reranked.slice(0, 3)
    const contextStr = topChunks.map(c => c.content).join('\n\n')
    
    const answer = await llm.generate({
      system: `Answer based on the context provided.`,
      context: contextStr,
      query
    })
    
    return {
      answer,
      confidence: 'medium'
    }
  }
  
  // HIGH CONFIDENCE: Multiple relevant chunks found
  const highQualityChunks = reranked.filter(c => c.score >= config.minTopScore).slice(0, 5)
  const contextStr = highQualityChunks.map(c => c.content).join('\n\n')
  
  const answer = await llm.generate({
    system: `Answer comprehensively using the provided context.`,
    context: contextStr,
    query
  })
  
  return {
    answer,
    confidence: 'high'
  }
}
```

### Confidence Score Distribution Analysis

```typescript
/**
 * Analyze retrieval quality over time
 */
interface ConfidenceAnalytics {
  high: number      // % queries with high confidence
  medium: number    // % queries with medium confidence
  low: number       // % queries with low confidence
  none: number      // % queries with no relevant context
  avgTopScore: number
  avgTopKScore: number
}

async function analyzeConfidence(
  timeWindow: number = 86400000  // 24 hours
): Promise<ConfidenceAnalytics> {
  const logs = await prisma.ragQuery.findMany({
    where: {
      timestamp: {
        gte: new Date(Date.now() - timeWindow)
      }
    }
  })
  
  const total = logs.length
  const confidenceCounts = {
    high: logs.filter(l => l.confidence === 'high').length,
    medium: logs.filter(l => l.confidence === 'medium').length,
    low: logs.filter(l => l.confidence === 'low').length,
    none: logs.filter(l => l.confidence === 'none').length
  }
  
  return {
    high: confidenceCounts.high / total,
    medium: confidenceCounts.medium / total,
    low: confidenceCounts.low / total,
    none: confidenceCounts.none / total,
    avgTopScore: average(logs.map(l => l.topScore)),
    avgTopKScore: average(logs.map(l => l.avgTopKScore))
  }
}
```

### Quality Thresholds by Domain

```typescript
/**
 * Different domains require different confidence levels
 */
const DOMAIN_THRESHOLDS: Record<string, ConfidenceConfig> = {
  'legal': {
    minTopScore: 0.7,              // Legal: Very high bar
    minAverageScore: 0.6,
    minChunksAboveThreshold: 3,
    bypassThreshold: 0.5
  },
  
  'medical': {
    minTopScore: 0.7,              // Medical: Very high bar
    minAverageScore: 0.6,
    minChunksAboveThreshold: 3,
    bypassThreshold: 0.5
  },
  
  'financial': {
    minTopScore: 0.6,              // Financial: High bar
    minAverageScore: 0.5,
    minChunksAboveThreshold: 2,
    bypassThreshold: 0.4
  },
  
  'customer-support': {
    minTopScore: 0.4,              // Support: Moderate bar
    minAverageScore: 0.3,
    minChunksAboveThreshold: 2,
    bypassThreshold: 0.2
  },
  
  'general': {
    minTopScore: 0.4,              // General: Moderate bar
    minAverageScore: 0.3,
    minChunksAboveThreshold: 2,
    bypassThreshold: 0.2
  }
}
```

### User Feedback Loop

```typescript
/**
 * Collect user feedback to tune confidence thresholds
 */
async function handleUserFeedback(
  queryId: string,
  feedback: 'helpful' | 'not_helpful' | 'incorrect',
  comment?: string
): Promise<void> {
  // 1. Get original query log
  const log = await prisma.ragQuery.findUnique({ where: { id: queryId } })
  
  // 2. Store feedback
  await prisma.ragFeedback.create({
    data: {
      queryId,
      confidence: log.confidence,
      topScore: log.topScore,
      feedback,
      comment,
      timestamp: new Date()
    }
  })
  
  // 3. Analyze patterns
  if (feedback === 'incorrect' && log.confidence === 'high') {
    // False positive: High confidence but wrong answer
    await alertQualityTeam({
      type: 'false_positive',
      query: log.query,
      topScore: log.topScore,
      comment
    })
  }
  
  if (feedback === 'helpful' && log.confidence === 'low') {
    // False negative: Low confidence but good answer
    // Consider lowering threshold
    await logThresholdAdjustment({
      type: 'lower_threshold',
      domain: log.domain,
      currentThreshold: log.threshold,
      suggestedThreshold: log.topScore - 0.05
    })
  }
}
```

### The Architect's Rule

**Confidence gates are MANDATORY for**:
- Legal AI (case law must be accurate)
- Medical AI (patient safety critical)
- Financial AI (regulatory compliance)
- Any domain where wrong answers cause harm

**Target Metrics**:
- **High confidence queries**: >70% (most queries should have good retrieval)
- **No confidence queries**: <5% (knowledge base should be comprehensive)
- **Low confidence incorrect**: <1% (when we warn low confidence, we should be right)
- **High confidence incorrect**: <0.1% (when we're confident, we MUST be right)

**The Pattern**: It's better to say "I don't know" than to give a confident wrong answer. Preserve trustworthiness by:
1. Measuring retrieval confidence (top-1 score, average top-K score)
2. Setting domain-appropriate thresholds
3. Bypassing LLM when confidence is too low
4. Collecting user feedback to tune thresholds


---

## 5. Query Transformation: Fixing Bad Questions

**Problem**: Users are bad at asking questions. "show part specs" doesn't include part number, acronyms are unclear.

**Solution**: Query rewriter cleans up prompts before hitting vector DB.

### Query Expansion & Rewriting

```typescript
interface QueryTransformation {
  original: string
  cleaned: string
  expanded: string[]
  context: string[]
}

class QueryRewriter {
  /**
   * Transform user query before retrieval
   */
  async transform(
    query: string,
    conversationHistory?: Message[]
  ): Promise<QueryTransformation> {
    // 1. Clean up query
    const cleaned = this.cleanQuery(query)

    // 2. Expand acronyms and abbreviations
    const expanded = await this.expandAcronyms(cleaned)

    // 3. Add conversation context
    const context = this.extractContext(conversationHistory || [])

    return {
      original: query,
      cleaned,
      expanded,
      context
    }
  }

  /**
   * Basic query cleaning
   */
  private cleanQuery(query: string): string {
    return query
      .trim()
      .toLowerCase()
      .replace(/[^\w\s#-]/g, '')  // Keep # for part numbers
  }

  /**
   * Expand acronyms using LLM
   */
  private async expandAcronyms(query: string): Promise<string[]> {
    const response = await anthropic.messages.create({
      model: 'claude-3-haiku-20240307',
      max_tokens: 200,
      messages: [{
        role: 'user',
        content: `Expand acronyms and rewrite this query in 3 different ways:

Query: "${query}"

Return JSON array of expanded queries:
["query1", "query2", "query3"]`
      }]
    })

    const text = response.content[0].type === 'text' ? response.content[0].text : '[]'
    return JSON.parse(text)
  }

  /**
   * Extract context from conversation history
   */
  private extractContext(history: Message[]): string[] {
    // Get last 3 messages for context
    return history
      .slice(-3)
      .filter(m => m.role === 'user')
      .map(m => m.content)
  }
}

/**
 * Use query rewriter in RAG pipeline
 */
async function smartRAGQuery(
  query: string,
  conversationHistory?: Message[]
): Promise<string> {
  // 1. Transform query
  const transformed = await queryRewriter.transform(query, conversationHistory)

  // 2. Search with expanded queries
  const allResults = await Promise.all([
    searchVectorDB(transformed.cleaned),
    ...transformed.expanded.map(q => searchVectorDB(q))
  ])

  // 3. Deduplicate and merge results
  const uniqueResults = deduplicateResults(allResults.flat())

  // 4. Rerank
  const reranked = await rerank(transformed.original, uniqueResults)

  // 5. Generate with top-3
  return await generateAnswer(reranked.slice(0, 3))
}
```

**Query Transformation Examples**:

| Original Query | Cleaned | Expanded |
|----------------|---------|----------|
| "show part specs" | "show part specs" | "show part specifications", "display part details", "part technical specifications" |
| "API docs?" | "api docs" | "API documentation", "API reference", "API guide" |
| "reset pw" | "reset pw" | "reset password", "password recovery", "change password" |

---

## 6. The "Lost in the Middle" Problem

**Key Finding**: More context isn't always better. Top-3 high-quality chunks usually beat top-10 noisy chunks.

**Problem**: LLMs struggle with many chunks - they ignore middle content, focus on first and last.

### Quality Over Quantity

```typescript
/**
 * Optimal chunk selection: Top-3 high-quality > Top-10 noisy
 */
async function selectOptimalChunks(
  query: string,
  candidates: Document[],
  maxChunks: number = 3  // NOT 10!
): Promise<Document[]> {
  // 1. Rerank all candidates
  const reranked = await rerank(query, candidates)

  // 2. Apply quality threshold
  const minRelevanceScore = 0.7
  const highQuality = reranked.filter(doc => doc.score &gt;= minRelevanceScore)

  // 3. Return top-K high-quality chunks (max 3-5)
  return highQuality.slice(0, maxChunks)
}

/**
 * Position-aware prompt construction
 * Put most relevant chunks at START and END (not middle)
 */
function buildRAGPrompt(query: string, chunks: Document[]): string {
  if (chunks.length === 0) {
    return `Question: ${query}\n\nI don't have any relevant information to answer this question.`
  }

  if (chunks.length === 1) {
    return `Context:\n${chunks[0].text}\n\nQuestion: ${query}\n\nAnswer:`
  }

  // Position most relevant at start and end
  const mostRelevant = chunks[0]
  const secondMost = chunks[chunks.length - 1]
  const rest = chunks.slice(1, -1)

  return `Context:

[Most Relevant]
${mostRelevant.text}

${rest.map((doc, i) => `[Reference ${i + 2}]\n${doc.text}`).join('\n\n')}

[Also Relevant]
${secondMost.text}

Question: ${query}

Answer based on the context above:`
}
```

**Research Findings** (Liu et al., 2023):
- **Top-3 chunks**: 85% answer accuracy
- **Top-10 chunks**: 72% answer accuracy (worse!)
- **Top-20 chunks**: 61% answer accuracy (much worse!)

**Why**: LLMs have "recency bias" and "primacy bias" - they remember first and last better than middle.

**Best Practices**:
- **Retrieve broadly**: Get 20-30 candidates from vector DB
- **Rerank aggressively**: Filter to top-3 high-quality chunks
- **Position strategically**: Put most relevant at start/end
- **Quality threshold**: Reject chunks with relevance &lt; 0.7

---

## 7. Evaluation: The North Star

**Key Finding**: You can't fix what you can't measure. Use RAGAS or LLM-as-judge to score faithfulness and relevance automatically.

### RAGAS Metrics

```typescript
interface RAGASMetrics {
  faithfulness: number        // 0-1: Answer is grounded in context
  answerRelevance: number    // 0-1: Answer addresses question
  contextPrecision: number   // 0-1: Retrieved chunks are relevant
  contextRecall: number      // 0-1: All relevant info was retrieved
}

/**
 * Evaluate RAG response using LLM-as-judge
 */
async function evaluateRAGResponse(
  question: string,
  answer: string,
  retrievedChunks: string[],
  groundTruth?: string
): Promise<RAGASMetrics> {
  // 1. Faithfulness: Is answer grounded in context?
  const faithfulness = await evaluateFaithfulness(answer, retrievedChunks)

  // 2. Answer Relevance: Does answer address question?
  const answerRelevance = await evaluateAnswerRelevance(question, answer)

  // 3. Context Precision: Are retrieved chunks relevant?
  const contextPrecision = await evaluateContextPrecision(question, retrievedChunks)

  // 4. Context Recall: Was all relevant info retrieved?
  const contextRecall = groundTruth
    ? await evaluateContextRecall(groundTruth, retrievedChunks)
    : null

  return {
    faithfulness,
    answerRelevance,
    contextPrecision,
    contextRecall: contextRecall || 0
  }
}

/**
 * Faithfulness: Check if answer is supported by context
 */
async function evaluateFaithfulness(
  answer: string,
  context: string[]
): Promise<number> {
  const prompt = `Evaluate if the answer is faithful to the context (no hallucinations).

Context:
${context.join('\n\n')}

Answer:
${answer}

Score 0-1 (0 = hallucinated, 1 = fully grounded):
Return only a number between 0 and 1.`

  const response = await anthropic.messages.create({
    model: 'claude-3-haiku-20240307',
    max_tokens: 10,
    messages: [{ role: 'user', content: prompt }]
  })

  const text = response.content[0].type === 'text' ? response.content[0].text : '0'
  return parseFloat(text.trim())
}

/**
 * Answer Relevance: Check if answer addresses question
 */
async function evaluateAnswerRelevance(
  question: string,
  answer: string
): Promise<number> {
  const prompt = `Evaluate if the answer relevantly addresses the question.

Question: ${question}

Answer: ${answer}

Score 0-1 (0 = irrelevant, 1 = highly relevant):
Return only a number between 0 and 1.`

  const response = await anthropic.messages.create({
    model: 'claude-3-haiku-20240307',
    max_tokens: 10,
    messages: [{ role: 'user', content: prompt }]
  })

  const text = response.content[0].type === 'text' ? response.content[0].text : '0'
  return parseFloat(text.trim())
}

/**
 * Context Precision: Check if retrieved chunks are relevant
 */
async function evaluateContextPrecision(
  question: string,
  chunks: string[]
): Promise<number> {
  let relevantCount = 0

  for (const chunk of chunks) {
    const prompt = `Is this context relevant to answering the question?

Question: ${question}

Context: ${chunk}

Answer: yes or no`

    const response = await anthropic.messages.create({
      model: 'claude-3-haiku-20240307',
      max_tokens: 5,
      messages: [{ role: 'user', content: prompt }]
    })

    const text = response.content[0].type === 'text' ? response.content[0].text.toLowerCase() : 'no'
    if (text.includes('yes')) {
      relevantCount++
    }
  }

  return relevantCount / chunks.length
}
```

### Continuous Evaluation Pipeline

```typescript
/**
 * Evaluate RAG system continuously in production
 */
class RAGEvaluator {
  async evaluateQuery(
    question: string,
    answer: string,
    retrievedChunks: string[]
  ): Promise<void> {
    // 1. Run evaluation
    const metrics = await evaluateRAGResponse(question, answer, retrievedChunks)

    // 2. Log metrics
    await this.logMetrics(metrics)

    // 3. Alert if quality drops
    if (metrics.faithfulness &lt; 0.7 || metrics.answerRelevance &lt; 0.7) {
      await this.alertQualityIssue(question, metrics)
    }

    // 4. Store for analytics
    await prisma.ragMetrics.create({
      data: {
        question,
        answer,
        faithfulness: metrics.faithfulness,
        answerRelevance: metrics.answerRelevance,
        contextPrecision: metrics.contextPrecision,
        timestamp: new Date()
      }
    })
  }

  /**
   * Calculate aggregate metrics for monitoring
   */
  async getAggregateMetrics(
    timeRange: { start: Date; end: Date }
  ): Promise<RAGASMetrics> {
    const metrics = await prisma.ragMetrics.findMany({
      where: {
        timestamp: {
          gte: timeRange.start,
          lte: timeRange.end
        }
      }
    })

    return {
      faithfulness: average(metrics.map(m => m.faithfulness)),
      answerRelevance: average(metrics.map(m => m.answerRelevance)),
      contextPrecision: average(metrics.map(m => m.contextPrecision)),
      contextRecall: average(metrics.map(m => m.contextRecall))
    }
  }

  private async alertQualityIssue(
    question: string,
    metrics: RAGASMetrics
  ): Promise<void> {
    await sendSlackAlert({
      channel: 'rag-quality',
      severity: 'warning',
      title: 'RAG Quality Issue Detected',
      message: `Question: "${question}"\nFaithfulness: ${metrics.faithfulness}\nRelevance: ${metrics.answerRelevance}`,
      action: 'Review retrieval and generation quality'
    })
  }
}
```

**Quality Thresholds**:
- **Faithfulness**: &gt; 0.8 (no hallucinations)
- **Answer Relevance**: &gt; 0.8 (answers the question)
- **Context Precision**: &gt; 0.7 (good retrieval)
- **Context Recall**: &gt; 0.9 (comprehensive coverage)

**Best Practices**:
- **Sample evaluation**: Evaluate 1-5% of queries (not all, too expensive)
- **Async evaluation**: Run evaluation in background, don't block user
- **Track trends**: Monitor weekly averages, alert on degradation
- **A/B testing**: Compare retrieval strategies using RAGAS scores

---

## 8. Production Readiness Checklist

Use this table to evaluate your RAG system's readiness:

| Concern | Prototype Level | Production/Architect Level |
|---------|----------------|---------------------------|
| **Search** | Simple vector search | Hybrid search (Vector + BM25) |
| **Query Quality** | Use raw user query | Query transformation & expansion |
| **Accuracy** | Top-K results only | Reranking with cross-encoder |
| **Context Size** | Top-10 chunks | Top-3 high-quality (avoid "Lost in Middle") |
| **Privacy** | Open access to all docs | Metadata filtering (ACL-based) |
| **Cost** | Every query hits LLM | Semantic caching (Redis/Disk) |
| **Scale** | One-off ingestion script | Async ingestion pipeline (queue-based) |
| **Data Lifecycle** | Manual updates | Incremental sync with CDC |
| **Security** | Basic auth | Row-level security, audit logs |
| **Evaluation** | Manual testing | RAGAS metrics (faithfulness, relevance) |
| **Monitoring** | None | Latency, cost, quality metrics dashboard |
| **Error Handling** | Fail fast | Retry with exponential backoff |
| **Multi-Tenancy** | Single tenant | ACL filtering, data isolation |

### Implementation Checklist

- [ ] **Hybrid Search**: Combine vector + BM25 keyword search
- [ ] **Reranking**: Add cross-encoder for top-K refinement
- [ ] **ACL Filtering**: Implement user permission checks
- [ ] **Semantic Caching**: Cache similar queries to reduce cost
- [ ] **Async Ingestion**: Queue-based pipeline for scalability
- [ ] **Incremental Sync**: CDC or scheduled sync for data freshness
- [ ] **Monitoring**: Track latency, cost, quality metrics
- [ ] **Error Handling**: Retry logic with exponential backoff
- [ ] **Audit Logs**: Log all queries and access for compliance
- [ ] **Load Testing**: Test with 10x expected production load


---

## Enterprise Scaling Challenge

> **Scenario**: You're the AI Architect for a SaaS company. Your RAG system works perfectly for 100 users in beta, with P95 latency of 200ms. After public launch, you now have **10,000 concurrent users**, and your **vector database latency has spiked to 5 seconds**. You are using a **single-node pgvector setup** on a db.m5.2xlarge Postgres instance.

### The Problem

**System Symptoms**:
```typescript
const metrics = {
  beta: {
    users: 100,
    concurrentQueries: 10,
    avgLatency: '200ms',
    p95Latency: '350ms',
    dbCPU: '25%',
    cost: '$150/month'
  },
  
  production: {
    users: 10_000,
    concurrentQueries: 500,
    avgLatency: '4,200ms',  // ❌ 21x slower
    p95Latency: '8,500ms',  // ❌ 24x slower
    dbCPU: '98%',           // ❌ Maxed out
    errorRate: '12%',       // ❌ Connection timeouts
    cost: '$150/month'      // Same hardware
  }
}
```

**User Impact**:
- 12% of queries timeout (connection pool exhausted)
- 88% of queries take 4-8 seconds (unacceptable UX)
- Support tickets: 2,400/day complaining about slow search
- Churn risk: 23% of users cite "slow AI" as reason for cancellation

### Your Task

**The CEO asks**: "Our AI worked great in beta. Why is it so slow now with more users? What's the architectural fix?"

You have **4 options**. Choose the best solution and explain why the others fail.

---

### Option A: Increase CPU on the Postgres Server

**Solution**: Upgrade from db.m5.2xlarge (8 vCPUs, 32GB RAM) to db.m5.12xlarge (48 vCPUs, 192GB RAM).

```typescript
// Before: db.m5.2xlarge
const currentInstance = {
  vcpus: 8,
  ram: '32GB',
  cost: '$150/month',
  performance: '5,000ms P95 (maxed out CPU)'
}

// After: db.m5.12xlarge
const upgradedInstance = {
  vcpus: 48,     // 6x more CPU
  ram: '192GB',  // 6x more RAM
  cost: '$900/month',  // 6x cost
  performance: '1,200ms P95 (better but still bad)'
}
```

**Would this work?** ❌ **No** (temporary fix, doesn't scale)

**Why it fails**:
- **Vertical scaling has limits**: Even 48 vCPUs can't handle 500 concurrent vector searches
- **Single-node bottleneck**: Postgres is single-writer, so you're still limited by one server
- **Cost explosion**: 6x cost increase for only 4x performance improvement
- **Doesn't address root cause**: Vector search is CPU-intensive; adding more CPUs to one box doesn't solve concurrency
- **Temporary fix**: When you hit 20,000 users, you're back to the same problem

**Real-world outcome**:
```typescript
const sixMonthsLater = {
  users: 20_000,
  p95Latency: '6,000ms',  // Degraded again
  cost: '$900/month',     // Still paying 6x
  solution: 'Need to upgrade again → unsustainable'
}
```

---

### Option B: Horizontal Scaling with Dedicated Vector DB + Read-Replica Strategy (✅ CORRECT)

**Solution**: Decouple specialized workloads. Use a dedicated vector database (Pinecone, Weaviate, or Milvus) for vector search, keep Postgres for relational data, and add read replicas for high-traffic metadata queries.

```typescript
/**
 * Horizontally scaled architecture
 * Separates vector search from relational queries
 */
interface ScaledArchitecture {
  // Vector search (specialized database)
  vectorDB: {
    provider: 'Pinecone' | 'Weaviate' | 'Milvus',
    nodes: number,
    replicas: number,
    shards: number
  },
  
  // Relational data (Postgres with read replicas)
  postgres: {
    primary: 'db.m5.2xlarge',       // Writes only
    readReplicas: string[],          // Reads distributed
    connectionPooling: 'PgBouncer'
  }
}

const productionArchitecture: ScaledArchitecture = {
  vectorDB: {
    provider: 'Pinecone',
    nodes: 3,              // Auto-scales
    replicas: 2,           // High availability
    shards: 4              // Parallel search
  },
  
  postgres: {
    primary: 'db.m5.2xlarge',
    readReplicas: [
      'db.m5.2xlarge-replica-1',
      'db.m5.2xlarge-replica-2'
    ],
    connectionPooling: 'PgBouncer'  // Handle 500 concurrent connections
  }
}
```

**Implementation**:

```typescript
/**
 * Hybrid architecture: Pinecone (vectors) + Postgres (metadata)
 */
class HorizontallyScaledRAG {
  private pinecone: PineconeClient
  private postgres: Pool
  private readReplicas: Pool[]
  
  constructor() {
    // Vector search → Pinecone (dedicated, auto-scaled)
    this.pinecone = new PineconeClient({ apiKey: process.env.PINECONE_API_KEY })
    
    // Primary → Writes only
    this.postgres = new Pool({
      host: 'postgres-primary.example.com',
      max: 20
    })
    
    // Read replicas → Distribute read load
    this.readReplicas = [
      new Pool({ host: 'postgres-replica-1.example.com', max: 50 }),
      new Pool({ host: 'postgres-replica-2.example.com', max: 50 })
    ]
  }
  
  async query(userQuery: string, userId: string): Promise<string> {
    // 1. Vector search → Pinecone (15ms, auto-scaled)
    const vectorResults = await this.pinecone.index('docs').query({
      vector: await embed(userQuery),
      topK: 10,
      filter: { tenant_id: await this.getTenantId(userId) }
    })
    
    // 2. Metadata enrichment → Read replica (5ms, load-balanced)
    const replica = this.getReadReplica()  // Round-robin
    const metadata = await replica.query(`
      SELECT document_id, title, url, author
      FROM documents
      WHERE id = ANY($1)
    `, [vectorResults.matches.map(m => m.id)])
    
    // 3. LLM generation
    const context = this.buildContext(vectorResults, metadata)
    return await this.llm.generate({ query: userQuery, context })
  }
  
  /**
   * Load balance across read replicas
   */
  private getReadReplica(): Pool {
    const index = Math.floor(Math.random() * this.readReplicas.length)
    return this.readReplicas[index]
  }
  
  /**
   * Get tenant ID from read replica (not primary)
   */
  private async getTenantId(userId: string): Promise<string> {
    const replica = this.getReadReplica()
    const result = await replica.query(
      'SELECT tenant_id FROM users WHERE id = $1',
      [userId]
    )
    return result.rows[0].tenant_id
  }
}
```

**Why this works?** ✅ **Yes**

**Architecture breakdown**:
1. **Vector search → Specialized DB**: Pinecone/Weaviate/Milvus are built for high-concurrency vector search
2. **Metadata queries → Read replicas**: Distribute read load across multiple Postgres instances
3. **Writes → Primary only**: Keep single source of truth for consistency
4. **Horizontal scaling**: Add more replicas as user base grows
5. **Auto-scaling**: Cloud vector DBs auto-scale based on load

**Performance comparison**:

| Metric | Before (Single Postgres) | After (Horizontal) | Improvement |
|--------|-------------------------|--------------------|-------------|
| **Vector search** | 4,500ms | 15ms | 300x faster |
| **Metadata query** | 1,200ms | 5ms | 240x faster |
| **Total P95** | 8,500ms | 250ms | 34x faster |
| **Concurrent capacity** | 20 queries/sec | 1,000 queries/sec | 50x more |
| **Cost** | $150/month | $450/month | 3x (but 50x capacity) |

**Scaling beyond 10,000 users**:
- **20K users**: Add 1 more read replica ($+150/month)
- **50K users**: Pinecone auto-scales (no action needed)
- **100K users**: Add 2 more read replicas ($+300/month)
- **1M users**: Still works with 5-6 read replicas + Pinecone scaling

**Cost analysis**:
```typescript
const horizontalScaling = {
  pinecone: {
    cost: '$300/month (p1.x1 pod)',
    capacity: '10M vectors, 1000 QPS',
    scaling: 'Automatic'
  },
  
  postgresReplicas: {
    primary: '$150/month',
    replicas: '$150/month × 2 = $300/month',
    total: '$450/month'
  },
  
  totalCost: '$750/month',
  
  comparison: {
    verticalScaling: {
      cost: '$900/month (db.m5.12xlarge)',
      capacity: '~50 QPS',
      verdict: 'More expensive, worse performance'
    },
    horizontalScaling: {
      cost: '$750/month',
      capacity: '1,000 QPS',
      verdict: 'Cheaper AND 20x better capacity'
    }
  }
}
```

---

### Option C: Reduce top_k from 5 to 1

**Solution**: Retrieve fewer vectors to reduce database load.

```typescript
// Before: Retrieve 5 chunks
const results = await vectorDB.query({ topK: 5 })

// After: Retrieve 1 chunk
const results = await vectorDB.query({ topK: 1 })
```

**Would this work?** ❌ **No** (makes quality worse)

**Why it fails**:
- **Barely reduces load**: Going from 5 to 1 vector only reduces DB load by 20% (not 80%)
- **Destroys quality**: Single chunk often lacks context for comprehensive answers
- **"Lost in the Middle" irrelevant**: You're not even hitting that problem anymore
- **Doesn't solve concurrency**: 500 concurrent single-vector queries still overwhelm single-node Postgres
- **Wrong optimization**: Trading quality for a minimal performance gain

**Real-world outcome**:
```typescript
const withTopK1 = {
  p95Latency: '4,000ms',        // Only 500ms improvement (not enough)
  answerQuality: '62%',          // DOWN from 91% (unacceptable)
  hallucinationRate: '28%',      // UP from 4% (dangerous)
  userSatisfaction: '2.1/5',     // DOWN from 4.6/5
  verdict: 'Slightly faster but much worse quality'
}
```

---

### Option D: Ask Users to Wait Longer for "High-Quality" Results

**Solution**: Set user expectations that AI searches take 5-10 seconds because they're "high quality."

```typescript
// Add loading message
const loadingMessage = "Searching through thousands of documents for the most accurate answer... This may take 5-10 seconds."
```

**Would this work?** ❌ **No** (product suicide)

**Why it fails**:
- **Users don't care about excuses**: Google returns results in 200ms; users won't tolerate 5,000ms
- **Competitors exist**: Other AI tools answer in <1 second; users will switch
- **Not an architectural solution**: You're admitting defeat instead of solving the problem
- **Doesn't scale**: When you hit 20K users, latency will be 10+ seconds (even worse)
- **Damages brand**: "Slow AI" reputation is hard to recover from

**Real-world outcome**:
```typescript
const userBehavior = {
  churnRate: '+47%',                  // Users leave for faster alternatives
  negativeReviews: '+340%',           // "Too slow, not worth it"
  competitorSwitching: '31%',         // Move to ChatGPT, Perplexity
  reputationDamage: 'Permanent',
  verdict: 'Product fails in market'
}
```

---

### Summary: Why Option B Wins

| Approach | Latency | Capacity | Cost | Scalability | Quality |
|----------|---------|----------|------|-------------|---------|
| **A: Bigger CPU** | 1,200ms | 50 QPS | $900/mo | Poor (hits limit again) | Same |
| **B: Horizontal** ✅ | 250ms | 1,000 QPS | $750/mo | Excellent (add replicas) | Same |
| **C: Reduce top_k** | 4,000ms | 25 QPS | $150/mo | Poor (still single-node) | ❌ Much worse |
| **D: Ask users to wait** | 8,500ms | 20 QPS | $150/mo | None | Same |

**The Correct Answer**: **Option B — Horizontal Scaling with Dedicated Vector DB + Read-Replica Strategy**

**The Architect's Principle**: When a specialized workload (vector search) bottlenecks a general-purpose database (Postgres), **decouple** them. Use the right tool for each job:
- **Vector search**: Specialized vector DB (Pinecone, Weaviate, Milvus)
- **Metadata queries**: Postgres read replicas (load balanced)
- **Writes**: Postgres primary (single source of truth)

**Implementation Checklist**:
- ✅ Migrate vectors to dedicated vector DB (Pinecone for fastest, Milvus for self-hosted)
- ✅ Set up Postgres read replicas (2-3 for 10K users, 5-6 for 100K users)
- ✅ Use connection pooling (PgBouncer) to handle high concurrency
- ✅ Implement round-robin load balancing for read queries
- ✅ Monitor separately: vector search latency vs metadata query latency
- ✅ Auto-scaling policies for vector DB based on QPS

**Scaling Path**:
```
10K users  → 3 nodes, 2 read replicas → $750/month
50K users  → 3 nodes, 4 read replicas → $1,050/month
100K users → 6 nodes, 6 read replicas → $1,650/month
1M users   → 12 nodes, 10 read replicas → $3,300/month
```

**Key Lesson**: **Vertical scaling (bigger box) fails. Horizontal scaling (more boxes, specialized tools) wins.**

### Production Performance Checklist

Before shipping, an AI Architect must verify these three **Stability Pillars**:

| Metric | Target | Why It Matters |
|--------|--------|----------------|
| **Token Pressure** | &lt; 80% Context Window | Avoids "Lost in the Middle" syndrome where LLMs ignore chunks in the middle of long contexts. Keep total prompt tokens (system + retrieved chunks + user query) under 80% of model's context limit (e.g., &lt;160K for Claude Sonnet 4.5's 200K limit). |
| **Index Choice** | HNSW for Speed | IVFFlat is easier to build and requires training, but HNSW (Hierarchical Navigable Small World) is faster for high-concurrency production apps. HNSW provides consistent low latency even under load. |
| **Cold-Start Latency** | &lt; 1 second | The first query after a period of inactivity often takes longer due to database "warming" (indexes not in memory, connections not pooled). Use connection pooling and warm up indexes on deployment. |

**Implementation guidance**:

```typescript
interface PerformanceConfig {
  maxContextTokens: number       // e.g., 160_000 for Claude Sonnet 4.5 (80% of 200K)
  maxChunkCount: number         // Enforce top-K limit (e.g., 3-5 chunks)
  vectorIndexType: 'hnsw' | 'ivfflat'
  connectionPoolSize: number    // e.g., 20 for high-concurrency
}

/**
 * Calculate token pressure before sending to LLM
 */
function checkTokenPressure(
  systemPrompt: string,
  retrievedChunks: string[],
  userQuery: string,
  config: PerformanceConfig
): { withinBudget: boolean; tokenCount: number; pressurePercent: number } {
  // Rough approximation: 1 token ≈ 4 characters
  const systemTokens = systemPrompt.length / 4
  const chunkTokens = retrievedChunks.reduce((sum, chunk) => sum + chunk.length / 4, 0)
  const queryTokens = userQuery.length / 4

  const totalTokens = systemTokens + chunkTokens + queryTokens
  const pressurePercent = (totalTokens / config.maxContextTokens) * 100

  return {
    withinBudget: totalTokens < config.maxContextTokens,
    tokenCount: Math.round(totalTokens),
    pressurePercent: Math.round(pressurePercent)
  }
}

/**
 * Postgres HNSW index configuration (pgvector)
 */
async function createProductionIndex() {
  await pool.query(`
    -- Create HNSW index for production
    CREATE INDEX idx_embedding_hnsw ON document_chunks
    USING hnsw (embedding vector_cosine_ops)
    WITH (m = 16, ef_construction = 64);

    -- m: Max connections per layer (higher = better recall, more memory)
    -- ef_construction: Size of dynamic candidate list (higher = better index quality, slower build)
  `)
}

/**
 * Connection pooling to avoid cold-start
 */
import { Pool } from 'pg'

const pool = new Pool({
  host: 'localhost',
  database: 'vectors',
  max: 20,                    // Connection pool size
  idleTimeoutMillis: 30000,   // Keep connections warm
  connectionTimeoutMillis: 2000
})

// Warm up connection pool on startup
async function warmupPool() {
  const promises = []
  for (let i = 0; i &lt; 5; i++) {
    promises.push(
      pool.query('SELECT 1')  // Dummy query to establish connections
    )
  }
  await Promise.all(promises)
}
```

**Performance monitoring**:

```typescript
interface PerformanceMetrics {
  queryLatency: number        // ms
  retrievalLatency: number    // ms
  llmLatency: number          // ms
  tokenCount: number
  tokenPressure: number       // percent
  cacheHit: boolean
}

async function monitoredRAGQuery(query: string): Promise<{ result: string; metrics: PerformanceMetrics }> {
  const startTime = Date.now()

  // 1. Check cache
  const cached = await checkCache(query)
  if (cached) {
    return {
      result: cached,
      metrics: {
        queryLatency: Date.now() - startTime,
        retrievalLatency: 0,
        llmLatency: 0,
        tokenCount: 0,
        tokenPressure: 0,
        cacheHit: true
      }
    }
  }

  // 2. Retrieve
  const retrievalStart = Date.now()
  const chunks = await hybridSearch(query)
  const retrievalLatency = Date.now() - retrievalStart

  // 3. Check token pressure
  const pressure = checkTokenPressure(SYSTEM_PROMPT, chunks, query, performanceConfig)

  if (!pressure.withinBudget) {
    // Trim chunks to fit budget
    console.warn(`Token pressure ${pressure.pressurePercent}% exceeds 80% threshold`)
    chunks.splice(3)  // Keep only top-3
  }

  // 4. Generate
  const llmStart = Date.now()
  const result = await generateAnswer(query, chunks)
  const llmLatency = Date.now() - llmStart

  return {
    result,
    metrics: {
      queryLatency: Date.now() - startTime,
      retrievalLatency,
      llmLatency,
      tokenCount: pressure.tokenCount,
      tokenPressure: pressure.pressurePercent,
      cacheHit: false
    }
  }
}
```

**Key insights**:
- **Token Pressure**: Monitor and enforce &lt; 80% to avoid "Lost in the Middle" degradation
- **HNSW vs IVFFlat**: HNSW is almost always better for production (consistent latency, no training needed)
- **Cold-Start**: Connection pooling + index warmup = consistent sub-second latency
- **Monitoring**: Track all three pillars in production to catch regressions early

---

## Key Takeaways

### Production vs. Prototype

**Prototype RAG** (tutorials):
- Simple vector search
- No access control
- Manual ingestion
- No caching

**Production RAG** (enterprise):
- Hybrid search (vector + keyword)
- ACL-based filtering
- Async ingestion pipeline
- Semantic caching
- Incremental syncing
- Monitoring and alerts

### Critical Improvements

1. **Hybrid Search**: 15-25% accuracy improvement
2. **Semantic Caching**: 50-70% cost reduction
3. **ACL Filtering**: Security and compliance
4. **Incremental Sync**: Data freshness and consistency

### Architecture Patterns

- **Queue-based ingestion**: Handle high volumes, retry failures
- **CDC for real-time**: Keep vector DB in sync with source data
- **Defense in depth**: Filter at DB level + application level
- **Over-fetch and rerank**: Retrieve 2x results, rerank to top-K

---

## Further Reading

### Within Week 3
- [RAG Memory Fundamentals](./rag-memory-fundamentals.mdx) - Three-phase RAG architecture basics
- [RAG Pipelines](./rag-pipelines.mdx) - Basic RAG implementation
- [Memory Systems](./memory-systems.mdx) - Context and conversation memory

### Week 9: Advanced Retrieval
- [Hybrid Search](../../week9/hybrid-search.mdx) - BM25, RRF, fusion techniques
- [Query Optimization](../../week9/query-optimization.mdx) - Query rewriting, expansion
- [Reranking Strategies](../../week9/reranking-strategies.mdx) - Cross-encoder, diversity filtering

### Week 6: Monitoring
- [Monitoring AI Systems](../../week6/monitoring-ai-systems.mdx) - Track RAG quality, latency, cost

### Week 12: Enterprise
- [Enterprise Architecture](../../week12/enterprise-architecture.mdx) - Multi-region, disaster recovery

## Next Steps

1. **Week 3**: Master production RAG patterns
2. **Week 9**: Deep dive into advanced retrieval techniques
3. **Week 6**: Implement monitoring and observability
4. **Week 12**: Scale to enterprise multi-region deployment
