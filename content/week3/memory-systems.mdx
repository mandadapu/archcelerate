---
title: "Conversation Memory Systems"
description: "Implement memory for AI agents and chatbots"
estimatedMinutes: 35
---

# Conversation Memory Systems

## Why Memory Matters

LLMs are stateless - they don't remember previous conversations. Memory systems solve this.

**Without Memory:**
```
User: "My name is Alice"
Bot: "Nice to meet you!"
User: "What's my name?"
Bot: "I don't know your name."
```

**With Memory:**
```
User: "My name is Alice"
Bot: "Nice to meet you, Alice!"
User: "What's my name?"
Bot: "Your name is Alice."
```

## Types of Memory

### The Architect's Dilemma: Memory Fidelity vs. Token Cost

Every conversational AI system faces this trade-off:
- **Full conversation history** = Perfect context, but exponential token costs
- **Compressed summary** = Cheap, but loses nuance and detail

**Production Systems Must Balance Four Constraints**:
1. **Token Budget**: LLM context window limits (200K for Claude Sonnet 4.5)
2. **Cost**: $0.003/1K input tokens (can add up fast)
3. **Latency**: More tokens = slower generation
4. **Fidelity**: How much detail can we afford to lose?

### Sliding Windows vs. Summary Buffers: The Decision Matrix

| Strategy | Memory Fidelity | Token Cost | Latency | Best For |
|----------|----------------|-----------|---------|----------|
| **Sliding Window** (last N messages) | High (100% for recent) | Medium ($0.05/conv) | Low (consistent) | Short sessions, high-stakes conversations |
| **Summary Buffer** (compress old) | Medium (70-80%) | Low ($0.01/conv) | Medium (summary step) | Long sessions, cost-constrained |
| **Hierarchical Summary** (rolling) | Medium-High (85-90%) | Low ($0.015/conv) | Medium-High | Very long sessions (50+ turns) |
| **Entity-Based Memory** | High for facts (95%+) | Very Low ($0.005/conv) | Low | Fact-heavy conversations (support, consulting) |

### Cost-Benefit Analysis: Real Numbers

**Scenario**: Customer support chatbot, 100K conversations/month, avg 20 turns/conversation

| Memory Strategy | Avg Tokens/Turn | Monthly Token Cost | Memory Fidelity | Recommendation |
|----------------|-----------------|-------------------|----------------|----------------|
| **No memory** | 500 (query only) | $150 | 0% | ❌ Terrible UX |
| **Full history** | 5,000 (cumulative) | $1,500 | 100% | ❌ Too expensive |
| **Sliding window (last 10)** | 2,000 | $600 | 95% | ⚠️ Good, but costly |
| **Summary buffer** | 1,000 | $300 | 80% | ✅ Best balance |
| **Entity extraction** | 800 | $240 | 85% | ✅ Best for support |

**The Sweet Spot**: Summary buffer or entity-based memory for most production systems. Sliding window only when accuracy is critical (medical, legal, financial).

---

## Memory Strategy Implementations

### 1. Short-Term Memory (Conversation Buffer)

Store recent conversation in context window.

```typescript
interface Message {
  role: 'user' | 'assistant'
  content: string
  timestamp: Date
}

class ConversationBuffer {
  private messages: Message[] = []
  private maxMessages: number = 10

  add(role: 'user' | 'assistant', content: string) {
    this.messages.push({
      role,
      content,
      timestamp: new Date()
    })

    // Keep only recent messages
    if (this.messages.length > this.maxMessages) {
      this.messages = this.messages.slice(-this.maxMessages)
    }
  }

  getHistory(): Message[] {
    return this.messages
  }

  formatForLLM(): Array<{ role: string; content: string }> {
    return this.messages.map(m => ({
      role: m.role,
      content: m.content
    }))
  }
}
```

### 2. Long-Term Memory (Vector Store)

Store all conversations in vector database for retrieval.

```typescript
class LongTermMemory {
  constructor(
    private vectorDB: VectorDB,
    private userId: string
  ) {}

  async remember(message: string, metadata: any = {}) {
    const embedding = await embed(message)

    await this.vectorDB.upsert({
      id: `${this.userId}-${Date.now()}`,
      vector: embedding,
      metadata: {
        userId: this.userId,
        text: message,
        timestamp: new Date().toISOString(),
        ...metadata
      }
    })
  }

  async recall(query: string, k: number = 5): Promise<Memory[]> {
    const queryEmbedding = await embed(query)

    const results = await this.vectorDB.query({
      vector: queryEmbedding,
      filter: { userId: this.userId },
      topK: k
    })

    return results.matches.map(m => ({
      text: m.metadata.text,
      timestamp: m.metadata.timestamp,
      score: m.score
    }))
  }
}
```

### 3. Summary Memory

Summarize old conversations to save tokens.

```typescript
class SummaryMemory {
  private summary: string = ''
  private recentMessages: Message[] = []
  private maxRecent: number = 5

  async addMessage(role: string, content: string) {
    this.recentMessages.push({ role, content, timestamp: new Date() })

    // When buffer full, summarize
    if (this.recentMessages.length > this.maxRecent) {
      await this.summarize()
    }
  }

  private async summarize() {
    const conversationText = this.recentMessages
      .map(m => `${m.role}: ${m.content}`)
      .join('\n')

    const newSummary = await llm.complete({
      prompt: `Current summary: ${this.summary}\n\n` +
              `New conversation:\n${conversationText}\n\n` +
              `Provide an updated summary of the conversation:`
    })

    this.summary = newSummary
    this.recentMessages = []
  }

  getContext(): string {
    const recent = this.recentMessages
      .map(m => `${m.role}: ${m.content}`)
      .join('\n')

    return `Summary of previous conversation: ${this.summary}\n\n` +
           `Recent messages:\n${recent}`
  }
}
```

## Entity Memory

Track facts about entities (people, places, things).

```typescript
interface Entity {
  name: string
  type: 'person' | 'place' | 'thing' | 'concept'
  facts: string[]
  lastMentioned: Date
}

class EntityMemory {
  private entities: Map<string, Entity> = new Map()

  async extractEntities(text: string): Promise<Entity[]> {
    const response = await llm.complete({
      prompt: `Extract entities and facts from this text:\n\n${text}\n\n` +
              `Return JSON: [{ name, type, facts: [] }]`
    })

    const entities = JSON.parse(response)

    entities.forEach((e: Entity) => {
      this.updateEntity(e)
    })

    return entities
  }

  private updateEntity(entity: Entity) {
    const existing = this.entities.get(entity.name)

    if (existing) {
      existing.facts.push(...entity.facts)
      existing.lastMentioned = new Date()
    } else {
      this.entities.set(entity.name, {
        ...entity,
        lastMentioned: new Date()
      })
    }
  }

  getEntity(name: string): Entity | undefined {
    return this.entities.get(name)
  }

  getRelevantEntities(limit: number = 5): Entity[] {
    return Array.from(this.entities.values())
      .sort((a, b) => b.lastMentioned.getTime() - a.lastMentioned.getTime())
      .slice(0, limit)
  }

  formatContext(): string {
    const entities = this.getRelevantEntities()

    return entities
      .map(e => `${e.name} (${e.type}): ${e.facts.join('; ')}`)
      .join('\n')
  }
}
```

## Hybrid Memory System

Combine multiple memory types.

```typescript
class HybridMemory {
  private shortTerm: ConversationBuffer
  private longTerm: LongTermMemory
  private entities: EntityMemory
  private summary: SummaryMemory

  constructor(userId: string, vectorDB: VectorDB) {
    this.shortTerm = new ConversationBuffer()
    this.longTerm = new LongTermMemory(vectorDB, userId)
    this.entities = new EntityMemory()
    this.summary = new SummaryMemory()
  }

  async addMessage(role: 'user' | 'assistant', content: string) {
    // Add to all memory systems
    this.shortTerm.add(role, content)
    await this.longTerm.remember(content, { role })
    await this.summary.addMessage(role, content)

    // Extract entities
    if (role === 'user') {
      await this.entities.extractEntities(content)
    }
  }

  async getContext(currentQuery: string): Promise<string> {
    // 1. Short-term (always include)
    const shortTermContext = this.shortTerm.formatForLLM()

    // 2. Relevant long-term memories
    const longTermMemories = await this.longTerm.recall(currentQuery, 3)

    // 3. Entity context
    const entityContext = this.entities.formatContext()

    // 4. Summary
    const summaryContext = this.summary.getContext()

    return `
Previous conversation summary:
${summaryContext}

Relevant past memories:
${longTermMemories.map(m => m.text).join('\n')}

Known facts:
${entityContext}

Recent conversation:
${shortTermContext.map(m => `${m.role}: ${m.content}`).join('\n')}
`
  }
}
```

---

## Production Persistence Layers: Redis Implementation

**The Problem**: In-memory storage loses all conversation state on server restart. Production systems need durable, multi-tenant memory with sub-100ms read latency.

**The Solution**: Redis + Vector DB for hybrid persistence.

### Architecture: Two-Tier Memory Persistence

```
┌──────────────────────────────────────────────────┐
│  APPLICATION LAYER                                │
│  ┌─────────────────┐    ┌──────────────────┐   │
│  │ Short-Term      │    │  Long-Term        │   │
│  │ Memory          │    │  Memory           │   │
│  │ (Redis)         │    │  (Vector DB)      │   │
│  └────────┬────────┘    └────────┬─────────┘   │
└───────────┼──────────────────────┼──────────────┘
            │                      │
            ▼                      ▼
   ┌─────────────────┐    ┌──────────────────┐
   │ Redis           │    │ pgvector/Pinecone│
   │ TTL: 7 days     │    │ TTL: 90 days     │
   │ Key: user:conv  │    │ Indexed by       │
   │ Type: List      │    │ user_id          │
   └─────────────────┘    └──────────────────┘
```

**Key Insight**: Redis for hot data (last 10 messages, <7 days old), Vector DB for cold data (semantic search across all history).

### Redis-Backed Conversation Memory

```typescript
import { Redis } from 'ioredis'
import { z } from 'zod'

interface RedisMemoryConfig {
  ttl: number              // Time-to-live in seconds (7 days = 604800)
  maxMessagesPerUser: number
  compressionThreshold: number  // Compress if message > N chars
}

const MessageSchema = z.object({
  role: z.enum(['user', 'assistant', 'system']),
  content: z.string(),
  timestamp: z.number(),
  metadata: z.record(z.any()).optional()
})

type Message = z.infer<typeof MessageSchema>

/**
 * Production-grade Redis-backed conversation memory
 * Handles multi-tenancy, TTL, compression, and failover
 */
class RedisConversationMemory {
  private redis: Redis
  private config: RedisMemoryConfig

  constructor(redisUrl: string, config: Partial<RedisMemoryConfig> = {}) {
    this.redis = new Redis(redisUrl, {
      retryStrategy: (times) => {
        const delay = Math.min(times * 50, 2000)
        return delay
      },
      enableReadyCheck: true,
      maxRetriesPerRequest: 3
    })

    this.config = {
      ttl: config.ttl || 7 * 24 * 60 * 60,  // 7 days
      maxMessagesPerUser: config.maxMessagesPerUser || 100,
      compressionThreshold: config.compressionThreshold || 1000
    }
  }

  /**
   * Add message to conversation history
   */
  async addMessage(
    userId: string,
    conversationId: string,
    message: Message
  ): Promise<void> {
    const key = this.getKey(userId, conversationId)

    // Serialize message
    const serialized = JSON.stringify(message)

    // Optional: Compress if large
    const data = serialized.length > this.config.compressionThreshold
      ? await this.compress(serialized)
      : serialized

    // Add to Redis list (LPUSH = prepend, so newest first)
    await this.redis
      .pipeline()
      .lpush(key, data)
      .ltrim(key, 0, this.config.maxMessagesPerUser - 1)  // Keep only last N
      .expire(key, this.config.ttl)  // Reset TTL
      .exec()
  }

  /**
   * Get recent conversation history (sliding window)
   */
  async getHistory(
    userId: string,
    conversationId: string,
    limit: number = 10
  ): Promise<Message[]> {
    const key = this.getKey(userId, conversationId)

    // Get last N messages (LRANGE 0 N-1)
    const messages = await this.redis.lrange(key, 0, limit - 1)

    // Decompress and parse
    const parsed = await Promise.all(
      messages.map(async (msg) => {
        const decompressed = msg.startsWith('{')
          ? msg
          : await this.decompress(msg)

        return MessageSchema.parse(JSON.parse(decompressed))
      })
    )

    // Reverse to get chronological order (oldest first)
    return parsed.reverse()
  }

  /**
   * Get conversation summary (for token budget management)
   */
  async getSummary(
    userId: string,
    conversationId: string
  ): Promise<string | null> {
    const summaryKey = `${this.getKey(userId, conversationId)}:summary`
    return await this.redis.get(summaryKey)
  }

  /**
   * Store conversation summary (compressed version of old messages)
   */
  async setSummary(
    userId: string,
    conversationId: string,
    summary: string
  ): Promise<void> {
    const summaryKey = `${this.getKey(userId, conversationId)}:summary`

    await this.redis
      .pipeline()
      .set(summaryKey, summary)
      .expire(summaryKey, this.config.ttl)
      .exec()
  }

  /**
   * Delete conversation (GDPR compliance, user request)
   */
  async deleteConversation(
    userId: string,
    conversationId: string
  ): Promise<void> {
    const key = this.getKey(userId, conversationId)
    const summaryKey = `${key}:summary`

    await this.redis.del(key, summaryKey)
  }

  /**
   * Get all active conversations for a user
   */
  async getUserConversations(userId: string): Promise<string[]> {
    const pattern = `conv:${userId}:*`
    const keys = await this.redis.keys(pattern)

    // Extract conversation IDs from keys
    return keys.map(key => key.split(':')[2])
  }

  private getKey(userId: string, conversationId: string): string {
    return `conv:${userId}:${conversationId}`
  }

  private async compress(data: string): Promise<string> {
    // Implement compression (zlib, gzip, etc.)
    return data  // Placeholder
  }

  private async decompress(data: string): Promise<string> {
    // Implement decompression
    return data  // Placeholder
  }
}
```

### Hierarchical Summary Strategy (Rolling Compression)

For very long conversations (50+ turns), use **rolling summarization** to maintain fidelity while controlling costs.

```typescript
/**
 * Hierarchical summary memory with rolling compression
 * Keeps last 10 messages + tiered summaries of older messages
 */
class HierarchicalMemory {
  private redis: RedisConversationMemory
  private turnsSinceLastSummary: number = 0
  private summaryTiers: Map<number, string> = new Map()  // tier -> summary

  constructor(private userId: string, private conversationId: string) {
    this.redis = new RedisConversationMemory(process.env.REDIS_URL!)
  }

  async addMessage(role: 'user' | 'assistant', content: string): Promise<void> {
    // Add to Redis
    await this.redis.addMessage(this.userId, this.conversationId, {
      role,
      content,
      timestamp: Date.now()
    })

    this.turnsSinceLastSummary++

    // Every 10 turns, create a summary
    if (this.turnsSinceLastSummary >= 10) {
      await this.createTierSummary()
      this.turnsSinceLastSummary = 0
    }
  }

  /**
   * Get context for LLM with hierarchical summaries
   */
  async getContext(): Promise<string> {
    // Layer 1: Recent messages (full fidelity)
    const recentMessages = await this.redis.getHistory(
      this.userId,
      this.conversationId,
      10
    )

    // Layer 2: Tiered summaries (compressed history)
    const summaries = Array.from(this.summaryTiers.entries())
      .sort(([tierA], [tierB]) => tierB - tierB)  // Newest first
      .map(([tier, summary]) => `[Turns ${tier * 10}-${(tier + 1) * 10}]: ${summary}`)

    return `
CONVERSATION HISTORY:

${summaries.length > 0 ? 'Earlier conversation summaries:\n' + summaries.join('\n\n') + '\n\n' : ''}

Recent messages:
${recentMessages.map(m => `${m.role}: ${m.content}`).join('\n')}
`
  }

  /**
   * Create summary of messages 11-20 (oldest tier)
   */
  private async createTierSummary(): Promise<void> {
    const messages = await this.redis.getHistory(
      this.userId,
      this.conversationId,
      20
    )

    // Messages 11-20 (older messages)
    const oldMessages = messages.slice(0, 10)

    if (oldMessages.length === 0) return

    // Use LLM to summarize
    const summary = await this.summarizeMessages(oldMessages)

    // Store in tier
    const tier = Math.floor(messages.length / 10) - 1
    this.summaryTiers.set(tier, summary)

    // Persist to Redis
    await this.redis.setSummary(
      this.userId,
      this.conversationId,
      JSON.stringify(Array.from(this.summaryTiers.entries()))
    )
  }

  private async summarizeMessages(messages: Message[]): Promise<string> {
    const conversationText = messages
      .map(m => `${m.role}: ${m.content}`)
      .join('\n')

    const response = await anthropic.messages.create({
      model: 'claude-3-haiku-20240307',
      max_tokens: 200,
      messages: [{
        role: 'user',
        content: `Summarize this conversation segment in 2-3 sentences, capturing key points and decisions:\n\n${conversationText}`
      }]
    })

    return response.content[0].type === 'text' ? response.content[0].text : ''
  }
}
```

### Multi-Tenant Memory with Rate Limiting

**Production Requirement**: Prevent memory abuse (user sends 10K messages → Redis OOM).

```typescript
/**
 * Multi-tenant memory manager with per-user quotas
 */
class MultiTenantMemoryManager {
  private redis: Redis

  constructor(redisUrl: string) {
    this.redis = new Redis(redisUrl)
  }

  /**
   * Check if user can add more messages (rate limit + quota)
   */
  async canAddMessage(userId: string, tier: 'free' | 'pro' | 'enterprise'): Promise<{
    allowed: boolean
    remaining: number
    resetAt: Date
  }> {
    const quotas = {
      free: { messagesPerHour: 100, maxConversations: 5 },
      pro: { messagesPerHour: 1000, maxConversations: 50 },
      enterprise: { messagesPerHour: 10000, maxConversations: 500 }
    }

    const quota = quotas[tier]
    const hourKey = `rate:${userId}:${this.getCurrentHour()}`

    // Check rate limit
    const count = await this.redis.incr(hourKey)

    if (count === 1) {
      // First message this hour - set expiry
      await this.redis.expire(hourKey, 3600)
    }

    if (count > quota.messagesPerHour) {
      return {
        allowed: false,
        remaining: 0,
        resetAt: this.getNextHour()
      }
    }

    return {
      allowed: true,
      remaining: quota.messagesPerHour - count,
      resetAt: this.getNextHour()
    }
  }

  /**
   * Get memory usage for billing/analytics
   */
  async getUserMemoryStats(userId: string): Promise<{
    conversationCount: number
    totalMessages: number
    storageBytes: number
  }> {
    const conversations = await this.getUserConversations(userId)

    let totalMessages = 0
    let storageBytes = 0

    for (const convId of conversations) {
      const key = `conv:${userId}:${convId}`
      const messages = await this.redis.llen(key)
      const memory = await this.redis.memory('USAGE', key)

      totalMessages += messages
      storageBytes += memory || 0
    }

    return {
      conversationCount: conversations.length,
      totalMessages,
      storageBytes
    }
  }

  private getCurrentHour(): string {
    return new Date().toISOString().slice(0, 13)  // '2026-02-05T14'
  }

  private getNextHour(): Date {
    const next = new Date()
    next.setHours(next.getHours() + 1, 0, 0, 0)
    return next
  }

  private async getUserConversations(userId: string): Promise<string[]> {
    const pattern = `conv:${userId}:*`
    const keys = await this.redis.keys(pattern)
    return keys.map(key => key.split(':')[2])
  }
}
```

### Production Metrics: Memory System Performance

| Metric | Target | How to Measure |
|--------|--------|----------------|
| **Read Latency** | <20ms (P95) | Redis LRANGE time |
| **Write Latency** | <10ms (P95) | Redis LPUSH time |
| **Memory Efficiency** | >80% (bytes used / bytes allocated) | Redis MEMORY USAGE |
| **Cache Hit Rate** | >90% | Redis INFO stats |
| **Token Efficiency** | <2000 tokens/turn avg | Count tokens in getContext() |

**Monitoring Setup**:
```typescript
async function monitorMemoryPerformance(
  userId: string,
  conversationId: string
): Promise<MemoryMetrics> {
  const startTime = Date.now()

  // Measure read latency
  const messages = await redis.getHistory(userId, conversationId, 10)
  const readLatency = Date.now() - startTime

  // Measure memory usage
  const key = `conv:${userId}:${conversationId}`
  const memoryBytes = await redis.redis.memory('USAGE', key)

  // Estimate token count
  const context = await redis.getContext()
  const estimatedTokens = Math.ceil(context.length / 4)

  return {
    readLatency,
    memoryBytes,
    messageCount: messages.length,
    estimatedTokens,
    tokensPerMessage: estimatedTokens / messages.length
  }
}
```

---

## Usage Example

```typescript
const memory = new HybridMemory(userId, vectorDB)

// Conversation 1
await memory.addMessage('user', 'My name is Alice and I love pizza')
const context1 = await memory.getContext('What do you know about me?')

const response1 = await llm.complete({
  system: 'You are a helpful assistant.',
  messages: [
    { role: 'user', content: context1 + '\n\nWhat do you know about me?' }
  ]
})
// Response: "Your name is Alice and you love pizza."

// Conversation 2 (later)
await memory.addMessage('user', 'What food do I like?')
const context2 = await memory.getContext('What food do I like?')

const response2 = await llm.complete({
  system: 'You are a helpful assistant.',
  messages: [
    { role: 'user', content: context2 + '\n\nWhat food do I like?' }
  ]
})
// Response: "You mentioned you love pizza."
```

## Token Management

Manage context window limits.

```typescript
class TokenBudgetMemory {
  private maxTokens: number = 4000

  async getContext(query: string): Promise<string> {
    let context = ''
    let tokens = 0

    // Priority 1: Current query (always include)
    tokens += countTokens(query)

    // Priority 2: Recent messages
    const recent = this.shortTerm.getHistory()
    for (const msg of recent.reverse()) {
      const msgTokens = countTokens(msg.content)
      if (tokens + msgTokens < this.maxTokens * 0.6) {
        context = `${msg.role}: ${msg.content}\n${context}`
        tokens += msgTokens
      }
    }

    // Priority 3: Relevant memories
    const memories = await this.longTerm.recall(query, 10)
    for (const mem of memories) {
      const memTokens = countTokens(mem.text)
      if (tokens + memTokens < this.maxTokens * 0.9) {
        context += `\nPast memory: ${mem.text}`
        tokens += memTokens
      }
    }

    return context
  }
}
```

## Forgetting Strategies

### Time-Based Decay

```typescript
async function forgetOldMemories(days: number = 90) {
  const cutoff = new Date()
  cutoff.setDate(cutoff.getDate() - days)

  await vectorDB.delete({
    filter: {
      timestamp: { $lt: cutoff.toISOString() }
    }
  })
}
```

### Relevance-Based Pruning

```typescript
async function pruneIrrelevantMemories() {
  // Keep only memories accessed in last 30 days
  const memories = await getAllMemories()

  for (const memory of memories) {
    if (memory.lastAccessed < thirtyDaysAgo) {
      await deleteMemory(memory.id)
    }
  }
}
```

## Privacy & Security

```typescript
class SecureMemory {
  async remember(userId: string, content: string) {
    // 1. Sanitize sensitive data
    const sanitized = await removePII(content)

    // 2. Encrypt before storage
    const encrypted = encrypt(sanitized, getUserKey(userId))

    // 3. Store with access controls
    await vectorDB.upsert({
      id: generateId(),
      vector: await embed(sanitized),
      metadata: {
        userId,
        encrypted,
        accessLevel: 'private'
      }
    })
  }

  async recall(userId: string, query: string): Promise<Memory[]> {
    // Only retrieve user's own memories
    const results = await vectorDB.query({
      vector: await embed(query),
      filter: {
        userId: { $eq: userId },
        accessLevel: { $eq: 'private' }
      }
    })

    // Decrypt before returning
    return results.map(r => ({
      ...r,
      text: decrypt(r.metadata.encrypted, getUserKey(userId))
    }))
  }
}
```

## Best Practices

1. **Hybrid approach**: Combine short-term and long-term memory
2. **Token budgets**: Always check context window limits
3. **Relevance ranking**: Retrieve most relevant memories first
4. **Privacy first**: Encrypt sensitive data
5. **Forgetting**: Implement data retention policies
6. **Entity tracking**: Extract and store key entities
7. **Summarization**: Compress old conversations

## Exercise

Build a memory system:
1. Implement conversation buffer
2. Add vector-based long-term memory
3. Extract and track entities
4. Test across multiple sessions
5. Add token budget management

## Resources

- [LangChain Memory](https://python.langchain.com/docs/modules/memory/)
- [Memory in LLM Applications](https://www.pinecone.io/learn/series/langchain/langchain-conversational-memory/)
- [Entity Memory Patterns](https://arxiv.org/abs/2304.03442)
