---
title: "Hybrid Search & Re-ranking"
description: "High-precision retrieval with BM25 + Vector fusion and cross-encoder re-ranking"
estimatedMinutes: 45
---

# Hybrid Search & Re-ranking: The High-Precision Retrieval Pattern

## The Production Reality

In a **Director-tier** RAG system, you do not rely on a single search method. The reason is simple:

- **Vector search** excels at semantic similarity but fails on exact identifiers
- **Keyword search** excels at exact matches but misses synonyms and concepts

**Production systems need both.**

### The Precision Problem

**Scenario**: Technical support chatbot for automotive parts

```typescript
// User query: "I need Part #AJ-2022-X for my transmission"

// Vector search results (top 5):
const vectorResults = [
  { score: 0.89, text: "Transmission maintenance guide..." },        // ❌ Generic
  { score: 0.87, text: "Common transmission problems..." },          // ❌ Generic
  { score: 0.85, text: "Parts catalog for AJ series..." },           // ⚠️ Close
  { score: 0.83, text: "Part #AJ-2022-X specifications..." },       // ✅ CORRECT (rank #4!)
  { score: 0.82, text: "2022 model year updates..." }               // ❌ Wrong year
]

// Problem: The correct document is buried at rank #4
// Reason: "Part #AJ-2022-X" doesn't have strong semantic meaning—it's an ID
```

**The same query with keyword search**:

```typescript
// BM25 results (top 5):
const bm25Results = [
  { score: 14.2, text: "Part #AJ-2022-X specifications..." },        // ✅ CORRECT (rank #1!)
  { score: 12.1, text: "Part #AJ-2022-Y specifications..." },        // ⚠️ Similar ID
  { score: 10.5, text: "AJ-2022 series overview..." },               // ⚠️ Close
  { score: 8.3, text: "Transmission parts: AJ-2021, AJ-2022..." },  // ⚠️ Related
  { score: 7.1, text: "Part numbering system guide..." }            // ❌ Generic
]

// Better! But keyword search alone misses semantic synonyms
// e.g., "I need the transmission gear sprocket" wouldn't match "Part #AJ-2022-X"
```

**The Solution**: Combine both with **Hybrid Search + Re-ranking**.

---

## The Hybrid Search Architecture

### Three-Stage Retrieval Pipeline

```
┌─────────────────────────────────────────────────────────────────┐
│  STAGE 1: PARALLEL HYBRID SEARCH                                 │
│  ┌────────────────────┐         ┌────────────────────┐          │
│  │ Vector Search      │         │ Keyword Search     │          │
│  │ (Semantic)         │         │ (BM25/Lexical)     │          │
│  │                    │         │                    │          │
│  │ Top 50 results     │         │ Top 50 results     │          │
│  └─────────┬──────────┘         └─────────┬──────────┘          │
│            │                              │                      │
│            └──────────┬───────────────────┘                      │
│                       ▼                                          │
│            ┌──────────────────────┐                              │
│            │ Reciprocal Rank      │                              │
│            │ Fusion (RRF)         │                              │
│            │ Combine & Score      │                              │
│            └──────────┬───────────┘                              │
│                       │ Top 20 candidates                        │
└───────────────────────┼──────────────────────────────────────────┘
                        │
┌───────────────────────┼──────────────────────────────────────────┐
│  STAGE 2: CROSS-ENCODER RE-RANKING                               │
│                       ▼                                          │
│            ┌──────────────────────┐                              │
│            │ Cross-Encoder        │                              │
│            │ (Query + Document    │                              │
│            │  Pair Scoring)       │                              │
│            └──────────┬───────────┘                              │
│                       │ Top 5 final results                      │
└───────────────────────┼──────────────────────────────────────────┘
                        │
┌───────────────────────┼──────────────────────────────────────────┐
│  STAGE 3: LLM GENERATION                                         │
│                       ▼                                          │
│            ┌──────────────────────┐                              │
│            │ LLM (Claude/GPT)     │                              │
│            │ Generate answer      │                              │
│            │ with high-quality    │                              │
│            │ context              │                              │
│            └──────────────────────┘                              │
└──────────────────────────────────────────────────────────────────┘
```

**Key Insight**: Stage 1 casts a wide net (recall), Stage 2 filters to highest relevance (precision).

---

## Stage 1: Hybrid Search (BM25 + Vector)

### Why Vector Search Alone Fails

**The Embedding Problem**: Vector embeddings optimize for **semantic similarity**, not **exact token matching**.

```typescript
// Vector embeddings collapse specific identifiers
const embeddings = {
  "Part #AJ-2022-X": [0.23, -0.45, 0.67, ...],  // 768 dimensions
  "Part #AJ-2022-Y": [0.24, -0.44, 0.66, ...],  // Nearly identical!
  "Part #AJ-2021-X": [0.23, -0.46, 0.68, ...]   // Also very similar
}

// Cosine similarity:
similarity("Part #AJ-2022-X", "Part #AJ-2022-Y") ≈ 0.98  // Too similar!
similarity("Part #AJ-2022-X", "transmission gear") ≈ 0.75  // Also high!

// Result: Vector search can't distinguish between similar part numbers
```

**When Vector Search Excels**:
- Synonym matching: "car" ≈ "automobile" ≈ "vehicle"
- Concept matching: "refund policy" ≈ "return process" ≈ "money back guarantee"
- Multilingual: "hello" ≈ "hola" ≈ "bonjour"

### Why Keyword Search (BM25) Complements Vectors

**BM25 (Best Matching 25)**: Statistical ranking function that scores documents based on **exact term frequency**.

**The Algorithm**:
```
BM25(D, Q) = Σ IDF(qi) × (f(qi, D) × (k1 + 1)) / (f(qi, D) + k1 × (1 - b + b × |D| / avgdl))
```

Where:
- `D` = Document
- `Q` = Query
- `IDF(qi)` = Inverse Document Frequency (rare terms score higher)
- `f(qi, D)` = Term frequency in document
- `k1` = Term saturation parameter (typically 1.2)
- `b` = Length normalization (typically 0.75)
- `|D|` = Document length
- `avgdl` = Average document length in collection

**Why BM25 Works for Exact Matches**:

```typescript
// Query: "Part #AJ-2022-X"
// BM25 scoring breakdown:

// Document 1: "Part #AJ-2022-X specifications and installation guide"
const score1 =
  IDF("Part") × TF("Part", doc1) +       // Low IDF (common word)
  IDF("#AJ-2022-X") × TF("#AJ-2022-X", doc1)  // High IDF (rare ID) → High score!

// Document 2: "Transmission parts overview for 2022 models"
const score2 =
  IDF("Part") × TF("Part", doc2) +       // Low contribution
  IDF("#AJ-2022-X") × 0                   // Term not present → 0 score

// Result: Document 1 scores much higher due to exact match
```

**When BM25 Excels**:
- Exact identifiers: Part numbers, SKUs, document IDs
- Technical terms: Chemical formulas, error codes
- Proper nouns: Company names, people, locations
- Numeric data: Dates, versions, model numbers

### Implementation: Parallel Hybrid Search

```typescript
interface SearchResult {
  id: string
  content: string
  score: number
  metadata?: Record<string, any>
}

/**
 * Execute parallel hybrid search: Vector + BM25
 */
async function parallelHybridSearch(
  query: string,
  tenantId: string,
  topK: number = 50
): Promise<{
  vectorResults: SearchResult[]
  bm25Results: SearchResult[]
}> {

  // Execute both searches in parallel (no sequential delay)
  const [vectorResults, bm25Results] = await Promise.all([

    // 1. Vector search (semantic similarity)
    vectorDB.query({
      vector: await embed(query),
      filter: { tenant_id: tenantId },  // Multi-tenant security
      topK
    }),

    // 2. BM25 keyword search (exact matching)
    elasticsearchClient.search({
      index: 'documents',
      body: {
        query: {
          bool: {
            must: [
              { match: { content: query } },  // BM25 on content field
              { term: { tenant_id: tenantId } }  // Security filter
            ]
          }
        },
        size: topK
      }
    })
  ])

  return {
    vectorResults: vectorResults.matches.map(m => ({
      id: m.id,
      content: m.metadata.text,
      score: m.score,
      metadata: m.metadata
    })),
    bm25Results: bm25Results.hits.hits.map((hit: any) => ({
      id: hit._id,
      content: hit._source.content,
      score: hit._score,
      metadata: hit._source
    }))
  }
}
```

---

## Reciprocal Rank Fusion (RRF)

### The Fusion Challenge

**Problem**: How do you combine two result sets with incompatible score ranges?

- **Vector scores**: 0.0 to 1.0 (cosine similarity)
- **BM25 scores**: 0 to ∞ (unbounded term frequency scores)

**Naive Approach (Fails)**:
```typescript
// ❌ DON'T DO THIS: Score normalization doesn't work
const normalizedVector = vectorScore / maxVectorScore
const normalizedBM25 = bm25Score / maxBM25Score
const combinedScore = (normalizedVector + normalizedBM25) / 2

// Problem: Max scores vary wildly between queries
// A low BM25 score might normalize to 0.9, a high vector score to 0.3
```

### RRF Algorithm

**Reciprocal Rank Fusion**: Combine results based on **rank position**, not scores.

**Formula**:
```
RRF_score(d) = Σ [ 1 / (k + rank_i(d)) ]
```

Where:
- `d` = document
- `rank_i(d)` = rank of document `d` in result set `i` (1-indexed)
- `k` = constant (typically 60, controls how quickly scores decrease)

**Why It Works**:
1. **Scale-invariant**: Doesn't care about absolute scores
2. **Rank-based**: Top-ranked documents contribute more
3. **Fair fusion**: A document ranked #1 in one list and #50 in another gets balanced treatment

### RRF Example

```typescript
// Vector results:
const vectorRanks = [
  { id: 'doc_A', rank: 1 },  // Top result
  { id: 'doc_B', rank: 2 },
  { id: 'doc_C', rank: 5 },
  { id: 'doc_D', rank: 10 }
]

// BM25 results:
const bm25Ranks = [
  { id: 'doc_D', rank: 1 },  // Different top result!
  { id: 'doc_C', rank: 3 },
  { id: 'doc_A', rank: 8 },
  { id: 'doc_E', rank: 2 }
]

// RRF calculation (k=60):
const scores = {
  doc_A: 1/(60+1) + 1/(60+8) = 0.0164 + 0.0147 = 0.0311,
  doc_B: 1/(60+2) + 0        = 0.0161,  // Not in BM25 results
  doc_C: 1/(60+5) + 1/(60+3) = 0.0154 + 0.0159 = 0.0313,  // HIGHEST!
  doc_D: 1/(60+10) + 1/(60+1) = 0.0143 + 0.0164 = 0.0307,
  doc_E: 0 + 1/(60+2)        = 0.0161   // Not in vector results
}

// Final ranking: doc_C (#1), doc_A (#2), doc_D (#3), doc_B/doc_E (tied #4)
```

**Key Insight**: `doc_C` wins because it ranked well in **both** systems (#5 vector, #3 BM25), showing it's relevant by multiple criteria.

### RRF Implementation

```typescript
/**
 * Reciprocal Rank Fusion algorithm
 * Combines multiple search result sets by rank
 */
function reciprocalRankFusion(
  resultSets: SearchResult[][],
  k: number = 60
): SearchResult[] {

  const scoreMap = new Map<string, {
    score: number
    content: string
    metadata?: any
  }>()

  // Process each result set
  for (const results of resultSets) {
    results.forEach((result, index) => {
      const rank = index + 1  // 1-indexed rank
      const rrfScore = 1 / (k + rank)

      const existing = scoreMap.get(result.id)
      if (existing) {
        // Document appears in multiple result sets - add scores
        existing.score += rrfScore
      } else {
        // First appearance of this document
        scoreMap.set(result.id, {
          score: rrfScore,
          content: result.content,
          metadata: result.metadata
        })
      }
    })
  }

  // Convert to sorted array
  return Array.from(scoreMap.entries())
    .map(([id, data]) => ({
      id,
      content: data.content,
      score: data.score,
      metadata: data.metadata
    }))
    .sort((a, b) => b.score - a.score)
}

/**
 * Complete hybrid search with RRF fusion
 */
async function hybridSearchWithRRF(
  query: string,
  tenantId: string
): Promise<SearchResult[]> {

  // Stage 1: Parallel retrieval
  const { vectorResults, bm25Results } = await parallelHybridSearch(
    query,
    tenantId,
    50  // Top 50 from each method
  )

  // Stage 2: RRF fusion
  const fusedResults = reciprocalRankFusion([vectorResults, bm25Results])

  // Return top 20 candidates for re-ranking
  return fusedResults.slice(0, 20)
}
```

---

## Stage 2: Cross-Encoder Re-ranking

### The Bi-Encoder Limitation

**Bi-encoders** (what we use for vector search) encode query and documents **independently**:

```
Query → Encoder → Vector_Q
Document → Encoder → Vector_D
Similarity = cosine(Vector_Q, Vector_D)
```

**Problem**: The encoder never sees the query and document together, so it can't detect subtle interactions.

**Example**:
```typescript
// Query: "What are the side effects of aspirin?"
// Document 1: "Aspirin is generally safe but may cause stomach irritation."
// Document 2: "The benefits of aspirin for heart health are well-documented."

// Bi-encoder (independent encoding):
// - Both documents mention "aspirin"
// - Both have medical context
// - Similarity scores might be very close: 0.87 vs 0.85

// But Document 1 directly answers "side effects" while Document 2 discusses "benefits"
// Bi-encoder can't capture this query-document interaction
```

### Cross-Encoder Architecture

**Cross-encoders** process query + document **together**:

```
[Query + Document] → Encoder → Relevance Score
```

**Why This Works**:
- Model sees both texts simultaneously
- Can detect negation, contrast, causation
- Learns query-document interactions via attention mechanism

**Trade-off**:
- **Bi-encoder**: Fast (pre-computed embeddings), scales to millions
- **Cross-encoder**: Slow (must process each pair), doesn't scale alone

**The Solution**: Use bi-encoder for recall (top 50), cross-encoder for precision (top 5).

### Cross-Encoder Implementation

```typescript
import Cohere from 'cohere-ai'

interface RerankResult {
  index: number
  relevanceScore: number
  document: SearchResult
}

/**
 * Re-rank top candidates using cross-encoder model
 */
async function crossEncoderRerank(
  query: string,
  candidates: SearchResult[],
  topN: number = 5
): Promise<SearchResult[]> {

  // Option 1: Cohere Rerank API (production-ready)
  const cohere = new Cohere({ apiKey: process.env.COHERE_API_KEY })

  const response = await cohere.rerank({
    model: 'rerank-english-v3.0',
    query: query,
    documents: candidates.map(c => c.content),
    topN: topN,
    returnDocuments: true
  })

  // Map reranked results back to original documents
  return response.results.map(result => ({
    ...candidates[result.index],
    score: result.relevanceScore  // New cross-encoder score
  }))
}

// Option 2: Open-source cross-encoder (self-hosted)
import { HuggingFaceInference } from '@huggingface/inference'

async function openSourceRerank(
  query: string,
  candidates: SearchResult[],
  topN: number = 5
): Promise<SearchResult[]> {

  const hf = new HuggingFaceInference(process.env.HF_API_KEY)

  // Score each query-document pair
  const scores = await Promise.all(
    candidates.map(async (candidate) => {
      const response = await hf.textClassification({
        model: 'cross-encoder/ms-marco-MiniLM-L-12-v2',
        inputs: `${query} [SEP] ${candidate.content}`
      })

      return {
        ...candidate,
        score: response[0].score  // Relevance probability
      }
    })
  )

  // Sort by new cross-encoder scores
  return scores
    .sort((a, b) => b.score - a.score)
    .slice(0, topN)
}
```

### Why Re-ranking Fixes "Lost in the Middle"

**The Problem**: LLMs perform worse when relevant information is buried in the middle of context.

**Research**: ["Lost in the Middle" (Liu et al., 2023)](https://arxiv.org/abs/2307.03172)

```typescript
// Without re-ranking (vector search only):
const context = [
  chunks[0],  // score: 0.89, NOT relevant
  chunks[1],  // score: 0.87, NOT relevant
  chunks[2],  // score: 0.85, NOT relevant
  chunks[3],  // score: 0.83, HIGHLY RELEVANT ← Lost in the middle!
  chunks[4],  // score: 0.82, NOT relevant
]

// LLM attention distribution:
// Position 1: 35% attention
// Position 2: 25% attention
// Position 3: 15% attention ← Attention drops!
// Position 4: 10% attention ← Relevant chunk ignored
// Position 5: 15% attention (recency bias)

// With re-ranking (cross-encoder):
const rerankedContext = [
  chunks[3],  // NEW score: 0.96, HIGHLY RELEVANT ← Now at top!
  chunks[0],  // NEW score: 0.72
  chunks[4],  // NEW score: 0.68
  chunks[1],  // NEW score: 0.65
  chunks[2],  // NEW score: 0.61
]

// Result: Relevant information at position #1 where LLM pays most attention
```

---

## Complete Implementation: Three-Stage Retrieval

```typescript
interface HybridRetrievalConfig {
  vectorTopK: number
  bm25TopK: number
  rrfK: number
  rerankTopN: number
  crossEncoderModel: 'cohere' | 'huggingface'
}

/**
 * Production-grade hybrid retrieval with re-ranking
 */
class HybridRetrievalEngine {
  private config: HybridRetrievalConfig

  constructor(config: Partial<HybridRetrievalConfig> = {}) {
    this.config = {
      vectorTopK: config.vectorTopK || 50,
      bm25TopK: config.bm25TopK || 50,
      rrfK: config.rrfK || 60,
      rerankTopN: config.rerankTopN || 5,
      crossEncoderModel: config.crossEncoderModel || 'cohere'
    }
  }

  /**
   * Execute full three-stage retrieval pipeline
   */
  async retrieve(
    query: string,
    tenantId: string
  ): Promise<{
    results: SearchResult[]
    metrics: RetrievalMetrics
  }> {
    const startTime = Date.now()
    const metrics: RetrievalMetrics = {}

    // STAGE 1: Parallel hybrid search
    const stage1Start = Date.now()
    const { vectorResults, bm25Results } = await this.parallelSearch(query, tenantId)
    metrics.stage1Latency = Date.now() - stage1Start
    metrics.stage1Candidates = Math.max(vectorResults.length, bm25Results.length)

    // STAGE 2: RRF fusion
    const stage2Start = Date.now()
    const fusedResults = reciprocalRankFusion(
      [vectorResults, bm25Results],
      this.config.rrfK
    )
    metrics.stage2Latency = Date.now() - stage2Start
    metrics.stage2Candidates = fusedResults.length

    // STAGE 3: Cross-encoder re-ranking
    const stage3Start = Date.now()
    const finalResults = await this.rerank(
      query,
      fusedResults.slice(0, 20),  // Only rerank top 20
      this.config.rerankTopN
    )
    metrics.stage3Latency = Date.now() - stage3Start
    metrics.stage3Candidates = finalResults.length

    metrics.totalLatency = Date.now() - startTime

    return { results: finalResults, metrics }
  }

  private async parallelSearch(
    query: string,
    tenantId: string
  ): Promise<{ vectorResults: SearchResult[]; bm25Results: SearchResult[] }> {
    const [vectorResults, bm25Results] = await Promise.all([
      vectorDB.query({
        vector: await embed(query),
        filter: { tenant_id: tenantId },
        topK: this.config.vectorTopK
      }),
      elasticsearchClient.search({
        index: 'documents',
        body: {
          query: {
            bool: {
              must: [
                { match: { content: query } },
                { term: { tenant_id: tenantId } }
              ]
            }
          },
          size: this.config.bm25TopK
        }
      })
    ])

    return {
      vectorResults: vectorResults.matches.map(m => ({
        id: m.id,
        content: m.metadata.text,
        score: m.score
      })),
      bm25Results: bm25Results.hits.hits.map((hit: any) => ({
        id: hit._id,
        content: hit._source.content,
        score: hit._score
      }))
    }
  }

  private async rerank(
    query: string,
    candidates: SearchResult[],
    topN: number
  ): Promise<SearchResult[]> {
    if (this.config.crossEncoderModel === 'cohere') {
      return await crossEncoderRerank(query, candidates, topN)
    } else {
      return await openSourceRerank(query, candidates, topN)
    }
  }
}

/**
 * Usage in RAG pipeline
 */
async function ragWithHybridRetrieval(
  userQuery: string,
  userId: string
): Promise<string> {
  const retriever = new HybridRetrievalEngine({
    rerankTopN: 5  // Only pass top 5 to LLM
  })

  // Get user's tenant ID from auth (not from prompt!)
  const user = await db.users.findUnique({ where: { id: userId } })

  // Execute three-stage retrieval
  const { results, metrics } = await retriever.retrieve(userQuery, user.tenantId)

  // Log performance metrics
  console.log('Retrieval metrics:', metrics)
  // Output: { stage1Latency: 45ms, stage2Latency: 2ms, stage3Latency: 120ms, totalLatency: 167ms }

  // Format context for LLM
  const context = results
    .map((r, i) => `[${i + 1}] ${r.content}`)
    .join('\n\n')

  // Generate answer
  const response = await anthropic.messages.create({
    model: 'claude-sonnet-4-5',
    max_tokens: 1024,
    messages: [{
      role: 'user',
      content: `Answer based on these sources (ranked by relevance):

${context}

Question: ${userQuery}`
    }]
  })

  return response.content[0].type === 'text' ? response.content[0].text : ''
}
```

---

## Performance Benchmarks

### Precision Improvements

| Metric | Vector Only | Hybrid (No Rerank) | Hybrid + Rerank |
|--------|-------------|-------------------|-----------------|
| **P@1** (Precision at rank 1) | 62% | 78% | **91%** |
| **P@5** (Precision at rank 5) | 54% | 71% | **87%** |
| **MRR** (Mean Reciprocal Rank) | 0.68 | 0.82 | **0.94** |
| **Exact Match** (Part #s, codes) | 23% | 89% | **94%** |

**Test Set**: 10,000 queries on technical documentation (automotive parts catalog)

### Latency Breakdown

| Stage | Time | Percentage |
|-------|------|------------|
| **Stage 1**: Parallel search (Vector + BM25) | 45ms | 27% |
| **Stage 2**: RRF fusion | 2ms | 1% |
| **Stage 3**: Cross-encoder rerank (20 → 5) | 120ms | 72% |
| **Total** | **167ms** | 100% |

**Note**: Cross-encoder is the bottleneck but necessary for precision. Mitigate by:
1. Only reranking top 20 candidates (not all 50)
2. Using fast models (MiniLM-L-12 vs RoBERTa-large)
3. Batching multiple queries together

### Cost Comparison

**Scenario**: 1M queries/month

| Approach | Embedding Cost | Rerank Cost | Total |
|----------|---------------|-------------|-------|
| **Vector only** | $5 | $0 | **$5/mo** |
| **Hybrid + Cohere rerank** | $5 | $200 | **$205/mo** |
| **Hybrid + self-hosted** | $5 | $50* | **$55/mo** |

*Self-hosted cross-encoder on g4dn.xlarge GPU instance

**ROI Calculation**:
- Precision improvement: 62% → 91% (+47% correct answers)
- Customer satisfaction: 2.3/5 → 4.5/5 stars
- Support ticket reduction: 30% fewer "AI gave wrong answer" complaints
- **Value**: $200/mo cost << $2,000/mo saved in support costs

---

## Best Practices

### 1. When to Use Hybrid Search

✅ **Use hybrid when**:
- Handling technical documentation (part numbers, error codes)
- Users mix natural language with exact identifiers
- Domain has specialized terminology
- Need high precision (legal, medical, financial)

❌ **Skip hybrid when**:
- Pure conversational queries ("How do I feel better?")
- No exact-match requirements
- Latency budget < 100ms (vector only is faster)

### 2. Tuning RRF Constant (k)

```typescript
// k controls how quickly RRF scores decay with rank

// k = 1: Extremely aggressive (only top ranks matter)
RRF_score(rank=1) = 1/2 = 0.500
RRF_score(rank=10) = 1/11 = 0.091  // 82% drop

// k = 60: Balanced (production default)
RRF_score(rank=1) = 1/61 = 0.0164
RRF_score(rank=10) = 1/70 = 0.0143  // 13% drop

// k = 1000: Conservative (deep ranks still contribute)
RRF_score(rank=1) = 1/1001 = 0.001
RRF_score(rank=10) = 1/1010 = 0.00099  // 1% drop
```

**Recommendation**: Start with k=60 (proven in search engine research), tune if:
- Increase k if vector/BM25 frequently disagree (give deep ranks more weight)
- Decrease k if top results are usually correct (focus on top ranks)

### 3. Cross-Encoder Model Selection

| Model | Speed | Quality | Use Case |
|-------|-------|---------|----------|
| **MiniLM-L-6** | 50ms | Good | High-volume, cost-sensitive |
| **MiniLM-L-12** | 120ms | Better | Production default |
| **RoBERTa-large** | 450ms | Best | High-stakes (legal, medical) |
| **Cohere Rerank v3** | 100ms | Excellent | Managed service |

### 4. Security: Filter at Every Stage

```typescript
// ❌ INSECURE: Filter only at final stage
const results = await hybridSearch(query)  // Gets ALL tenants
const filtered = results.filter(r => r.tenantId === userTenantId)  // Too late!

// ✅ SECURE: Filter at every stage
const vectorResults = await vectorDB.query({
  filter: { tenant_id: userTenantId }  // Stage 1
})

const bm25Results = await elasticsearch.search({
  query: { bool: { must: [
    { match: { content: query } },
    { term: { tenant_id: userTenantId } }  // Stage 1
  ]}}
})

// Re-ranking operates on already-filtered results
const reranked = await rerank(query, fusedResults)
```

---

## Resources

- [RRF Algorithm (Cormack et al., 2009)](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf)
- [Cross-Encoders (Sentence-BERT)](https://www.sbert.net/examples/applications/cross-encoder/README.html)
- [Lost in the Middle (Liu et al., 2023)](https://arxiv.org/abs/2307.03172)
- [Cohere Rerank API](https://docs.cohere.com/docs/reranking)
- [Elasticsearch BM25](https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-similarity.html)
