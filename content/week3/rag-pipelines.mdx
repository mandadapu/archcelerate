---
title: "Building RAG Pipelines"
description: "Build production retrieval-augmented generation systems"
estimatedMinutes: 40
---

# Building RAG Pipelines

## What is RAG?

**Retrieval-Augmented Generation (RAG)** combines information retrieval with LLM generation to answer questions using external knowledge.

**Without RAG:**
```
User: "What are our Q4 revenue numbers?"
LLM: "I don't have access to that information."
```

**With RAG:**
```
User: "What are our Q4 revenue numbers?"
→ Retrieve: [Q4 Financial Report: Revenue was $2.4M...]
→ Generate: "According to the Q4 financial report, revenue was $2.4M."
```

## RAG Pipeline Architecture

```
┌─────────┐     ┌──────────┐     ┌─────────┐     ┌─────────┐
│  Query  │ ──> │ Retrieve │ ──> │ Rerank  │ ──> │Generate │
└─────────┘     └──────────┘     └─────────┘     └─────────┘
                     │                               │
                     ▼                               ▼
              Vector Database                    LLM API
```

## Implementation

### Basic RAG System

```typescript
import { embed } from './embeddings'
import { pinecone } from './vector-db'
import { anthropic } from '@anthropic-ai/sdk'

async function ragQuery(question: string): Promise<string> {
  // 1. Embed the question
  const queryEmbedding = await embed(question)

  // 2. Retrieve relevant documents
  const results = await pinecone.index('docs').query({
    vector: queryEmbedding,
    topK: 5,
    includeMetadata: true
  })

  // 3. Format context
  const context = results.matches
    .map(match => match.metadata?.text)
    .join('\n\n---\n\n')

  // 4. Generate answer
  const response = await anthropic.messages.create({
    model: 'claude-3-sonnet-20240229',
    max_tokens: 1024,
    messages: [{
      role: 'user',
      content: `Answer this question using ONLY the context below. If the answer isn't in the context, say "I don't know."

Context:
${context}

Question: ${question}`
    }]
  })

  return response.content[0].text
}

### The Naive RAG Problem

**Reality Check**: The basic 4-step RAG (Embed → Retrieve → Format → Generate) suffers from **low precision**—it retrieves 100 potential matches but struggles to identify the 1 true answer.

> **Architect Perspective**: Vector databases are great at **recall** (finding 100 potential matches) but poor at **precision** (identifying the 1 true answer). Never pass raw vector results directly to your prompt. Use a **Cross-Encoder Re-ranker** (like Cohere or a local BGE model) to re-score the top 20-50 chunks. This ensures the most semantically relevant data is at the **top** of the context window, solving the "Lost in the Middle" problem.

## Two-Stage Retrieval: Recall → Precision

For full implementation details of two-stage retrieval with cross-encoder re-ranking, see the production patterns below. The key insight: bi-encoders (vector search) cast a wide net for recall, then cross-encoders re-rank for precision.

### Production RAG Pipeline

```typescript
interface RAGConfig {
  retrievalK: number
  rerankK: number
  minScore: number
  includeMetadata: boolean
}

class RAGPipeline {
  constructor(
    private vectorDB: VectorDB,
    private llm: LLM,
    private config: RAGConfig
  ) {}

  async query(question: string): Promise<RAGResponse> {
    // 1. Query preprocessing
    const processedQuery = await this.preprocessQuery(question)

    // 2. Retrieval
    const retrieved = await this.retrieve(processedQuery)

    // 3. Reranking
    const reranked = await this.rerank(question, retrieved)

    // 4. Context assembly
    const context = this.assembleContext(reranked)

    // 5. Generation
    const answer = await this.generate(question, context)

    // 6. Citation extraction
    const citations = this.extractCitations(answer, reranked)

    return {
      answer: answer.text,
      citations,
      sources: reranked.map(d => d.metadata)
    }
  }

  private async preprocessQuery(query: string): Promise<string> {
    // Expand query with synonyms, fix typos, etc.
    return query
  }

  private async retrieve(query: string): Promise<Document[]> {
    const embedding = await embed(query)

    const results = await this.vectorDB.query({
      vector: embedding,
      topK: this.config.retrievalK,
      includeMetadata: true
    })

    return results.matches
      .filter(m => m.score &gt;= this.config.minScore)
      .map(m => ({
        id: m.id,
        text: m.metadata.text,
        metadata: m.metadata,
        score: m.score
      }))
  }

  private async rerank(
    query: string,
    documents: Document[]
  ): Promise<Document[]> {
    // Use cross-encoder or LLM for reranking
    const reranked = await rerankWithCrossEncoder(query, documents)

    return reranked.slice(0, this.config.rerankK)
  }

  private assembleContext(documents: Document[]): string {
    return documents
      .map((doc, i) => `[${i + 1}] ${doc.text}\nSource: ${doc.metadata.source}`)
      .join('\n\n---\n\n')
  }

  private async generate(
    question: string,
    context: string
  ): Promise<{ text: string }> {
    const response = await this.llm.complete({
      system: `You are a helpful assistant. Answer questions using the provided context.
Always cite sources using [1], [2], etc.
If the answer isn't in the context, say "I don't have enough information to answer that."`,
      messages: [{
        role: 'user',
        content: `Context:\n${context}\n\nQuestion: ${question}`
      }]
    })

    return { text: response.content }
  }

  private extractCitations(
    answer: string,
    sources: Document[]
  ): Citation[] {
    const citationPattern = /\[(\d+)\]/g
    const matches = [...answer.matchAll(citationPattern)]

    return matches.map(match => {
      const index = parseInt(match[1]) - 1
      return {
        text: match[0],
        source: sources[index]
      }
    })
  }
}
```

## Document Chunking Strategies

The quality of your RAG system is determined by chunk quality. Bad chunking = bad retrieval = hallucinations. This section covers production-grade chunking strategies that preserve semantic meaning across boundaries.

### The Chunking Problem

**Bad Chunking** (context lost):
```
Chunk 1: "...our revenue grew by"
Chunk 2: "25% in Q4 2025 due to..."
```
Query: "What was Q4 revenue growth?" → **Retrieves Chunk 2 only** → Missing context → Hallucination

**Good Chunking** (context preserved):
```
Chunk 1: "...our revenue grew by 25% in Q4 2025 due to strong enterprise sales..."
Chunk 2 (overlap): "Q4 2025 due to strong enterprise sales. The growth was driven by..."
```
Query: "What was Q4 revenue growth?" → **Retrieves Chunk 1 with full context** → Accurate answer

### Chunking Strategy Decision Matrix

| Strategy | Best For | Chunk Size | Pros | Cons | Use When |
|----------|----------|-----------|------|------|----------|
| **Fixed-Size** | Generic text | 500-1000 tokens | Fast, predictable | Splits mid-sentence | No structure available |
| **Recursive Character** | Mixed content | 500-1500 tokens | Respects boundaries | Slower | Markdown, code, structured docs |
| **Semantic** | Dense content | Variable (200-2000) | Meaning-preserving | Very slow, needs LLM | High-stakes RAG (legal, medical) |
| **Document-Aware** | Technical docs | Variable by section | Natural boundaries | Format-specific | Known document structure |

### Overlap Engineering: The Sliding Window Technique

**The Key Finding**: Overlapping chunks improve retrieval recall by 15-30% with only 10-20% storage increase.

```typescript
interface ChunkingConfig {
  chunkSize: number      // Target chunk size in tokens
  overlapSize: number    // Overlap in tokens
  overlapRatio: number   // overlapSize / chunkSize
  minChunkSize: number   // Don't create tiny chunks
}

/**
 * Optimal overlap configuration based on content type
 */
const CHUNKING_PROFILES: Record<string, ChunkingConfig> = {
  'technical-docs': {
    chunkSize: 800,
    overlapSize: 160,      // 20% overlap
    overlapRatio: 0.20,
    minChunkSize: 200
  },
  'conversational': {
    chunkSize: 500,
    overlapSize: 100,      // 20% overlap
    overlapRatio: 0.20,
    minChunkSize: 100
  },
  'code': {
    chunkSize: 1000,
    overlapSize: 200,      // 20% overlap (preserve function context)
    overlapRatio: 0.20,
    minChunkSize: 300
  },
  'legal-contracts': {
    chunkSize: 1500,
    overlapSize: 300,      // 20% overlap (critical context)
    overlapRatio: 0.20,
    minChunkSize: 500
  }
}

/**
 * Production-grade recursive character splitter
 * Respects natural boundaries: paragraphs, sentences, words
 */
class RecursiveCharacterTextSplitter {
  private separators: string[] = [
    '\n\n',      // Paragraphs (highest priority)
    '\n',        // Lines
    '. ',        // Sentences
    '? ',
    '! ',
    '; ',
    ', ',        // Clauses
    ' ',         // Words (last resort)
    ''           // Characters (absolute last resort)
  ]

  constructor(private config: ChunkingConfig) {}

  split(text: string): Array<{ content: string; metadata: ChunkMetadata }> {
    const chunks: Array<{ content: string; metadata: ChunkMetadata }> = []

    // Approximate: 1 token ≈ 4 characters
    const chunkChars = this.config.chunkSize * 4
    const overlapChars = this.config.overlapSize * 4

    let startIndex = 0
    let chunkIndex = 0

    while (startIndex < text.length) {
      // Calculate end index
      let endIndex = Math.min(startIndex + chunkChars, text.length)

      // Try to find natural boundary
      endIndex = this.findNaturalBoundary(text, startIndex, endIndex)

      // Extract chunk with overlap
      const chunkContent = text.slice(startIndex, endIndex)

      // Ensure minimum size
      if (chunkContent.length &gt;= this.config.minChunkSize * 4 || endIndex === text.length) {
        chunks.push({
          content: chunkContent.trim(),
          metadata: {
            chunkIndex,
            startChar: startIndex,
            endChar: endIndex,
            overlapWithPrevious: chunkIndex &gt; 0 ? overlapChars : 0,
            strategy: 'recursive-character'
          }
        })
        chunkIndex++
      }

      // Move start forward, accounting for overlap
      startIndex = endIndex - overlapChars

      // Prevent infinite loop
      if (startIndex &gt;= endIndex - 10) {
        startIndex = endIndex
      }
    }

    return chunks
  }

  /**
   * Find natural boundary using separator hierarchy
   */
  private findNaturalBoundary(
    text: string,
    start: number,
    end: number
  ): number {
    // If we're at the end, return as-is
    if (end &gt;= text.length) {
      return text.length
    }

    // Search backwards from end for natural separators
    const searchWindow = text.slice(start, end + 100)  // Look a bit ahead

    for (const separator of this.separators) {
      const lastIndex = searchWindow.lastIndexOf(separator, end - start)

      if (lastIndex !== -1 && lastIndex > (end - start) / 2) {
        // Found separator in second half of chunk
        return start + lastIndex + separator.length
      }
    }

    // No separator found, return original end
    return end
  }
}
```

### Overlap Strategy: The 20% Rule

**Research Finding** (from Pinecone Labs):
- **0% overlap**: Baseline retrieval recall
- **10% overlap**: +8% recall improvement
- **20% overlap**: +15% recall improvement ⭐ (optimal)
- **50% overlap**: +18% recall improvement (diminishing returns, 2.5x storage cost)

**The 20% Rule**: Overlap 20% of your chunk size for optimal recall/cost trade-off.

```typescript
/**
 * Visualize overlap strategy
 */
interface OverlapVisualization {
  chunk: string
  overlap: string
  unique: string
}

function demonstrateOverlap(text: string, chunkSize: number, overlapRatio: number): void {
  const overlapSize = chunkSize * overlapRatio

  console.log('Chunk 1:')
  console.log('  Unique content:', text.slice(0, chunkSize - overlapSize))
  console.log('  Overlap region:', text.slice(chunkSize - overlapSize, chunkSize))

  console.log('\nChunk 2:')
  console.log('  Overlap region:', text.slice(chunkSize - overlapSize, chunkSize))
  console.log('  Unique content:', text.slice(chunkSize, chunkSize * 2 - overlapSize))
}

// Example:
// Chunk 1: [........................aaaaaaa]
// Chunk 2:                     [aaaaaaa........................]
//          ^----------------^  ^-----^  ^----------------^
//          Unique (Chunk 1)    Overlap  Unique (Chunk 2)
```

### Production Chunking: Semantic-Aware Strategy

For high-stakes applications (legal, medical, financial), use LLM-powered semantic chunking:

```typescript
/**
 * Semantic chunking using LLM to identify natural breaks
 */
class SemanticChunker {
  async chunk(
    document: string,
    config: ChunkingConfig
  ): Promise<Array<{ content: string; reasoning: string }>> {
    // Step 1: Split into candidate chunks
    const splitter = new RecursiveCharacterTextSplitter(config)
    const candidates = splitter.split(document)

    // Step 2: Use LLM to evaluate and refine boundaries
    const refinedChunks: Array<{ content: string; reasoning: string }> = []

    for (let i = 0; i < candidates.length - 1; i++) {
      const current = candidates[i]
      const next = candidates[i + 1]

      // Ask LLM: Should these be merged or split differently?
      const decision = await this.evaluateBoundary(current.content, next.content)

      if (decision.shouldMerge) {
        // Merge current and next
        refinedChunks.push({
          content: current.content + ' ' + next.content,
          reasoning: decision.reasoning
        })
        i++  // Skip next since we merged
      } else if (decision.betterBoundary) {
        // Split at LLM-suggested boundary
        const [part1, part2] = this.splitAtBoundary(
          current.content + next.content,
          decision.betterBoundary
        )
        refinedChunks.push(
          { content: part1, reasoning: decision.reasoning },
          { content: part2, reasoning: 'Continuation after natural break' }
        )
        i++
      } else {
        // Keep as-is
        refinedChunks.push({ content: current.content, reasoning: 'Natural boundary' })
      }
    }

    return refinedChunks
  }

  private async evaluateBoundary(
    chunk1: string,
    chunk2: string
  ): Promise<{
    shouldMerge: boolean
    betterBoundary?: number
    reasoning: string
  }> {
    const response = await anthropic.messages.create({
      model: 'claude-3-haiku-20240307',
      max_tokens: 200,
      messages: [{
        role: 'user',
        content: `Evaluate this chunk boundary:

Chunk 1: "${chunk1.slice(-200)}"
Chunk 2: "${chunk2.slice(0, 200)}"

Questions:
1. Does Chunk 1 end mid-thought/mid-sentence?
2. Does Chunk 2 start without necessary context?
3. Should these be merged or is the boundary good?

Return JSON:
{
  "shouldMerge": boolean,
  "reasoning": "explanation"
}`
      }]
    })

    const text = response.content[0].type === 'text' ? response.content[0].text : '{}'
    return JSON.parse(text)
  }

  private splitAtBoundary(combined: string, boundary: number): [string, string] {
    return [combined.slice(0, boundary), combined.slice(boundary)]
  }
}
```

### Code-Aware Chunking

For code repositories, respect function and class boundaries:

```typescript
/**
 * Code-aware chunking that preserves function boundaries
 */
class CodeChunker {
  /**
   * Chunk by top-level declarations (functions, classes, etc.)
   */
  async chunkByDeclarations(code: string, language: string): Promise<CodeChunk[]> {
    // Parse AST (Abstract Syntax Tree)
    const ast = parseCode(code, language)

    const chunks: CodeChunk[] = []

    for (const node of ast.topLevelDeclarations) {
      const chunk: CodeChunk = {
        content: code.slice(node.start, node.end),
        metadata: {
          type: node.type,  // 'function' | 'class' | 'interface'
          name: node.name,
          startLine: node.startLine,
          endLine: node.endLine,
          imports: this.extractImports(node),
          exports: this.extractExports(node)
        }
      }

      // Add context: include relevant imports
      if (chunk.metadata.imports.length &gt; 0) {
        const importContext = this.getImportContext(ast, chunk.metadata.imports)
        chunk.content = importContext + '\n\n' + chunk.content
      }

      chunks.push(chunk)
    }

    return chunks
  }

  /**
   * Sliding window for long functions
   */
  chunkLongFunction(functionCode: string, maxLines: number = 100): CodeChunk[] {
    const lines = functionCode.split('\n')

    if (lines.length &lt;= maxLines) {
      return [{ content: functionCode, metadata: {} }]
    }

    const chunks: CodeChunk[] = []
    const overlapLines = Math.floor(maxLines * 0.2)  // 20% overlap

    for (let i = 0; i < lines.length; i += maxLines - overlapLines) {
      const chunkLines = lines.slice(i, i + maxLines)
      chunks.push({
        content: chunkLines.join('\n'),
        metadata: {
          startLine: i,
          endLine: i + chunkLines.length,
          isPartial: chunkLines.length < maxLines
        }
      })
    }

    return chunks
  }

  private extractImports(node: ASTNode): string[] {
    // Extract import statements used by this node
    return []  // Implementation depends on language
  }

  private extractExports(node: ASTNode): string[] {
    // Extract what this node exports
    return []
  }

  private getImportContext(ast: AST, imports: string[]): string {
    // Return relevant import statements as context
    return ''
  }
}
```

### Chunk Quality Metrics

**How to evaluate if your chunking strategy is working:**

```typescript
interface ChunkQualityMetrics {
  avgChunkSize: number
  chunkSizeStdDev: number
  boundaryQuality: number     // 0-1, how many chunks end at natural boundaries
  overlapEfficiency: number   // Recall improvement per % overlap
  retrievalRecall: number     // % of relevant chunks retrieved
}

/**
 * Evaluate chunking quality
 */
async function evaluateChunking(
  chunks: Chunk[],
  testQueries: string[],
  groundTruth: Map<string, string[]>  // query -> relevant chunk IDs
): Promise<ChunkQualityMetrics> {
  // 1. Calculate size statistics
  const sizes = chunks.map(c => countTokens(c.content))
  const avgChunkSize = average(sizes)
  const chunkSizeStdDev = standardDeviation(sizes)

  // 2. Evaluate boundary quality
  const naturalBoundaries = chunks.filter(c =>
    c.content.endsWith('.') ||
    c.content.endsWith('!') ||
    c.content.endsWith('?') ||
    c.content.endsWith('\n')
  ).length
  const boundaryQuality = naturalBoundaries / chunks.length

  // 3. Measure retrieval recall
  let totalRecall = 0
  for (const query of testQueries) {
    const retrieved = await vectorSearch(query, chunks, 5)
    const relevant = groundTruth.get(query) || []
    const hits = retrieved.filter(r => relevant.includes(r.id)).length
    totalRecall += hits / relevant.length
  }
  const retrievalRecall = totalRecall / testQueries.length

  return {
    avgChunkSize,
    chunkSizeStdDev,
    boundaryQuality,
    overlapEfficiency: 0.75,  // Requires A/B test
    retrievalRecall
  }
}
```

**Target Metrics for Production**:
- **Boundary Quality**: &gt; 80% (chunks end at natural breaks)
- **Size Std Dev**: &lt; 30% of average (consistent sizes)
- **Retrieval Recall**: &gt; 85% (most relevant chunks found)

---

### Fixed-Size Chunking

```typescript
function chunkByTokens(text: string, chunkSize: number, overlap: number): string[] {
  const tokens = tokenize(text)
  const chunks: string[] = []

  for (let i = 0; i < tokens.length; i += chunkSize - overlap) {
    const chunk = tokens.slice(i, i + chunkSize)
    chunks.push(chunk.join(' '))
  }

  return chunks
}

// Usage
const document = "long text..."
const chunks = chunkByTokens(document, 500, 50)
```

### Semantic Chunking

```typescript
async function semanticChunk(text: string): Promise<string[]> {
  // Split by paragraphs
  const paragraphs = text.split('\n\n')

  const chunks: string[] = []
  let currentChunk = ''

  for (const para of paragraphs) {
    const combined = currentChunk + '\n\n' + para

    // If too large, start new chunk
    if (tokenCount(combined) &gt; 500) {
      if (currentChunk) chunks.push(currentChunk)
      currentChunk = para
    } else {
      currentChunk = combined
    }
  }

  if (currentChunk) chunks.push(currentChunk)

  return chunks
}
```

### Markdown-Aware Chunking

```typescript
function chunkMarkdown(markdown: string): Chunk[] {
  const sections = parseMarkdownSections(markdown)

  return sections.map(section => ({
    text: section.content,
    metadata: {
      heading: section.heading,
      level: section.level,
      path: section.path  // e.g., "Introduction > Setup > Prerequisites"
    }
  }))
}
```

## Indexing Pipeline

```typescript
class DocumentIndexer {
  async indexDocument(document: Document): Promise<void> {
    // 1. Extract text
    const text = await this.extractText(document)

    // 2. Chunk
    const chunks = await this.chunk(text)

    // 3. Embed
    const embeddings = await this.embedBatch(chunks)

    // 4. Store in vector DB
    await this.store(chunks, embeddings, document.metadata)
  }

  private async extractText(document: Document): Promise<string> {
    // Handle different file types
    switch (document.type) {
      case 'pdf':
        return await extractPdfText(document.path)
      case 'docx':
        return await extractDocxText(document.path)
      case 'html':
        return await extractHtmlText(document.content)
      default:
        return document.content
    }
  }

  private async chunk(text: string): Promise<string[]> {
    // Use appropriate chunking strategy
    return chunkByTokens(text, 500, 50)
  }

  private async embedBatch(chunks: string[]): Promise<number[][]> {
    // Batch for efficiency
    const BATCH_SIZE = 100

    const embeddings: number[][] = []

    for (let i = 0; i < chunks.length; i += BATCH_SIZE) {
      const batch = chunks.slice(i, i + BATCH_SIZE)
      const batchEmbeddings = await embed(batch)
      embeddings.push(...batchEmbeddings)
    }

    return embeddings
  }

  private async store(
    chunks: string[],
    embeddings: number[][],
    metadata: any
  ): Promise<void> {
    const vectors = chunks.map((chunk, i) => ({
      id: `${metadata.id}-chunk-${i}`,
      values: embeddings[i],
      metadata: {
        ...metadata,
        text: chunk,
        chunkIndex: i
      }
    }))

    await this.vectorDB.upsert(vectors)
  }
}
```

## Advanced Techniques

### Query Rewriting with HyDE (Hypothetical Document Embeddings)

> **Architect Perspective**: Users are bad at writing search queries. Instead of embedding the user's messy question directly, ask a fast model (like Haiku) to generate a **hypothetical answer** first. Embed that answer to perform the search. Since the hypothetical answer looks more like the documents in your database, your retrieval accuracy will spike by 20-30%.

**The Problem**: User queries don't match document style (casual vs. formal language).

**The Solution**: Generate what a good answer would look like, then search with that.

```typescript
async function hydeRAG(userQuery: string): Promise<string> {
  // 1. Generate hypothetical answer
  const hypothetical = await anthropic.messages.create({
    model: 'claude-haiku-4',
    max_tokens: 300,
    messages: [{
      role: 'user',
      content: `Write a formal documentation answer to: "${userQuery}"`
    }]
  })

  const hypotheticalText = hypothetical.content[0].type === 'text' 
    ? hypothetical.content[0].text 
    : ''

  // 2. Embed hypothetical (not the user query!)
  const embedding = await openai.embeddings.create({
    model: 'text-embedding-3-small',
    input: hypotheticalText
  })

  // 3. Search with hypothetical embedding
  const results = await vectorDB.query({
    vector: embedding.data[0].embedding,
    topK: 5
  })

  // 4. Generate real answer
  const context = results.map(r => r.content).join('\n\n')
  
  const answer = await anthropic.messages.create({
    model: 'claude-sonnet-4-5',
    max_tokens: 1024,
    messages: [{
      role: 'user',
      content: `Context:\n${context}\n\nQuestion: ${userQuery}`
    }]
  })

  return answer.content[0].type === 'text' ? answer.content[0].text : ''
}
```

**Why it works**: Hypothetical answers use formal language that matches how documents are written, improving retrieval accuracy by 20-30%.

---

### Citation-First Architecture

> **Architect Perspective**: In enterprise AI, a response without a citation is a **liability**. Architect your prompt to require the LLM to output its answer in a **structured format** (e.g., JSON) where every claim is linked to a `chunk_id`. This allows your UI to highlight exactly which sentence in the source PDF justified the AI's answer, which is the **#1 requirement** for legal and medical "Right to Explanation" compliance.

**The Compliance Problem**: For regulated industries, AI answers without traceable sources create legal liability.

```typescript
interface CitedAnswer {
  answer: string
  claims: Array<{
    text: string
    chunkIds: string[]
    confidence: number
  }>
  confidence: number
}

async function citationFirstRAG(query: string): Promise<CitedAnswer> {
  // 1. Retrieve chunks
  const chunks = await vectorSearch(query, 5)

  // 2. Format with chunk IDs
  const context = chunks
    .map(c => `[CHUNK_ID: ${c.id}]\n${c.content}`)
    .join('\n\n---\n\n')

  // 3. Prompt for structured output with citations
  const response = await anthropic.messages.create({
    model: 'claude-sonnet-4-5',
    max_tokens: 2048,
    messages: [{
      role: 'user',
      content: `Answer using ONLY the context. Cite CHUNK_IDs for every claim.

Context:
${context}

Question: "${query}"

Return JSON:
{
  "answer": "Your answer",
  "claims": [
    {"text": "claim text", "chunkIds": ["chunk-1"], "confidence": 0.95}
  ],
  "confidence": 0.92
}`
    }]
  })

  const text = response.content[0].type === 'text' ? response.content[0].text : '{}'
  return JSON.parse(text)
}
```

**Why it's mandatory**: GDPR, HIPAA, and SOX require audit trails. Citations make AI answers legally defensible.

### Query Decomposition

```typescript
async function decomposeQuery(complexQuestion: string): Promise<string> {
  // Break complex question into sub-questions
  const subQuestions = await llm.complete({
    prompt: `Break this question into simpler sub-questions:\n${complexQuestion}`
  })

  // Answer each sub-question
  const answers = await Promise.all(
    subQuestions.map(q => ragQuery(q))
  )

  // Synthesize final answer
  return await llm.complete({
    prompt: `Synthesize these answers into one:\n${answers.join('\n\n')}`
  })
}
```

## Evaluation

```typescript
interface EvaluationMetrics {
  retrievalPrecision: number
  retrievalRecall: number
  answerQuality: number
  citationAccuracy: number
}

async function evaluateRAG(
  questions: string[],
  groundTruth: string[]
): Promise<EvaluationMetrics> {
  const results = await Promise.all(
    questions.map(q => ragQuery(q))
  )

  return {
    retrievalPrecision: calculatePrecision(results),
    retrievalRecall: calculateRecall(results),
    answerQuality: scoreAnswers(results, groundTruth),
    citationAccuracy: verifyCitations(results)
  }
}
```

## Best Practices

1. **Two-stage retrieval**: Use vector search for recall, cross-encoder for precision
2. **Chunk overlap**: Use 10-20% overlap to preserve context
3. **HyDE**: Generate hypothetical answers for better query-document matching
4. **Citation-first**: Require structured output with chunk IDs for compliance
5. **Reranking**: Always rerank top results with cross-encoder
6. **Fallback handling**: Handle "no relevant docs" gracefully


## Architectural Optimization Challenge

> **Scenario**: Your RAG system retrieves 5 relevant chunks, and you've verified the correct answer **is** in those chunks. However, the LLM consistently responds with **"I don't know"** or **hallucinates** based on training data.

### The Problem

**System Behavior**: LLM ignores retrieved context despite it containing the answer.

**User Impact**:
- 47% of queries receive "I don't know" despite relevant docs
- 23% contain hallucinated information
- Customer trust: 2.1/5 stars

### Your Task

**The CEO asks**: "The right information is being retrieved. Why isn't the LLM using it?"

Choose the best architectural fix:

**Option A: Increase top_k from 5 to 50**
- ❌ **Fails**: More noise = worse performance ("Lost in the Middle")
- Result: 38% accuracy (-9%), 5x slower, 15x cost

**Option B: Implement Re-ranking & Contextual Compression** ✅ **CORRECT**
- ✅ **Works**: Cross-encoder ensures answer is at top, compression removes noise
- Result: 91% accuracy (+44%), only +200ms latency, +$0.0002/query
- **Why**: Most RAG failures are from context noise, not retrieval quality

**Option C: Use Larger Model (Opus)**
- ❌ **Fails**: Smarter model doesn't fix noisy context
- Result: 62% accuracy (+15%), 5x cost, slower than Option B

**Option D: Re-index with Different Embedding Model**
- ❌ **Fails**: Problem is ranking, not retrieval
- Result: 47% accuracy (no change), $4K cost, 8 hours downtime

### The Correct Answer: B

**The Architect's Principle**: When RAG retrieves correct chunks but LLM doesn't use them, fix **context noise** with:
1. Two-stage retrieval (vector → cross-encoder)
2. Keep top-K small (3-5 chunks)
3. Extract only relevant sentences
4. Position answer at top of context

**ROI**: +94% correct answers, 83% fewer hallucinations, minimal cost increase.

## Resources

- [LangChain RAG Tutorial](https://python.langchain.com/docs/use_cases/question_answering/)
- [RAG Best Practices](https://www.pinecone.io/learn/retrieval-augmented-generation/)
- [Advanced RAG Techniques](https://arxiv.org/abs/2312.10997)
