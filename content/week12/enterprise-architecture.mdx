---
title: "Enterprise AI Architecture Patterns"
description: "Design AI systems for enterprise scale"
estimatedMinutes: 45
---

# Enterprise AI Architecture Patterns

## Why Enterprise Architecture Differs

**Simple Explanation**: Building an AI app for 100 users is different from building for 100 companies with 10,000 users each. Enterprise systems need multi-tenancy, high availability, disaster recovery, and strict security.

**Enterprise Requirements**:
1. **Multi-Tenancy**: Isolate data between companies
2. **High Availability**: 99.9%+ uptime (&lt; 8.7 hours downtime per year)
3. **Scalability**: Handle 10x traffic spikes
4. **Security**: SOC 2, GDPR, HIPAA compliance
5. **Disaster Recovery**: Backup and restore capabilities
6. **Observability**: Comprehensive logging and monitoring
7. **Performance SLAs**: Response time guarantees

**Cost of Downtime**:
- Small business: $8,000 per hour
- Mid-size business: $74,000 per hour
- Enterprise: $540,000 per hour
(Source: Gartner)

## Multi-Tenant Architecture

**Simple Explanation**: Multiple companies (tenants) use your AI system, but each company's data must be completely isolated from others. Like an apartment building - everyone shares the infrastructure but has their own private unit.

### Tenant Isolation Strategies

```typescript
// Strategy 1: Database per Tenant (Strongest Isolation)
class DatabasePerTenant {
  private connections = new Map<string, PrismaClient>()

  async getConnection(tenantId: string): Promise<PrismaClient> {
    if (!this.connections.has(tenantId)) {
      // Each tenant gets their own database
      const dbUrl = `postgresql://user:pass@localhost:5432/tenant_${tenantId}`

      const prisma = new PrismaClient({
        datasources: { db: { url: dbUrl } }
      })

      this.connections.set(tenantId, prisma)
    }

    return this.connections.get(tenantId)!
  }

  async query(tenantId: string, userId: string, query: string) {
    const prisma = await this.getConnection(tenantId)

    // All queries automatically scoped to this tenant's database
    const conversations = await prisma.conversation.findMany({
      where: { userId }
    })

    return conversations
  }
}

// Pros: Perfect isolation, easy to backup/restore individual tenants
// Cons: Higher cost (one DB per tenant), complex migrations
// Use when: Handling regulated data (healthcare, finance)
```

```typescript
// Strategy 2: Schema per Tenant (Good Isolation)
class SchemaPerTenant {
  private prisma: PrismaClient

  async query(tenantId: string, userId: string) {
    // Set schema for this tenant
    await this.prisma.$executeRaw`SET search_path TO tenant_${tenantId}`

    // All subsequent queries use this schema
    const conversations = await this.prisma.conversation.findMany({
      where: { userId }
    })

    return conversations
  }
}

// Pros: Better isolation than shared schema, easier migrations than DB-per-tenant
// Cons: All tenants share same database connection pool
// Use when: Need good isolation without full database separation
```

```typescript
// Strategy 3: Shared Schema with Tenant ID (Most Efficient)
class TenantIsolation {
  async query(tenantId: string, userId: string, query: string) {
    // Tenant ID column in every table
    const namespace = `tenant_${tenantId}`

    // 1. Query vector DB with tenant filter
    const vectorResults = await vectorDB.query({
      namespace,
      vector: await embed(query),
      filter: {
        tenantId: { $eq: tenantId },  // CRITICAL: Always filter by tenant
        accessibleBy: { $in: [userId, 'public'] }
      },
      topK: 10
    })

    // 2. Query SQL database with tenant filter
    const conversations = await prisma.conversation.findMany({
      where: {
        tenantId: tenantId,  // CRITICAL: Always filter by tenant
        userId: userId
      }
    })

    return { vectorResults, conversations }
  }
}

// Pros: Most cost-effective, easiest to manage
// Cons: Risk of data leakage if tenant filter forgotten
// Use when: Cost optimization is priority
```

### Enforcing Tenant Isolation

**Simple Explanation**: Use middleware to automatically inject tenant filters into every query. This prevents accidental data leaks.

```typescript
// Middleware to enforce tenant context
class TenantMiddleware {
  async withTenantContext<T>(
    tenantId: string,
    operation: () => Promise<T>
  ): Promise<T> {
    // Set tenant context in async local storage
    return await asyncLocalStorage.run({ tenantId }, async () => {
      return await operation()
    })
  }
}

// Prisma middleware to auto-inject tenant filter
prisma.$use(async (params, next) => {
  const tenantId = asyncLocalStorage.getStore()?.tenantId

  if (!tenantId) {
    throw new Error('Tenant context not set - possible security issue!')
  }

  // Automatically add tenantId to all queries
  if (params.action === 'findMany' || params.action === 'findFirst') {
    params.args.where = {
      ...params.args.where,
      tenantId: tenantId
    }
  }

  if (params.action === 'create') {
    params.args.data = {
      ...params.args.data,
      tenantId: tenantId
    }
  }

  return next(params)
})

// Usage in API route
export async function GET(req: Request) {
  const tenantId = await getTenantFromRequest(req)

  return tenantMiddleware.withTenantContext(tenantId, async () => {
    // All database queries automatically scoped to this tenant
    const users = await prisma.user.findMany() // Automatically includes tenantId filter

    return NextResponse.json(users)
  })
}
```

### Cell-Based Global Architecture

**Architect's Tip — Regional Data Sovereignty (The Cell Pattern)**: "In 2026, multi-tenancy isn't just about database rows; it's about **Geography**. For a hardened enterprise system, you must deploy **Cells** — self-contained instances of your entire stack (DB, Vector Store, LLM Gateway) — in specific regions (e.g., EU-West-1 for GDPR, US-East-1 for HIPAA). Use a **Global Tenant Router** to ensure that data for an EU client never leaves the EU cell, even for processing. This is the only way to satisfy high-level compliance audits."

```typescript
/**
 * Cell-Based Architecture: Regional Data Sovereignty
 *
 * Problem: A single-region deployment can't satisfy GDPR's
 * data residency requirements. EU data processed in US-East-1
 * is a compliance violation — regardless of encryption.
 *
 * Solution: Deploy self-contained "Cells" per region. Each Cell
 * has its own DB, Vector Store, LLM Gateway, and Audit Log.
 * A Global Tenant Router inspects the tenant's region assignment
 * and forwards all traffic to the correct Cell.
 *
 * Interview Defense: "We deploy Cell-Based Architecture with
 * regional sovereignty. An EU tenant's data never leaves EU-West-1 —
 * not for processing, not for logging, not for backups. The Global
 * Router enforces this at the network layer, not the application layer."
 */

interface RegionalCell {
  region: string                    // e.g., 'eu-west-1', 'us-east-1'
  complianceFramework: 'GDPR' | 'HIPAA' | 'CCPA' | 'PDPA'
  endpoints: {
    api: string                     // Cell API gateway
    database: string                // Cell-local PostgreSQL
    vectorStore: string             // Cell-local Pinecone/pgvector
    llmGateway: string              // Cell-local LLM proxy
    auditLog: string                // Cell-local audit storage
  }
  capacity: {
    maxTenants: number
    currentTenants: number
    maxRPS: number                  // Requests per second
  }
}

interface TenantCellAssignment {
  tenantId: string
  primaryCell: string               // e.g., 'eu-west-1'
  backupCell: string                // e.g., 'eu-central-1' (same jurisdiction)
  dataResidency: string             // Legal jurisdiction
  assignedAt: Date
}

class GlobalTenantRouter {
  private cellRegistry: Map<string, RegionalCell>
  private tenantAssignments: Map<string, TenantCellAssignment>

  async routeRequest(
    tenantId: string,
    request: IncomingRequest
  ): Promise<Response> {
    // Step 1: Look up tenant's Cell assignment
    const assignment = this.tenantAssignments.get(tenantId)
    if (!assignment) {
      throw new Error(`Tenant ${tenantId} has no Cell assignment`)
    }

    // Step 2: Get the target Cell
    const cell = this.cellRegistry.get(assignment.primaryCell)
    if (!cell) {
      // Failover to backup Cell (must be same jurisdiction)
      const backupCell = this.cellRegistry.get(assignment.backupCell)
      if (!backupCell) {
        throw new Error(`No available Cell for tenant ${tenantId}`)
      }
      return this.forwardToCell(backupCell, tenantId, request)
    }

    // Step 3: Enforce data residency — NEVER route cross-jurisdiction
    if (request.sourceRegion &&
        !this.isSameJurisdiction(request.sourceRegion, cell.region)) {
      await this.logSovereigntyViolationAttempt(tenantId, request)
      throw new Error(
        `Data sovereignty violation: cannot route ${request.sourceRegion} ` +
        `traffic to ${cell.region}`
      )
    }

    // Step 4: Forward to the correct Cell
    return this.forwardToCell(cell, tenantId, request)
  }

  private async forwardToCell(
    cell: RegionalCell,
    tenantId: string,
    request: IncomingRequest
  ): Promise<Response> {
    return fetch(cell.endpoints.api, {
      method: request.method,
      headers: {
        ...request.headers,
        'X-Tenant-Id': tenantId,
        'X-Cell-Region': cell.region,
        'X-Compliance-Framework': cell.complianceFramework
      },
      body: request.body
    })
  }

  private isSameJurisdiction(
    sourceRegion: string,
    cellRegion: string
  ): boolean {
    // EU regions can route to other EU regions (GDPR allows intra-EU)
    const EU_REGIONS = ['eu-west-1', 'eu-west-2', 'eu-central-1']
    const US_REGIONS = ['us-east-1', 'us-west-2']

    if (EU_REGIONS.includes(sourceRegion) &&
        EU_REGIONS.includes(cellRegion)) return true
    if (US_REGIONS.includes(sourceRegion) &&
        US_REGIONS.includes(cellRegion)) return true

    return sourceRegion === cellRegion
  }

  private async logSovereigntyViolationAttempt(
    tenantId: string,
    request: IncomingRequest
  ): Promise<void> {
    console.error('[SOVEREIGNTY VIOLATION ATTEMPT]', {
      tenantId,
      sourceRegion: request.sourceRegion,
      timestamp: new Date().toISOString()
    })
  }
}

// Cell deployment topology:
//
// ┌─────────────────────────────────────────────────────────┐
// │                 Global Tenant Router                     │
// │         (DNS-based routing + tenant lookup)              │
// └────────┬──────────────────┬──────────────────┬──────────┘
//          │                  │                  │
//    ┌─────▼─────┐     ┌─────▼─────┐     ┌─────▼─────┐
//    │ EU-West-1 │     │ US-East-1 │     │ AP-SE-1   │
//    │   Cell    │     │   Cell    │     │   Cell    │
//    │           │     │           │     │           │
//    │ ┌───────┐ │     │ ┌───────┐ │     │ ┌───────┐ │
//    │ │  DB   │ │     │ │  DB   │ │     │ │  DB   │ │
//    │ │Vector │ │     │ │Vector │ │     │ │Vector │ │
//    │ │  LLM  │ │     │ │  LLM  │ │     │ │  LLM  │ │
//    │ │ Audit │ │     │ │ Audit │ │     │ │ Audit │ │
//    │ └───────┘ │     │ └───────┘ │     │ └───────┘ │
//    │  GDPR     │     │  HIPAA    │     │  PDPA     │
//    └───────────┘     └───────────┘     └───────────┘
//
// Key: Each Cell is a COMPLETE, ISOLATED stack.
// EU data never leaves EU. US data never leaves US.
// Backup Cells must be in the SAME jurisdiction.
```

### Tenant-Specific Configuration

```typescript
interface TenantConfig {
  tenantId: string
  name: string
  settings: {
    llmModel: 'claude-3-5-sonnet' | 'gpt-4-turbo'
    maxTokens: number
    rateLimit: number  // requests per hour
    features: string[] // enabled features
    customPrompts?: Record<string, string>
  }
  billing: {
    plan: 'free' | 'pro' | 'enterprise'
    costPerToken: number
    monthlyQuota: number
  }
}

class TenantConfigService {
  private cache = new LRUCache<string, TenantConfig>({ max: 1000 })

  async getConfig(tenantId: string): Promise<TenantConfig> {
    // Check cache first
    const cached = this.cache.get(tenantId)
    if (cached) return cached

    // Load from database
    const config = await prisma.tenantConfig.findUnique({
      where: { tenantId }
    })

    if (!config) throw new Error(`Tenant ${tenantId} not found`)

    this.cache.set(tenantId, config)
    return config
  }

  async getLLMSettings(tenantId: string) {
    const config = await this.getConfig(tenantId)

    return {
      model: config.settings.llmModel,
      maxTokens: config.settings.maxTokens,
      systemPrompt: config.settings.customPrompts?.system || DEFAULT_PROMPT
    }
  }
}

// Usage
const tenantConfig = new TenantConfigService()

async function generateResponse(tenantId: string, userMessage: string) {
  const settings = await tenantConfig.getLLMSettings(tenantId)

  const response = await anthropic.messages.create({
    model: settings.model,
    max_tokens: settings.maxTokens,
    system: settings.systemPrompt,
    messages: [{ role: 'user', content: userMessage }]
  })

  return response.content[0].text
}
```

## Token-Bucket Rate Limiting

**Architect's Tip — Tiered Throughput Engine (The Noisy Neighbor Shield)**: "Don't let one noisy neighbor crash the system for everyone. Implement a **Tiered Throughput Engine**: assign each tenant a **Token Bucket** based on their contract (e.g., Bronze: 10 RPM, Platinum: 1000 RPM). When the bucket is empty, return a `429 Too Many Requests` with a `Retry-After` header. This protects your High Availability (99.9%+) by ensuring the underlying LLM provider's rate limits are never exceeded by a single runaway script."

```typescript
/**
 * Token-Bucket Rate Limiting per Tenant
 *
 * Problem: A single tenant running a batch job at 2 AM can
 * exhaust the entire LLM provider's rate limit, causing 429
 * errors for all other tenants. One "noisy neighbor" degrades
 * service for everyone.
 *
 * Solution: Assign each tenant a Token Bucket with a refill
 * rate based on their contract tier. The bucket drains on each
 * request and refills at a constant rate. When empty, the
 * request is rejected with 429 + Retry-After header.
 *
 * Interview Defense: "We implement per-tenant token bucket rate
 * limiting backed by Redis. Each tier has a defined bucket size
 * and refill rate. A Platinum tenant gets 1000 RPM; a Bronze
 * tenant gets 10 RPM. If a tenant's bucket empties, they get
 * a 429 with Retry-After — but every other tenant is unaffected."
 */

interface TenantTier {
  name: 'bronze' | 'silver' | 'gold' | 'platinum'
  bucketSize: number         // Max tokens in bucket
  refillRate: number         // Tokens added per second
  maxRequestsPerMinute: number
  maxTokensPerRequest: number
  priority: number           // Higher = processed first under contention
}

const TIER_CONFIG: Record<string, TenantTier> = {
  bronze:   {
    name: 'bronze',   bucketSize: 10,   refillRate: 0.17,
    maxRequestsPerMinute: 10,   maxTokensPerRequest: 1000,  priority: 1
  },
  silver:   {
    name: 'silver',   bucketSize: 50,   refillRate: 0.83,
    maxRequestsPerMinute: 50,   maxTokensPerRequest: 2000,  priority: 2
  },
  gold:     {
    name: 'gold',     bucketSize: 200,  refillRate: 3.33,
    maxRequestsPerMinute: 200,  maxTokensPerRequest: 4000,  priority: 3
  },
  platinum: {
    name: 'platinum', bucketSize: 1000, refillRate: 16.67,
    maxRequestsPerMinute: 1000, maxTokensPerRequest: 8000,  priority: 4
  },
}

class TokenBucketRateLimiter {
  private redis: Redis

  constructor(redis: Redis) {
    this.redis = redis
  }

  async consumeToken(
    tenantId: string,
    tier: string
  ): Promise<{
    allowed: boolean
    remaining: number
    retryAfterMs: number | null
  }> {
    const config = TIER_CONFIG[tier]
    if (!config) throw new Error(`Unknown tier: ${tier}`)

    const key = `ratelimit:${tenantId}`
    const now = Date.now()

    // Atomic token bucket operation via Lua script
    const result = await this.redis.eval(
      TOKEN_BUCKET_LUA_SCRIPT,
      1,
      key,
      config.bucketSize,       // Max tokens
      config.refillRate,        // Tokens per second
      now,                      // Current timestamp
      1                         // Tokens to consume
    ) as [number, number, number]

    const [allowed, remaining, retryAfterMs] = result

    return {
      allowed: allowed === 1,
      remaining,
      retryAfterMs: allowed === 1 ? null : retryAfterMs
    }
  }
}

// Lua script for atomic token bucket (runs in Redis)
const TOKEN_BUCKET_LUA_SCRIPT = `
  local key = KEYS[1]
  local max_tokens = tonumber(ARGV[1])
  local refill_rate = tonumber(ARGV[2])
  local now = tonumber(ARGV[3])
  local requested = tonumber(ARGV[4])

  -- Get current bucket state
  local bucket = redis.call('HMGET', key, 'tokens', 'last_refill')
  local tokens = tonumber(bucket[1]) or max_tokens
  local last_refill = tonumber(bucket[2]) or now

  -- Refill tokens based on elapsed time
  local elapsed = (now - last_refill) / 1000
  tokens = math.min(max_tokens, tokens + (elapsed * refill_rate))

  -- Try to consume
  if tokens >= requested then
    tokens = tokens - requested
    redis.call('HMSET', key, 'tokens', tokens, 'last_refill', now)
    redis.call('EXPIRE', key, 120)
    return {1, math.floor(tokens), 0}
  else
    -- Calculate wait time until enough tokens are available
    local deficit = requested - tokens
    local wait_ms = math.ceil((deficit / refill_rate) * 1000)
    redis.call('HMSET', key, 'tokens', tokens, 'last_refill', now)
    redis.call('EXPIRE', key, 120)
    return {0, math.floor(tokens), wait_ms}
  end
`

// Middleware integration
async function rateLimitMiddleware(
  req: Request,
  tenantId: string
): Promise<Response | null> {
  const config = await tenantConfigService.getConfig(tenantId)
  const limiter = new TokenBucketRateLimiter(redis)

  const result = await limiter.consumeToken(
    tenantId, config.billing.plan
  )

  if (!result.allowed) {
    return new Response(
      JSON.stringify({
        error: 'Too Many Requests',
        message: `Rate limit exceeded for tier: ${config.billing.plan}`,
        retryAfterMs: result.retryAfterMs,
        remaining: result.remaining,
        upgradeUrl: '/billing/upgrade'
      }),
      {
        status: 429,
        headers: {
          'Retry-After': String(
            Math.ceil(result.retryAfterMs! / 1000)
          ),
          'X-RateLimit-Remaining': String(result.remaining),
          'X-RateLimit-Tier': config.billing.plan
        }
      }
    )
  }

  // Request allowed — continue processing
  return null
}

// Rate limit tiers:
//
// | Tier     | Bucket | Refill   | RPM  | Max Tokens | Priority |
// |----------|--------|----------|------|------------|----------|
// | Bronze   | 10     | 0.17/s   | 10   | 1,000      | 1 (Low)  |
// | Silver   | 50     | 0.83/s   | 50   | 2,000      | 2        |
// | Gold     | 200    | 3.33/s   | 200  | 4,000      | 3        |
// | Platinum | 1,000  | 16.67/s  | 1000 | 8,000      | 4 (High) |
//
// When a Bronze tenant hits 10 RPM, they get 429 + Retry-After.
// Platinum tenants are unaffected. No single tenant can starve others.
```

## High Availability

**Simple Explanation**: Your system should stay online even if servers crash, databases fail, or entire data centers go down.

### Multi-Region Deployment

```yaml
# Kubernetes deployment across multiple regions
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ai-api
spec:
  replicas: 6  # 2 per region
  selector:
    matchLabels:
      app: ai-api
  template:
    metadata:
      labels:
        app: ai-api
    spec:
      # Spread pods across availability zones
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: ai-api

      containers:
      - name: api
        image: ai-api:latest
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "1000m"

        # Health checks
        livenessProbe:
          httpGet:
            path: /health
            port: 3000
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3

        readinessProbe:
          httpGet:
            path: /ready
            port: 3000
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 2

        # Environment variables from secrets
        env:
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: db-credentials
              key: url
        - name: ANTHROPIC_API_KEY
          valueFrom:
            secretKeyRef:
              name: api-keys
              key: anthropic
```

### Health Checks and Circuit Breakers

```typescript
class HealthCheckService {
  async check(): Promise<HealthStatus> {
    const checks = await Promise.allSettled([
      this.checkDatabase(),
      this.checkRedis(),
      this.checkLLMAPI(),
      this.checkVectorDB(),
      this.checkDiskSpace(),
      this.checkMemory()
    ])

    const results = checks.map((check, i) => ({
      name: ['database', 'redis', 'llm_api', 'vector_db', 'disk', 'memory'][i],
      status: check.status === 'fulfilled' && check.value ? 'healthy' : 'unhealthy',
      details: check.status === 'fulfilled' ? check.value : { error: check.reason }
    }))

    const allHealthy = results.every(r => r.status === 'healthy')

    return {
      status: allHealthy ? 'healthy' : 'degraded',
      checks: results,
      timestamp: new Date().toISOString()
    }
  }

  private async checkDatabase(): Promise<boolean> {
    try {
      await prisma.$queryRaw`SELECT 1`
      return true
    } catch (error) {
      console.error('Database health check failed:', error)
      return false
    }
  }

  private async checkRedis(): Promise<boolean> {
    try {
      await redis.ping()
      return true
    } catch (error) {
      console.error('Redis health check failed:', error)
      return false
    }
  }

  private async checkLLMAPI(): Promise<boolean> {
    try {
      // Simple ping to verify API is reachable
      const response = await fetch('https://api.anthropic.com', {
        method: 'HEAD',
        signal: AbortSignal.timeout(5000)
      })
      return response.ok
    } catch (error) {
      console.error('LLM API health check failed:', error)
      return false
    }
  }
}

// Circuit breaker to prevent cascade failures
class CircuitBreaker {
  private failures = 0
  private lastFailureTime = 0
  private state: 'closed' | 'open' | 'half-open' = 'closed'

  private readonly failureThreshold = 5
  private readonly resetTimeout = 60000 // 1 minute

  async execute<T>(operation: () => Promise<T>): Promise<T> {
    if (this.state === 'open') {
      // Check if we should try again
      if (Date.now() - this.lastFailureTime > this.resetTimeout) {
        this.state = 'half-open'
      } else {
        throw new Error('Circuit breaker is OPEN - service unavailable')
      }
    }

    try {
      const result = await operation()

      // Success - reset circuit
      if (this.state === 'half-open') {
        this.state = 'closed'
        this.failures = 0
      }

      return result
    } catch (error) {
      this.failures++
      this.lastFailureTime = Date.now()

      if (this.failures &gt;= this.failureThreshold) {
        this.state = 'open'
        console.error(`Circuit breaker opened after ${this.failures} failures`)
      }

      throw error
    }
  }

  getState() {
    return {
      state: this.state,
      failures: this.failures,
      lastFailureTime: this.lastFailureTime
    }
  }
}

// Usage
const llmCircuitBreaker = new CircuitBreaker()

async function callLLM(prompt: string) {
  return await llmCircuitBreaker.execute(async () => {
    const response = await anthropic.messages.create({
      model: 'claude-3-5-sonnet-20241022',
      max_tokens: 1024,
      messages: [{ role: 'user', content: prompt }]
    })

    return response.content[0].text
  })
}
```

## Disaster Recovery

**Simple Explanation**: Plan for the worst - entire data centers going offline, accidental data deletion, ransomware attacks. You need backups and a recovery plan.

```typescript
class DisasterRecovery {
  // Automated backup strategy
  async performBackup(): Promise<BackupResult> {
    const backupId = `backup_${Date.now()}`
    console.log(`Starting backup: ${backupId}`)

    try {
      // 1. Database backup
      const dbBackup = await this.backupDatabase(backupId)

      // 2. Vector DB snapshot
      const vectorBackup = await this.backupVectorDB(backupId)

      // 3. Configuration backup
      const configBackup = await this.backupConfiguration(backupId)

      // 4. File storage backup
      const filesBackup = await this.backupFiles(backupId)

      // 5. Replicate to multiple regions
      await this.replicateToBackupRegions(backupId, [
        dbBackup,
        vectorBackup,
        configBackup,
        filesBackup
      ])

      // 6. Verify backup integrity
      await this.verifyBackup(backupId)

      console.log(`Backup completed: ${backupId}`)

      return {
        backupId,
        timestamp: new Date(),
        components: {
          database: dbBackup,
          vectorDB: vectorBackup,
          config: configBackup,
          files: filesBackup
        },
        regions: ['us-east-1', 'us-west-2', 'eu-west-1'],
        verified: true
      }
    } catch (error) {
      console.error(`Backup failed: ${backupId}`, error)
      await this.alertOps({ type: 'backup_failed', backupId, error })
      throw error
    }
  }

  private async backupDatabase(backupId: string): Promise<string> {
    // PostgreSQL backup
    const filename = `${backupId}_database.sql.gz`

    await exec(`
      pg_dump ${DATABASE_URL} | gzip > /backups/${filename}
    `)

    // Upload to S3
    await this.uploadToS3(filename, 'database-backups')

    return filename
  }

  private async backupVectorDB(backupId: string): Promise<string> {
    // Chroma/Pinecone snapshot
    const snapshot = await vectorDB.createSnapshot({
      name: backupId
    })

    return snapshot.id
  }

  // Recovery process
  async restore(backupId: string): Promise<void> {
    console.log(`Starting restore from backup: ${backupId}`)

    // 1. Verify backup exists and is valid
    const backup = await this.getBackup(backupId)
    if (!backup.verified) {
      throw new Error('Backup not verified - refusing to restore')
    }

    // 2. Create point-in-time snapshot before restore (safety)
    const preRestoreSnapshot = await this.performBackup()
    console.log(`Pre-restore snapshot created: ${preRestoreSnapshot.backupId}`)

    try {
      // 3. Restore database
      await this.restoreDatabase(backup.components.database)

      // 4. Restore vector DB
      await this.restoreVectorDB(backup.components.vectorDB)

      // 5. Restore configuration
      await this.restoreConfiguration(backup.components.config)

      // 6. Restore files
      await this.restoreFiles(backup.components.files)

      // 7. Run post-restore validation
      await this.validateRestore()

      console.log(`Restore completed successfully from ${backupId}`)
    } catch (error) {
      console.error('Restore failed, rolling back to pre-restore snapshot')
      await this.restore(preRestoreSnapshot.backupId)
      throw error
    }
  }

  // RTO (Recovery Time Objective): How long can you be down?
  // RPO (Recovery Point Objective): How much data can you lose?

  async getRTORPO(): Promise<{ rto: string; rpo: string }> {
    return {
      rto: '4 hours',    // Max downtime
      rpo: '15 minutes'  // Max data loss (backup every 15 min)
    }
  }
}

// Automated backup schedule
const dr = new DisasterRecovery()

// Full backup daily
cron.schedule('0 2 * * *', async () => {
  await dr.performBackup()
})

// Incremental backup every 15 minutes
cron.schedule('*/15 * * * *', async () => {
  await dr.performIncrementalBackup()
})
```

## Observability

**Simple Explanation**: You can't fix what you can't see. Comprehensive monitoring, logging, and tracing help you understand system behavior and debug issues.

```typescript
// Structured logging
class Logger {
  log(level: 'info' | 'warn' | 'error', message: string, metadata?: any) {
    const logEntry = {
      timestamp: new Date().toISOString(),
      level,
      message,
      ...metadata,
      // Add context
      environment: process.env.NODE_ENV,
      service: 'ai-api',
      version: process.env.APP_VERSION,
      // Correlation ID for tracing
      traceId: asyncLocalStorage.getStore()?.traceId
    }

    // Send to logging service (DataDog, CloudWatch, etc.)
    console.log(JSON.stringify(logEntry))

    // Also send errors to error tracking
    if (level === 'error') {
      Sentry.captureException(new Error(message), { extra: metadata })
    }
  }
}

// Metrics tracking
class Metrics {
  async recordLLMCall(metadata: {
    model: string
    tokens: number
    latency: number
    cost: number
    tenantId: string
  }) {
    // Send to metrics service (Prometheus, DataDog, etc.)
    await statsd.timing('llm.latency', metadata.latency, {
      model: metadata.model,
      tenant: metadata.tenantId
    })

    await statsd.increment('llm.calls', {
      model: metadata.model,
      tenant: metadata.tenantId
    })

    await statsd.gauge('llm.cost', metadata.cost, {
      tenant: metadata.tenantId
    })
  }
}

// Distributed tracing
import { trace } from '@opentelemetry/api'

async function handleRequest(req: Request) {
  const tracer = trace.getTracer('ai-api')

  return await tracer.startActiveSpan('handleRequest', async (span) => {
    span.setAttribute('http.method', req.method)
    span.setAttribute('http.url', req.url)

    try {
      const result = await processRequest(req)
      span.setStatus({ code: 1 }) // OK
      return result
    } catch (error) {
      span.setStatus({ code: 2, message: error.message }) // ERROR
      span.recordException(error)
      throw error
    } finally {
      span.end()
    }
  })
}
```

## Sovereign Audit Log: The Capstone Requirement

**Critical Requirement**: Your Week 12 capstone project is **NOT considered complete** unless it generates a **Sovereign Audit Log** that tracks every blocked PII attempt and governance violation.

### What CTOs Demand: Proof of Governance

Enterprise buyers don't accept "We have good security practices." They demand: **"Show me the audit log that proves your system blocked unauthorized access."**

The Sovereign Audit Log is your **proof of governance**—an immutable, cryptographically-verified record that demonstrates:
1. **Every PII redaction event** (what was blocked, when, by whom)
2. **Every governance violation attempt** (prompt injections, policy bypasses, unauthorized access)
3. **Every compliance decision** (why a request was allowed or denied)

This isn't optional—it's what makes your AI system **insurable** and **enterprise-deployable**.

**Architect's Tip — Tamper-Evident Cryptographic Audit Chain**: "An audit log that a DBA can delete is not a Sovereign Log. Implement a **Hash Chain**: every log entry must contain the hash of the previous entry. Periodically **anchor** these hashes into Write-Once-Read-Many (WORM) storage. If a single entry is altered or deleted, the Chain of Trust breaks, providing immutable proof to auditors that the governance record is intact. Without cryptographic chaining, your audit log is just a database table that anyone with admin access can quietly modify."

### The Sovereign Audit Log Architecture

```typescript
/**
 * Sovereign Audit Log: Immutable governance proof
 */

interface SovereignAuditEntry {
  // Event Identity
  auditId: string              // Unique event ID
  timestamp: Date              // When event occurred
  eventType: 'PII_BLOCKED' | 'GOVERNANCE_VIOLATION' | 'POLICY_DECISION' | 'ACCESS_DENIED'

  // Governance Context
  tenantId: string
  userId: string
  requestId: string

  // Violation Details
  violation: {
    type: 'PII_DETECTION' | 'PROMPT_INJECTION' | 'POLICY_BREACH' | 'RATE_LIMIT' | 'UNAUTHORIZED_ACCESS'
    severity: 'LOW' | 'MEDIUM' | 'HIGH' | 'CRITICAL'
    details: string
    blockedContent: string      // Redacted/hashed
    policyRule: string          // Which policy was violated
  }

  // Decision Rationale
  decision: {
    action: 'BLOCKED' | 'REDACTED' | 'ESCALATED' | 'ALLOWED_WITH_WARNING'
    reasoning: string           // Why this decision was made
    policyVersion: string       // Which policy version was active
    confidence: number          // 0-1 confidence in decision
  }

  // Cryptographic Proof
  hash: string                  // SHA-256 hash of entry
  previousHash: string          // Hash of previous entry (blockchain-style)
  signature: string             // Digital signature for tamper-proof audit

  // Compliance Metadata
  complianceFramework: 'HIPAA' | 'GDPR' | 'SOC2' | 'CCPA'
  regulatoryBasis: string       // Which regulation triggered this
  retentionRequired: number     // Years to retain (HIPAA = 6)
}

class SovereignAuditLogger {
  private previousHash: string = '0000000000000000000000000000000000000000000000000000000000000000'

  async logViolation(entry: Omit<SovereignAuditEntry, 'auditId' | 'hash' | 'previousHash' | 'signature'>): Promise<string> {
    const auditId = crypto.randomUUID()

    // Calculate hash
    const entryData = JSON.stringify({
      auditId,
      ...entry,
      previousHash: this.previousHash
    })

    const hash = await crypto.subtle.digest('SHA-256', new TextEncoder().encode(entryData))
    const hashHex = Array.from(new Uint8Array(hash))
      .map(b => b.toString(16).padStart(2, '0'))
      .join('')

    // Digital signature (in production, use private key)
    const signature = await this.signEntry(entryData)

    const completeEntry: SovereignAuditEntry = {
      auditId,
      ...entry,
      hash: hashHex,
      previousHash: this.previousHash,
      signature
    }

    // Write to immutable storage (append-only)
    await this.writeToImmutableStorage(completeEntry)

    // Update chain
    this.previousHash = hashHex

    // Alert if critical
    if (entry.violation.severity === 'CRITICAL') {
      await this.alertSecurityTeam(completeEntry)
    }

    return auditId
  }

  private async writeToImmutableStorage(entry: SovereignAuditEntry): Promise<void> {
    // Option 1: Write-once database table (no UPDATE/DELETE permissions)
    await prisma.$executeRaw`
      INSERT INTO sovereign_audit_log (
        audit_id, timestamp, event_type, tenant_id, user_id,
        violation, decision, hash, previous_hash, signature,
        compliance_framework, regulatory_basis, retention_required
      ) VALUES (
        ${entry.auditId}, ${entry.timestamp}, ${entry.eventType},
        ${entry.tenantId}, ${entry.userId},
        ${JSON.stringify(entry.violation)}, ${JSON.stringify(entry.decision)},
        ${entry.hash}, ${entry.previousHash}, ${entry.signature},
        ${entry.complianceFramework}, ${entry.regulatoryBasis}, ${entry.retentionRequired}
      )
    `

    // Option 2: Also write to S3 with object lock (WORM - Write Once Read Many)
    await s3.putObject({
      Bucket: 'sovereign-audit-logs',
      Key: `${entry.tenantId}/${entry.auditId}.json`,
      Body: JSON.stringify(entry, null, 2),
      ObjectLockMode: 'GOVERNANCE',  // Prevents deletion
      ObjectLockRetainUntilDate: new Date(Date.now() + (entry.retentionRequired * 365 * 24 * 60 * 60 * 1000))
    })

    // Option 3: Blockchain-based audit log for maximum immutability
    // await blockchain.appendBlock(entry)
  }

  async verifyAuditChain(entries: SovereignAuditEntry[]): Promise<boolean> {
    // Verify hash chain integrity
    for (let i = 1; i < entries.length; i++) {
      if (entries[i].previousHash !== entries[i - 1].hash) {
        console.error(`Audit chain broken at entry ${entries[i].auditId}`)
        return false
      }

      // Verify signature
      const isValid = await this.verifySignature(entries[i])
      if (!isValid) {
        console.error(`Invalid signature at entry ${entries[i].auditId}`)
        return false
      }
    }

    return true
  }

  async generateComplianceReport(
    tenantId: string,
    startDate: Date,
    endDate: Date
  ): Promise<SovereignAuditReport> {
    const entries = await prisma.sovereignAuditLog.findMany({
      where: {
        tenantId,
        timestamp: {
          gte: startDate,
          lte: endDate
        }
      },
      orderBy: { timestamp: 'asc' }
    })

    // Verify chain integrity
    const chainValid = await this.verifyAuditChain(entries)

    // Aggregate statistics
    const stats = {
      totalEvents: entries.length,
      byType: this.groupBy(entries, 'eventType'),
      bySeverity: this.groupBy(entries.map(e => e.violation), 'severity'),
      piiBlocked: entries.filter(e => e.violation.type === 'PII_DETECTION').length,
      promptInjections: entries.filter(e => e.violation.type === 'PROMPT_INJECTION').length,
      criticalViolations: entries.filter(e => e.violation.severity === 'CRITICAL').length
    }

    return {
      tenantId,
      period: { start: startDate, end: endDate },
      chainIntegrityVerified: chainValid,
      statistics: stats,
      entries: entries.map(e => ({
        auditId: e.auditId,
        timestamp: e.timestamp,
        eventType: e.eventType,
        violation: e.violation,
        decision: e.decision
      })),
      certificationStatement: chainValid
        ? 'This audit log has been cryptographically verified and contains no tampering.'
        : 'WARNING: Audit log integrity check FAILED. Chain has been tampered with.'
    }
  }

  private groupBy(items: any[], key: string): Record<string, number> {
    return items.reduce((acc, item) => {
      const value = item[key]
      acc[value] = (acc[value] || 0) + 1
      return acc
    }, {})
  }

  private async signEntry(data: string): Promise<string> {
    // In production: Use private key to sign
    // For now, return hash as signature
    return crypto.createHash('sha256').update(data).digest('hex')
  }

  private async verifySignature(entry: SovereignAuditEntry): Promise<boolean> {
    // In production: Verify with public key
    return true
  }

  private async alertSecurityTeam(entry: SovereignAuditEntry): Promise<void> {
    console.error('[CRITICAL SECURITY EVENT]', {
      auditId: entry.auditId,
      violation: entry.violation,
      tenant: entry.tenantId
    })
    // Send to PagerDuty, Slack, etc.
  }
}

interface SovereignAuditReport {
  tenantId: string
  period: { start: Date; end: Date }
  chainIntegrityVerified: boolean
  statistics: {
    totalEvents: number
    byType: Record<string, number>
    bySeverity: Record<string, number>
    piiBlocked: number
    promptInjections: number
    criticalViolations: number
  }
  entries: any[]
  certificationStatement: string
}

// Integration Example: Log PII Blocking
async function processUserMessage(tenantId: string, userId: string, message: string) {
  const piiDetector = new PHIDetector()
  const result = await piiDetector.detect(message)

  if (result.totalMatches > 0) {
    // Log to Sovereign Audit Log
    const auditLogger = new SovereignAuditLogger()

    await auditLogger.logViolation({
      timestamp: new Date(),
      eventType: 'PII_BLOCKED',
      tenantId,
      userId,
      requestId: crypto.randomUUID(),
      violation: {
        type: 'PII_DETECTION',
        severity: 'HIGH',
        details: `Detected ${result.totalMatches} PII items: ${result.matches.map(m => m.type).join(', ')}`,
        blockedContent: '[REDACTED]',  // Never log actual PII
        policyRule: 'HIPAA §164.312 - Technical Safeguards'
      },
      decision: {
        action: 'REDACTED',
        reasoning: 'PII detected and automatically redacted before LLM processing',
        policyVersion: 'v2.1.0',
        confidence: 0.95
      },
      complianceFramework: 'HIPAA',
      regulatoryBasis: 'HIPAA Privacy Rule §164.312',
      retentionRequired: 6  // HIPAA requires 6-year retention
    })

    // Continue with redacted message
    const redacted = await piiDetector.redact(message, userId)
    return await sendToLLM(redacted.redactedText)
  }

  return await sendToLLM(message)
}
```

### Merkle Tree Anchoring: Periodic Tamper-Proof Checkpoints

```typescript
/**
 * Merkle Tree Anchoring
 *
 * Problem: Hash chains prove sequential integrity, but verifying
 * the entire chain is O(n) — slow for audit logs with millions
 * of entries. An auditor shouldn't have to replay every entry.
 *
 * Solution: Batch entries into Merkle Trees every N entries.
 * Write the Merkle Root to WORM storage (S3 Object Lock or
 * private blockchain). Auditors verify any single entry in
 * O(log n) by checking its Merkle Proof against the anchored root.
 *
 * Interview Defense: "We anchor a Merkle Root to WORM storage
 * every 1,000 audit entries. An auditor can verify any single
 * entry in O(log n) without replaying the entire chain. If
 * any entry was tampered with, the Merkle Proof fails."
 */

class MerkleAnchorService {
  private readonly BATCH_SIZE = 1000

  async anchorBatch(
    entries: SovereignAuditEntry[]
  ): Promise<MerkleAnchor> {
    // Step 1: Build leaf hashes from entry hashes
    const leaves = entries.map(e => e.hash)

    // Step 2: Build Merkle Tree
    const merkleRoot = this.buildMerkleRoot(leaves)

    // Step 3: Create anchor record
    const anchor: MerkleAnchor = {
      anchorId: crypto.randomUUID(),
      merkleRoot,
      entryCount: entries.length,
      firstEntryId: entries[0].auditId,
      lastEntryId: entries[entries.length - 1].auditId,
      timestamp: new Date(),
      batchRange: {
        from: entries[0].timestamp,
        to: entries[entries.length - 1].timestamp
      }
    }

    // Step 4: Write to WORM storage (immutable)
    await this.writeToWORM(anchor)

    return anchor
  }

  private buildMerkleRoot(leaves: string[]): string {
    if (leaves.length === 0) return ''
    if (leaves.length === 1) return leaves[0]

    const nextLevel: string[] = []
    for (let i = 0; i < leaves.length; i += 2) {
      const left = leaves[i]
      const right = leaves[i + 1] || left // Duplicate last if odd
      const combined = crypto
        .createHash('sha256')
        .update(left + right)
        .digest('hex')
      nextLevel.push(combined)
    }

    return this.buildMerkleRoot(nextLevel)
  }

  async verifyEntry(
    entry: SovereignAuditEntry,
    proof: string[],
    anchor: MerkleAnchor
  ): Promise<boolean> {
    // Reconstruct root from entry hash + proof path
    let currentHash = entry.hash

    for (const sibling of proof) {
      currentHash = crypto
        .createHash('sha256')
        .update(currentHash + sibling)
        .digest('hex')
    }

    // Compare against anchored Merkle Root
    return currentHash === anchor.merkleRoot
  }

  private async writeToWORM(anchor: MerkleAnchor): Promise<void> {
    // S3 Object Lock — cannot be deleted or overwritten
    await s3.putObject({
      Bucket: 'sovereign-merkle-anchors',
      Key: `anchors/${anchor.anchorId}.json`,
      Body: JSON.stringify(anchor, null, 2),
      ObjectLockMode: 'COMPLIANCE',  // Even root account cannot delete
      ObjectLockRetainUntilDate: new Date(
        Date.now() + 7 * 365 * 24 * 60 * 60 * 1000 // 7-year retention
      )
    })
  }
}

interface MerkleAnchor {
  anchorId: string
  merkleRoot: string
  entryCount: number
  firstEntryId: string
  lastEntryId: string
  timestamp: Date
  batchRange: { from: Date; to: Date }
}

// Anchor flow:
//
// Audit Entries (sequential):
// [E1] → [E2] → [E3] → ... → [E1000]
//
// Merkle Tree (batch of 1000):
//            [ROOT]              ← Anchored to WORM
//           /      \
//      [H1-2]     [H3-4]
//      /    \     /    \
//   [E1]  [E2] [E3]  [E4] ...
//
// Verification: O(log n) — auditor needs only the
// Merkle Proof (sibling hashes) to verify any single entry.
// If E2 was tampered with, H1-2 changes, ROOT changes,
// and the anchored ROOT in WORM storage won't match.
```

### Capstone Completion Criteria

Your Week 12 capstone project must demonstrate:

1. **Sovereign Audit Log Implementation** ✅
   - Immutable storage (append-only database or S3 object lock)
   - Cryptographic verification (hash chain or blockchain)
   - All PII blocking events logged
   - All governance violations logged
   - All policy decisions logged

2. **Compliance Reporting** ✅
   - Generate audit reports for any date range
   - Verify hash chain integrity
   - Export for regulatory audits (PDF, JSON, CSV)
   - Statistics: PII blocked, violations prevented, policy decisions made

3. **Real-Time Monitoring** ✅
   - Dashboard showing governance events in real-time
   - Alerts for critical violations (CRITICAL severity)
   - Metrics: Events per hour, violation types, blocked content types

**Proof of Completion**:
Submit a compliance report from your Sovereign Audit Log showing:
- ≥50 logged governance events
- Zero hash chain breaks (100% integrity)
- At least 3 different violation types (PII, prompt injection, policy breach)
- Cryptographic signatures verified
- HIPAA/GDPR/SOC2 compliance metadata present

**Without this Sovereign Audit Log, your project is NOT enterprise-ready and NOT considered complete.**

This is what separates student projects from **production-grade AI systems that CTOs will trust with regulated data**.

---

## Architect Challenge: The Enterprise Boardroom

**A Fortune 500 client wants to use your AI platform but requires that their data "never touches a shared database" and that you provide "immutable proof" of every PII violation blocked. How does your architecture satisfy this?**

**A)** Tell them you use SSL encryption and regular backups.

**B)** Deploy a **Dedicated Cell** with a **Sovereign Audit Log**. You provide a Siloed Stack (Strategy 1: Database per Tenant) so their data is physically isolated. You then provide a **Cryptographically Signed Audit Log** that proves every governance event was captured and cannot be retroactively altered. This satisfies both the Data Privacy and Compliance Accountability requirements of a global enterprise.

**C)** Just give them a discount on the subscription price.

**D)** Promise them that your engineers are very careful with data.

<details>
<summary><strong>Click to reveal the correct answer</strong></summary>

### Correct Answer: B — Dedicated Cell + Sovereign Audit Log

An Architect wins enterprise contracts by providing **Structural Guarantees** rather than **Policy Promises**.

**The Architecture:**

```typescript
// What you deploy for a Fortune 500 client:
//
// 1. DEDICATED CELL (Data Isolation)
//    - Cell in their required region (e.g., US-East-1 for HIPAA)
//    - Isolated PostgreSQL instance (not shared schema)
//    - Dedicated vector store namespace
//    - LLM Gateway with their API keys
//    - No data ever leaves this Cell
//
// 2. SOVEREIGN AUDIT LOG (Compliance Proof)
//    - Hash-chained audit entries (tamper-evident)
//    - Merkle Root anchored to WORM storage every 1,000 entries
//    - Every PII detection → logged with policy rule + decision
//    - Every governance event → cryptographically signed
//    - Compliance report generated on demand
//
// 3. TOKEN-BUCKET RATE LIMITING (SLA Protection)
//    - Platinum tier: 1,000 RPM guaranteed
//    - Isolated from other tenants' traffic
//    - 99.9% uptime SLA backed by Cell isolation
//
// What the CTO hears:
//   "Your data is physically isolated in a dedicated Cell.
//    Every PII event is cryptographically logged and tamper-proof.
//    We can generate a compliance report in 30 seconds.
//    Here's the Merkle Root anchored in WORM storage as proof."
//
// What the CTO's lawyers hear:
//   "Structural guarantees, not policy promises."
```

**Why other answers fail:**

- **A) SSL encryption and backups** — SSL is table stakes, not a differentiator. Doesn't address data isolation or immutable audit proof. Any junior developer can enable SSL.
- **C) Discount** — Enterprise clients don't buy on price; they buy on trust and compliance guarantees. A discount signals desperation, not competence.
- **D) "We're careful"** — Policy promises are worthless in a compliance audit. "Our engineers are careful" fails every SOC 2 audit. Auditors demand structural proof, not intentions.

**The Architect's Principle:** "Enterprise sales are won in the architecture diagram, not the pricing spreadsheet. A Dedicated Cell + Sovereign Audit Log gives the CTO two things they can show their board: **physical isolation** they can verify and **cryptographic proof** they can audit."

</details>

---

## Best Practices

1. **Design for Failure**: Assume everything will fail eventually
2. **Automate Everything**: Deployments, backups, scaling, monitoring
3. **Test Disaster Recovery**: Run fire drills quarterly
4. **Monitor Business Metrics**: Not just technical metrics
5. **Cost Attribution**: Track costs per tenant
6. **Security in Depth**: Multiple layers of security
7. **Documentation**: Runbooks for common incidents
8. **Sovereign Audit Logs**: Immutable proof of governance (capstone requirement)

## Resources
- [AWS Well-Architected Framework](https://aws.amazon.com/architecture/well-architected/)
- [Google SRE Book](https://sre.google/sre-book/table-of-contents/)
- [Kubernetes Best Practices](https://kubernetes.io/docs/concepts/configuration/overview/)
- [HIPAA Audit Log Requirements](https://www.hhs.gov/hipaa/for-professionals/security/laws-regulations/index.html)
