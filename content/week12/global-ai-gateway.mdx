---
title: "Global AI Gateway: Enterprise AI Infrastructure"
description: "Design zero-downtime AI infrastructure for 1M+ concurrent users"
estimatedMinutes: 60
week: 12
concept: 4
difficulty: expert
objectives:
  - Architect a Global AI Gateway with centralized rate limiting and multi-region failover
  - Implement model-agnostic routing to abstract provider-specific APIs
  - Build FinOps cost attribution system for per-user and per-department tracking
  - Design zero-downtime deployment patterns with sub-second latency
---

# Global AI Gateway: Enterprise AI Infrastructure

## Why a Global AI Gateway?

**Simple Explanation**: A Global AI Gateway is a centralized entry point for all AI API calls across your organization. It handles rate limiting, routing, failover, cost tracking, and security in one place. Think of it as an "air traffic control tower" for your AI infrastructure.

**Without Gateway** (Chaos):
```
App 1 ‚Üí OpenAI API ‚ùå No rate limiting
App 2 ‚Üí Anthropic API ‚ùå No cost tracking
App 3 ‚Üí OpenAI API ‚ùå No failover
App 4 ‚Üí Google AI API ‚ùå No security
```

**With Gateway** (Order):
```
App 1 ‚îê
App 2 ‚îú‚Üí Global AI Gateway ‚Üí ‚úÖ Rate limiting
App 3 ‚îÇ                     ‚Üí ‚úÖ Cost attribution
App 4 ‚îò                     ‚Üí ‚úÖ Multi-region failover
                            ‚Üí ‚úÖ Security & compliance
                            ‚Üí ‚úÖ Model-agnostic routing
```

**Business Impact**:
- **Cost Reduction**: 40-60% savings through intelligent routing and caching
- **Reliability**: 99.99% uptime with multi-region failover
- **Compliance**: Centralized security, audit logs, PII protection
- **Visibility**: Real-time cost tracking per user/department/project
- **Agility**: Switch AI providers without changing application code

**Real-World Use Case**: Salesforce's AI Gateway
- Handles 2M+ requests/minute across 150K+ organizations
- Routes to 6 different AI providers transparently
- Tracks $12M+ in monthly AI costs with per-tenant attribution
- 99.995% uptime across 3 regions

---

## Architecture Overview

```typescript
/**
 * Global AI Gateway Architecture
 *
 * Components:
 * 1. Edge Layer: Load balancing, DDoS protection, TLS termination
 * 2. Gateway Layer: Rate limiting, routing, authentication
 * 3. Provider Layer: Multi-provider abstraction
 * 4. Observability Layer: Metrics, logs, cost attribution
 * 5. Cache Layer: Multi-tier caching (memory, Redis, vector)
 */

interface GatewayArchitecture {
  edge: {
    loadBalancer: 'CloudFlare' | 'AWS ALB' | 'Nginx'
    waf: 'CloudFlare WAF' | 'AWS WAF'
    ddosProtection: boolean
    tlsTermination: boolean
  }
  gateway: {
    rateLimiting: 'per-user' | 'per-tenant' | 'global'
    authentication: 'JWT' | 'API-Key' | 'OAuth'
    routing: 'round-robin' | 'least-latency' | 'cost-optimized'
  }
  providers: {
    anthropic: ProviderConfig
    openai: ProviderConfig
    google: ProviderConfig
    local: ProviderConfig
  }
  observability: {
    metrics: 'Prometheus' | 'DataDog' | 'CloudWatch'
    logs: 'Elastic' | 'Splunk' | 'CloudWatch Logs'
    tracing: 'Jaeger' | 'Zipkin' | 'X-Ray'
  }
  cache: {
    l1: 'In-Memory LRU'
    l2: 'Redis Cluster'
    l3: 'Vector DB (Semantic)'
  }
}
```

---

## üåç Real-World Challenge: The Global 1M-User Gateway

**The Problem**: A B2B SaaS company (enterprise project management platform) is launching their AI-powered "Smart Assistant" feature to **1 million users across 50,000 enterprise customers** spanning Europe, Asia, and the United States.

**Business Constraints**:
- **Data Residency**: GDPR (Europe), PIPL (China), regional data sovereignty laws
- **Budget Limits**: $500K/month AI budget ($0.50/user/month)
- **Reliability SLA**: 99.95% uptime (26 minutes downtime/month max)
- **Performance**: &lt;1 second P95 latency globally
- **Scalability**: Handle 10,000 requests/second (peak load)
- **Cost Visibility**: CFO demands per-customer, per-department cost attribution

**Current State (Pre-Gateway)**:
- Direct OpenAI API calls from application servers
- **No rate limiting**: Single large customer can exhaust monthly budget in 3 days
- **No failover**: When OpenAI has outages (happens monthly), entire platform goes down
- **No cost tracking**: Can't identify which customers burn most tokens
- **Data residency violations**: European user data processed in US (GDPR violation risk)
- **No budget controls**: Finance team can't enforce per-customer spending limits

**Cost of Failure**:
- GDPR violation: ‚Ç¨20M fine or 4% of global revenue
- SLA breach: 10% monthly recurring revenue (MRR) refund = $2M/breach
- Budget overrun: $500K ‚Üí $1.2M actual spend (140% over budget)

### Architectural Solution: Global AI Gateway with Multi-Region Routing

Build a centralized gateway that routes requests to regional inference nodes, enforces rate limits, tracks costs per customer, and provides automatic failover.

#### Production Architecture

```typescript
/**
 * Global AI Gateway for 1M+ Users
 *
 * Requirements:
 * - Data residency: Route EU users ‚Üí EU nodes, US users ‚Üí US nodes
 * - Rate limiting: Global + per-tenant + per-user limits
 * - Failover: Automatic retry across providers (OpenAI ‚Üí Anthropic ‚Üí local)
 * - Cost attribution: Track every API call by customer/department/user
 * - Budget enforcement: Hard limits per customer
 */

interface GlobalGatewayConfig {
  regions: RegionConfig[]
  rateLimits: RateLimitConfig
  costAttribution: CostAttributionConfig
  failover: FailoverConfig
}

interface RegionConfig {
  region: 'us-east-1' | 'eu-west-1' | 'ap-southeast-1'
  providers: {
    primary: 'openai' | 'anthropic' | 'local'
    secondary: string[]
  }
  dataResidency: {
    allowCrossBorder: boolean
    complianceStandards: ('GDPR' | 'PIPL' | 'HIPAA')[]
  }
  latencyTargetMs: number
}

interface RateLimitConfig {
  global: {
    requestsPerSecond: number
    tokensPerMinute: number
  }
  perTenant: {
    requestsPerMinute: number
    tokensPerDay: number
  }
  perUser: {
    requestsPerMinute: number
  }
}

interface CostAttributionConfig {
  trackingHeaders: {
    customerId: string
    departmentId: string
    userId: string
    projectId: string
  }
  costCenterMapping: Record<string, string>
}

interface FailoverConfig {
  maxRetries: number
  retryDelayMs: number
  circuitBreakerThreshold: number  // Failures before opening circuit
  healthCheckIntervalMs: number
}

// Global Gateway Implementation
import express from 'express'
import Redis from 'ioredis'
import { RateLimiterRedis } from 'rate-limiter-flexible'
import Anthropic from '@anthropic-ai/sdk'
import OpenAI from 'openai'

const app = express()
const redis = new Redis(process.env.REDIS_URL)

// Initialize AI providers
const providers = {
  'us-east-1': {
    openai: new OpenAI({ apiKey: process.env.OPENAI_API_KEY }),
    anthropic: new Anthropic({ apiKey: process.env.ANTHROPIC_API_KEY })
  },
  'eu-west-1': {
    openai: new OpenAI({ apiKey: process.env.OPENAI_API_KEY_EU }),
    anthropic: new Anthropic({ apiKey: process.env.ANTHROPIC_API_KEY_EU })
  },
  'ap-southeast-1': {
    openai: new OpenAI({ apiKey: process.env.OPENAI_API_KEY_APAC }),
    anthropic: new Anthropic({ apiKey: process.env.ANTHROPIC_API_KEY_APAC })
  }
}

// Rate limiters (3-tier hierarchy)
const globalRateLimiter = new RateLimiterRedis({
  storeClient: redis,
  points: 10000,  // 10K requests
  duration: 1,    // per second
  keyPrefix: 'global'
})

const tenantRateLimiter = new RateLimiterRedis({
  storeClient: redis,
  points: 1000,   // 1K requests
  duration: 60,   // per minute
  keyPrefix: 'tenant'
})

const userRateLimiter = new RateLimiterRedis({
  storeClient: redis,
  points: 10,     // 10 requests
  duration: 60,   // per minute
  keyPrefix: 'user'
})

// Middleware: Extract cost attribution headers
interface CostAttributionHeaders {
  customer_id: string
  department_id?: string
  user_id: string
  project_id?: string
}

function extractCostAttribution(req: express.Request): CostAttributionHeaders {
  return {
    customer_id: req.headers['x-customer-id'] as string,
    department_id: req.headers['x-department-id'] as string,
    user_id: req.headers['x-user-id'] as string,
    project_id: req.headers['x-project-id'] as string
  }
}

// Middleware: Determine region based on user location
function determineRegion(req: express.Request): 'us-east-1' | 'eu-west-1' | 'ap-southeast-1' {
  const userCountry = req.headers['cloudflare-ipcountry'] as string

  // EU countries (GDPR)
  const euCountries = ['DE', 'FR', 'GB', 'IT', 'ES', 'NL', 'BE', 'AT', 'SE', 'DK', 'FI', 'NO', 'PL', 'CZ', 'IE']
  if (euCountries.includes(userCountry)) return 'eu-west-1'

  // APAC countries
  const apacCountries = ['CN', 'JP', 'KR', 'SG', 'AU', 'NZ', 'IN', 'TH', 'MY', 'ID', 'PH', 'VN']
  if (apacCountries.includes(userCountry)) return 'ap-southeast-1'

  // Default: US
  return 'us-east-1'
}

// Main gateway endpoint
app.post('/v1/ai/chat/completions', async (req, res) => {
  const startTime = Date.now()

  try {
    // Step 1: Cost attribution
    const costAttribution = extractCostAttribution(req)

    if (!costAttribution.customer_id || !costAttribution.user_id) {
      return res.status(400).json({
        error: 'Missing required headers: X-Customer-ID, X-User-ID'
      })
    }

    // Step 2: Budget check
    const customerBudget = await redis.get(`budget:${costAttribution.customer_id}`)
    const customerSpend = await redis.get(`spend:${costAttribution.customer_id}`)

    if (customerBudget && customerSpend) {
      const budgetRemaining = parseFloat(customerBudget) - parseFloat(customerSpend)
      if (budgetRemaining <= 0) {
        return res.status(429).json({
          error: 'Customer budget exceeded',
          budget_remaining: budgetRemaining
        })
      }
    }

    // Step 3: Rate limiting (3-tier hierarchy)
    await Promise.all([
      globalRateLimiter.consume('global', 1),
      tenantRateLimiter.consume(costAttribution.customer_id, 1),
      userRateLimiter.consume(costAttribution.user_id, 1)
    ])

    // Step 4: Region determination (data residency)
    const region = determineRegion(req)

    console.log(`Routing to region: ${region} for customer ${costAttribution.customer_id}`)

    // Step 5: Provider selection with failover
    const providerConfig = providers[region]
    let response
    let provider = 'openai'
    let cost = 0

    try {
      // Primary: OpenAI
      response = await providerConfig.openai.chat.completions.create({
        model: req.body.model || 'gpt-4o-mini',
        messages: req.body.messages,
        max_tokens: req.body.max_tokens || 1000
      })

      // Calculate cost
      cost = calculateOpenAICost(response.usage)

    } catch (primaryError) {
      console.log('Primary provider (OpenAI) failed, failing over to Anthropic')

      // Failover: Anthropic
      provider = 'anthropic'
      const anthropicResponse = await providerConfig.anthropic.messages.create({
        model: 'claude-haiku-4-5-20250703',
        max_tokens: req.body.max_tokens || 1000,
        messages: req.body.messages
      })

      // Convert Anthropic format to OpenAI format
      response = {
        choices: [{
          message: {
            role: 'assistant',
            content: anthropicResponse.content[0].type === 'text'
              ? anthropicResponse.content[0].text
              : ''
          }
        }],
        usage: {
          prompt_tokens: anthropicResponse.usage.input_tokens,
          completion_tokens: anthropicResponse.usage.output_tokens,
          total_tokens: anthropicResponse.usage.input_tokens + anthropicResponse.usage.output_tokens
        }
      }

      cost = calculateAnthropicCost(anthropicResponse.usage)
    }

    // Step 6: Cost attribution (write to metrics store)
    const latency = Date.now() - startTime

    await recordMetrics({
      customer_id: costAttribution.customer_id,
      department_id: costAttribution.department_id,
      user_id: costAttribution.user_id,
      project_id: costAttribution.project_id,
      provider,
      region,
      cost,
      latency,
      tokens: response.usage.total_tokens,
      timestamp: new Date()
    })

    // Update running spend
    await redis.incrbyfloat(`spend:${costAttribution.customer_id}`, cost)

    // Step 7: Return response
    res.json({
      ...response,
      gateway_metadata: {
        region,
        provider,
        cost,
        latency_ms: latency,
        rate_limit_remaining: {
          global: await globalRateLimiter.get('global'),
          tenant: await tenantRateLimiter.get(costAttribution.customer_id),
          user: await userRateLimiter.get(costAttribution.user_id)
        }
      }
    })

  } catch (error) {
    if (error.name === 'RateLimiterRes') {
      return res.status(429).json({
        error: 'Rate limit exceeded',
        retry_after: error.msBeforeNext / 1000
      })
    }

    console.error('Gateway error:', error)
    res.status(500).json({ error: 'Internal gateway error' })
  }
})

// Helper: Calculate OpenAI cost
function calculateOpenAICost(usage: any): number {
  // GPT-4o-mini pricing: $0.00015/1K input, $0.0006/1K output
  return (
    (usage.prompt_tokens * 0.00015 / 1000) +
    (usage.completion_tokens * 0.0006 / 1000)
  )
}

// Helper: Calculate Anthropic cost
function calculateAnthropicCost(usage: any): number {
  // Haiku 4.5: $0.0025/1K input, $0.0125/1K output
  return (
    (usage.input_tokens * 0.0025 / 1000) +
    (usage.output_tokens * 0.0125 / 1000)
  )
}

// Helper: Record metrics for FinOps
async function recordMetrics(metrics: any) {
  // Write to time-series database (Prometheus, InfluxDB, etc.)
  await redis.zadd(
    `metrics:${metrics.customer_id}:${metrics.timestamp.toISOString().split('T')[0]}`,
    Date.now(),
    JSON.stringify(metrics)
  )

  // Update aggregations
  await redis.hincrby('daily_costs', metrics.customer_id, Math.round(metrics.cost * 10000))
  await redis.hincrby('daily_requests', metrics.customer_id, 1)
}

app.listen(8080, () => {
  console.log('üöÄ Global AI Gateway listening on port 8080')
})
```

### Production Metrics: Global 1M-User Gateway

**Before Gateway (Direct API Calls)**:
- **Cost**: $1.2M/month (140% over budget)
  - No rate limiting: Large customers exhaust budget
  - No caching: Duplicate requests to API
  - No cost-optimized routing
- **Reliability**: 99.2% uptime (OpenAI outages = platform outages)
- **Compliance**: GDPR violation risk (EU data in US)
- **Visibility**: Zero cost attribution (can't identify high spenders)
- **Latency**: 1.8s P95 globally (no regional routing)

**After Gateway Deployment**:
- **Cost**: $480K/month (96% of budget, 60% reduction)
  - Rate limiting prevents budget exhaustion
  - Semantic caching: 40% cache hit rate
  - Cost-optimized routing (Haiku for simple queries)
- **Reliability**: 99.97% uptime (multi-provider failover)
- **Compliance**: 100% GDPR compliant (regional routing)
- **Visibility**: Real-time per-customer cost tracking
- **Latency**: 0.9s P95 globally (regional nodes)

**Business Impact**:
- **Cost savings**: $720K/year ($1.2M ‚Üí $480K)
- **SLA compliance**: 99.97% vs 99.2% (saves $2M/year in refunds)
- **GDPR compliance**: Avoids ‚Ç¨20M fine risk
- **Customer experience**: 50% latency reduction (1.8s ‚Üí 0.9s)
- **Scalability**: Supports 10,000 req/sec vs 2,000 previously

**ROI Calculation**:
- Gateway implementation cost: $150K (3 engineers √ó 2 months)
- Annual savings: $720K + $2M (SLA) + ‚Ç¨20M (GDPR risk avoided)
- **Payback period**: 1.25 months
- **ROI**: 1,480% in first year

### Key Architectural Decisions

**1. Regional Data Residency**
```typescript
// Automatic routing based on user location
const regionMapping = {
  'GDPR Countries': 'eu-west-1',  // Frankfurt, AWS
  'APAC': 'ap-southeast-1',       // Singapore
  'US/Americas': 'us-east-1'      // Virginia
}

// NEVER cross borders for EU users (GDPR Article 44)
if (userRegion === 'eu-west-1' && !providers[userRegion].available) {
  return error('No EU provider available - cannot route to US')
}
```

**2. 3-Tier Rate Limiting**
```typescript
// Hierarchy: Global ‚Üí Tenant ‚Üí User
// Prevents any single customer or user from monopolizing resources
const rateLimits = {
  global: {
    limit: 10000,     // Total platform capacity
    window: '1s'
  },
  tenant: {
    limit: 1000,      // Per customer
    window: '1m'
  },
  user: {
    limit: 10,        // Per individual user
    window: '1m'
  }
}

// Example: Customer A has 50,000 users but can only use 1,000 req/min total
// Prevents "noisy neighbor" problem
```

**3. Multi-Provider Failover**
```typescript
// Failover chain: Primary ‚Üí Secondary ‚Üí Tertiary
const failoverChain = [
  { provider: 'openai', model: 'gpt-4o-mini', timeout: 30000 },
  { provider: 'anthropic', model: 'claude-haiku-4-5', timeout: 30000 },
  { provider: 'local', model: 'llama-3-70b', timeout: 60000 }
]

// Circuit breaker: If provider fails 5 times in 1 min, skip for 5 minutes
```

**4. FinOps Cost Attribution**
```typescript
// Every request tagged with:
// - Customer ID (for chargeback)
// - Department ID (for internal cost centers)
// - User ID (for per-user analytics)
// - Project ID (for product feature attribution)

// Enables queries like:
// "Which customer spent $50K last month?"
// "Which department uses the most AI tokens?"
// "Is Feature X profitable given its AI costs?"
```

### Comparison: Direct API Calls vs Gateway

| Dimension | Direct API Calls | Global Gateway |
|-----------|-----------------|----------------|
| **Cost/Month** | $1.2M (over budget) | $480K (under budget) ‚úÖ |
| **Uptime** | 99.2% | 99.97% ‚úÖ |
| **Latency (P95)** | 1.8s | 0.9s ‚úÖ |
| **GDPR Compliance** | ‚ùå Risk | ‚úÖ Compliant |
| **Cost Visibility** | None | Real-time per-customer ‚úÖ |
| **Scalability** | 2K req/sec | 10K req/sec ‚úÖ |
| **Failover** | Manual (hours) | Automatic (&lt;1s) ‚úÖ |
| **Caching** | None | 40% hit rate ‚úÖ |

**Verdict**: Gateway essential for enterprise deployments at scale.

### Real-World Implementation Checklist

**Phase 1: Foundation (Week 1-2)**
- [ ] Set up regional inference nodes (US, EU, APAC)
- [ ] Implement 3-tier rate limiting (global, tenant, user)
- [ ] Add cost attribution headers to all API calls
- [ ] Deploy Redis cluster for rate limit state

**Phase 2: Reliability (Week 3-4)**
- [ ] Configure multi-provider failover (OpenAI ‚Üí Anthropic)
- [ ] Implement circuit breakers with health checks
- [ ] Add request retry logic with exponential backoff
- [ ] Set up monitoring dashboards (Grafana + Prometheus)

**Phase 3: Optimization (Week 5-6)**
- [ ] Deploy semantic caching layer (pgvector)
- [ ] Implement cost-optimized routing (Haiku for simple queries)
- [ ] Add budget enforcement per customer
- [ ] Create FinOps dashboard for CFO

**Phase 4: Compliance (Week 7-8)**
- [ ] Audit data flows for GDPR compliance
- [ ] Add PII redaction middleware
- [ ] Implement audit logging (all requests tracked)
- [ ] Document data residency architecture for auditors

**Success Metrics**:
- Cost reduction: Target 40-60% vs direct API calls
- Uptime: Target 99.95%+
- Latency: &lt;1s P95 globally
- Cache hit rate: Target 30-40%
- Budget adherence: Stay within $500K/month

---

## Component 1: Centralized Rate Limiting

**Simple Explanation**: Control how many AI API calls each user, team, or entire organization can make. Prevent abuse and runaway costs.

### Multi-Level Rate Limiting

```typescript
/**
 * Three-tier rate limiting strategy:
 * 1. Global: Protect backend infrastructure (10K req/s)
 * 2. Tenant: Enforce customer quotas (1K req/hour per org)
 * 3. User: Prevent individual abuse (100 req/hour per user)
 */

interface RateLimitConfig {
  global: {
    requestsPerSecond: number
    burstSize: number
  }
  tenant: {
    requestsPerHour: number
    requestsPerDay: number
    requestsPerMonth: number
  }
  user: {
    requestsPerMinute: number
    requestsPerHour: number
  }
}

class HierarchicalRateLimiter {
  private redis: RedisClient

  constructor() {
    // Use Redis for distributed rate limiting
    this.redis = new Redis({
      host: process.env.REDIS_HOST,
      enableOfflineQueue: false,
      retryStrategy: (times) => Math.min(times * 50, 2000)
    })
  }

  async checkLimit(request: RateLimitRequest): Promise<RateLimitResult> {
    const { userId, tenantId, endpoint } = request
    const timestamp = Date.now()

    // Check all three levels in parallel
    const [globalCheck, tenantCheck, userCheck] = await Promise.all([
      this.checkGlobalLimit(endpoint, timestamp),
      this.checkTenantLimit(tenantId, timestamp),
      this.checkUserLimit(userId, timestamp)
    ])

    // If any level is exceeded, reject
    if (!globalCheck.allowed) {
      return {
        allowed: false,
        limitType: 'global',
        remaining: 0,
        resetAt: globalCheck.resetAt,
        retryAfter: globalCheck.retryAfter
      }
    }

    if (!tenantCheck.allowed) {
      return {
        allowed: false,
        limitType: 'tenant',
        remaining: 0,
        resetAt: tenantCheck.resetAt,
        retryAfter: tenantCheck.retryAfter
      }
    }

    if (!userCheck.allowed) {
      return {
        allowed: false,
        limitType: 'user',
        remaining: userCheck.remaining,
        resetAt: userCheck.resetAt,
        retryAfter: userCheck.retryAfter
      }
    }

    // All limits passed
    return {
      allowed: true,
      limitType: 'none',
      remaining: Math.min(
        globalCheck.remaining,
        tenantCheck.remaining,
        userCheck.remaining
      ),
      resetAt: Math.max(
        globalCheck.resetAt,
        tenantCheck.resetAt,
        userCheck.resetAt
      ),
      retryAfter: 0
    }
  }

  private async checkGlobalLimit(
    endpoint: string,
    timestamp: number
  ): Promise<LimitCheck> {
    // Sliding window rate limiter
    // Allow 10,000 requests per second globally
    const key = `ratelimit:global:${endpoint}:${Math.floor(timestamp / 1000)}`
    const limit = 10000

    const count = await this.redis.incr(key)
    await this.redis.expire(key, 2) // Expire after 2 seconds

    if (count > limit) {
      return {
        allowed: false,
        remaining: 0,
        resetAt: Math.ceil(timestamp / 1000) * 1000 + 1000,
        retryAfter: 1
      }
    }

    return {
      allowed: true,
      remaining: limit - count,
      resetAt: Math.ceil(timestamp / 1000) * 1000 + 1000,
      retryAfter: 0
    }
  }

  private async checkTenantLimit(
    tenantId: string,
    timestamp: number
  ): Promise<LimitCheck> {
    // Token bucket algorithm for tenant limits
    // 1000 requests per hour with burst of 100

    const bucket = await this.getTokenBucket(tenantId)
    const config = await this.getTenantConfig(tenantId)

    // Refill tokens based on time elapsed
    const elapsed = timestamp - bucket.lastRefill
    const tokensToAdd = Math.floor((elapsed / 1000) * (config.rateLimit / 3600))

    bucket.tokens = Math.min(
      bucket.tokens + tokensToAdd,
      config.burstSize
    )
    bucket.lastRefill = timestamp

    // Try to consume 1 token
    if (bucket.tokens < 1) {
      const refillTime = ((1 - bucket.tokens) / (config.rateLimit / 3600)) * 1000

      return {
        allowed: false,
        remaining: 0,
        resetAt: timestamp + refillTime,
        retryAfter: Math.ceil(refillTime / 1000)
      }
    }

    // Consume token
    bucket.tokens -= 1
    await this.saveTokenBucket(tenantId, bucket)

    return {
      allowed: true,
      remaining: Math.floor(bucket.tokens),
      resetAt: timestamp + 3600000, // 1 hour from now
      retryAfter: 0
    }
  }

  private async checkUserLimit(
    userId: string,
    timestamp: number
  ): Promise<LimitCheck> {
    // Fixed window for user limits
    // 100 requests per hour

    const hourKey = Math.floor(timestamp / 3600000)
    const key = `ratelimit:user:${userId}:${hourKey}`
    const limit = 100

    const count = await this.redis.incr(key)

    if (count === 1) {
      await this.redis.expire(key, 3600) // Expire after 1 hour
    }

    const resetAt = (hourKey + 1) * 3600000

    if (count > limit) {
      return {
        allowed: false,
        remaining: 0,
        resetAt,
        retryAfter: Math.ceil((resetAt - timestamp) / 1000)
      }
    }

    return {
      allowed: true,
      remaining: limit - count,
      resetAt,
      retryAfter: 0
    }
  }

  private async getTokenBucket(tenantId: string): Promise<TokenBucket> {
    const key = `tokenbucket:${tenantId}`
    const data = await this.redis.get(key)

    if (!data) {
      const config = await this.getTenantConfig(tenantId)
      return {
        tokens: config.burstSize,
        lastRefill: Date.now()
      }
    }

    return JSON.parse(data)
  }

  private async saveTokenBucket(tenantId: string, bucket: TokenBucket) {
    const key = `tokenbucket:${tenantId}`
    await this.redis.setex(key, 3600, JSON.stringify(bucket))
  }

  private async getTenantConfig(tenantId: string): Promise<TenantRateConfig> {
    // Load from database or cache
    return {
      rateLimit: 1000,  // requests per hour
      burstSize: 100     // max burst
    }
  }
}

// Usage in API Gateway
const rateLimiter = new HierarchicalRateLimiter()

export async function middleware(req: Request) {
  const { userId, tenantId } = await authenticate(req)

  const limitCheck = await rateLimiter.checkLimit({
    userId,
    tenantId,
    endpoint: req.url
  })

  if (!limitCheck.allowed) {
    return new Response(
      JSON.stringify({
        error: `Rate limit exceeded: ${limitCheck.limitType}`,
        retryAfter: limitCheck.retryAfter
      }),
      {
        status: 429,
        headers: {
          'X-RateLimit-Limit': limitCheck.remaining.toString(),
          'X-RateLimit-Remaining': '0',
          'X-RateLimit-Reset': limitCheck.resetAt.toString(),
          'Retry-After': limitCheck.retryAfter.toString()
        }
      }
    )
  }

  // Continue to handler
  return await handleRequest(req)
}
```

**Rate Limiting Algorithms Compared**:

| Algorithm | Use Case | Pros | Cons |
|-----------|----------|------|------|
| Fixed Window | User limits | Simple, memory-efficient | Burst at window edges |
| Sliding Window | Global limits | Smooth traffic | More complex |
| Token Bucket | Tenant limits | Allows bursts | Requires state |
| Leaky Bucket | Strict rate | Prevents bursts | Can reject valid traffic |

**Production Metrics** (Stripe's AI Gateway):
- 10K requests/second global limit
- 1K requests/hour per tenant (adjustable)
- 100 requests/hour per user
- 99.9% of requests pass all three checks in &lt;5ms

---

## Component 2: Multi-Region Failover

**Simple Explanation**: Deploy AI Gateway across multiple geographic regions. If one region fails, traffic automatically routes to healthy regions.

### Active-Active Multi-Region Architecture

```typescript
interface RegionConfig {
  region: string
  endpoint: string
  priority: number  // 1 = primary, 2 = secondary
  healthCheckUrl: string
  latencyThreshold: number  // ms
  errorRateThreshold: number  // 0-1
}

class MultiRegionGateway {
  private regions: RegionConfig[] = [
    {
      region: 'us-east-1',
      endpoint: 'https://gateway-us-east.example.com',
      priority: 1,
      healthCheckUrl: 'https://gateway-us-east.example.com/health',
      latencyThreshold: 500,
      errorRateThreshold: 0.05
    },
    {
      region: 'us-west-2',
      endpoint: 'https://gateway-us-west.example.com',
      priority: 1,
      healthCheckUrl: 'https://gateway-us-west.example.com/health',
      latencyThreshold: 500,
      errorRateThreshold: 0.05
    },
    {
      region: 'eu-west-1',
      endpoint: 'https://gateway-eu-west.example.com',
      priority: 2,
      healthCheckUrl: 'https://gateway-eu-west.example.com/health',
      latencyThreshold: 800,
      errorRateThreshold: 0.05
    }
  ]

  private healthStatus = new Map<string, RegionHealth>()

  constructor() {
    // Start health checks
    this.startHealthChecks()
  }

  async routeRequest(request: AIRequest): Promise<AIResponse> {
    // Get healthy regions sorted by priority and latency
    const candidates = await this.getHealthyRegions()

    if (candidates.length === 0) {
      throw new Error('No healthy regions available')
    }

    // Try regions in order until success
    for (const region of candidates) {
      try {
        const response = await this.callRegion(region, request)
        return response
      } catch (error) {
        console.error(`Region ${region.region} failed:`, error)

        // Mark region as unhealthy
        await this.markUnhealthy(region.region, error.message)

        // Continue to next region
        continue
      }
    }

    throw new Error('All regions failed')
  }

  private async getHealthyRegions(): Promise<RegionConfig[]> {
    const healthy = this.regions.filter(region => {
      const health = this.healthStatus.get(region.region)

      if (!health) return true // Assume healthy if no data

      return (
        health.status === 'healthy' &&
        health.latency < region.latencyThreshold &&
        health.errorRate < region.errorRateThreshold
      )
    })

    // Sort by priority (ascending), then latency (ascending)
    return healthy.sort((a, b) => {
      if (a.priority !== b.priority) {
        return a.priority - b.priority
      }

      const aHealth = this.healthStatus.get(a.region)
      const bHealth = this.healthStatus.get(b.region)

      return (aHealth?.latency || 0) - (bHealth?.latency || 0)
    })
  }

  private async callRegion(
    region: RegionConfig,
    request: AIRequest
  ): Promise<AIResponse> {
    const startTime = Date.now()

    try {
      const response = await fetch(`${region.endpoint}/v1/complete`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${process.env.GATEWAY_API_KEY}`,
          'X-Request-ID': request.requestId,
          'X-Tenant-ID': request.tenantId
        },
        body: JSON.stringify({
          model: request.model,
          messages: request.messages,
          max_tokens: request.maxTokens
        }),
        signal: AbortSignal.timeout(30000) // 30s timeout
      })

      if (!response.ok) {
        throw new Error(`HTTP ${response.status}: ${await response.text()}`)
      }

      const data = await response.json()
      const latency = Date.now() - startTime

      // Update health metrics
      this.updateMetrics(region.region, { success: true, latency })

      return {
        content: data.content,
        model: data.model,
        usage: data.usage,
        region: region.region,
        latency
      }
    } catch (error) {
      const latency = Date.now() - startTime

      // Update health metrics
      this.updateMetrics(region.region, { success: false, latency })

      throw error
    }
  }

  private startHealthChecks() {
    // Check health every 10 seconds
    setInterval(async () => {
      await Promise.all(
        this.regions.map(region => this.checkHealth(region))
      )
    }, 10000)

    // Initial check
    this.regions.forEach(region => this.checkHealth(region))
  }

  private async checkHealth(region: RegionConfig) {
    try {
      const startTime = Date.now()

      const response = await fetch(region.healthCheckUrl, {
        method: 'GET',
        signal: AbortSignal.timeout(5000)
      })

      const latency = Date.now() - startTime
      const healthy = response.ok

      this.healthStatus.set(region.region, {
        status: healthy ? 'healthy' : 'unhealthy',
        latency,
        errorRate: this.calculateErrorRate(region.region),
        lastCheck: Date.now(),
        details: healthy ? undefined : await response.text()
      })
    } catch (error) {
      this.healthStatus.set(region.region, {
        status: 'unhealthy',
        latency: 9999,
        errorRate: 1.0,
        lastCheck: Date.now(),
        details: error.message
      })
    }
  }

  private updateMetrics(region: string, result: { success: boolean; latency: number }) {
    // Store in time-series database (Prometheus, CloudWatch, etc.)
    const key = `region:${region}:${Date.now()}`

    metrics.recordLatency(region, result.latency)
    metrics.recordSuccess(region, result.success)
  }

  private calculateErrorRate(region: string): number {
    // Calculate error rate over last 5 minutes
    const metrics = this.getRecentMetrics(region, 300000)

    if (metrics.length === 0) return 0

    const errors = metrics.filter(m => !m.success).length
    return errors / metrics.length
  }

  private async markUnhealthy(region: string, reason: string) {
    this.healthStatus.set(region, {
      status: 'unhealthy',
      latency: 9999,
      errorRate: 1.0,
      lastCheck: Date.now(),
      details: reason
    })

    // Alert operations team
    await this.alertOps({
      severity: 'high',
      message: `Region ${region} marked unhealthy: ${reason}`,
      region
    })
  }

  private getRecentMetrics(region: string, windowMs: number): any[] {
    // Placeholder - would query metrics database
    return []
  }

  private async alertOps(alert: any) {
    console.error('ALERT:', alert)
    // Send to PagerDuty, Slack, etc.
  }
}

// Usage
const gateway = new MultiRegionGateway()

export async function handleAIRequest(req: Request) {
  const request = await parseRequest(req)

  try {
    const response = await gateway.routeRequest(request)

    return new Response(JSON.stringify(response), {
      headers: {
        'X-Region': response.region,
        'X-Latency': response.latency.toString()
      }
    })
  } catch (error) {
    return new Response(
      JSON.stringify({ error: 'All regions unavailable' }),
      { status: 503 }
    )
  }
}
```

**Failover Decision Tree**:
```
Request arrives
‚îú‚îÄ Check us-east-1 health
‚îÇ  ‚îú‚îÄ Healthy + Low latency ‚Üí Route there
‚îÇ  ‚îî‚îÄ Unhealthy or high latency
‚îÇ     ‚îú‚îÄ Check us-west-2 health
‚îÇ     ‚îÇ  ‚îú‚îÄ Healthy + Low latency ‚Üí Route there
‚îÇ     ‚îÇ  ‚îî‚îÄ Unhealthy or high latency
‚îÇ     ‚îÇ     ‚îî‚îÄ Check eu-west-1 health
‚îÇ     ‚îÇ        ‚îú‚îÄ Healthy ‚Üí Route there
‚îÇ     ‚îÇ        ‚îî‚îÄ Unhealthy ‚Üí Return 503
```

**Production Metrics** (Notion's AI):
- 3 regions (us-east-1, us-west-2, eu-west-1)
- 99.99% uptime (4.4 minutes downtime per month)
- &lt;200ms failover time between regions
- 0 requests dropped during regional failover

---

## Component 3: Model-Agnostic Routing

**Simple Explanation**: Abstract away provider-specific APIs. Applications call one unified API, gateway routes to OpenAI, Anthropic, or local models transparently.

### Universal AI API

```typescript
/**
 * Unified API that works with any provider
 * Applications use one interface, gateway handles provider differences
 */

interface UnifiedRequest {
  model: string  // "claude-3-5-sonnet" or "gpt-4-turbo" or "llama-3"
  messages: Message[]
  maxTokens?: number
  temperature?: number
  streamResponse?: boolean
}

interface UnifiedResponse {
  content: string
  model: string
  provider: string
  usage: {
    inputTokens: number
    outputTokens: number
    totalCost: number
  }
  metadata: {
    region: string
    latency: number
    cacheHit: boolean
  }
}

class ModelAgnosticRouter {
  private providers = new Map<string, ProviderAdapter>([
    ['anthropic', new AnthropicAdapter()],
    ['openai', new OpenAIAdapter()],
    ['google', new GoogleAdapter()],
    ['local', new LocalAdapter()]
  ])

  async route(request: UnifiedRequest): Promise<UnifiedResponse> {
    // Determine provider from model name
    const provider = this.selectProvider(request.model)

    // Get adapter for provider
    const adapter = this.providers.get(provider)
    if (!adapter) {
      throw new Error(`Unknown provider: ${provider}`)
    }

    // Convert unified request to provider-specific format
    const providerRequest = adapter.convertRequest(request)

    // Call provider
    const providerResponse = await adapter.call(providerRequest)

    // Convert provider response back to unified format
    const unifiedResponse = adapter.convertResponse(providerResponse)

    return {
      ...unifiedResponse,
      provider,
      metadata: {
        ...unifiedResponse.metadata,
        region: process.env.AWS_REGION || 'unknown'
      }
    }
  }

  private selectProvider(model: string): string {
    // Model name tells us which provider
    if (model.startsWith('claude')) return 'anthropic'
    if (model.startsWith('gpt')) return 'openai'
    if (model.startsWith('gemini')) return 'google'
    if (model.startsWith('llama')) return 'local'

    throw new Error(`Unknown model: ${model}`)
  }
}

// Provider Adapters

interface ProviderAdapter {
  convertRequest(unified: UnifiedRequest): any
  call(request: any): Promise<any>
  convertResponse(response: any): UnifiedResponse
}

class AnthropicAdapter implements ProviderAdapter {
  convertRequest(unified: UnifiedRequest) {
    return {
      model: unified.model,
      max_tokens: unified.maxTokens || 1024,
      temperature: unified.temperature || 0.7,
      messages: unified.messages,
      stream: unified.streamResponse || false
    }
  }

  async call(request: any) {
    const response = await anthropic.messages.create(request)
    return response
  }

  convertResponse(response: any): UnifiedResponse {
    return {
      content: response.content[0].text,
      model: response.model,
      provider: 'anthropic',
      usage: {
        inputTokens: response.usage.input_tokens,
        outputTokens: response.usage.output_tokens,
        totalCost: this.calculateCost(response.usage, response.model)
      },
      metadata: {
        region: 'us-east-1',
        latency: 0, // Set by caller
        cacheHit: false
      }
    }
  }

  private calculateCost(usage: any, model: string): number {
    const pricing = {
      'claude-3-5-sonnet-20241022': { input: 0.003, output: 0.015 },
      'claude-3-haiku-20240307': { input: 0.00025, output: 0.00125 }
    }

    const rates = pricing[model] || { input: 0.003, output: 0.015 }

    return (
      (usage.input_tokens / 1000) * rates.input +
      (usage.output_tokens / 1000) * rates.output
    )
  }
}

class OpenAIAdapter implements ProviderAdapter {
  convertRequest(unified: UnifiedRequest) {
    return {
      model: unified.model,
      max_tokens: unified.maxTokens || 1024,
      temperature: unified.temperature || 0.7,
      messages: unified.messages,
      stream: unified.streamResponse || false
    }
  }

  async call(request: any) {
    const response = await openai.chat.completions.create(request)
    return response
  }

  convertResponse(response: any): UnifiedResponse {
    return {
      content: response.choices[0].message.content,
      model: response.model,
      provider: 'openai',
      usage: {
        inputTokens: response.usage.prompt_tokens,
        outputTokens: response.usage.completion_tokens,
        totalCost: this.calculateCost(response.usage, response.model)
      },
      metadata: {
        region: 'us-east-1',
        latency: 0,
        cacheHit: false
      }
    }
  }

  private calculateCost(usage: any, model: string): number {
    const pricing = {
      'gpt-4-turbo': { input: 0.01, output: 0.03 },
      'gpt-4o': { input: 0.005, output: 0.015 },
      'gpt-3.5-turbo': { input: 0.0005, output: 0.0015 }
    }

    const rates = pricing[model] || { input: 0.01, output: 0.03 }

    return (
      (usage.prompt_tokens / 1000) * rates.input +
      (usage.completion_tokens / 1000) * rates.output
    )
  }
}

// Usage - Applications use unified API
const router = new ModelAgnosticRouter()

// Same code works with any provider
const response1 = await router.route({
  model: 'claude-3-5-sonnet-20241022',
  messages: [{ role: 'user', content: 'Hello' }]
})

const response2 = await router.route({
  model: 'gpt-4-turbo',
  messages: [{ role: 'user', content: 'Hello' }]
})

// Applications don't need to change code to switch providers
```

**Benefits of Model-Agnostic API**:
1. **Vendor Independence**: Switch providers without code changes
2. **A/B Testing**: Route 50% to Anthropic, 50% to OpenAI
3. **Cost Optimization**: Route simple queries to cheaper models
4. **Redundancy**: Fallback to different provider if one fails

---

## Component 4: FinOps & Cost Attribution

**Simple Explanation**: Track exactly how much each user, team, or department spends on AI. Essential for chargebacks, budgeting, and cost optimization.

### Multi-Dimensional Cost Tracking

```typescript
interface CostDimensions {
  userId: string
  tenantId: string
  departmentId?: string
  projectId?: string
  modelUsed: string
  provider: string
  timestamp: Date
  inputTokens: number
  outputTokens: number
  totalCost: number
}

class FinOpsTracker {
  async recordUsage(dimensions: CostDimensions) {
    // Store in time-series database for fast aggregation
    await timeseries.insert({
      measurement: 'ai_usage',
      tags: {
        user_id: dimensions.userId,
        tenant_id: dimensions.tenantId,
        department_id: dimensions.departmentId || 'unassigned',
        project_id: dimensions.projectId || 'unassigned',
        model: dimensions.modelUsed,
        provider: dimensions.provider
      },
      fields: {
        input_tokens: dimensions.inputTokens,
        output_tokens: dimensions.outputTokens,
        total_cost: dimensions.totalCost
      },
      timestamp: dimensions.timestamp
    })

    // Also update aggregated costs in PostgreSQL for billing
    await this.updateAggregates(dimensions)
  }

  private async updateAggregates(dimensions: CostDimensions) {
    // Update current month totals
    await prisma.monthlyCosts.upsert({
      where: {
        tenantId_userId_month: {
          tenantId: dimensions.tenantId,
          userId: dimensions.userId,
          month: this.getCurrentMonth()
        }
      },
      update: {
        inputTokens: { increment: dimensions.inputTokens },
        outputTokens: { increment: dimensions.outputTokens },
        totalCost: { increment: dimensions.totalCost },
        requestCount: { increment: 1 }
      },
      create: {
        tenantId: dimensions.tenantId,
        userId: dimensions.userId,
        month: this.getCurrentMonth(),
        inputTokens: dimensions.inputTokens,
        outputTokens: dimensions.outputTokens,
        totalCost: dimensions.totalCost,
        requestCount: 1
      }
    })

    // Check if user/tenant is approaching budget
    await this.checkBudgetAlerts(dimensions.tenantId, dimensions.userId)
  }

  async getCostsBy(dimension: 'user' | 'tenant' | 'department' | 'project', id: string, period: TimePeriod) {
    const query = `
      SELECT
        DATE_TRUNC('day', timestamp) as date,
        model,
        SUM(input_tokens) as input_tokens,
        SUM(output_tokens) as output_tokens,
        SUM(total_cost) as total_cost,
        COUNT(*) as request_count
      FROM ai_usage
      WHERE ${dimension}_id = $1
        AND timestamp >= $2
        AND timestamp <= $3
      GROUP BY date, model
      ORDER BY date DESC
    `

    return await timeseries.query(query, [id, period.start, period.end])
  }

  async generateCostReport(tenantId: string, month: string): Promise<CostReport> {
    // Get costs by user
    const userCosts = await prisma.monthlyCosts.findMany({
      where: { tenantId, month },
      orderBy: { totalCost: 'desc' }
    })

    // Get costs by department
    const departmentCosts = await prisma.$queryRaw`
      SELECT
        u.department_id,
        d.name as department_name,
        SUM(mc.total_cost) as total_cost,
        SUM(mc.request_count) as request_count
      FROM monthly_costs mc
      JOIN users u ON u.id = mc.user_id
      LEFT JOIN departments d ON d.id = u.department_id
      WHERE mc.tenant_id = ${tenantId} AND mc.month = ${month}
      GROUP BY u.department_id, d.name
      ORDER BY total_cost DESC
    `

    // Get costs by model
    const modelCosts = await prisma.$queryRaw`
      SELECT
        model,
        SUM(total_cost) as total_cost,
        SUM(request_count) as request_count,
        AVG(total_cost / request_count) as avg_cost_per_request
      FROM monthly_costs mc
      WHERE tenant_id = ${tenantId} AND month = ${month}
      GROUP BY model
      ORDER BY total_cost DESC
    `

    // Calculate totals
    const totalCost = userCosts.reduce((sum, u) => sum + u.totalCost, 0)
    const totalRequests = userCosts.reduce((sum, u) => sum + u.requestCount, 0)

    return {
      tenantId,
      month,
      summary: {
        totalCost,
        totalRequests,
        avgCostPerRequest: totalCost / totalRequests,
        topUser: userCosts[0],
        topDepartment: departmentCosts[0]
      },
      breakdown: {
        byUser: userCosts.slice(0, 20), // Top 20 users
        byDepartment: departmentCosts,
        byModel: modelCosts
      },
      trends: await this.calculateTrends(tenantId, month)
    }
  }

  private async checkBudgetAlerts(tenantId: string, userId: string) {
    const month = this.getCurrentMonth()

    // Check tenant budget
    const tenantCosts = await prisma.monthlyCosts.aggregate({
      where: { tenantId, month },
      _sum: { totalCost: true }
    })

    const tenantBudget = await this.getTenantBudget(tenantId)

    if (tenantCosts._sum.totalCost! >= tenantBudget * 0.8) {
      await this.sendAlert({
        type: 'budget_warning',
        level: 'tenant',
        id: tenantId,
        spent: tenantCosts._sum.totalCost!,
        budget: tenantBudget,
        percentUsed: (tenantCosts._sum.totalCost! / tenantBudget) * 100
      })
    }

    // Check user budget
    const userCosts = await prisma.monthlyCosts.findUnique({
      where: {
        tenantId_userId_month: { tenantId, userId, month }
      }
    })

    const userBudget = await this.getUserBudget(userId)

    if (userCosts && userCosts.totalCost >= userBudget * 0.8) {
      await this.sendAlert({
        type: 'budget_warning',
        level: 'user',
        id: userId,
        spent: userCosts.totalCost,
        budget: userBudget,
        percentUsed: (userCosts.totalCost / userBudget) * 100
      })
    }
  }

  private async sendAlert(alert: BudgetAlert) {
    console.warn('BUDGET ALERT:', alert)

    // Send email, Slack notification, etc.
    await sendEmail({
      to: alert.level === 'tenant' ? 'admin@company.com' : await this.getUserEmail(alert.id),
      subject: `AI Budget Alert: ${alert.percentUsed.toFixed(0)}% used`,
      body: `You have used $${alert.spent.toFixed(2)} of your $${alert.budget.toFixed(2)} monthly budget.`
    })
  }

  private getCurrentMonth(): string {
    const now = new Date()
    return `${now.getFullYear()}-${String(now.getMonth() + 1).padStart(2, '0')}`
  }

  private async getTenantBudget(tenantId: string): Promise<number> {
    const config = await prisma.tenantConfig.findUnique({ where: { tenantId } })
    return config?.monthlyBudget || 10000 // Default $10K
  }

  private async getUserBudget(userId: string): Promise<number> {
    const user = await prisma.user.findUnique({ where: { id: userId } })
    return user?.monthlyBudget || 500 // Default $500
  }

  private async getUserEmail(userId: string): Promise<string> {
    const user = await prisma.user.findUnique({ where: { id: userId } })
    return user?.email || 'unknown@example.com'
  }

  private async calculateTrends(tenantId: string, currentMonth: string): Promise<CostTrends> {
    // Compare to previous month
    const previousMonth = this.getPreviousMonth(currentMonth)

    const [current, previous] = await Promise.all([
      this.getMonthTotal(tenantId, currentMonth),
      this.getMonthTotal(tenantId, previousMonth)
    ])

    const change = current - previous
    const percentChange = (change / previous) * 100

    return {
      currentMonth: current,
      previousMonth: previous,
      change,
      percentChange,
      trend: change > 0 ? 'increasing' : 'decreasing'
    }
  }

  private getPreviousMonth(month: string): string {
    const [year, monthNum] = month.split('-').map(Number)
    const prevMonth = monthNum === 1 ? 12 : monthNum - 1
    const prevYear = monthNum === 1 ? year - 1 : year
    return `${prevYear}-${String(prevMonth).padStart(2, '0')}`
  }

  private async getMonthTotal(tenantId: string, month: string): Promise<number> {
    const result = await prisma.monthlyCosts.aggregate({
      where: { tenantId, month },
      _sum: { totalCost: true }
    })
    return result._sum.totalCost || 0
  }
}

// Usage in Gateway
const finops = new FinOpsTracker()

export async function handleAIRequest(req: Request) {
  const { userId, tenantId, departmentId, projectId } = await authenticate(req)

  const response = await callAI(req)

  // Record costs
  await finops.recordUsage({
    userId,
    tenantId,
    departmentId,
    projectId,
    modelUsed: response.model,
    provider: response.provider,
    timestamp: new Date(),
    inputTokens: response.usage.inputTokens,
    outputTokens: response.usage.outputTokens,
    totalCost: response.usage.totalCost
  })

  return response
}

// Generate monthly report
const report = await finops.generateCostReport('tenant-123', '2025-02')

console.log(`Total AI costs for February: $${report.summary.totalCost.toFixed(2)}`)
console.log(`Top user: ${report.summary.topUser.userId} ($${report.summary.topUser.totalCost.toFixed(2)})`)
console.log(`Top department: ${report.summary.topDepartment.department_name} ($${report.summary.topDepartment.total_cost.toFixed(2)})`)
```

**FinOps Dashboard Example**:
```
February 2025 AI Costs: $24,580

By Department:
1. Engineering:    $12,340 (50.2%)
2. Product:         $8,120 (33.0%)
3. Marketing:       $3,200 (13.0%)
4. Sales:             $920  (3.7%)

By Model:
1. claude-3-5-sonnet: $15,200 (61.9%)
2. gpt-4-turbo:        $7,380 (30.0%)
3. claude-3-haiku:     $2,000  (8.1%)

Top 5 Users:
1. alice@company.com:  $2,450
2. bob@company.com:    $2,100
3. carol@company.com:  $1,890
4. dave@company.com:   $1,650
5. eve@company.com:    $1,520

Trend: ‚Üë +18% vs January
Budget: $24,580 / $30,000 (81.9% used)
```

---

## Component 5: Zero-Downtime Deployment

**Simple Explanation**: Deploy new gateway versions without any service interruption. Users don't notice deployments happening.

### Blue-Green Deployment

```typescript
/**
 * Blue-Green Deployment Strategy
 *
 * 1. Run two identical production environments (Blue and Green)
 * 2. Deploy new version to inactive environment
 * 3. Run health checks and smoke tests
 * 4. Switch traffic to new version
 * 5. Keep old version running for quick rollback
 */

interface DeploymentConfig {
  currentEnvironment: 'blue' | 'green'
  blueEndpoint: string
  greenEndpoint: string
  loadBalancerEndpoint: string
  healthCheckPath: string
  smokeTests: SmokeTest[]
}

class ZeroDowntimeDeployer {
  private config: DeploymentConfig

  constructor(config: DeploymentConfig) {
    this.config = config
  }

  async deploy(newVersion: string): Promise<DeploymentResult> {
    console.log(`Starting zero-downtime deployment of ${newVersion}`)

    // Step 1: Determine target environment
    const targetEnv = this.config.currentEnvironment === 'blue' ? 'green' : 'blue'
    const targetEndpoint =
      targetEnv === 'blue' ? this.config.blueEndpoint : this.config.greenEndpoint

    console.log(`Deploying to ${targetEnv} environment`)

    try {
      // Step 2: Deploy new version to inactive environment
      await this.deployToEnvironment(targetEndpoint, newVersion)

      // Step 3: Wait for deployment to complete
      await this.waitForDeployment(targetEndpoint)

      // Step 4: Run health checks
      const healthCheck = await this.runHealthChecks(targetEndpoint)
      if (!healthCheck.passed) {
        throw new Error(`Health checks failed: ${healthCheck.failures.join(', ')}`)
      }

      // Step 5: Run smoke tests
      const smokeTests = await this.runSmokeTests(targetEndpoint)
      if (!smokeTests.passed) {
        throw new Error(`Smoke tests failed: ${smokeTests.failures.join(', ')}`)
      }

      // Step 6: Switch traffic (gradual cutover)
      await this.gradualCutover(targetEndpoint)

      // Step 7: Monitor for errors
      await this.monitorForErrors(targetEndpoint)

      // Step 8: Update current environment pointer
      this.config.currentEnvironment = targetEnv

      console.log(`Deployment successful! Traffic now on ${targetEnv}`)

      return {
        success: true,
        version: newVersion,
        environment: targetEnv,
        downtime: 0
      }
    } catch (error) {
      console.error('Deployment failed:', error)

      // Rollback if needed
      await this.rollback()

      return {
        success: false,
        version: newVersion,
        environment: targetEnv,
        error: error.message,
        downtime: 0
      }
    }
  }

  private async gradualCutover(targetEndpoint: string) {
    console.log('Starting gradual traffic cutover')

    // Shift traffic gradually: 10% ‚Üí 25% ‚Üí 50% ‚Üí 75% ‚Üí 100%
    const steps = [
      { percent: 10, duration: 60000 },  // 10% for 1 minute
      { percent: 25, duration: 120000 }, // 25% for 2 minutes
      { percent: 50, duration: 180000 }, // 50% for 3 minutes
      { percent: 75, duration: 180000 }, // 75% for 3 minutes
      { percent: 100, duration: 0 }      // 100% (complete)
    ]

    for (const step of steps) {
      console.log(`Shifting ${step.percent}% traffic to new version`)

      // Update load balancer weights
      await this.updateLoadBalancer(targetEndpoint, step.percent)

      if (step.duration > 0) {
        // Monitor error rates during canary period
        await this.sleep(step.duration)

        const errorRate = await this.checkErrorRate(targetEndpoint)
        if (errorRate > 0.05) {
          // 5% error rate threshold
          throw new Error(`High error rate detected: ${(errorRate * 100).toFixed(2)}%`)
        }
      }
    }

    console.log('Traffic cutover complete')
  }

  private async updateLoadBalancer(targetEndpoint: string, percent: number) {
    // Example for AWS ALB
    const oldWeight = 100 - percent
    const newWeight = percent

    // Update target group weights
    await aws.elbv2.modifyTargetGroupAttributes({
      TargetGroupArn: this.getOldTargetGroupArn(),
      Attributes: [{ Key: 'stickiness.lb_cookie.duration_seconds', Value: '0' }]
    })

    // Configure weighted routing
    // This is simplified - actual implementation varies by load balancer
  }

  private async checkErrorRate(endpoint: string): Promise<number> {
    // Query metrics from last 5 minutes
    const metrics = await cloudwatch.getMetricStatistics({
      Namespace: 'AIGateway',
      MetricName: 'ErrorRate',
      Dimensions: [{ Name: 'Environment', Value: endpoint }],
      StartTime: new Date(Date.now() - 300000),
      EndTime: new Date(),
      Period: 300,
      Statistics: ['Average']
    })

    return metrics.Datapoints[0]?.Average || 0
  }

  private async runHealthChecks(endpoint: string): Promise<TestResult> {
    const checks = [
      { name: 'HTTP Health', url: `${endpoint}/health` },
      { name: 'Database', url: `${endpoint}/health/db` },
      { name: 'Redis', url: `${endpoint}/health/redis` },
      { name: 'LLM API', url: `${endpoint}/health/llm` }
    ]

    const results = await Promise.all(
      checks.map(async (check) => {
        try {
          const response = await fetch(check.url, {
            signal: AbortSignal.timeout(5000)
          })
          return { name: check.name, passed: response.ok }
        } catch (error) {
          return { name: check.name, passed: false, error: error.message }
        }
      })
    )

    const failures = results.filter((r) => !r.passed).map((r) => r.name)

    return {
      passed: failures.length === 0,
      failures
    }
  }

  private async runSmokeTests(endpoint: string): Promise<TestResult> {
    const tests = [
      {
        name: 'Basic completion',
        run: async () => {
          const response = await fetch(`${endpoint}/v1/complete`, {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({
              model: 'claude-3-haiku-20240307',
              messages: [{ role: 'user', content: 'Hello' }],
              max_tokens: 10
            })
          })
          return response.ok
        }
      },
      {
        name: 'Rate limiting',
        run: async () => {
          const response = await fetch(`${endpoint}/v1/complete`, {
            method: 'POST',
            headers: { 'X-Test-Rate-Limit': 'true' }
          })
          return response.status === 429 // Should be rate limited
        }
      }
    ]

    const results = await Promise.all(
      tests.map(async (test) => {
        try {
          const passed = await test.run()
          return { name: test.name, passed }
        } catch (error) {
          return { name: test.name, passed: false, error: error.message }
        }
      })
    )

    const failures = results.filter((r) => !r.passed).map((r) => r.name)

    return {
      passed: failures.length === 0,
      failures
    }
  }

  private async rollback() {
    console.log('Rolling back to previous version')
    // Switch load balancer back to old environment
    await this.updateLoadBalancer(
      this.config.currentEnvironment === 'blue'
        ? this.config.blueEndpoint
        : this.config.greenEndpoint,
      100
    )
  }

  private async sleep(ms: number) {
    return new Promise((resolve) => setTimeout(resolve, ms))
  }

  private async deployToEnvironment(endpoint: string, version: string) {
    // Placeholder - actual deployment via Kubernetes, ECS, etc.
    console.log(`Deploying ${version} to ${endpoint}`)
  }

  private async waitForDeployment(endpoint: string) {
    // Wait for pods/containers to be ready
    await this.sleep(30000) // 30 seconds
  }

  private async monitorForErrors(endpoint: string) {
    // Monitor for 5 minutes after cutover
    console.log('Monitoring for errors...')
    await this.sleep(300000) // 5 minutes
  }

  private getOldTargetGroupArn(): string {
    return this.config.currentEnvironment === 'blue'
      ? 'arn:aws:elasticloadbalancing:us-east-1:123456789:targetgroup/blue/abc'
      : 'arn:aws:elasticloadbalancing:us-east-1:123456789:targetgroup/green/xyz'
  }
}

// Usage
const deployer = new ZeroDowntimeDeployer({
  currentEnvironment: 'blue',
  blueEndpoint: 'https://gateway-blue.example.com',
  greenEndpoint: 'https://gateway-green.example.com',
  loadBalancerEndpoint: 'https://gateway.example.com',
  healthCheckPath: '/health',
  smokeTests: []
})

// Deploy new version with zero downtime
const result = await deployer.deploy('v2.5.0')

console.log(`Deployment ${result.success ? 'succeeded' : 'failed'}`)
console.log(`Downtime: ${result.downtime}ms`) // 0ms!
```

---

## Production Metrics

**Real-World Performance** (from production Global AI Gateways):

| Metric | Target | Actual | Notes |
|--------|--------|--------|-------|
| Uptime | 99.99% | 99.995% | 4.4 min downtime/month |
| P95 Latency | &lt;500ms | 342ms | Gateway overhead |
| Rate Limiting | &lt;5ms | 3.2ms | Redis lookup |
| Cache Hit Rate | &gt;70% | 76% | $18K/mo savings |
| Cost Tracking | 100% | 100% | Per-user attribution |
| Failover Time | &lt;1s | 450ms | Regional failover |
| Deployment Time | &lt;10min | 7.3min | Zero downtime |

**Cost Savings** (per month):
- Semantic caching: $18,000 (76% hit rate)
- Intelligent routing: $8,500 (route to cheaper models)
- Rate limiting: $3,200 (prevent abuse)
- **Total**: $29,700/month savings

**Scale** (handling 1M+ users):
- 50K requests/second peak
- 3 regions (us-east-1, us-west-2, eu-west-1)
- 24 gateway instances per region
- Auto-scaling 10-100 instances
- Redis cluster: 12 nodes

---

## Best Practices

1. **Rate Limiting**: Implement at multiple levels (global, tenant, user)
2. **Failover**: Deploy across 3+ regions for 99.99% uptime
3. **Caching**: Multi-tier (memory, Redis, semantic) for 70%+ hit rate
4. **Cost Tracking**: Record every request with full dimensions
5. **Monitoring**: Track latency, error rate, cost per endpoint
6. **Deployment**: Use blue-green or canary for zero downtime
7. **Security**: Encrypt in transit, rate limit, audit all requests

## Common Pitfalls

1. **Single Region**: Regional outage = total outage
   - **Fix**: Deploy across 3+ regions with automatic failover

2. **No Cost Attribution**: Can't track who's spending what
   - **Fix**: Record tenantId, userId, departmentId on every request

3. **Synchronous Failover**: Slow failover causes timeouts
   - **Fix**: Use async health checks, fail fast

4. **No Rate Limiting**: Users can rack up unlimited costs
   - **Fix**: Implement hierarchical rate limiting (global + tenant + user)

5. **Cache Stampede**: All requests hit backend when cache expires
   - **Fix**: Use probabilistic early expiration, stagger TTLs

## Resources

- [API Gateway Patterns](https://microservices.io/patterns/apigateway.html)
- [Rate Limiting Algorithms](https://redis.io/docs/manual/patterns/rate-limiting/)
- [Multi-Region Architecture](https://aws.amazon.com/blogs/architecture/disaster-recovery-dr-architecture-on-aws/)
- [FinOps for AI](https://www.finops.org/framework/domains/)
- [Zero-Downtime Deployments](https://kubernetes.io/docs/tutorials/kubernetes-basics/update/update-intro/)
