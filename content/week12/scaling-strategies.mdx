---
title: "Scaling AI Systems to Production"
description: "Scale AI applications to handle enterprise load"
estimatedMinutes: 40
---

# Scaling AI Systems to Production

## Why Scaling Matters

**Simple Explanation**: Your AI app might work great with 10 users, but what happens with 10,000 users? Or 1 million? Scaling is about making your system handle increased load without breaking or becoming too expensive.

**Scaling Challenges for AI**:
1. **LLM Latency**: AI responses take 2-10 seconds (vs &lt;100ms for traditional APIs)
2. **Cost Per Request**: $0.001-0.05 per request (vs $0.000001 for database queries)
3. **Rate Limits**: Most AI APIs limit requests per minute
4. **Stateful Conversations**: Need to maintain context across requests
5. **Unpredictable Load**: Usage patterns can be spiky and unpredictable

**Real Example**: AI chatbot
- **Day 1**: 100 users → Works fine → $10/day
- **Week 2**: Goes viral → 10,000 users → API rate limits hit → $5,000/day → System crashes
- **Need**: Scalable architecture that handles growth without manual intervention

## Load Balancing

**Simple Explanation**: Don't put all your eggs in one basket. Use multiple LLM providers and distribute requests across them.

```typescript
interface ProviderConfig {
  name: string
  client: any          // The API client (Anthropic, OpenAI, etc.)
  weight: number       // 0-100, higher = more traffic
  rateLimit: number    // requests per minute
  cost: number         // cost per 1K tokens
  latency: number      // average ms
}

class LoadBalancedLLM {
  private providers: ProviderConfig[] = [
    {
      name: 'anthropic',
      client: anthropic,
      weight: 60,
      rateLimit: 1000,
      cost: 0.015,
      latency: 2500
    },
    {
      name: 'openai',
      client: openai,
      weight: 40,
      rateLimit: 500,
      cost: 0.01,
      latency: 3000
    }
  ]

  private usage = new Map<string, ProviderUsage>()

  async complete(prompt: string, options?: CompletionOptions): Promise<string> {
    // Select provider based on weights and current usage
    const provider = this.selectProvider()

    try {
      const startTime = Date.now()

      // Make request to selected provider
      const response = await this.callProvider(provider, prompt, options)

      // Track success
      this.recordSuccess(provider.name, Date.now() - startTime)

      return response
    } catch (error) {
      console.error(`${provider.name} failed:`, error)

      // Try fallback provider
      return await this.fallback(prompt, provider.name, options)
    }
  }

  private selectProvider(): ProviderConfig {
    // Get current usage for each provider
    const now = Date.now()
    const windowMs = 60000 // 1 minute

    // Filter out providers that are at rate limit
    const availableProviders = this.providers.filter(p => {
      const usage = this.usage.get(p.name)
      if (!usage) return true

      const recentRequests = usage.requests.filter(
        time => now - time < windowMs
      )

      return recentRequests.length < p.rateLimit
    })

    if (availableProviders.length === 0) {
      throw new Error('All providers at rate limit')
    }

    // Weighted random selection
    const totalWeight = availableProviders.reduce(
      (sum, p) => sum + p.weight,
      0
    )

    let random = Math.random() * totalWeight

    for (const provider of availableProviders) {
      random -= provider.weight
      if (random <= 0) return provider
    }

    return availableProviders[0]
  }

  private async callProvider(
    provider: ProviderConfig,
    prompt: string,
    options?: CompletionOptions
  ): Promise<string> {
    // Record request for rate limiting
    this.recordRequest(provider.name)

    if (provider.name === 'anthropic') {
      const response = await provider.client.messages.create({
        model: options?.model || 'claude-3-5-sonnet-20241022',
        max_tokens: options?.maxTokens || 1024,
        messages: [{ role: 'user', content: prompt }]
      })
      return response.content[0].text
    } else if (provider.name === 'openai') {
      const response = await provider.client.chat.completions.create({
        model: options?.model || 'gpt-4-turbo',
        max_tokens: options?.maxTokens || 1024,
        messages: [{ role: 'user', content: prompt }]
      })
      return response.choices[0].message.content
    }

    throw new Error(`Unknown provider: ${provider.name}`)
  }

  private async fallback(
    prompt: string,
    failedProvider: string,
    options?: CompletionOptions
  ): Promise<string> {
    // Try other providers
    const otherProviders = this.providers.filter(
      p => p.name !== failedProvider
    )

    for (const provider of otherProviders) {
      try {
        console.log(`Falling back to ${provider.name}`)
        return await this.callProvider(provider, prompt, options)
      } catch (error) {
        console.error(`${provider.name} also failed:`, error)
        continue
      }
    }

    throw new Error('All providers failed')
  }

  private recordRequest(providerName: string) {
    if (!this.usage.has(providerName)) {
      this.usage.set(providerName, { requests: [], errors: 0, totalLatency: 0 })
    }

    const usage = this.usage.get(providerName)!
    usage.requests.push(Date.now())

    // Clean old requests (older than 1 minute)
    const now = Date.now()
    usage.requests = usage.requests.filter(time => now - time < 60000)
  }

  private recordSuccess(providerName: string, latency: number) {
    const usage = this.usage.get(providerName)!
    usage.totalLatency += latency
  }

  // Monitor provider health
  getProviderStats(): ProviderStats[] {
    return this.providers.map(p => {
      const usage = this.usage.get(p.name)
      const requestCount = usage?.requests.length || 0
      const errorRate = usage ? usage.errors / Math.max(usage.requests.length, 1) : 0
      const avgLatency = usage ? usage.totalLatency / Math.max(usage.requests.length, 1) : 0

      return {
        name: p.name,
        requestsPerMinute: requestCount,
        rateLimit: p.rateLimit,
        utilizationPercent: (requestCount / p.rateLimit) * 100,
        errorRate: errorRate,
        avgLatency: avgLatency
      }
    })
  }
}

// Usage
const llm = new LoadBalancedLLM()

// 60% of requests go to Anthropic, 40% to OpenAI
// Automatic fallback if one provider fails
// Rate limiting handled automatically

const response = await llm.complete('Explain load balancing')

console.log(llm.getProviderStats())
// [
//   {
//     name: 'anthropic',
//     requestsPerMinute: 45,
//     rateLimit: 1000,
//     utilizationPercent: 4.5,
//     errorRate: 0.01,
//     avgLatency: 2500
//   },
//   {
//     name: 'openai',
//     requestsPerMinute: 30,
//     rateLimit: 500,
//     utilizationPercent: 6,
//     errorRate: 0.02,
//     avgLatency: 3000
//   }
// ]
```

**Benefits**:
- **Redundancy**: If one provider goes down, traffic shifts to others
- **Rate Limit Avoidance**: Distribute load across multiple rate limits
- **Cost Optimization**: Route to cheaper providers when possible
- **Performance**: Route to faster providers for time-sensitive requests

## Auto-Scaling

**Simple Explanation**: Automatically add more servers when traffic increases, remove them when traffic decreases. Like a restaurant adding more waiters during rush hour.

```typescript
interface ScalingConfig {
  minPods: number          // Always keep this many running
  maxPods: number          // Never exceed this many
  targetCPU: number        // Scale up if CPU > this percent
  targetLatency: number    // Scale up if latency > this ms
  scaleUpCooldown: number  // Wait this long before scaling up again (ms)
  scaleDownCooldown: number // Wait this long before scaling down (ms)
}

class AutoScaler {
  private config: ScalingConfig = {
    minPods: 2,
    maxPods: 20,
    targetCPU: 70,
    targetLatency: 500,
    scaleUpCooldown: 60000,   // 1 minute
    scaleDownCooldown: 300000 // 5 minutes
  }

  private lastScaleUp = 0
  private lastScaleDown = 0

  async checkAndScale(metrics: SystemMetrics): Promise<ScalingAction> {
    const currentPods = await this.getCurrentPodCount()
    const now = Date.now()

    // Decide if we need to scale
    const decision = this.makeScalingDecision(metrics, currentPods)

    if (decision.action === 'scale_up') {
      // Check cooldown
      if (now - this.lastScaleUp < this.config.scaleUpCooldown) {
        console.log('Scale up on cooldown, skipping')
        return { action: 'none', reason: 'cooldown' }
      }

      const newPodCount = Math.min(
        currentPods + decision.podDelta,
        this.config.maxPods
      )

      if (newPodCount === currentPods) {
        return { action: 'none', reason: 'at_max_capacity' }
      }

      await this.scaleUp(newPodCount)
      this.lastScaleUp = now

      return {
        action: 'scale_up',
        from: currentPods,
        to: newPodCount,
        reason: decision.reason
      }
    }

    if (decision.action === 'scale_down') {
      // Check cooldown (longer for scale down to avoid flapping)
      if (now - this.lastScaleDown < this.config.scaleDownCooldown) {
        console.log('Scale down on cooldown, skipping')
        return { action: 'none', reason: 'cooldown' }
      }

      const newPodCount = Math.max(
        currentPods - decision.podDelta,
        this.config.minPods
      )

      if (newPodCount === currentPods) {
        return { action: 'none', reason: 'at_min_capacity' }
      }

      await this.scaleDown(newPodCount)
      this.lastScaleDown = now

      return {
        action: 'scale_down',
        from: currentPods,
        to: newPodCount,
        reason: decision.reason
      }
    }

    return { action: 'none', reason: 'metrics_within_target' }
  }

  private makeScalingDecision(
    metrics: SystemMetrics,
    currentPods: number
  ): ScalingDecision {
    // Multiple triggers for scaling up
    const triggers: ScalingTrigger[] = []

    // Trigger 1: High CPU
    if (metrics.avgCPU > this.config.targetCPU) {
      triggers.push({
        name: 'high_cpu',
        severity: (metrics.avgCPU - this.config.targetCPU) / 100,
        desiredPods: Math.ceil(currentPods * (metrics.avgCPU / this.config.targetCPU))
      })
    }

    // Trigger 2: High latency
    if (metrics.p95Latency > this.config.targetLatency) {
      triggers.push({
        name: 'high_latency',
        severity: (metrics.p95Latency - this.config.targetLatency) / this.config.targetLatency,
        desiredPods: Math.ceil(currentPods * 1.5) // 50% more pods
      })
    }

    // Trigger 3: Queue depth
    if (metrics.queueDepth > 100) {
      triggers.push({
        name: 'high_queue_depth',
        severity: metrics.queueDepth / 100,
        desiredPods: Math.ceil(currentPods + (metrics.queueDepth / 50))
      })
    }

    // Trigger 4: Error rate
    if (metrics.errorRate > 0.05) {  // 5%
      triggers.push({
        name: 'high_error_rate',
        severity: metrics.errorRate,
        desiredPods: Math.ceil(currentPods * 1.3) // 30% more pods
      })
    }

    // Any triggers? Scale up!
    if (triggers.length > 0) {
      const maxDesiredPods = Math.max(...triggers.map(t => t.desiredPods))
      const podDelta = maxDesiredPods - currentPods

      return {
        action: 'scale_up',
        podDelta: podDelta,
        reason: triggers.map(t => t.name).join(', ')
      }
    }

    // Check for scale down conditions
    const shouldScaleDown =
      metrics.avgCPU < (this.config.targetCPU * 0.5) &&  // CPU < 35%
      metrics.p95Latency < (this.config.targetLatency * 0.7) &&  // Latency good
      metrics.queueDepth < 10 &&  // Queue empty
      currentPods > this.config.minPods

    if (shouldScaleDown) {
      // Conservative scale down (1 pod at a time)
      return {
        action: 'scale_down',
        podDelta: 1,
        reason: 'low_utilization'
      }
    }

    return {
      action: 'none',
      podDelta: 0,
      reason: 'metrics_within_target'
    }
  }

  private async getCurrentPodCount(): Promise<number> {
    // In Kubernetes, would query the API
    // For example: kubectl get deployment my-ai-api -o json | jq '.spec.replicas'
    return 3  // Placeholder
  }

  private async scaleUp(newPodCount: number) {
    console.log(`Scaling up to ${newPodCount} pods`)
    // In Kubernetes: kubectl scale deployment my-ai-api --replicas=${newPodCount}
    // In AWS ECS: Update service desired count
    // In cloud functions: Increase max instances
  }

  private async scaleDown(newPodCount: number) {
    console.log(`Scaling down to ${newPodCount} pods`)
    // Same as scale up, but with lower number
  }
}

// Kubernetes YAML for HPA (Horizontal Pod Autoscaler)
```

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: ai-api-scaler
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: ai-api
  minReplicas: 2
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Pods
    pods:
      metric:
        name: http_request_duration_p95
      target:
        type: AverageValue
        averageValue: "500m"  # 500ms
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 50  # Max 50% increase at once
        periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300  # Wait 5 min before scale down
      policies:
      - type: Pods
        value: 1  # Remove 1 pod at a time
        periodSeconds: 60
```

**Real-World Scaling Example**:
```
Time    Traffic    CPU    Pods    Action
08:00   100 req/m  30%    2       None (baseline)
09:00   500 req/m  75%    2       Scale up to 4 (CPU > 70%)
09:05   500 req/m  40%    4       None (cooldown)
12:00   2000 req/m 85%    4       Scale up to 8 (high CPU + queue)
14:00   1500 req/m 60%    8       None (metrics okay)
18:00   300 req/m  20%    8       Scale down to 4 (low utilization)
23:00   50 req/m   10%    4       Scale down to 2 (min pods)
```

## Caching at Scale

**Simple Explanation**: Store answers to common questions so you don't have to call the expensive AI API every time.

```typescript
// Multi-tier caching for maximum performance
class CacheHierarchy {
  private memoryCache: LRUCache       // L1: In-memory (fastest, smallest)
  private redisCache: RedisClient     // L2: Redis (fast, medium)
  private semanticCache: VectorDB     // L3: Semantic (slower, largest)

  constructor() {
    // L1: In-memory cache - 1000 items, < 1ms access
    this.memoryCache = new LRUCache({ max: 1000 })

    // L2: Redis cache - 100K items, ~5ms access
    this.redisCache = new Redis({
      host: process.env.REDIS_HOST,
      maxRetriesPerRequest: 3
    })

    // L3: Semantic cache - unlimited, ~50ms access
    this.semanticCache = new ChromaDB()
  }

  async get(query: string): Promise<CachedResponse | null> {
    const key = this.hashQuery(query)

    // L1: Check memory cache (< 1ms)
    const memoryResult = this.memoryCache.get(key)
    if (memoryResult) {
      console.log('L1 cache hit (memory)')
      return { ...memoryResult, cacheLevel: 'L1' }
    }

    // L2: Check Redis (~ 5ms)
    const redisResult = await this.redisCache.get(key)
    if (redisResult) {
      console.log('L2 cache hit (Redis)')
      const parsed = JSON.parse(redisResult)

      // Backfill L1
      this.memoryCache.set(key, parsed)

      return { ...parsed, cacheLevel: 'L2' }
    }

    // L3: Check semantic cache (~50ms)
    const semanticResult = await this.findSimilarQuery(query)
    if (semanticResult && semanticResult.similarity > 0.95) {
      console.log('L3 cache hit (semantic)')

      // Backfill L2 and L1
      await this.redisCache.setex(key, 3600, JSON.stringify(semanticResult.response))
      this.memoryCache.set(key, semanticResult.response)

      return { ...semanticResult.response, cacheLevel: 'L3' }
    }

    // Cache miss - need to call LLM
    console.log('Cache miss - calling LLM')
    return null
  }

  async set(query: string, response: any, ttl: number = 3600) {
    const key = this.hashQuery(query)

    // Store in all cache levels
    this.memoryCache.set(key, response)
    await this.redisCache.setex(key, ttl, JSON.stringify(response))
    await this.storeSemanticCache(query, response)
  }

  private async findSimilarQuery(query: string): Promise<SemanticMatch | null> {
    // Embed the query
    const embedding = await this.embed(query)

    // Search for similar queries
    const results = await this.semanticCache.query({
      queryEmbeddings: [embedding],
      nResults: 1
    })

    if (results.length === 0) return null

    const topMatch = results[0]

    return {
      similarity: topMatch.distance,
      query: topMatch.document,
      response: topMatch.metadata.response
    }
  }

  private async storeSemanticCache(query: string, response: any) {
    const embedding = await this.embed(query)

    await this.semanticCache.add({
      ids: [this.hashQuery(query)],
      embeddings: [embedding],
      documents: [query],
      metadatas: [{ response, timestamp: Date.now() }]
    })
  }

  private hashQuery(query: string): string {
    // Simple hash for exact matching
    return crypto.createHash('md5').update(query).digest('hex')
  }

  private async embed(text: string): Promise<number[]> {
    // Use embedding model (e.g., OpenAI ada-002)
    const response = await openai.embeddings.create({
      model: 'text-embedding-ada-002',
      input: text
    })
    return response.data[0].embedding
  }
}

// Usage with metrics
const cache = new CacheHierarchy()

async function getAnswer(query: string): Promise<string> {
  // Try cache first
  const cached = await cache.get(query)

  if (cached) {
    console.log(`Cache hit! Saved $${COST_PER_LLM_CALL}`)
    return cached.response
  }

  // Cache miss - call LLM
  const response = await llm.complete(query)

  // Store in cache
  await cache.set(query, response)

  return response
}

// Performance over time
// Day 1:  Cache hit rate 20% → Save $200/day
// Day 7:  Cache hit rate 45% → Save $900/day
// Day 30: Cache hit rate 75% → Save $3,000/day
```

**Cache Hit Rate Impact**:
```
Without cache:
- 10,000 requests/day
- $0.05 per request
- Cost: $500/day = $15,000/month

With 75% cache hit rate:
- 2,500 LLM calls/day (25%)
- 7,500 cache hits/day (75%)
- Cost: $125/day = $3,750/month
- Savings: $11,250/month (75% reduction!)
```

## Rate Limiting at Scale

**Simple Explanation**: Prevent users from overusing your AI API and racking up costs. Like a speed limit on a highway.

```typescript
// Distributed rate limiting using Redis
class DistributedRateLimiter {
  private redis: RedisClient

  async checkLimit(
    userId: string,
    tier: 'free' | 'pro' | 'enterprise'
  ): Promise<RateLimitResult> {
    const limits = {
      free: { requests: 10, window: 3600 },       // 10 per hour
      pro: { requests: 1000, window: 3600 },      // 1000 per hour
      enterprise: { requests: 100000, window: 3600 } // 100K per hour
    }

    const limit = limits[tier]
    const key = `ratelimit:${userId}:${tier}`

    // Increment counter
    const count = await this.redis.incr(key)

    // Set expiry on first request
    if (count === 1) {
      await this.redis.expire(key, limit.window)
    }

    // Check limit
    const allowed = count <= limit.requests

    if (!allowed) {
      const ttl = await this.redis.ttl(key)
      return {
        allowed: false,
        remaining: 0,
        resetIn: ttl,
        retryAfter: ttl
      }
    }

    return {
      allowed: true,
      remaining: limit.requests - count,
      resetIn: await this.redis.ttl(key),
      retryAfter: 0
    }
  }
}

// Usage in API route
export async function POST(req: Request) {
  const { userId, tier } = await getUserInfo(req)

  const rateLimiter = new DistributedRateLimiter()
  const limitCheck = await rateLimiter.checkLimit(userId, tier)

  if (!limitCheck.allowed) {
    return NextResponse.json(
      {
        error: 'Rate limit exceeded',
        retryAfter: limitCheck.retryAfter
      },
      {
        status: 429,
        headers: {
          'X-RateLimit-Remaining': '0',
          'X-RateLimit-Reset': limitCheck.resetIn.toString(),
          'Retry-After': limitCheck.retryAfter.toString()
        }
      }
    )
  }

  // Process request...
  const response = await processAIRequest(req)

  return NextResponse.json(response, {
    headers: {
      'X-RateLimit-Remaining': limitCheck.remaining.toString(),
      'X-RateLimit-Reset': limitCheck.resetIn.toString()
    }
  })
}
```

## Best Practices

1. **Monitor Everything**: Track latency, costs, errors, cache hit rates
2. **Start Conservative**: Scale up gradually, don't over-provision
3. **Use Circuit Breakers**: Stop calling failing services
4. **Implement Graceful Degradation**: Serve cached/partial results when backends are slow
5. **Load Test Before Launch**: Know your breaking points
6. **Set Cost Alerts**: Get notified if costs spike unexpectedly

## Resources
- [Scaling Strategies](https://aws.amazon.com/architecture/scalability/)
- [Production Best Practices](https://kubernetes.io/docs/concepts/configuration/overview/)
- [Caching Strategies](https://redis.io/docs/manual/patterns/)
