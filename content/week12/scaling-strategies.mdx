---
title: "Scaling AI Systems to Production"
description: "Scale AI applications to handle enterprise load"
estimatedMinutes: 40
---

# Scaling AI Systems to Production

## Why Scaling Matters

**Simple Explanation**: Your AI app might work great with 10 users, but what happens with 10,000 users? Or 1 million? Scaling is about making your system handle increased load without breaking or becoming too expensive.

**Scaling Challenges for AI**:
1. **LLM Latency**: AI responses take 2-10 seconds (vs &lt;100ms for traditional APIs)
2. **Cost Per Request**: $0.001-0.05 per request (vs $0.000001 for database queries)
3. **Rate Limits**: Most AI APIs limit requests per minute
4. **Stateful Conversations**: Need to maintain context across requests
5. **Unpredictable Load**: Usage patterns can be spiky and unpredictable

**Real Example**: AI chatbot
- **Day 1**: 100 users → Works fine → $10/day
- **Week 2**: Goes viral → 10,000 users → API rate limits hit → $5,000/day → System crashes
- **Need**: Scalable architecture that handles growth without manual intervention

**Architect's Tip — Asynchronous Streaming + Optimistic State (The Latency Decoupler)**: "To handle 1 million users, you cannot hold an open HTTP connection for 10 seconds while the LLM generates a response. An Architect uses **Server-Sent Events (SSE)** to stream tokens as they're generated, and an **Event-Driven Architecture** to decouple API latency from LLM generation time. When the user sends a message, write it to your State Store immediately and return an ACK to the UI. The LLM processing happens in a background worker. This uncouples your web server from the LLM bottleneck, allowing it to handle **10x more concurrent users** with the same infrastructure."

```typescript
/**
 * Asynchronous Streaming + Optimistic State Updates
 *
 * Problem: A synchronous request/response model holds an HTTP
 * connection open for 2-10 seconds while the LLM generates.
 * At 10,000 concurrent users × 5 seconds each = 50,000 open
 * connections. Most web servers cap at 10,000 connections.
 *
 * Solution: Decouple the request lifecycle into 3 phases:
 * 1. ACK Phase (< 50ms): Accept message, store in state, return ACK
 * 2. Processing Phase (background): LLM generates in a worker
 * 3. Streaming Phase (SSE): Push tokens to the client as they arrive
 *
 * Result: Web server handles ACK in 50ms (not 5,000ms),
 * freeing the connection for other users. 10x throughput.
 *
 * Interview Defense: "We decouple API latency from LLM generation
 * time using SSE and background workers. Our web servers return
 * an ACK in 50ms while the LLM processes asynchronously. This
 * lets us handle 100K concurrent users on infrastructure that
 * would only support 10K with synchronous requests."
 */

import { Redis } from 'ioredis'

const redis = new Redis(process.env.REDIS_URL!)

interface ConversationMessage {
  id: string
  conversationId: string
  role: 'user' | 'assistant'
  content: string
  status: 'pending' | 'streaming' | 'complete' | 'failed'
  timestamp: number
}

// Phase 1: Optimistic State Update (< 50ms)
async function handleUserMessage(
  conversationId: string,
  userMessage: string
): Promise<{ messageId: string; status: 'accepted' }> {
  const messageId = crypto.randomUUID()

  // Write user message to state store immediately
  await redis.rpush(
    `conversation:${conversationId}`,
    JSON.stringify({
      id: messageId,
      conversationId,
      role: 'user',
      content: userMessage,
      status: 'complete',
      timestamp: Date.now()
    })
  )

  // Create placeholder for assistant response
  const assistantMessageId = crypto.randomUUID()
  await redis.rpush(
    `conversation:${conversationId}`,
    JSON.stringify({
      id: assistantMessageId,
      conversationId,
      role: 'assistant',
      content: '',
      status: 'pending',
      timestamp: Date.now()
    })
  )

  // Enqueue LLM processing (background worker picks this up)
  await redis.lpush('llm:queue', JSON.stringify({
    conversationId,
    userMessageId: messageId,
    assistantMessageId,
    userMessage
  }))

  // Return ACK immediately (< 50ms total)
  return { messageId: assistantMessageId, status: 'accepted' }
}

// Phase 2 + 3: Background Worker with SSE Streaming
async function processLLMQueue() {
  while (true) {
    // Block until a job is available
    const job = await redis.brpop('llm:queue', 0)
    if (!job) continue

    const task = JSON.parse(job[1])

    // Stream tokens via SSE channel
    const stream = await anthropic.messages.stream({
      model: 'claude-sonnet-4-5-20250929',
      max_tokens: 1024,
      messages: [{ role: 'user', content: task.userMessage }]
    })

    let fullContent = ''

    for await (const chunk of stream) {
      if (chunk.type === 'content_block_delta') {
        fullContent += chunk.delta.text

        // Publish each token to SSE channel
        await redis.publish(
          `sse:${task.conversationId}`,
          JSON.stringify({
            messageId: task.assistantMessageId,
            delta: chunk.delta.text,
            status: 'streaming'
          })
        )
      }
    }

    // Mark message as complete in state store
    await redis.set(
      `message:${task.assistantMessageId}`,
      JSON.stringify({ content: fullContent, status: 'complete' })
    )
  }
}

// Throughput comparison:
//
// SYNCHRONOUS (hold connection open):
//   Request duration: 5,000ms (LLM generation)
//   Max connections: 10,000
//   Max concurrent users: 10,000
//   Throughput: 2,000 req/s (10K connections / 5s)
//
// ASYNC + SSE (decouple with background worker):
//   ACK duration: 50ms
//   Max connections: 10,000 (same infrastructure)
//   Max concurrent users: 100,000+ (ACK frees connection)
//   Throughput: 200,000 req/s (10K connections / 0.05s)
//
// Same infrastructure, 100x more throughput for the ACK phase
// LLM generation still takes 5s, but it's in a background worker
// Users see tokens streaming in real-time via SSE
```

## Load Balancing

**Simple Explanation**: Don't put all your eggs in one basket. Use multiple LLM providers and distribute requests across them.

```typescript
interface ProviderConfig {
  name: string
  client: any          // The API client (Anthropic, OpenAI, etc.)
  weight: number       // 0-100, higher = more traffic
  rateLimit: number    // requests per minute
  cost: number         // cost per 1K tokens
  latency: number      // average ms
}

class LoadBalancedLLM {
  private providers: ProviderConfig[] = [
    {
      name: 'anthropic',
      client: anthropic,
      weight: 60,
      rateLimit: 1000,
      cost: 0.015,
      latency: 2500
    },
    {
      name: 'openai',
      client: openai,
      weight: 40,
      rateLimit: 500,
      cost: 0.01,
      latency: 3000
    }
  ]

  private usage = new Map<string, ProviderUsage>()

  async complete(prompt: string, options?: CompletionOptions): Promise<string> {
    // Select provider based on weights and current usage
    const provider = this.selectProvider()

    try {
      const startTime = Date.now()

      // Make request to selected provider
      const response = await this.callProvider(provider, prompt, options)

      // Track success
      this.recordSuccess(provider.name, Date.now() - startTime)

      return response
    } catch (error) {
      console.error(`${provider.name} failed:`, error)

      // Try fallback provider
      return await this.fallback(prompt, provider.name, options)
    }
  }

  private selectProvider(): ProviderConfig {
    // Get current usage for each provider
    const now = Date.now()
    const windowMs = 60000 // 1 minute

    // Filter out providers that are at rate limit
    const availableProviders = this.providers.filter(p => {
      const usage = this.usage.get(p.name)
      if (!usage) return true

      const recentRequests = usage.requests.filter(
        time => now - time < windowMs
      )

      return recentRequests.length < p.rateLimit
    })

    if (availableProviders.length === 0) {
      throw new Error('All providers at rate limit')
    }

    // Weighted random selection
    const totalWeight = availableProviders.reduce(
      (sum, p) => sum + p.weight,
      0
    )

    let random = Math.random() * totalWeight

    for (const provider of availableProviders) {
      random -= provider.weight
      if (random &lt;= 0) return provider
    }

    return availableProviders[0]
  }

  private async callProvider(
    provider: ProviderConfig,
    prompt: string,
    options?: CompletionOptions
  ): Promise<string> {
    // Record request for rate limiting
    this.recordRequest(provider.name)

    if (provider.name === 'anthropic') {
      const response = await provider.client.messages.create({
        model: options?.model || 'claude-3-5-sonnet-20241022',
        max_tokens: options?.maxTokens || 1024,
        messages: [{ role: 'user', content: prompt }]
      })
      return response.content[0].text
    } else if (provider.name === 'openai') {
      const response = await provider.client.chat.completions.create({
        model: options?.model || 'gpt-4-turbo',
        max_tokens: options?.maxTokens || 1024,
        messages: [{ role: 'user', content: prompt }]
      })
      return response.choices[0].message.content
    }

    throw new Error(`Unknown provider: ${provider.name}`)
  }

  private async fallback(
    prompt: string,
    failedProvider: string,
    options?: CompletionOptions
  ): Promise<string> {
    // Try other providers
    const otherProviders = this.providers.filter(
      p => p.name !== failedProvider
    )

    for (const provider of otherProviders) {
      try {
        console.log(`Falling back to ${provider.name}`)
        return await this.callProvider(provider, prompt, options)
      } catch (error) {
        console.error(`${provider.name} also failed:`, error)
        continue
      }
    }

    throw new Error('All providers failed')
  }

  private recordRequest(providerName: string) {
    if (!this.usage.has(providerName)) {
      this.usage.set(providerName, { requests: [], errors: 0, totalLatency: 0 })
    }

    const usage = this.usage.get(providerName)!
    usage.requests.push(Date.now())

    // Clean old requests (older than 1 minute)
    const now = Date.now()
    usage.requests = usage.requests.filter(time => now - time &lt; 60000)
  }

  private recordSuccess(providerName: string, latency: number) {
    const usage = this.usage.get(providerName)!
    usage.totalLatency += latency
  }

  // Monitor provider health
  getProviderStats(): ProviderStats[] {
    return this.providers.map(p => {
      const usage = this.usage.get(p.name)
      const requestCount = usage?.requests.length || 0
      const errorRate = usage ? usage.errors / Math.max(usage.requests.length, 1) : 0
      const avgLatency = usage ? usage.totalLatency / Math.max(usage.requests.length, 1) : 0

      return {
        name: p.name,
        requestsPerMinute: requestCount,
        rateLimit: p.rateLimit,
        utilizationPercent: (requestCount / p.rateLimit) * 100,
        errorRate: errorRate,
        avgLatency: avgLatency
      }
    })
  }
}

// Usage
const llm = new LoadBalancedLLM()

// 60% of requests go to Anthropic, 40% to OpenAI
// Automatic fallback if one provider fails
// Rate limiting handled automatically

const response = await llm.complete('Explain load balancing')

console.log(llm.getProviderStats())
// [
//   {
//     name: 'anthropic',
//     requestsPerMinute: 45,
//     rateLimit: 1000,
//     utilizationPercent: 4.5,
//     errorRate: 0.01,
//     avgLatency: 2500
//   },
//   {
//     name: 'openai',
//     requestsPerMinute: 30,
//     rateLimit: 500,
//     utilizationPercent: 6,
//     errorRate: 0.02,
//     avgLatency: 3000
//   }
// ]
```

**Benefits**:
- **Redundancy**: If one provider goes down, traffic shifts to others
- **Rate Limit Avoidance**: Distribute load across multiple rate limits
- **Cost Optimization**: Route to cheaper providers when possible
- **Performance**: Route to faster providers for time-sensitive requests

**Architect's Tip — Dynamic Provider Switching (The Tiered Failover Strategy)**: "Never rely on a single LLM provider for a production-critical feature. An Architect implements a **Tiered Provider Strategy**: if your primary model (e.g., Claude Sonnet) returns a **429 (Rate Limit)** or **503 (Overloaded)**, your load balancer should instantly failover to an equivalent secondary model (e.g., GPT-4o), or a faster, cheaper **Safe Fallback** (e.g., Claude Haiku). This ensures **99.99% availability** even when a major AI provider suffers a regional outage. The key is that the failover is **automatic and invisible to the user** — they never see an error, just a slightly different model behind the scenes."

```typescript
/**
 * Dynamic Provider Switching with Tiered Failover
 *
 * Problem: Single-provider architectures fail catastrophically.
 * When OpenAI has a 2-hour outage (happens ~monthly), your
 * entire product goes down. When you hit rate limits during
 * a traffic spike, users see errors.
 *
 * Solution: Tiered failover with automatic 429/503 detection.
 * Primary → Secondary → Safe Fallback → Cached Response.
 * Each tier is pre-configured and tested, so failover is instant.
 *
 * Interview Defense: "We maintain a 3-tier provider strategy.
 * If our primary returns a 429 or 503, we failover to the
 * secondary in under 100ms. If both are down, we serve from
 * our safe fallback tier. Our users have never seen a
 * provider outage because the failover is invisible."
 */

interface ProviderTier {
  name: string
  model: string
  priority: number          // 1 = primary, 2 = secondary, 3 = fallback
  costPer1KTokens: number
  avgLatencyMs: number
  isHealthy: boolean
  consecutiveFailures: number
  lastFailure: number
}

class TieredProviderRouter {
  private tiers: ProviderTier[] = [
    {
      name: 'primary',
      model: 'claude-sonnet-4-5-20250929',
      priority: 1,
      costPer1KTokens: 0.003,
      avgLatencyMs: 1200,
      isHealthy: true,
      consecutiveFailures: 0,
      lastFailure: 0
    },
    {
      name: 'secondary',
      model: 'gpt-4o',
      priority: 2,
      costPer1KTokens: 0.005,
      avgLatencyMs: 1500,
      isHealthy: true,
      consecutiveFailures: 0,
      lastFailure: 0
    },
    {
      name: 'safe-fallback',
      model: 'claude-haiku-4-5-20251001',
      priority: 3,
      costPer1KTokens: 0.0008,
      avgLatencyMs: 400,
      isHealthy: true,
      consecutiveFailures: 0,
      lastFailure: 0
    }
  ]

  async route(prompt: string): Promise<{
    response: string
    provider: string
    failoverUsed: boolean
  }> {
    // Sort by priority, filter to healthy providers
    const available = this.tiers
      .filter(t => t.isHealthy)
      .sort((a, b) => a.priority - b.priority)

    for (const tier of available) {
      try {
        const response = await this.callProvider(tier, prompt)

        // Reset failure count on success
        tier.consecutiveFailures = 0

        return {
          response,
          provider: tier.name,
          failoverUsed: tier.priority > 1
        }
      } catch (error: any) {
        const statusCode = error?.status || error?.statusCode

        // 429 = Rate Limited, 503 = Overloaded, 529 = Overloaded
        if ([429, 503, 529].includes(statusCode)) {
          tier.consecutiveFailures++

          // Mark unhealthy after 3 consecutive failures
          if (tier.consecutiveFailures >= 3) {
            tier.isHealthy = false
            tier.lastFailure = Date.now()

            // Schedule health check recovery (try again in 60s)
            setTimeout(() => {
              tier.isHealthy = true
              tier.consecutiveFailures = 0
            }, 60_000)
          }

          // Continue to next tier (automatic failover)
          continue
        }

        // Non-retryable error — still try next tier
        continue
      }
    }

    // All providers failed — serve from cache as last resort
    throw new Error('All provider tiers exhausted')
  }

  private async callProvider(
    tier: ProviderTier,
    prompt: string
  ): Promise<string> {
    // Provider-specific API calls
    // (Implementation depends on provider SDK)
    throw new Error('Provider call implementation')
  }

  getStatus(): { tier: string; healthy: boolean; failures: number }[] {
    return this.tiers.map(t => ({
      tier: t.name,
      healthy: t.isHealthy,
      failures: t.consecutiveFailures
    }))
  }
}

// Failover scenario (real-world):
//
// 10:00 AM — Normal traffic
//   All requests → Primary (Claude Sonnet) ✅
//
// 10:15 AM — Traffic spike, Claude returns 429
//   Request 1 → Primary (429) → Secondary (GPT-4o) ✅
//   Request 2 → Primary (429) → Secondary (GPT-4o) ✅
//   Request 3 → Primary (429) → Primary marked UNHEALTHY
//   Request 4 → Secondary (GPT-4o) ✅ (Primary skipped)
//
// 10:20 AM — GPT-4o also rate-limited
//   Request 5 → Secondary (429) → Safe Fallback (Haiku) ✅
//   Request 6 → Safe Fallback (Haiku) ✅
//   // Users get slightly less capable responses, but ZERO errors
//
// 10:16 AM — Primary health check passes
//   Request 7 → Primary (Claude Sonnet) ✅ (recovered)
//
// User impact: ZERO errors. Response quality degraded for ~5 min.
// Without tiered failover: 100% of requests fail for ~5 min.
```

## Auto-Scaling

**Simple Explanation**: Automatically add more servers when traffic increases, remove them when traffic decreases. Like a restaurant adding more waiters during rush hour.

```typescript
interface ScalingConfig {
  minPods: number          // Always keep this many running
  maxPods: number          // Never exceed this many
  targetCPU: number        // Scale up if CPU > this percent
  targetLatency: number    // Scale up if latency &gt; this ms
  scaleUpCooldown: number  // Wait this long before scaling up again (ms)
  scaleDownCooldown: number // Wait this long before scaling down (ms)
}

class AutoScaler {
  private config: ScalingConfig = {
    minPods: 2,
    maxPods: 20,
    targetCPU: 70,
    targetLatency: 500,
    scaleUpCooldown: 60000,   // 1 minute
    scaleDownCooldown: 300000 // 5 minutes
  }

  private lastScaleUp = 0
  private lastScaleDown = 0

  async checkAndScale(metrics: SystemMetrics): Promise<ScalingAction> {
    const currentPods = await this.getCurrentPodCount()
    const now = Date.now()

    // Decide if we need to scale
    const decision = this.makeScalingDecision(metrics, currentPods)

    if (decision.action === 'scale_up') {
      // Check cooldown
      if (now - this.lastScaleUp < this.config.scaleUpCooldown) {
        console.log('Scale up on cooldown, skipping')
        return { action: 'none', reason: 'cooldown' }
      }

      const newPodCount = Math.min(
        currentPods + decision.podDelta,
        this.config.maxPods
      )

      if (newPodCount === currentPods) {
        return { action: 'none', reason: 'at_max_capacity' }
      }

      await this.scaleUp(newPodCount)
      this.lastScaleUp = now

      return {
        action: 'scale_up',
        from: currentPods,
        to: newPodCount,
        reason: decision.reason
      }
    }

    if (decision.action === 'scale_down') {
      // Check cooldown (longer for scale down to avoid flapping)
      if (now - this.lastScaleDown < this.config.scaleDownCooldown) {
        console.log('Scale down on cooldown, skipping')
        return { action: 'none', reason: 'cooldown' }
      }

      const newPodCount = Math.max(
        currentPods - decision.podDelta,
        this.config.minPods
      )

      if (newPodCount === currentPods) {
        return { action: 'none', reason: 'at_min_capacity' }
      }

      await this.scaleDown(newPodCount)
      this.lastScaleDown = now

      return {
        action: 'scale_down',
        from: currentPods,
        to: newPodCount,
        reason: decision.reason
      }
    }

    return { action: 'none', reason: 'metrics_within_target' }
  }

  private makeScalingDecision(
    metrics: SystemMetrics,
    currentPods: number
  ): ScalingDecision {
    // Multiple triggers for scaling up
    const triggers: ScalingTrigger[] = []

    // Trigger 1: High CPU
    if (metrics.avgCPU > this.config.targetCPU) {
      triggers.push({
        name: 'high_cpu',
        severity: (metrics.avgCPU - this.config.targetCPU) / 100,
        desiredPods: Math.ceil(currentPods * (metrics.avgCPU / this.config.targetCPU))
      })
    }

    // Trigger 2: High latency
    if (metrics.p95Latency > this.config.targetLatency) {
      triggers.push({
        name: 'high_latency',
        severity: (metrics.p95Latency - this.config.targetLatency) / this.config.targetLatency,
        desiredPods: Math.ceil(currentPods * 1.5) // 50% more pods
      })
    }

    // Trigger 3: Queue depth
    if (metrics.queueDepth &gt; 100) {
      triggers.push({
        name: 'high_queue_depth',
        severity: metrics.queueDepth / 100,
        desiredPods: Math.ceil(currentPods + (metrics.queueDepth / 50))
      })
    }

    // Trigger 4: Error rate
    if (metrics.errorRate &gt; 0.05) {  // 5%
      triggers.push({
        name: 'high_error_rate',
        severity: metrics.errorRate,
        desiredPods: Math.ceil(currentPods * 1.3) // 30% more pods
      })
    }

    // Any triggers? Scale up!
    if (triggers.length &gt; 0) {
      const maxDesiredPods = Math.max(...triggers.map(t => t.desiredPods))
      const podDelta = maxDesiredPods - currentPods

      return {
        action: 'scale_up',
        podDelta: podDelta,
        reason: triggers.map(t => t.name).join(', ')
      }
    }

    // Check for scale down conditions
    const shouldScaleDown =
      metrics.avgCPU < (this.config.targetCPU * 0.5) &&  // CPU &lt; 35%
      metrics.p95Latency < (this.config.targetLatency * 0.7) &&  // Latency good
      metrics.queueDepth &lt; 10 &&  // Queue empty
      currentPods > this.config.minPods

    if (shouldScaleDown) {
      // Conservative scale down (1 pod at a time)
      return {
        action: 'scale_down',
        podDelta: 1,
        reason: 'low_utilization'
      }
    }

    return {
      action: 'none',
      podDelta: 0,
      reason: 'metrics_within_target'
    }
  }

  private async getCurrentPodCount(): Promise<number> {
    // In Kubernetes, would query the API
    // For example: kubectl get deployment my-ai-api -o json | jq '.spec.replicas'
    return 3  // Placeholder
  }

  private async scaleUp(newPodCount: number) {
    console.log(`Scaling up to ${newPodCount} pods`)
    // In Kubernetes: kubectl scale deployment my-ai-api --replicas=${newPodCount}
    // In AWS ECS: Update service desired count
    // In cloud functions: Increase max instances
  }

  private async scaleDown(newPodCount: number) {
    console.log(`Scaling down to ${newPodCount} pods`)
    // Same as scale up, but with lower number
  }
}

// Kubernetes YAML for HPA (Horizontal Pod Autoscaler)
```

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: ai-api-scaler
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: ai-api
  minReplicas: 2
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Pods
    pods:
      metric:
        name: http_request_duration_p95
      target:
        type: AverageValue
        averageValue: "500m"  # 500ms
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 50  # Max 50% increase at once
        periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300  # Wait 5 min before scale down
      policies:
      - type: Pods
        value: 1  # Remove 1 pod at a time
        periodSeconds: 60
```

**Real-World Scaling Example**:
```
Time    Traffic    CPU    Pods    Action
08:00   100 req/m  30%    2       None (baseline)
09:00   500 req/m  75%    2       Scale up to 4 (CPU &gt; 70%)
09:05   500 req/m  40%    4       None (cooldown)
12:00   2000 req/m 85%    4       Scale up to 8 (high CPU + queue)
14:00   1500 req/m 60%    8       None (metrics okay)
18:00   300 req/m  20%    8       Scale down to 4 (low utilization)
23:00   50 req/m   10%    4       Scale down to 2 (min pods)
```

## Caching at Scale

**Simple Explanation**: Store answers to common questions so you don't have to call the expensive AI API every time.

```typescript
// Multi-tier caching for maximum performance
class CacheHierarchy {
  private memoryCache: LRUCache       // L1: In-memory (fastest, smallest)
  private redisCache: RedisClient     // L2: Redis (fast, medium)
  private semanticCache: VectorDB     // L3: Semantic (slower, largest)

  constructor() {
    // L1: In-memory cache - 1000 items, &lt; 1ms access
    this.memoryCache = new LRUCache({ max: 1000 })

    // L2: Redis cache - 100K items, ~5ms access
    this.redisCache = new Redis({
      host: process.env.REDIS_HOST,
      maxRetriesPerRequest: 3
    })

    // L3: Semantic cache - unlimited, ~50ms access
    this.semanticCache = new ChromaDB()
  }

  async get(query: string): Promise<CachedResponse | null> {
    const key = this.hashQuery(query)

    // L1: Check memory cache (&lt; 1ms)
    const memoryResult = this.memoryCache.get(key)
    if (memoryResult) {
      console.log('L1 cache hit (memory)')
      return { ...memoryResult, cacheLevel: 'L1' }
    }

    // L2: Check Redis (~ 5ms)
    const redisResult = await this.redisCache.get(key)
    if (redisResult) {
      console.log('L2 cache hit (Redis)')
      const parsed = JSON.parse(redisResult)

      // Backfill L1
      this.memoryCache.set(key, parsed)

      return { ...parsed, cacheLevel: 'L2' }
    }

    // L3: Check semantic cache (~50ms)
    const semanticResult = await this.findSimilarQuery(query)
    if (semanticResult && semanticResult.similarity &gt; 0.95) {
      console.log('L3 cache hit (semantic)')

      // Backfill L2 and L1
      await this.redisCache.setex(key, 3600, JSON.stringify(semanticResult.response))
      this.memoryCache.set(key, semanticResult.response)

      return { ...semanticResult.response, cacheLevel: 'L3' }
    }

    // Cache miss - need to call LLM
    console.log('Cache miss - calling LLM')
    return null
  }

  async set(query: string, response: any, ttl: number = 3600) {
    const key = this.hashQuery(query)

    // Store in all cache levels
    this.memoryCache.set(key, response)
    await this.redisCache.setex(key, ttl, JSON.stringify(response))
    await this.storeSemanticCache(query, response)
  }

  private async findSimilarQuery(query: string): Promise<SemanticMatch | null> {
    // Embed the query
    const embedding = await this.embed(query)

    // Search for similar queries
    const results = await this.semanticCache.query({
      queryEmbeddings: [embedding],
      nResults: 1
    })

    if (results.length === 0) return null

    const topMatch = results[0]

    return {
      similarity: topMatch.distance,
      query: topMatch.document,
      response: topMatch.metadata.response
    }
  }

  private async storeSemanticCache(query: string, response: any) {
    const embedding = await this.embed(query)

    await this.semanticCache.add({
      ids: [this.hashQuery(query)],
      embeddings: [embedding],
      documents: [query],
      metadatas: [{ response, timestamp: Date.now() }]
    })
  }

  private hashQuery(query: string): string {
    // Simple hash for exact matching
    return crypto.createHash('md5').update(query).digest('hex')
  }

  private async embed(text: string): Promise<number[]> {
    // Use embedding model (e.g., OpenAI ada-002)
    const response = await openai.embeddings.create({
      model: 'text-embedding-ada-002',
      input: text
    })
    return response.data[0].embedding
  }
}

// Usage with metrics
const cache = new CacheHierarchy()

async function getAnswer(query: string): Promise<string> {
  // Try cache first
  const cached = await cache.get(query)

  if (cached) {
    console.log(`Cache hit! Saved $${COST_PER_LLM_CALL}`)
    return cached.response
  }

  // Cache miss - call LLM
  const response = await llm.complete(query)

  // Store in cache
  await cache.set(query, response)

  return response
}

// Performance over time
// Day 1:  Cache hit rate 20% → Save $200/day
// Day 7:  Cache hit rate 45% → Save $900/day
// Day 30: Cache hit rate 75% → Save $3,000/day
```

**Cache Hit Rate Impact**:
```
Without cache:
- 10,000 requests/day
- $0.05 per request
- Cost: $500/day = $15,000/month

With 75% cache hit rate:
- 2,500 LLM calls/day (25%)
- 7,500 cache hits/day (75%)
- Cost: $125/day = $3,750/month
- Savings: $11,250/month (75% reduction!)
```

## Rate Limiting at Scale

**Simple Explanation**: Prevent users from overusing your AI API and racking up costs. Like a speed limit on a highway.

```typescript
// Distributed rate limiting using Redis
class DistributedRateLimiter {
  private redis: RedisClient

  async checkLimit(
    userId: string,
    tier: 'free' | 'pro' | 'enterprise'
  ): Promise<RateLimitResult> {
    const limits = {
      free: { requests: 10, window: 3600 },       // 10 per hour
      pro: { requests: 1000, window: 3600 },      // 1000 per hour
      enterprise: { requests: 100000, window: 3600 } // 100K per hour
    }

    const limit = limits[tier]
    const key = `ratelimit:${userId}:${tier}`

    // Increment counter
    const count = await this.redis.incr(key)

    // Set expiry on first request
    if (count === 1) {
      await this.redis.expire(key, limit.window)
    }

    // Check limit
    const allowed = count &lt;= limit.requests

    if (!allowed) {
      const ttl = await this.redis.ttl(key)
      return {
        allowed: false,
        remaining: 0,
        resetIn: ttl,
        retryAfter: ttl
      }
    }

    return {
      allowed: true,
      remaining: limit.requests - count,
      resetIn: await this.redis.ttl(key),
      retryAfter: 0
    }
  }
}

// Usage in API route
export async function POST(req: Request) {
  const { userId, tier } = await getUserInfo(req)

  const rateLimiter = new DistributedRateLimiter()
  const limitCheck = await rateLimiter.checkLimit(userId, tier)

  if (!limitCheck.allowed) {
    return NextResponse.json(
      {
        error: 'Rate limit exceeded',
        retryAfter: limitCheck.retryAfter
      },
      {
        status: 429,
        headers: {
          'X-RateLimit-Remaining': '0',
          'X-RateLimit-Reset': limitCheck.resetIn.toString(),
          'Retry-After': limitCheck.retryAfter.toString()
        }
      }
    )
  }

  // Process request...
  const response = await processAIRequest(req)

  return NextResponse.json(response, {
    headers: {
      'X-RateLimit-Remaining': limitCheck.remaining.toString(),
      'X-RateLimit-Reset': limitCheck.resetIn.toString()
    }
  })
}
```

**Architect's Tip — Token-Aware Circuit Breaker (The Budget Shield)**: "A user sending 100 simple questions costs you $1, but a user sending 100 large PDFs for summarization costs you $50. Request-count rate limiting is blind to this 50x cost difference. An Architect sets limits based on **Estimated Token Impact**, not request count. If a tenant exceeds their monthly Token Budget, the system triggers a **Circuit Breaker** that switches them to a smaller model or enforces a strict delay. This prevents a single 'malicious' or 'inefficient' user from bankrupting your project in a single afternoon."

```typescript
/**
 * Token-Aware Circuit Breaker
 *
 * Problem: Request-count rate limiting treats all requests equally.
 * A 10-token "What time is it?" and a 50,000-token PDF summary
 * both count as 1 request. A user who uploads 100 PDFs can spend
 * $500 in a single session while staying under the request limit.
 *
 * Solution: Track ESTIMATED token usage per tenant, not just
 * request count. When a tenant approaches their token budget,
 * trigger progressive degradation:
 *   80% budget → Warn + switch to smaller model
 *   95% budget → Hard limit + delay enforcement
 *   100% budget → Circuit breaker OPEN (block until reset)
 *
 * Interview Defense: "Our rate limiter is token-aware. We estimate
 * input tokens before calling the LLM and track cumulative spend
 * per tenant. When a tenant hits 80% of their budget, we
 * automatically switch them to Haiku. At 95%, we enforce delays.
 * This saved us from a $12,000 bill when a single enterprise
 * tenant ran a bulk extraction job without warning."
 */

interface TenantBudget {
  tenantId: string
  tier: 'free' | 'pro' | 'enterprise'
  monthlyTokenBudget: number     // Max tokens per billing period
  tokensUsed: number             // Cumulative tokens this period
  estimatedCostUsed: number      // Dollar cost this period
  circuitState: 'closed' | 'degraded' | 'open'
}

const TIER_BUDGETS = {
  free:       { tokens: 100_000,     costCap: 1.00 },
  pro:        { tokens: 5_000_000,   costCap: 50.00 },
  enterprise: { tokens: 100_000_000, costCap: 1000.00 }
}

class TokenAwareCircuitBreaker {
  private redis: Redis

  constructor(redis: Redis) {
    this.redis = redis
  }

  async evaluateRequest(
    tenantId: string,
    tier: 'free' | 'pro' | 'enterprise',
    estimatedInputTokens: number
  ): Promise<{
    allowed: boolean
    model: string
    delay: number            // Enforced delay in ms (0 = no delay)
    budgetRemaining: number  // Percentage remaining
    action: string
  }> {
    const budget = TIER_BUDGETS[tier]
    const key = `budget:${tenantId}:${this.getCurrentPeriod()}`

    // Get current usage
    const used = parseInt(await this.redis.get(key) || '0')
    const projectedUsage = used + estimatedInputTokens
    const utilizationPercent = (projectedUsage / budget.tokens) * 100

    // Circuit breaker logic
    if (utilizationPercent >= 100) {
      // OPEN — hard block
      return {
        allowed: false,
        model: 'none',
        delay: 0,
        budgetRemaining: 0,
        action: 'CIRCUIT_OPEN: Token budget exhausted. Resets next billing period.'
      }
    }

    if (utilizationPercent >= 95) {
      // DEGRADED + DELAY — allow but throttle aggressively
      await this.redis.incrby(key, estimatedInputTokens)
      return {
        allowed: true,
        model: 'claude-haiku-4-5-20251001',  // Force cheapest model
        delay: 5000,                          // 5-second enforced delay
        budgetRemaining: Math.round(100 - utilizationPercent),
        action: 'DEGRADED: Forced to Haiku + 5s delay. Budget nearly exhausted.'
      }
    }

    if (utilizationPercent >= 80) {
      // DEGRADED — switch to cheaper model, no delay
      await this.redis.incrby(key, estimatedInputTokens)
      return {
        allowed: true,
        model: 'claude-haiku-4-5-20251001',  // Switch to cheaper model
        delay: 0,
        budgetRemaining: Math.round(100 - utilizationPercent),
        action: 'DEGRADED: Switched to Haiku to preserve budget.'
      }
    }

    // CLOSED — normal operation
    await this.redis.incrby(key, estimatedInputTokens)
    return {
      allowed: true,
      model: 'claude-sonnet-4-5-20250929',   // Full-power model
      delay: 0,
      budgetRemaining: Math.round(100 - utilizationPercent),
      action: 'NORMAL: Full model access.'
    }
  }

  // Estimate tokens before calling LLM (cheap, fast)
  estimateTokens(input: string): number {
    // Rough estimate: 1 token ≈ 4 characters for English text
    return Math.ceil(input.length / 4)
  }

  private getCurrentPeriod(): string {
    const now = new Date()
    return `${now.getFullYear()}-${String(now.getMonth() + 1).padStart(2, '0')}`
  }
}

// Real-world scenario:
//
// Tenant "AcmeCorp" (Pro tier, 5M token budget):
//
// Day 1-20: Normal usage, 3.2M tokens used (64%)
//   → All requests use Sonnet, no restrictions
//
// Day 21: Intern runs bulk PDF extraction (500 PDFs)
//   → Estimated: 200K tokens per PDF = 100M tokens total
//   → After 9 PDFs (1.8M tokens): Budget hits 100% (5M)
//   → Circuit breaker OPENS on PDF #10
//   → Intern gets: "Token budget exhausted. Contact admin."
//   → Total damage: $50 (budget cap), NOT $5,000 (uncapped)
//
// Without token-aware limiting:
//   → 500 PDFs × $10 each = $5,000 in a single afternoon
//   → Request-count limiter saw only 500 requests (under 1K/hour limit)
//   → The bill arrives 30 days later. Surprise.
//
// | Scenario              | Request Limiter | Token-Aware Limiter |
// |-----------------------|-----------------|---------------------|
// | 100 simple questions  | 100 requests ✅  | 10K tokens ✅        |
// | 100 PDF summaries     | 100 requests ✅  | 5M tokens → BLOCKED |
// | Bulk extraction job   | 500 requests ✅  | 100M tokens → OPEN  |
// | Cost exposure         | Unlimited       | Capped at tier max  |
```

## Best Practices

1. **Monitor Everything**: Track latency, costs, errors, cache hit rates
2. **Start Conservative**: Scale up gradually, don't over-provision
3. **Use Circuit Breakers**: Stop calling failing services
4. **Implement Graceful Degradation**: Serve cached/partial results when backends are slow
5. **Load Test Before Launch**: Know your breaking points
6. **Set Cost Alerts**: Get notified if costs spike unexpectedly

---

## Architect Challenge: The "Scale-or-Die" CTO Crisis

**Your AI platform just went viral on social media. Traffic has increased 50x in one hour.**

**The Situation:**

Your product was featured by a major tech influencer. Before the video, you handled **2,000 requests/hour**. Now you're getting **100,000 requests/hour** and climbing. Your OpenAI rate limits are maxed out at 10,000 requests/minute. Your cloud bill is spiking vertically — you've already burned through $4,000 in the last hour alone. Your CEO is on a live stream demo in 30 minutes. If the platform goes down on camera, the viral moment becomes a viral disaster.

**Your options:**

**A)** Manually contact OpenAI support to request a higher rate limit and hope they respond within 30 minutes.

**B)** Activate **"Economic Mode" and Provider Failover**. Your system automatically detects the 429 errors from OpenAI and routes new traffic to a **fine-tuned Llama-3-70B instance** you keep on standby for exactly this scenario. For non-premium users, you temporarily switch to a **Cache-First Strategy** — serving only cached results for the 500 most common queries (which cover 60% of traffic during viral spikes). Premium users continue to get live LLM responses via your secondary provider. This saves the system from crashing and protects your margins during the viral peak.

**C)** Shut down the registration page to stop new users from signing up until the traffic subsides.

**D)** Increase subscription prices to slow down the flood of new users.

<details>
<summary><strong>Click to reveal the correct answer</strong></summary>

### Correct Answer: B — Economic Mode + Provider Failover

An Architect designs **"Elastic Degradation"** — the system survives success by gracefully trading quality for availability during traffic spikes.

**The Math:**

```typescript
// Before viral spike (2,000 req/hour):
//   Primary: OpenAI GPT-4o (100% of traffic)
//   Cost: 2,000 × $0.05 = $100/hour
//   Latency: 1,500ms average
//   Status: All systems nominal

// During viral spike (100,000 req/hour):
//
// WITHOUT Elastic Degradation:
//   OpenAI rate limit: 10,000 req/min → 90,000 requests REJECTED
//   Error rate: 90% → Platform appears "down"
//   Cost: $5,000/hour (for the 10K that get through)
//   User experience: Catastrophic — 9 out of 10 users see errors
//
// WITH Elastic Degradation (Option B):
//   Tier 1 — Premium users (10%): Secondary provider (Claude) → $0.04/req
//   Tier 2 — Cache hits (60%): Cached responses → $0.001/req
//   Tier 3 — Remaining (30%): Llama-3-70B standby → $0.008/req
//
//   Cost breakdown:
//     Premium:  10,000 × $0.04  = $400
//     Cached:   60,000 × $0.001 = $60
//     Standby:  30,000 × $0.008 = $240
//     Total:    $700/hour (vs $5,000 without degradation)
//
//   Error rate: 0% — every user gets a response
//   Quality: 60% get cached (instant), 30% get slightly lower quality
//   Savings: $4,300/hour during spike
//
// After spike subsides (6 hours later):
//   Auto-recovery: Primary provider health check passes
//   Traffic: Routes back to primary automatically
//   Total spike cost: $4,200 (vs $30,000 without degradation)
```

**Why other answers fail:**

- **A) Contact OpenAI for higher limits** — Provider support doesn't respond in 30 minutes during a traffic spike. Even if they did, higher limits mean higher costs with no cost control. You're solving the rate limit problem but not the economics problem. An Architect never designs a system that requires human intervention to survive a predictable failure mode.

- **C) Shut down registration** — You're turning away the exact users who came because of the viral moment. The influencer's audience is watching RIGHT NOW. Every minute the site is closed, you lose thousands of potential users permanently. This is the business equivalent of closing your restaurant during a food critic's visit because "the kitchen is busy."

- **D) Increase prices** — Price changes take days to implement, communicate, and deploy. Even if instant, it penalizes existing users for a temporary spike. It also doesn't solve the technical rate limit problem — you'll still get 429 errors regardless of pricing.

**The Architect's Principle:** "An Architect designs systems that **survive success**. Viral growth is not a crisis — it's a test of your architecture. The system should have a pre-configured 'Economic Mode' that activates automatically when traffic exceeds provider capacity. The degradation is **invisible to the user** — they get a response, just from a different model or cache. The CEO's live demo works perfectly. The viral moment converts users instead of losing them. And the cloud bill stays under control because your circuit breakers enforce budget caps at every tier."

</details>

---

## Resources
- [Scaling Strategies](https://aws.amazon.com/architecture/scalability/)
- [Production Best Practices](https://kubernetes.io/docs/concepts/configuration/overview/)
- [Caching Strategies](https://redis.io/docs/manual/patterns/)
