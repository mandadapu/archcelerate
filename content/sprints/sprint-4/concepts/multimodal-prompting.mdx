---
id: multimodal-prompting
title: Multimodal Prompting Techniques
description: Master techniques for combining text and image inputs effectively
difficulty: intermediate
estimatedMinutes: 75
order: 2
prerequisites: [vision-models]
tags: [prompting, multimodal, vision, best-practices]
---

# Multimodal Prompting Techniques

Effective multimodal prompting is both an art and a science. While vision models can "see" images, the quality of their analysis depends heavily on how you structure your prompts, combine modalities, and guide the model's attention. This lesson teaches you practical techniques for getting the best results from multimodal AI.

## Learning Objectives

By the end of this lesson, you'll master:
- How to combine text and image inputs effectively
- Proven patterns for multimodal prompt engineering
- Image encoding formats and API best practices
- Techniques for multi-image analysis
- Advanced prompting strategies for vision tasks
- Common pitfalls and how to avoid them
- Production-ready practices for accuracy and reliability

## The Fundamentals of Multimodal Prompting

### What Makes Multimodal Prompting Different?

Unlike text-only prompting, multimodal prompting requires you to:
1. **Coordinate** between visual and textual information
2. **Direct attention** to specific parts of images
3. **Provide context** that images alone might not convey
4. **Structure requests** to leverage both modalities
5. **Handle ambiguity** inherent in visual interpretation

### The Multimodal Input Spectrum

```typescript
// Spectrum: Text-Only → Text+Image → Multi-Image → Complex Multimodal

// 1. Text-Only (baseline)
const textOnly = {
  role: 'user',
  content: 'What is a receipt?'
}

// 2. Single Image + Question
const singleImage = {
  role: 'user',
  content: [
    { type: 'image', source: { type: 'base64', media_type: 'image/jpeg', data: image1 } },
    { type: 'text', text: 'What is the total amount on this receipt?' }
  ]
}

// 3. Multiple Images + Comparative Question
const multiImage = {
  role: 'user',
  content: [
    { type: 'text', text: 'Compare these two receipts:' },
    { type: 'image', source: { type: 'base64', media_type: 'image/jpeg', data: receipt1 } },
    { type: 'image', source: { type: 'base64', media_type: 'image/jpeg', data: receipt2 } },
    { type: 'text', text: 'Which purchase was more expensive and by how much?' }
  ]
}

// 4. Complex: Multiple Images + Context + Structured Output
const complexMultimodal = {
  role: 'user',
  content: [
    { type: 'text', text: 'You are analyzing expense reports. Here are three receipts from a business trip:' },
    { type: 'image', source: { type: 'base64', media_type: 'image/jpeg', data: day1Receipt } },
    { type: 'text', text: 'Day 1 - Breakfast' },
    { type: 'image', source: { type: 'base64', media_type: 'image/jpeg', data: day2Receipt } },
    { type: 'text', text: 'Day 2 - Lunch' },
    { type: 'image', source: { type: 'base64', media_type: 'image/jpeg', data: day3Receipt } },
    { type: 'text', text: 'Day 3 - Dinner. Calculate total expenses and categorize by meal type.' }
  ]
}
```

### Key Principle: Image + Text > Image Alone

Images rarely speak for themselves in API contexts. Always pair images with clear textual guidance.

```typescript
// Poor: Image without context
const poor = {
  role: 'user',
  content: [
    { type: 'image', source: { type: 'base64', media_type: 'image/jpeg', data: screenshot } }
  ]
}
// Model doesn't know what you want it to do with the image

// Better: Image with specific question
const better = {
  role: 'user',
  content: [
    { type: 'image', source: { type: 'base64', media_type: 'image/jpeg', data: screenshot } },
    { type: 'text', text: 'What UI framework is this application using?' }
  ]
}

// Best: Image + Context + Specific Request + Output Format
const best = {
  role: 'user',
  content: [
    { type: 'text', text: 'You are a UI/UX expert analyzing a web application screenshot.' },
    { type: 'image', source: { type: 'base64', media_type: 'image/jpeg', data: screenshot } },
    { type: 'text', text: `Identify:
1. UI framework/library (React, Vue, etc.)
2. Component library (Material-UI, Tailwind, etc.)
3. Key UI patterns used
4. Accessibility considerations

Format as JSON.` }
  ]
}
```

## Image Encoding & API Formats

### Base64 Encoding

Base64 is the most common format for sending images to Claude's API.

```typescript
import fs from 'fs'
import Anthropic from '@anthropic-ai/sdk'

const client = new Anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY,
})

// Encoding an image to base64
function encodeImage(imagePath: string): { data: string; mediaType: string } {
  const imageBuffer = fs.readFileSync(imagePath)
  const base64Data = imageBuffer.toString('base64')

  // Determine media type from file extension
  const extension = imagePath.split('.').pop()?.toLowerCase()
  const mediaTypeMap: Record<string, string> = {
    'jpg': 'image/jpeg',
    'jpeg': 'image/jpeg',
    'png': 'image/png',
    'gif': 'image/gif',
    'webp': 'image/webp',
  }

  const mediaType = mediaTypeMap[extension || 'jpeg'] || 'image/jpeg'

  return { data: base64Data, mediaType }
}

// Using the encoded image
async function analyzeImage(imagePath: string, prompt: string) {
  const { data, mediaType } = encodeImage(imagePath)

  const response = await client.messages.create({
    model: 'claude-3-5-sonnet-20250129',
    max_tokens: 1024,
    messages: [
      {
        role: 'user',
        content: [
          {
            type: 'image',
            source: {
              type: 'base64',
              media_type: mediaType,
              data: data,
            },
          },
          {
            type: 'text',
            text: prompt,
          },
        ],
      },
    ],
  })

  return response.content[0].text
}
```

### URL-Based Images

For images already hosted online, you can use URLs directly.

```typescript
async function analyzeImageURL(imageUrl: string, prompt: string) {
  const response = await client.messages.create({
    model: 'claude-3-5-sonnet-20250129',
    max_tokens: 1024,
    messages: [
      {
        role: 'user',
        content: [
          {
            type: 'image',
            source: {
              type: 'url',
              url: imageUrl,
            },
          },
          {
            type: 'text',
            text: prompt,
          },
        ],
      },
    ],
  })

  return response.content[0].text
}

// Example usage
const result = await analyzeImageURL(
  'https://example.com/product.jpg',
  'Describe this product in detail for an e-commerce listing.'
)
```

### Handling Multiple Image Formats

Create a flexible helper that handles both local files and URLs.

```typescript
interface ImageSource {
  type: 'base64' | 'url'
  media_type?: string
  data?: string
  url?: string
}

function createImageSource(input: string): ImageSource {
  // Check if input is a URL
  if (input.startsWith('http://') || input.startsWith('https://')) {
    return {
      type: 'url',
      url: input,
    }
  }

  // Otherwise, treat as local file path
  const { data, mediaType } = encodeImage(input)
  return {
    type: 'base64',
    media_type: mediaType,
    data: data,
  }
}

// Flexible analysis function
async function analyzeFlexible(
  images: string | string[],
  prompt: string
) {
  const imageArray = Array.isArray(images) ? images : [images]

  const content = imageArray.map(img => ({
    type: 'image' as const,
    source: createImageSource(img),
  }))

  content.push({
    type: 'text' as const,
    text: prompt,
  })

  const response = await client.messages.create({
    model: 'claude-3-5-sonnet-20250129',
    max_tokens: 2048,
    messages: [{ role: 'user', content }],
  })

  return response.content[0].text
}

// Works with local files
await analyzeFlexible('./receipt.jpg', 'Extract the total')

// Works with URLs
await analyzeFlexible('https://example.com/image.png', 'Describe this')

// Works with multiple images
await analyzeFlexible(
  ['./before.jpg', 'https://example.com/after.jpg'],
  'What changed between these images?'
)
```

## Effective Multimodal Prompt Patterns

### Pattern 1: Context → Image → Question

Provide context before the image, then ask your question.

```typescript
async function contextImageQuestion(imagePath: string) {
  const { data, mediaType } = encodeImage(imagePath)

  const response = await client.messages.create({
    model: 'claude-3-5-sonnet-20250129',
    max_tokens: 1024,
    messages: [
      {
        role: 'user',
        content: [
          {
            type: 'text',
            text: 'I am building a product catalog for an online furniture store. I need detailed product descriptions that highlight key features and materials.',
          },
          {
            type: 'image',
            source: { type: 'base64', media_type: mediaType, data },
          },
          {
            type: 'text',
            text: 'Create a product description for this furniture item. Include dimensions if visible, materials, style, and key selling points.',
          },
        ],
      },
    ],
  })

  return response.content[0].text
}
```

**Why this works**: The context sets the model's "role" and expectations before it sees the image, leading to more focused analysis.

### Pattern 2: Image → Labels → Question

Show the image, label what it represents, then ask specific questions.

```typescript
async function imageLabelQuestion(images: string[], labels: string[], question: string) {
  const content = []

  for (let i = 0; i < images.length; i++) {
    const { data, mediaType } = encodeImage(images[i])

    content.push({
      type: 'image' as const,
      source: { type: 'base64' as const, media_type: mediaType, data },
    })

    content.push({
      type: 'text' as const,
      text: `Image ${i + 1}: ${labels[i]}`,
    })
  }

  content.push({
    type: 'text' as const,
    text: question,
  })

  const response = await client.messages.create({
    model: 'claude-3-5-sonnet-20250129',
    max_tokens: 2048,
    messages: [{ role: 'user', content }],
  })

  return response.content[0].text
}

// Example: Comparing UI states
const result = await imageLabelQuestion(
  ['./login-before.png', './login-after.png'],
  ['Login page - before redesign', 'Login page - after redesign'],
  'List all visual changes between these two versions of the login page.'
)
```

**Why this works**: Labels help the model understand the relationship between images and what each represents.

### Pattern 3: Multi-Step Reasoning

Break complex analysis into steps within the prompt.

```typescript
async function multiStepAnalysis(imagePath: string) {
  const { data, mediaType } = encodeImage(imagePath)

  const response = await client.messages.create({
    model: 'claude-3-5-sonnet-20250129',
    max_tokens: 2048,
    messages: [
      {
        role: 'user',
        content: [
          {
            type: 'image',
            source: { type: 'base64', media_type: mediaType, data },
          },
          {
            type: 'text',
            text: `Analyze this dashboard screenshot using these steps:

Step 1: Identify all visible metrics and their values
Step 2: Determine the dashboard's primary purpose
Step 3: Assess the data visualization quality (charts, graphs)
Step 4: Identify any potential issues or anomalies in the data
Step 5: Suggest improvements to the dashboard design

Provide your analysis following this step-by-step structure.`,
          },
        ],
      },
    ],
  })

  return response.content[0].text
}
```

**Why this works**: Explicit steps guide the model through complex reasoning, leading to more thorough and structured analysis.

### Pattern 4: Role + Task + Constraints

Set a role, define the task, and add constraints for focused output.

```typescript
async function roleTaskConstraints(imagePath: string) {
  const { data, mediaType } = encodeImage(imagePath)

  const response = await client.messages.create({
    model: 'claude-3-5-sonnet-20250129',
    max_tokens: 1024,
    messages: [
      {
        role: 'user',
        content: [
          {
            type: 'text',
            text: `Role: You are a professional photo curator for a stock photography website.

Task: Write a title, description, and tags for this image.

Constraints:
- Title: max 10 words, descriptive and searchable
- Description: 2-3 sentences, highlight visual elements
- Tags: 8-12 relevant keywords, comma-separated
- Focus on what makes this image commercially valuable`,
          },
          {
            type: 'image',
            source: { type: 'base64', media_type: mediaType, data },
          },
        ],
      },
    ],
  })

  return response.content[0].text
}
```

**Why this works**: Clear role, task, and constraints create boundaries that shape the output format and focus.

### Pattern 5: Few-Shot with Images

Provide examples of the desired input-output pattern.

```typescript
async function fewShotImageAnalysis(targetImagePath: string) {
  // Example images showing the pattern
  const example1 = encodeImage('./examples/receipt1.jpg')
  const example2 = encodeImage('./examples/receipt2.jpg')
  const target = encodeImage(targetImagePath)

  const response = await client.messages.create({
    model: 'claude-3-5-sonnet-20250129',
    max_tokens: 1024,
    messages: [
      {
        role: 'user',
        content: [
          { type: 'text', text: 'Extract receipt data in JSON format. Here are examples:' },
          { type: 'text', text: 'Example 1:' },
          { type: 'image', source: { type: 'base64', media_type: 'image/jpeg', data: example1.data } },
        ],
      },
      {
        role: 'assistant',
        content: '{"merchant": "Coffee Shop", "date": "2024-01-15", "total": 12.50, "items": ["Latte", "Croissant"]}',
      },
      {
        role: 'user',
        content: [
          { type: 'text', text: 'Example 2:' },
          { type: 'image', source: { type: 'base64', media_type: 'image/jpeg', data: example2.data } },
        ],
      },
      {
        role: 'assistant',
        content: '{"merchant": "Grocery Store", "date": "2024-01-16", "total": 45.30, "items": ["Milk", "Bread", "Eggs", "Cheese"]}',
      },
      {
        role: 'user',
        content: [
          { type: 'text', text: 'Now extract from this receipt:' },
          { type: 'image', source: { type: 'base64', media_type: 'image/jpeg', data: target.data } },
        ],
      },
    ],
  })

  return JSON.parse(response.content[0].text)
}
```

**Why this works**: Examples teach the model the exact format and level of detail you expect.

### Pattern 6: Negative Instructions

Tell the model what NOT to do to avoid common mistakes.

```typescript
async function withNegativeInstructions(imagePath: string) {
  const { data, mediaType } = encodeImage(imagePath)

  const response = await client.messages.create({
    model: 'claude-3-5-sonnet-20250129',
    max_tokens: 1024,
    messages: [
      {
        role: 'user',
        content: [
          {
            type: 'image',
            source: { type: 'base64', media_type: mediaType, data },
          },
          {
            type: 'text',
            text: `Count the number of people in this image.

IMPORTANT:
- Do NOT guess if you're uncertain
- Do NOT count partially visible people unless you're confident
- Do NOT include reflections or images-within-images
- Do NOT round up for crowds (give exact count or range)

If the count is uncertain, provide a range (e.g., "15-18 people").`,
          },
        ],
      },
    ],
  })

  return response.content[0].text
}
```

**Why this works**: Negative instructions prevent common errors by explicitly highlighting what to avoid.

## Multi-Image Analysis Techniques

### Technique 1: Sequential Comparison

Analyze images in sequence, building understanding progressively.

```typescript
async function sequentialComparison(imagePaths: string[]) {
  const content: any[] = [
    {
      type: 'text',
      text: 'I will show you a sequence of images showing the progression of a UI design. Analyze how the design evolved.',
    },
  ]

  imagePaths.forEach((path, index) => {
    const { data, mediaType } = encodeImage(path)
    content.push({
      type: 'text',
      text: `Version ${index + 1}:`,
    })
    content.push({
      type: 'image',
      source: { type: 'base64', media_type: mediaType, data },
    })
  })

  content.push({
    type: 'text',
    text: `For each version, identify:
1. Major changes from the previous version
2. Improvements made
3. Potential issues introduced

Then provide an overall assessment of the design evolution.`,
  })

  const response = await client.messages.create({
    model: 'claude-3-5-sonnet-20250129',
    max_tokens: 3000,
    messages: [{ role: 'user', content }],
  })

  return response.content[0].text
}
```

### Technique 2: Comparative Analysis

Direct comparison between specific images.

```typescript
interface ComparisonResult {
  similarities: string[]
  differences: string[]
  winner?: string
  reasoning: string
}

async function compareImages(
  image1Path: string,
  image2Path: string,
  criteria: string
): Promise<ComparisonResult> {
  const img1 = encodeImage(image1Path)
  const img2 = encodeImage(image2Path)

  const response = await client.messages.create({
    model: 'claude-3-5-sonnet-20250129',
    max_tokens: 1500,
    messages: [
      {
        role: 'user',
        content: [
          { type: 'text', text: 'Image A:' },
          { type: 'image', source: { type: 'base64', media_type: img1.mediaType, data: img1.data } },
          { type: 'text', text: 'Image B:' },
          { type: 'image', source: { type: 'base64', media_type: img2.mediaType, data: img2.data } },
          {
            type: 'text',
            text: `Compare these two images based on: ${criteria}

Return JSON:
{
  "similarities": ["list of similarities"],
  "differences": ["list of differences"],
  "winner": "A or B (or null if tie)",
  "reasoning": "explanation of your assessment"
}`,
          },
        ],
      },
    ],
  })

  return JSON.parse(response.content[0].text)
}

// Example: Compare product photos
const result = await compareImages(
  './product-photo-v1.jpg',
  './product-photo-v2.jpg',
  'professional photography quality, lighting, composition, and product visibility'
)

console.log('Winner:', result.winner)
console.log('Reasoning:', result.reasoning)
```

### Technique 3: Multi-Image Aggregation

Aggregate information across multiple images.

```typescript
interface AggregatedData {
  totalItems: number
  categories: Record<string, number>
  insights: string[]
  summary: string
}

async function aggregateMultipleImages(
  imagePaths: string[],
  aggregationType: string
): Promise<AggregatedData> {
  const content: any[] = [
    {
      type: 'text',
      text: `You will analyze ${imagePaths.length} images to ${aggregationType}. Here are the images:`,
    },
  ]

  imagePaths.forEach((path, index) => {
    const { data, mediaType } = encodeImage(path)
    content.push({
      type: 'text',
      text: `Image ${index + 1}:`,
    })
    content.push({
      type: 'image',
      source: { type: 'base64', media_type: mediaType, data },
    })
  })

  content.push({
    type: 'text',
    text: `Analyze all images together and return JSON:
{
  "totalItems": number (total count across all images),
  "categories": {"category": count},
  "insights": ["key insight 1", "insight 2"],
  "summary": "overall summary of findings"
}`,
  })

  const response = await client.messages.create({
    model: 'claude-3-5-sonnet-20250129',
    max_tokens: 2048,
    messages: [{ role: 'user', content }],
  })

  return JSON.parse(response.content[0].text)
}

// Example: Inventory analysis
const inventory = await aggregateMultipleImages(
  ['./shelf1.jpg', './shelf2.jpg', './shelf3.jpg'],
  'count total products and categorize by type'
)

console.log('Total products:', inventory.totalItems)
console.log('Categories:', inventory.categories)
```

### Technique 4: Before/After Analysis

Specialized pattern for temporal comparisons.

```typescript
interface BeforeAfterAnalysis {
  changes: {
    added: string[]
    removed: string[]
    modified: string[]
  }
  impact: 'positive' | 'negative' | 'neutral' | 'mixed'
  details: string
}

async function beforeAfterAnalysis(
  beforePath: string,
  afterPath: string,
  context: string
): Promise<BeforeAfterAnalysis> {
  const before = encodeImage(beforePath)
  const after = encodeImage(afterPath)

  const response = await client.messages.create({
    model: 'claude-3-5-sonnet-20250129',
    max_tokens: 1500,
    messages: [
      {
        role: 'user',
        content: [
          { type: 'text', text: `Context: ${context}\n\nBEFORE:` },
          { type: 'image', source: { type: 'base64', media_type: before.mediaType, data: before.data } },
          { type: 'text', text: 'AFTER:' },
          { type: 'image', source: { type: 'base64', media_type: after.mediaType, data: after.data } },
          {
            type: 'text',
            text: `Analyze what changed between BEFORE and AFTER. Return JSON:
{
  "changes": {
    "added": ["new elements"],
    "removed": ["removed elements"],
    "modified": ["changed elements"]
  },
  "impact": "positive|negative|neutral|mixed",
  "details": "detailed explanation of changes and their significance"
}`,
          },
        ],
      },
    ],
  })

  return JSON.parse(response.content[0].text)
}

// Example: Website redesign analysis
const analysis = await beforeAfterAnalysis(
  './homepage-old.png',
  './homepage-new.png',
  'Homepage redesign for better user engagement'
)

console.log('Added:', analysis.changes.added)
console.log('Removed:', analysis.changes.removed)
console.log('Impact:', analysis.impact)
```

## Advanced Prompting Strategies

### Strategy 1: Chain of Thought for Vision

Guide the model through visual reasoning step-by-step.

```typescript
async function chainOfThoughtVision(imagePath: string, question: string) {
  const { data, mediaType } = encodeImage(imagePath)

  const response = await client.messages.create({
    model: 'claude-3-5-sonnet-20250129',
    max_tokens: 2048,
    messages: [
      {
        role: 'user',
        content: [
          {
            type: 'image',
            source: { type: 'base64', media_type: mediaType, data },
          },
          {
            type: 'text',
            text: `Question: ${question}

Before answering, think through this step by step:
1. What do I see in the image? (describe key elements)
2. What clues are relevant to the question?
3. What can I infer from these clues?
4. What is my conclusion?

Provide your reasoning for each step, then give your final answer.`,
          },
        ],
      },
    ],
  })

  return response.content[0].text
}

// Example
const result = await chainOfThoughtVision(
  './store-shelf.jpg',
  'Is this store well-stocked for the holiday season?'
)
```

### Strategy 2: Confidence Scoring

Ask the model to assess its own confidence.

```typescript
interface ConfidentAnalysis {
  answer: string
  confidence: 'high' | 'medium' | 'low'
  uncertainties: string[]
  assumptions: string[]
}

async function confidenceBasedAnalysis(
  imagePath: string,
  question: string
): Promise<ConfidentAnalysis> {
  const { data, mediaType } = encodeImage(imagePath)

  const response = await client.messages.create({
    model: 'claude-3-5-sonnet-20250129',
    max_tokens: 1024,
    messages: [
      {
        role: 'user',
        content: [
          {
            type: 'image',
            source: { type: 'base64', media_type: mediaType, data },
          },
          {
            type: 'text',
            text: `${question}

Provide your answer along with:
- Confidence level (high/medium/low)
- Any uncertainties or ambiguities
- Assumptions you're making

Return as JSON:
{
  "answer": "your answer",
  "confidence": "high|medium|low",
  "uncertainties": ["list of things you're uncertain about"],
  "assumptions": ["assumptions you're making"]
}`,
          },
        ],
      },
    ],
  })

  return JSON.parse(response.content[0].text)
}

// Example: Product condition assessment
const assessment = await confidenceBasedAnalysis(
  './used-item.jpg',
  'What condition is this item in? (new/like-new/good/fair/poor)'
)

if (assessment.confidence === 'low') {
  console.log('Low confidence. Uncertainties:', assessment.uncertainties)
  // Perhaps request manual review
}
```

### Strategy 3: Focused Attention

Direct the model's attention to specific regions.

```typescript
async function focusedAttention(imagePath: string) {
  const { data, mediaType } = encodeImage(imagePath)

  const response = await client.messages.create({
    model: 'claude-3-5-sonnet-20250129',
    max_tokens: 1024,
    messages: [
      {
        role: 'user',
        content: [
          {
            type: 'image',
            source: { type: 'base64', media_type: mediaType, data },
          },
          {
            type: 'text',
            text: `Focus your attention on the TOP-RIGHT corner of this image.

What text or numbers are visible in that specific region?

Ignore everything else in the image.`,
          },
        ],
      },
    ],
  })

  return response.content[0].text
}
```

### Strategy 4: Iterative Refinement

Use multi-turn conversations to refine analysis.

```typescript
async function iterativeImageAnalysis(imagePath: string) {
  const { data, mediaType } = encodeImage(imagePath)

  // First pass: broad analysis
  const response1 = await client.messages.create({
    model: 'claude-3-5-sonnet-20250129',
    max_tokens: 1024,
    messages: [
      {
        role: 'user',
        content: [
          { type: 'image', source: { type: 'base64', media_type: mediaType, data } },
          { type: 'text', text: 'Describe this document at a high level. What type of document is it?' },
        ],
      },
    ],
  })

  const docType = response1.content[0].text

  // Second pass: targeted extraction based on document type
  const response2 = await client.messages.create({
    model: 'claude-3-5-sonnet-20250129',
    max_tokens: 2048,
    messages: [
      {
        role: 'user',
        content: [
          { type: 'image', source: { type: 'base64', media_type: mediaType, data } },
          { type: 'text', text: 'Describe this document at a high level. What type of document is it?' },
        ],
      },
      {
        role: 'assistant',
        content: docType,
      },
      {
        role: 'user',
        content: `Based on it being a ${docType}, extract all relevant structured data in JSON format.`,
      },
    ],
  })

  return {
    documentType: docType,
    extractedData: JSON.parse(response2.content[0].text),
  }
}
```

### Strategy 5: Structured Output with Schema

Provide explicit JSON schemas for complex extractions.

```typescript
interface ProductSchema {
  name: string
  brand?: string
  price?: {
    amount: number
    currency: string
  }
  features: string[]
  condition: 'new' | 'like-new' | 'used' | 'refurbished'
  category: string
}

async function structuredExtraction(imagePath: string): Promise<ProductSchema> {
  const { data, mediaType } = encodeImage(imagePath)

  const schema = `{
  "name": "product name (required)",
  "brand": "brand name (optional)",
  "price": {
    "amount": number,
    "currency": "USD|EUR|GBP"
  },
  "features": ["feature 1", "feature 2"],
  "condition": "new|like-new|used|refurbished",
  "category": "product category"
}`

  const response = await client.messages.create({
    model: 'claude-3-5-sonnet-20250129',
    max_tokens: 1024,
    messages: [
      {
        role: 'user',
        content: [
          {
            type: 'image',
            source: { type: 'base64', media_type: mediaType, data },
          },
          {
            type: 'text',
            text: `Extract product information from this image.

Required output schema:
${schema}

Rules:
- If price is not visible, omit the price field
- List all visible features
- Infer condition from visual appearance
- Use null for truly unknown fields (but avoid if possible)

Return valid JSON only.`,
          },
        ],
      },
    ],
  })

  return JSON.parse(response.content[0].text)
}
```

## Common Pitfalls and Solutions

### Pitfall 1: Vague Prompts

**Problem**: Generic prompts yield generic results.

```typescript
// Bad: Too vague
const bad = await analyzeImage(
  './screenshot.png',
  'What is this?'
)
// Result: "This appears to be a screenshot of a web application."

// Good: Specific and actionable
const good = await analyzeImage(
  './screenshot.png',
  `Identify this web application and provide:
1. Application name/type (if recognizable)
2. Primary functionality based on visible UI
3. Technology stack indicators (frameworks, libraries)
4. User role/permissions visible
5. Current page or view name`
)
// Result: Detailed, structured analysis
```

**Solution**: Always be specific about what you want extracted, analyzed, or described.

### Pitfall 2: Ignoring Image Quality

**Problem**: Poor image quality leads to poor results.

```typescript
function validateImageQuality(imagePath: string): {
  valid: boolean
  issues: string[]
} {
  const stats = fs.statSync(imagePath)
  const sizeInMB = stats.size / (1024 * 1024)
  const issues: string[] = []

  // Check file size
  if (sizeInMB > 5) {
    issues.push('Image exceeds 5MB limit')
  }

  if (sizeInMB < 0.01) {
    issues.push('Image is very small, may be low quality')
  }

  // You could add more checks:
  // - Image dimensions using sharp or similar
  // - File corruption checks
  // - Format validation

  return {
    valid: issues.length === 0,
    issues,
  }
}

async function safeImageAnalysis(imagePath: string, prompt: string) {
  const validation = validateImageQuality(imagePath)

  if (!validation.valid) {
    console.warn('Image quality issues:', validation.issues)
    // Optionally enhance or reject the image
  }

  return await analyzeImage(imagePath, prompt)
}
```

**Solution**: Validate images before sending them to the API.

### Pitfall 3: Not Handling Uncertainty

**Problem**: Treating all model outputs as equally reliable.

```typescript
// Bad: Assuming confidence without checking
const count = await analyzeImage('./crowd.jpg', 'How many people?')
const number = parseInt(count) // Dangerous if model expressed uncertainty

// Good: Explicitly handle uncertainty
interface CountResult {
  count: number | null
  range?: { min: number; max: number }
  confidence: 'high' | 'medium' | 'low'
  explanation: string
}

async function robustCounting(imagePath: string, objectType: string): Promise<CountResult> {
  const { data, mediaType } = encodeImage(imagePath)

  const response = await client.messages.create({
    model: 'claude-3-5-sonnet-20250129',
    max_tokens: 512,
    messages: [
      {
        role: 'user',
        content: [
          { type: 'image', source: { type: 'base64', media_type: mediaType, data } },
          {
            type: 'text',
            text: `Count ${objectType} in this image. Return JSON:
{
  "count": exact_number or null if uncertain,
  "range": {"min": number, "max": number} if count is approximate,
  "confidence": "high|medium|low",
  "explanation": "explain your count and confidence level"
}`,
          },
        ],
      },
    ],
  })

  return JSON.parse(response.content[0].text)
}

const result = await robustCounting('./parking-lot.jpg', 'cars')
if (result.confidence !== 'high') {
  console.log(`Uncertain count: ${result.range?.min}-${result.range?.max}`)
}
```

**Solution**: Build uncertainty handling into your prompts and data structures.

### Pitfall 4: Overloading Single Requests

**Problem**: Trying to do too much in one API call.

```typescript
// Bad: Too many tasks at once
const overloaded = await analyzeImage(
  './complex-scene.jpg',
  `1. Count all people
2. Identify all vehicles
3. Read all visible text
4. Describe the weather
5. Determine the location
6. Assess the time of day
7. Identify all brands/logos
8. Describe everyone's clothing
9. List all colors present
10. Estimate the photo's purpose`
)
// Result: Rushed, incomplete analysis

// Good: Break into focused requests or use structured approach
async function comprehensiveAnalysis(imagePath: string) {
  const { data, mediaType } = encodeImage(imagePath)

  // Request 1: Scene understanding
  const scene = await client.messages.create({
    model: 'claude-3-5-sonnet-20250129',
    max_tokens: 1024,
    messages: [
      {
        role: 'user',
        content: [
          { type: 'image', source: { type: 'base64', media_type: mediaType, data } },
          { type: 'text', text: 'Describe the overall scene: setting, time of day, weather, general activity.' },
        ],
      },
    ],
  })

  // Request 2: Object counting
  const objects = await client.messages.create({
    model: 'claude-3-5-sonnet-20250129',
    max_tokens: 512,
    messages: [
      {
        role: 'user',
        content: [
          { type: 'image', source: { type: 'base64', media_type: mediaType, data } },
          { type: 'text', text: 'Count people and vehicles. Return JSON: {"people": number, "vehicles": number}' },
        ],
      },
    ],
  })

  // Request 3: Text extraction
  const text = await client.messages.create({
    model: 'claude-3-5-sonnet-20250129',
    max_tokens: 1024,
    messages: [
      {
        role: 'user',
        content: [
          { type: 'image', source: { type: 'base64', media_type: mediaType, data } },
          { type: 'text', text: 'Extract all visible text and identify brands/logos.' },
        ],
      },
    ],
  })

  return {
    scene: scene.content[0].text,
    objects: JSON.parse(objects.content[0].text),
    text: text.content[0].text,
  }
}
```

**Solution**: Break complex analysis into multiple focused requests.

### Pitfall 5: Assuming Perfect OCR

**Problem**: Expecting 100% accurate text extraction from images.

```typescript
// Bad: No error handling for OCR
const text = await analyzeImage('./receipt.jpg', 'Extract all text')
const total = parseFloat(text.match(/Total: \$(\d+\.\d+)/)[1]) // Can crash

// Good: Robust text extraction with validation
interface OCRResult {
  text: string
  confidence: number
  warnings: string[]
}

async function robustOCR(imagePath: string): Promise<OCRResult> {
  const { data, mediaType } = encodeImage(imagePath)

  const response = await client.messages.create({
    model: 'claude-3-5-sonnet-20250129',
    max_tokens: 2048,
    messages: [
      {
        role: 'user',
        content: [
          { type: 'image', source: { type: 'base64', media_type: mediaType, data } },
          {
            type: 'text',
            text: `Extract all text from this image. Return JSON:
{
  "text": "full extracted text",
  "confidence": 0-100 (your confidence in accuracy),
  "warnings": ["any issues: blurry areas, unclear text, etc."]
}`,
          },
        ],
      },
    ],
  })

  const result = JSON.parse(response.content[0].text)

  if (result.confidence < 70) {
    console.warn('Low OCR confidence:', result.warnings)
    // Maybe request human review
  }

  return result
}
```

**Solution**: Always validate OCR results and handle edge cases.

## Best Practices for Production

### Practice 1: Implement Retry Logic

```typescript
async function analyzeImageWithRetry(
  imagePath: string,
  prompt: string,
  maxRetries = 3
): Promise<string> {
  let lastError: Error | null = null

  for (let attempt = 1; attempt <= maxRetries; attempt++) {
    try {
      return await analyzeImage(imagePath, prompt)
    } catch (error) {
      lastError = error as Error
      console.log(`Attempt ${attempt} failed:`, error.message)

      if (attempt < maxRetries) {
        // Exponential backoff
        const delay = Math.pow(2, attempt) * 1000
        await new Promise(resolve => setTimeout(resolve, delay))
      }
    }
  }

  throw new Error(`Failed after ${maxRetries} attempts: ${lastError?.message}`)
}
```

### Practice 2: Cache Results

```typescript
import crypto from 'crypto'

const cache = new Map<string, any>()

function getCacheKey(imagePath: string, prompt: string): string {
  const imageHash = crypto
    .createHash('md5')
    .update(fs.readFileSync(imagePath))
    .digest('hex')

  const promptHash = crypto
    .createHash('md5')
    .update(prompt)
    .digest('hex')

  return `${imageHash}-${promptHash}`
}

async function cachedImageAnalysis(
  imagePath: string,
  prompt: string
): Promise<string> {
  const cacheKey = getCacheKey(imagePath, prompt)

  if (cache.has(cacheKey)) {
    console.log('Cache hit!')
    return cache.get(cacheKey)
  }

  console.log('Cache miss, analyzing...')
  const result = await analyzeImage(imagePath, prompt)
  cache.set(cacheKey, result)

  return result
}
```

### Practice 3: Monitor Costs

```typescript
interface UsageMetrics {
  totalRequests: number
  totalImages: number
  totalTokens: number
  estimatedCost: number
}

class VisionAPIMonitor {
  private metrics: UsageMetrics = {
    totalRequests: 0,
    totalImages: 0,
    totalTokens: 0,
    estimatedCost: 0,
  }

  async trackAnalysis(
    imagePath: string,
    prompt: string,
    operation: () => Promise<string>
  ): Promise<string> {
    this.metrics.totalRequests++
    this.metrics.totalImages++

    const result = await operation()

    // Estimate tokens (very rough)
    const imageTokens = 1000 // Average tokens for an image
    const textTokens = result.length / 4 // Rough estimate
    this.metrics.totalTokens += imageTokens + textTokens

    // Rough cost estimate (update with actual pricing)
    this.metrics.estimatedCost += (imageTokens + textTokens) * 0.000003

    return result
  }

  getMetrics(): UsageMetrics {
    return { ...this.metrics }
  }

  reset() {
    this.metrics = {
      totalRequests: 0,
      totalImages: 0,
      totalTokens: 0,
      estimatedCost: 0,
    }
  }
}

// Usage
const monitor = new VisionAPIMonitor()

const result = await monitor.trackAnalysis(
  './image.jpg',
  'Analyze this',
  () => analyzeImage('./image.jpg', 'Analyze this')
)

console.log('Metrics:', monitor.getMetrics())
```

### Practice 4: Validate Outputs

```typescript
function validateJSON<T>(
  jsonString: string,
  schema: (obj: any) => boolean
): T {
  let parsed: any

  try {
    parsed = JSON.parse(jsonString)
  } catch (error) {
    throw new Error('Invalid JSON response from model')
  }

  if (!schema(parsed)) {
    throw new Error('Response does not match expected schema')
  }

  return parsed as T
}

// Example schema validator
function isReceiptData(obj: any): boolean {
  return (
    typeof obj === 'object' &&
    typeof obj.merchant === 'string' &&
    typeof obj.total === 'number' &&
    Array.isArray(obj.items)
  )
}

async function extractReceiptSafely(imagePath: string) {
  const result = await analyzeImage(imagePath, 'Extract receipt data as JSON...')

  return validateJSON(result, isReceiptData)
}
```

## Summary and Key Takeaways

### Core Principles

1. **Context is crucial**: Always provide context before images
2. **Be specific**: Vague prompts yield vague results
3. **Structure matters**: Use clear patterns and formats
4. **Handle uncertainty**: Build in confidence checking
5. **Test thoroughly**: Vision models aren't perfect

### Effective Patterns

- Context → Image → Question
- Image → Label → Question
- Multi-step reasoning
- Role + Task + Constraints
- Few-shot learning with examples

### Multi-Image Strategies

- Sequential comparison for progressions
- Direct comparison for differences
- Aggregation for combined analysis
- Before/after for temporal changes

### Production Checklist

- [ ] Validate image quality before processing
- [ ] Implement retry logic for failures
- [ ] Cache results to reduce costs
- [ ] Monitor usage and costs
- [ ] Validate outputs against schemas
- [ ] Handle uncertainty explicitly
- [ ] Break complex tasks into focused requests
- [ ] Provide clear examples when possible

### When Multimodal Prompting Excels

Use these techniques for:
- Document processing and data extraction
- Visual quality assessment
- Comparative analysis
- Content categorization
- Accessibility features
- Visual search and matching
- Multi-step visual reasoning

### Common Mistakes to Avoid

- Sending images without clear instructions
- Expecting perfect accuracy without validation
- Overloading single requests
- Ignoring image quality issues
- Not handling model uncertainty
- Skipping output validation

## Practice Exercises

Test your multimodal prompting skills with these exercises:

1. **Receipt Comparison**: Create a function that compares two receipts and identifies which had better deals

2. **UI Evolution Tracker**: Build a system that analyzes a series of UI screenshots and generates a changelog

3. **Product Catalog Builder**: Extract structured product data from 10 different product photos with varying quality

4. **Document Router**: Create a classifier that routes different document types to specialized extraction functions

5. **Before/After Analyzer**: Build a tool that analyzes home renovation photos and estimates improvement value

6. **Multi-Image Story**: Take 5 sequential images and generate a narrative description of what happened

Remember: Great multimodal prompting is about combining visual understanding with clear textual guidance. The best results come from treating images and text as complementary information sources, each strengthening the other.
