{
  "id": "lab-3-monitoring",
  "title": "Production Monitoring with Sentry",
  "description": "Implement comprehensive monitoring and error tracking for AI applications using Sentry, custom metrics, and alerting",
  "difficulty": "intermediate",
  "estimatedMinutes": 180,
  "language": "typescript",
  "starterCode": "// Custom monitoring utilities\nimport * as Sentry from '@sentry/nextjs'\n\ninterface AIMetrics {\n  requestCount: number\n  totalLatency: number\n  totalTokens: number\n  totalCost: number\n  errorCount: number\n}\n\nexport class AIMonitor {\n  // TODO: Track AI request metrics\n  static async trackAIRequest(\n    operation: string,\n    callback: () => Promise<any>\n  ) {\n    // 1. Start Sentry transaction\n    // 2. Record start time\n    // 3. Execute callback\n    // 4. Record end time and metrics\n    // 5. Add custom tags and measurements\n    // 6. Finish transaction\n  }\n  \n  // TODO: Record custom metric\n  static recordMetric(name: string, value: number, unit: string) {\n    // Send custom metric to Sentry\n  }\n  \n  // TODO: Track AI cost\n  static trackCost(model: string, inputTokens: number, outputTokens: number) {\n    // Calculate cost based on model pricing\n    // Record as custom metric\n  }\n  \n  // TODO: Set user context\n  static setUserContext(userId: string, tier: string, email?: string) {\n    Sentry.setUser({\n      id: userId,\n      // Add additional context\n    })\n  }\n  \n  // TODO: Add breadcrumb for user action\n  static addBreadcrumb(message: string, category: string, data?: any) {\n    Sentry.addBreadcrumb({\n      message,\n      category,\n      data,\n      level: 'info',\n    })\n  }\n  \n  // TODO: Capture exception with context\n  static captureException(error: Error, context?: Record<string, any>) {\n    if (context) {\n      Sentry.setContext('error_context', context)\n    }\n    Sentry.captureException(error)\n  }\n}",
  "instructions": "Implement comprehensive monitoring and error tracking for AI applications using Sentry.\n\n**Objectives:**\n- Integrate Sentry SDK in Next.js application (client + server)\n- Track all errors with full stack traces and context\n- Monitor AI API performance (response time, token usage, costs)\n- Create custom metrics for AI-specific monitoring\n- Set up alerts for critical errors and performance issues\n- Implement user feedback mechanism for errors\n\n**Implementation Steps:**\n\n1. **Set up Sentry:**\n   - Install @sentry/nextjs package\n   - Run: npx @sentry/wizard@latest -i nextjs\n   - Configure sentry.client.config.ts and sentry.server.config.ts\n   - Set environment variables (NEXT_PUBLIC_SENTRY_DSN)\n   - Enable source maps in next.config.js\n\n2. **Configure Sentry client (sentry.client.config.ts):**\n   - Set appropriate sample rate (tracesSampleRate: 1.0 for dev, 0.1 for prod)\n   - Enable debug mode in development\n   - Set environment (development, staging, production)\n   - Add beforeSend hook to filter sensitive data\n\n3. **Configure Sentry server (sentry.server.config.ts):**\n   - Same configuration as client\n   - Add server-specific integrations\n\n4. **Implement AIMonitor.trackAIRequest:**\n   - Start Sentry transaction with Sentry.startTransaction()\n   - Record start time with Date.now()\n   - Execute callback and catch errors\n   - Record end time and calculate latency\n   - Add custom measurements (latency, tokens, cost)\n   - Add tags (operation, model, user_tier)\n   - Finish transaction\n\n5. **Implement AIMonitor.trackCost:**\n   - Calculate cost based on model pricing:\n     - GPT-4: $0.03/1K input tokens, $0.06/1K output tokens\n     - GPT-3.5-turbo: $0.0015/1K input, $0.002/1K output\n   - Record as custom metric with Sentry\n\n6. **Implement AIMonitor.setUserContext:**\n   - Use Sentry.setUser() with user ID, tier, email\n   - Add custom context with Sentry.setContext()\n\n7. **Implement AIMonitor.addBreadcrumb:**\n   - Use Sentry.addBreadcrumb() to track user actions\n   - Include message, category, data, timestamp\n\n8. **Implement AIMonitor.captureException:**\n   - Set error context with Sentry.setContext()\n   - Capture exception with Sentry.captureException()\n\n9. **Create monitored AI endpoint:**\n   - Set user context at request start\n   - Add breadcrumb for request received\n   - Track AI request with AIMonitor.trackAIRequest\n   - Track costs with AIMonitor.trackCost\n   - Capture exceptions with context\n\n10. **Set up alerts in Sentry:**\n    - Configure alert for high error rate (>5%)\n    - Configure alert for slow responses (>10s)\n    - Configure alert for cost spikes (>$10/hour)\n    - Set up Slack/email notifications\n\n**Custom Metrics to Track:**\n- AI requests per minute\n- Average latency\n- Error rate\n- Cost per hour\n- Token usage\n\n**Testing:**\n- Trigger test errors and verify they appear in Sentry\n- Verify stack traces show original TypeScript code (source maps)\n- Check performance transactions are recorded\n- Simulate error spike and verify alert fires\n- Verify user context helps debugging",
  "testCases": [
    {
      "name": "Unhandled error captured by Sentry",
      "input": {
        "triggerError": "throw new Error('Test error')"
      },
      "expected": {
        "errorInSentry": true,
        "hasStackTrace": true,
        "hasBreadcrumbs": true,
        "appearsWithinMinutes": 1
      }
    },
    {
      "name": "AI API performance tracked",
      "input": {
        "apiRequest": true
      },
      "expected": {
        "transactionRecorded": true,
        "hasLatency": true,
        "hasTokenUsage": true,
        "customMetricsUpdated": true
      }
    },
    {
      "name": "Alert fires on high error rate",
      "input": {
        "errorCount": 6,
        "timeWindowMinutes": 1
      },
      "expected": {
        "alertFires": true,
        "notificationReceived": true,
        "alertDetailsAccurate": true
      }
    },
    {
      "name": "Cost tracking accurate",
      "input": {
        "model": "gpt-4",
        "inputTokens": 1000,
        "outputTokens": 500
      },
      "expected": {
        "calculatedCost": 0.06,
        "costMetricRecorded": true
      }
    },
    {
      "name": "User context helps debugging",
      "input": {
        "userId": "user123",
        "tier": "pro",
        "triggerError": true
      },
      "expected": {
        "eventHasUserId": true,
        "eventHasTier": true,
        "contextRelevant": true
      }
    },
    {
      "name": "Source maps work correctly",
      "input": {
        "triggerError": true
      },
      "expected": {
        "stackTraceShowsTypeScript": true,
        "notCompiledJS": true,
        "lineNumbersAccurate": true
      }
    }
  ],
  "hints": [
    "Run npx @sentry/wizard@latest -i nextjs to auto-configure Sentry",
    "Enable source maps in next.config.js for debugging production errors",
    "Use Sentry.startTransaction() for performance monitoring",
    "Add spans for sub-operations (DB queries, API calls)",
    "Set custom measurements for token usage and costs",
    "In production, use tracesSampleRate: 0.1 to reduce overhead",
    "Use tracesSampler function for dynamic sampling by transaction type",
    "Calculate costs: GPT-4 = $0.03/1K input + $0.06/1K output tokens",
    "Configure alerts in Sentry web UI for error spikes and performance issues",
    "Set up Slack or email notifications for critical alerts",
    "Use Sentry.setUser() and Sentry.setContext() for debugging context",
    "Filter sensitive data in beforeSend hook"
  ]
}
