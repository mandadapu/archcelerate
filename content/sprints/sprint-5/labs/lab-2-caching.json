{
  "id": "lab-2-caching",
  "title": "Implement AI Response Caching",
  "description": "Build an intelligent caching system for AI responses using semantic similarity to reduce API costs and improve response times",
  "difficulty": "advanced",
  "estimatedMinutes": 240,
  "language": "typescript",
  "starterCode": "// AI response caching with semantic similarity\nimport { Redis } from '@upstash/redis'\nimport { OpenAI } from 'openai'\n\nconst redis = new Redis({\n  url: process.env.UPSTASH_REDIS_REST_URL!,\n  token: process.env.UPSTASH_REDIS_REST_TOKEN!,\n})\n\nconst openai = new OpenAI({\n  apiKey: process.env.OPENAI_API_KEY,\n})\n\ninterface CachedResponse {\n  query: string\n  embedding: number[]\n  response: string\n  timestamp: number\n  model: string\n}\n\nexport interface CacheStats {\n  hits: number\n  misses: number\n  apiCalls: number\n  costSaved: number\n}\n\nconst SIMILARITY_THRESHOLD = 0.95\nconst CACHE_TTL = 3600 // 1 hour in seconds\n\nexport async function generateEmbedding(text: string): Promise<number[]> {\n  // TODO: Generate embedding using OpenAI embeddings API\n  // Use model: 'text-embedding-3-small'\n  return []\n}\n\nexport function cosineSimilarity(a: number[], b: number[]): number {\n  // TODO: Calculate cosine similarity between two vectors\n  // Formula: dot(a,b) / (magnitude(a) * magnitude(b))\n  return 0\n}\n\nexport async function getCachedResponse(\n  query: string\n): Promise<string | null> {\n  // TODO: Check cache for similar query\n  // 1. Generate embedding for query\n  // 2. Find similar cached response\n  // 3. Update cache statistics (hit)\n  // 4. Return cached response if found\n  return null\n}\n\nexport async function cacheResponse(\n  query: string,\n  response: string,\n  model: string\n): Promise<void> {\n  // TODO: Cache AI response\n  // 1. Generate embedding for query\n  // 2. Create cache entry with query, embedding, response\n  // 3. Store in Redis with TTL\n  // 4. Update cache statistics (API call)\n}",
  "instructions": "Build an intelligent caching system for AI responses using semantic similarity.\n\n**Objectives:**\n- Cache AI responses in Redis with TTL\n- Implement semantic similarity matching (cosine similarity > 0.95 = cache hit)\n- Return cached responses for similar queries instead of calling API\n- Track cache hit/miss rates and cost savings\n- Implement cache invalidation strategies\n\n**Implementation Steps:**\n\n1. **Implement generateEmbedding function:**\n   - Use OpenAI embeddings API with model 'text-embedding-3-small'\n   - Pass the text to openai.embeddings.create()\n   - Extract and return the embedding vector\n   - Handle errors gracefully\n\n2. **Implement cosineSimilarity function:**\n   - Calculate dot product: sum of (a[i] * b[i]) for all i\n   - Calculate magnitude of a: sqrt(sum of a[i]^2)\n   - Calculate magnitude of b: sqrt(sum of b[i]^2)\n   - Return: dot product / (magnitude_a * magnitude_b)\n\n3. **Implement findSimilarCache function:**\n   - Get all cached query embeddings from Redis\n   - Calculate similarity with each cached embedding\n   - Find best match if similarity > SIMILARITY_THRESHOLD (0.95)\n   - Return cached response or null\n\n4. **Implement getCachedResponse function:**\n   - Generate embedding for query\n   - Search for similar cached response\n   - If found, update cache hit statistics\n   - Return cached response or null\n\n5. **Implement cacheResponse function:**\n   - Generate embedding for query\n   - Create cache entry with query, embedding, response, timestamp\n   - Store in Redis with hash key like 'cache:query:{id}'\n   - Set TTL to CACHE_TTL (1 hour)\n   - Update cache statistics (API call counter)\n\n6. **Implement getCacheStats function:**\n   - Get statistics from Redis (hits, misses, API calls)\n   - Calculate cost saved based on API call pricing\n   - Return CacheStats object\n\n7. **Create AI chat endpoint with caching:**\n   - Check cache first with getCachedResponse\n   - If cache hit, return cached response\n   - If cache miss, call OpenAI API\n   - Cache the response with cacheResponse\n   - Return response with cache indicator\n\n**Technical Requirements:**\n- Use Upstash Redis for distributed caching\n- Generate embeddings for similarity comparison\n- Store embeddings and responses efficiently\n- Handle cache misses by calling AI API\n- Add cache statistics and monitoring\n\n**Cost Calculation:**\n- OpenAI GPT-4: ~$0.03 per 1K input tokens, ~$0.06 per 1K output tokens\n- Embeddings: ~$0.00002 per 1K tokens\n- Calculate savings based on average response length\n\n**Testing:**\n- Test exact query match returns cached response\n- Test similar query matches semantically (>0.95 similarity)\n- Test different query misses cache (<0.95 similarity)\n- Test cache expires after TTL\n- Test cache statistics track accurately",
  "testCases": [
    {
      "name": "Exact query match returns cached response",
      "input": {
        "query": "What is machine learning?",
        "secondQuery": "What is machine learning?"
      },
      "expected": {
        "firstCallsAPI": true,
        "secondFromCache": true,
        "responseTimeLessThan": 100
      }
    },
    {
      "name": "Similar query matches semantically",
      "input": {
        "query": "Explain neural networks",
        "similarQuery": "What are neural networks?"
      },
      "expected": {
        "similarity": ">0.95",
        "returnsCachedResponse": true,
        "noAPICall": true
      }
    },
    {
      "name": "Different query misses cache",
      "input": {
        "cachedQuery": "What is AI?",
        "differentQuery": "How to bake a cake?"
      },
      "expected": {
        "similarity": "<0.95",
        "cacheMiss": true,
        "callsAPI": true
      }
    },
    {
      "name": "Cache expires after TTL",
      "input": {
        "query": "Test query",
        "ttl": 60,
        "waitSeconds": 61
      },
      "expected": {
        "cacheHitWithinTTL": true,
        "cacheMissAfterTTL": true,
        "newAPICall": true
      }
    },
    {
      "name": "Cache statistics track accurately",
      "input": {
        "uniqueQueries": 5,
        "duplicateQueries": 5
      },
      "expected": {
        "apiCalls": 5,
        "cacheHits": 5,
        "hitRate": 50
      }
    },
    {
      "name": "Cosine similarity calculation correct",
      "input": {
        "vectorA": [1, 0, 0],
        "vectorB": [1, 0, 0]
      },
      "expected": {
        "similarity": 1.0
      }
    }
  ],
  "hints": [
    "Use OpenAI text-embedding-3-small model for cost-effective embeddings",
    "Cosine similarity formula: dot(a,b) / (||a|| * ||b||)",
    "Threshold of 0.95 balances accuracy and hit rate",
    "Store embeddings as JSON arrays in Redis",
    "Use hash keys like 'cache:query:{id}' with fields for embedding, response, timestamp",
    "Set EXPIRE on Redis keys for automatic cleanup",
    "For production, consider Redis Stack with vector similarity search (VSS)",
    "Cache embeddings to avoid regenerating for same queries",
    "Track API calls saved to calculate cost savings",
    "Handle OpenAI API errors with proper fallbacks"
  ]
}
