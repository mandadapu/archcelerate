---
id: chunking-strategies
title: Chunking Strategies & Document Processing
description: Master techniques for breaking documents into optimal chunks for retrieval
difficulty: intermediate
estimatedMinutes: 60
order: 2
prerequisites: [vector-embeddings]
tags: [rag, chunking, preprocessing, document-processing]
---

# Chunking Strategies & Document Processing

The quality of your RAG system depends heavily on how you chunk documents. Good chunking improves retrieval accuracy and answer quality.

## Why Chunking Matters

### The Problem

LLMs have context limits, but documents can be huge:
- 100-page PDF = 50,000+ tokens
- Can't embed entire document as one vector
- Can't fit entire document in LLM context

### The Solution

Break documents into **chunks**: smaller, self-contained segments that can be:
- Embedded individually
- Retrieved independently
- Fit in LLM context window

## Chunking Trade-offs

### Chunk Size

**Small Chunks (100-200 tokens)**
- ✅ More precise retrieval
- ✅ Less noise in context
- ❌ May lose broader context
- ❌ More chunks to manage

**Large Chunks (500-1000 tokens)**
- ✅ More context per chunk
- ✅ Fewer chunks to store
- ❌ Less precise retrieval
- ❌ More irrelevant info

**Sweet Spot**: 200-500 tokens (~150-400 words)

### Overlap

**Purpose**: Ensure important info isn't split across chunk boundaries

```
Chunk 1: [0...500] tokens
Chunk 2: [450...950] tokens  ← 50 token overlap
Chunk 3: [900...1400] tokens
```

**Typical overlap**: 10-20% of chunk size (50-100 tokens)

## Chunking Strategies

### 1. Fixed-Size Chunking

Split text every N tokens/characters:

```typescript
function fixedSizeChunk(
  text: string,
  chunkSize: number = 500,
  overlap: number = 50
): string[] {
  const chunks: string[] = []
  let start = 0

  while (start < text.length) {
    const end = Math.min(start + chunkSize, text.length)
    chunks.push(text.slice(start, end))
    start = end - overlap
  }

  return chunks
}
```

**Pros**: Simple, predictable
**Cons**: May split sentences/paragraphs awkwardly

**Best for**: Unstructured text, quick prototypes

### 2. Sentence-Aware Chunking

Respect sentence boundaries:

```typescript
function sentenceAwareChunk(
  text: string,
  targetSize: number = 500
): string[] {
  const sentences = text.match(/[^.!?]+[.!?]+/g) || []
  const chunks: string[] = []
  let currentChunk = ''

  for (const sentence of sentences) {
    if ((currentChunk + sentence).length > targetSize && currentChunk) {
      chunks.push(currentChunk.trim())
      currentChunk = sentence
    } else {
      currentChunk += sentence
    }
  }

  if (currentChunk) chunks.push(currentChunk.trim())
  return chunks
}
```

**Pros**: Natural boundaries, coherent chunks
**Cons**: Variable chunk sizes

**Best for**: Articles, blog posts, general prose

### 3. Paragraph-Based Chunking

Use paragraph breaks as natural boundaries:

```typescript
function paragraphChunk(
  text: string,
  targetSize: number = 500
): string[] {
  const paragraphs = text.split(/\n\n+/)
  const chunks: string[] = []
  let currentChunk = ''

  for (const para of paragraphs) {
    if ((currentChunk + para).length > targetSize && currentChunk) {
      chunks.push(currentChunk.trim())
      currentChunk = para
    } else {
      currentChunk += '\n\n' + para
    }
  }

  if (currentChunk) chunks.push(currentChunk.trim())
  return chunks
}
```

**Pros**: Preserves document structure
**Cons**: Variable sizes, some paragraphs too large

**Best for**: Structured documents, markdown

### 4. Semantic Chunking

Use NLP to find topically coherent segments:

```typescript
// Conceptual example - requires ML model
async function semanticChunk(text: string): Promise<string[]> {
  // 1. Embed each sentence
  const sentences = splitIntoSentences(text)
  const embeddings = await embedSentences(sentences)

  // 2. Find similarity breakpoints
  const chunks: string[] = []
  let currentChunk: string[] = []

  for (let i = 0; i < sentences.length - 1; i++) {
    currentChunk.push(sentences[i])

    // If similarity drops, start new chunk
    const similarity = cosineSimilarity(embeddings[i], embeddings[i + 1])
    if (similarity < 0.7) {
      chunks.push(currentChunk.join(' '))
      currentChunk = []
    }
  }

  return chunks
}
```

**Pros**: Topically coherent, best retrieval quality
**Cons**: Expensive (requires embeddings), slower

**Best for**: High-value documents, production systems

### 5. Markdown-Aware Chunking

Respect markdown structure (headers, code blocks):

```typescript
function markdownChunk(markdown: string, targetSize: number = 500): string[] {
  const sections = markdown.split(/^#{1,6} /gm)
  const chunks: string[] = []

  for (const section of sections) {
    if (section.length &lt;= targetSize) {
      chunks.push(section.trim())
    } else {
      // Fall back to paragraph chunking for large sections
      chunks.push(...paragraphChunk(section, targetSize))
    }
  }

  return chunks
}
```

**Pros**: Preserves document hierarchy
**Cons**: Complex parsing

**Best for**: Documentation, technical content

## Metadata Enrichment

Add context to each chunk:

```typescript
interface EnrichedChunk {
  content: string
  metadata: {
    documentId: string
    documentTitle: string
    chunkIndex: number
    pageNumber?: number
    section?: string
    subsection?: string
    author?: string
    createdAt: Date
  }
}

function enrichChunk(
  chunk: string,
  index: number,
  doc: Document
): EnrichedChunk {
  return {
    content: chunk,
    metadata: {
      documentId: doc.id,
      documentTitle: doc.title,
      chunkIndex: index,
      pageNumber: extractPageNumber(chunk, doc),
      section: extractSection(chunk, doc),
      author: doc.author,
      createdAt: doc.createdAt,
    },
  }
}
```

**Why metadata matters**:
- Better citations ("From page 5 of Contract.pdf")
- Filtering ("Only show results from 2024")
- Reranking (prefer recent documents)

## Document Processing Pipeline

### Complete RAG Preprocessing

```typescript
async function processDocument(filePath: string): Promise<void> {
  // 1. Extract text
  const text = await extractText(filePath)

  // 2. Clean text
  const cleaned = cleanText(text)

  // 3. Chunk strategically
  const chunks = sentenceAwareChunk(cleaned, 500)

  // 4. Enrich with metadata
  const enriched = chunks.map((chunk, i) => enrichChunk(chunk, i, doc))

  // 5. Generate embeddings
  const embeddings = await batchEmbed(enriched.map(c => c.content))

  // 6. Store in vector DB
  await storeChunks(enriched, embeddings)
}
```

### Text Extraction

```typescript
import pdf from 'pdf-parse'
import mammoth from 'mammoth'

async function extractText(filePath: string): Promise<string> {
  const ext = path.extname(filePath).toLowerCase()

  if (ext === '.pdf') {
    const dataBuffer = fs.readFileSync(filePath)
    const data = await pdf(dataBuffer)
    return data.text
  }

  if (ext === '.docx') {
    const result = await mammoth.extractRawText({ path: filePath })
    return result.value
  }

  if (ext === '.txt' || ext === '.md') {
    return fs.readFileSync(filePath, 'utf-8')
  }

  throw new Error(`Unsupported file type: ${ext}`)
}
```

### Text Cleaning

```typescript
function cleanText(text: string): string {
  return text
    .replace(/\s+/g, ' ')           // Normalize whitespace
    .replace(/\n{3,}/g, '\n\n')     // Max 2 newlines
    .replace(/[^\S\n]+/g, ' ')      // Remove weird spaces
    .trim()
}
```

## Advanced Techniques

### 1. Hierarchical Chunking

Store multiple chunk sizes:

```
Document
├─ Large chunks (1000 tokens) → For context
└─ Small chunks (200 tokens)  → For retrieval
```

Search small chunks, but return large chunks to LLM.

### 2. Sliding Window with Context

Include surrounding chunks in metadata:

```typescript
interface ChunkWithContext {
  content: string
  previousChunk?: string
  nextChunk?: string
}
```

Helps LLM understand chunk in broader context.

### 3. Parent-Child Chunking

Small chunks for search, retrieve parent for context:

```typescript
{
  childChunk: "The contract expires on Dec 31, 2024",
  parentChunk: "Section 3: Term and Termination. This agreement..."
}
```

## Common Mistakes

### 1. Too Small Chunks
Chunks like "Yes" or "See above" have no context.
**Fix**: Minimum 100 tokens per chunk

### 2. No Overlap
Information at chunk boundaries gets lost.
**Fix**: 10-20% overlap

### 3. Ignoring Structure
Splitting mid-sentence or mid-code block.
**Fix**: Use structure-aware chunking

### 4. No Metadata
Can't cite sources or filter results.
**Fix**: Always store document metadata

## Choosing the Right Strategy

| Document Type | Best Strategy | Typical Size |
|--------------|---------------|--------------|
| Legal docs | Paragraph + metadata | 400-600 tokens |
| Technical docs | Markdown-aware | 300-500 tokens |
| Chatbot FAQs | Semantic | 200-400 tokens |
| Code repos | File/function-based | 200-800 tokens |
| General articles | Sentence-aware | 400-600 tokens |

## Testing Your Chunking

### Evaluation Criteria

1. **Chunk coherence**: Does each chunk make sense alone?
2. **Retrieval precision**: Do queries find the right chunks?
3. **Answer quality**: Can LLM answer from retrieved chunks?
4. **Coverage**: Are all important parts captured?

### Simple Test

```typescript
// Test: Can you answer questions from retrieved chunks?
const testCases = [
  { question: "What is the payment term?", expectedChunk: "Payment due within 30 days..." },
  { question: "Who is the contractor?", expectedChunk: "Contractor: Acme Corp..." },
]

for (const test of testCases) {
  const retrieved = await search(test.question)
  assert(retrieved[0].content.includes(test.expectedChunk))
}
```

## Next Steps

In the next lesson, you'll learn how to build complete retrieval pipelines with citation tracking, combining chunking, embedding, and generation.

## Resources

- [LangChain Text Splitters](https://js.langchain.com/docs/modules/data_connection/document_transformers/)
- [Pinecone Chunking Guide](https://www.pinecone.io/learn/chunking-strategies/)
- [pdf-parse npm package](https://www.npmjs.com/package/pdf-parse)
