{
  "id": "lab-3-prompt-optimizer",
  "title": "Automated Prompt Optimizer",
  "description": "Build a system that automatically optimizes prompts using A/B testing and metrics",
  "difficulty": "advanced",
  "estimatedMinutes": 50,
  "language": "typescript",
  "starterCode": "import Anthropic from '@anthropic-ai/sdk'\n\ninterface PromptVariant {\n  id: string\n  template: string\n  metrics?: {\n    accuracy: number\n    avgTokens: number\n    avgCost: number\n  }\n}\n\ninterface OptimizationResult {\n  winner: string // ID of best variant\n  improvement: number // % improvement over baseline\n  recommendations: string[]\n}\n\n/**\n * Optimize a prompt by testing variants and measuring metrics\n * @param baseline Baseline prompt template\n * @param variants Array of prompt variants to test\n * @param testCases Evaluation test cases\n * @returns Best performing variant and improvement metrics\n */\nasync function optimizePrompt(\n  baseline: PromptVariant,\n  variants: PromptVariant[],\n  testCases: Array<{ input: string; expectedOutput: string }>\n): Promise<OptimizationResult> {\n  // Your code here\n  return {\n    winner: '',\n    improvement: 0,\n    recommendations: []\n  }\n}\n",
  "instructions": "Implement `optimizePrompt()` that:\n1. Evaluates baseline prompt on all test cases\n2. Tests each variant against the same test cases\n3. Measures accuracy, token usage, and cost for each variant\n4. Compares variants to find the best performer\n5. Calculates improvement percentage vs baseline\n6. Generates recommendations for further optimization\n\nConsider multiple metrics: accuracy, cost, latency",
  "testCases": [
    {
      "input": "baseline + 3 variants, 10 test cases",
      "expectedOutput": "Returns winner ID and improvement %",
      "description": "Basic optimization"
    },
    {
      "input": "Variant with higher accuracy but 2x cost",
      "expectedOutput": "Recommendations include cost-accuracy tradeoff",
      "description": "Multi-metric optimization"
    },
    {
      "input": "All variants perform similarly",
      "expectedOutput": "Winner is lowest cost variant",
      "description": "Tie-breaking on cost"
    },
    {
      "input": "Variant improves accuracy by 20%",
      "expectedOutput": "improvement = 20",
      "description": "Calculate improvement correctly"
    },
    {
      "input": "Baseline is already optimal",
      "expectedOutput": "winner = baseline.id, improvement = 0",
      "description": "Baseline wins"
    },
    {
      "input": "Variant reduces tokens by 40%",
      "expectedOutput": "Recommendations include 'Reduced token usage'",
      "description": "Token optimization recognition"
    },
    {
      "input": "Empty variants array",
      "expectedOutput": "Returns baseline as winner",
      "description": "Handle edge case"
    },
    {
      "input": "Test cases with diverse difficulty levels",
      "expectedOutput": "Optimization considers performance across all difficulties",
      "description": "Comprehensive evaluation"
    }
  ],
  "hints": [
    "Test each variant on all test cases independently",
    "Calculate accuracy: correct answers / total cases",
    "Track token usage from API response.usage",
    "Calculate cost: (input_tokens × $3 + output_tokens × $15) / 1M",
    "Overall score: weighted combination of accuracy, cost, tokens",
    "Example weights: 50% accuracy, 30% cost, 20% tokens",
    "Generate recommendations based on what improved",
    "If accuracy improved: 'Better accuracy (+X%)'",
    "If tokens reduced: 'More concise (-X% tokens)'",
    "If cost reduced: 'Cost savings (-$X per 1K requests)'",
    "Consider statistical significance for small improvements",
    "Store all variant metrics for comparison"
  ]
}
