{
  "id": "lab-2-approach-comparison",
  "title": "Compare Prompt vs RAG vs Agent Approaches",
  "description": "Implement and benchmark three different AI architectures for the same task",
  "difficulty": "advanced",
  "estimatedMinutes": 60,
  "language": "typescript",
  "starterCode": "import Anthropic from '@anthropic-ai/sdk'\n\ninterface BenchmarkResult {\n  approach: 'prompt' | 'rag' | 'agent'\n  accuracy: number\n  avgLatency: number\n  avgCost: number\n  successRate: number\n}\n\n/**\n * Compare different AI approaches on the same task\n * @param task The task to solve (e.g., 'Answer customer questions')\n * @param testQuestions Array of test questions\n * @param knowledgeBase Optional knowledge base for RAG\n * @returns Comparison of all three approaches\n */\nasync function compareApproaches(\n  task: string,\n  testQuestions: string[],\n  knowledgeBase?: string[]\n): Promise<BenchmarkResult[]> {\n  // Your code here\n  return []\n}\n",
  "instructions": "Implement `compareApproaches()` that:\n1. Implements three approaches:\n   - Prompt: Simple prompt engineering\n   - RAG: Retrieve relevant docs + generate\n   - Agent: Multi-step reasoning with tools\n2. Runs all test questions through each approach\n3. Measures accuracy, latency, and cost for each\n4. Returns comparative benchmark results\n\nEach approach should solve the same task using its pattern.",
  "testCases": [
    {
      "input": "task='QA', testQuestions=['What is AI?', 'Define ML']",
      "expectedOutput": "Returns 3 BenchmarkResults (prompt, rag, agent)",
      "description": "Basic comparison"
    },
    {
      "input": "RAG with knowledgeBase provided",
      "expectedOutput": "RAG accuracy > Prompt accuracy",
      "description": "RAG should improve with knowledge base"
    },
    {
      "input": "Simple questions",
      "expectedOutput": "Prompt latency < RAG latency < Agent latency",
      "description": "Latency increases with complexity"
    },
    {
      "input": "10 test questions",
      "expectedOutput": "All approaches have successRate between 0 and 1",
      "description": "Success rate calculation"
    },
    {
      "input": "Cost comparison",
      "expectedOutput": "Prompt cost < RAG cost < Agent cost (typical)",
      "description": "Cost increases with approach complexity"
    },
    {
      "input": "Empty test questions",
      "expectedOutput": "Returns results with 0 accuracy, latency, cost",
      "description": "Handle empty input"
    },
    {
      "input": "Complex multi-step questions",
      "expectedOutput": "Agent accuracy >= Prompt accuracy",
      "description": "Agent excels at complex tasks"
    },
    {
      "input": "Questions requiring specific knowledge",
      "expectedOutput": "RAG with knowledgeBase outperforms others",
      "description": "RAG advantage with domain knowledge"
    }
  ],
  "hints": [
    "Prompt approach: Direct question answering with no external data",
    "RAG approach: Search knowledgeBase, retrieve top 3, include in prompt",
    "Agent approach: Use Claude with tools for multi-step reasoning",
    "Measure latency: Date.now() before and after each call",
    "Calculate cost: Use token counts Ã— pricing (input + output)",
    "Accuracy: Compare answers to expected/correct answers",
    "Success rate: Successful completions / total attempts",
    "Use same model for fair comparison (claude-3-5-sonnet)",
    "Implement simple vector search for RAG (cosine similarity)",
    "For agent, provide tools like 'search' and 'calculate'",
    "Average metrics across all test questions",
    "Handle API errors gracefully (count as failures)"
  ]
}
