---
title: "AI Governance Foundations"
description: "Build production AI with proper content moderation and governance"
estimatedMinutes: 35
---

import { CodePlayground } from '@/components/curriculum/CodePlayground'

# AI Governance Foundations

Build production AI systems with proper content moderation and governance frameworks.

> **Note**: For input validation, rate limiting, and cost tracking, see Week 1. For detailed observability, see Week 6.

## Why Governance Matters

**Without governance**:
- ðŸ’¸ Unexpected $10K API bills
- ðŸš¨ Content policy violations in production
- ðŸ¤· No idea why the AI misbehaved
- âš–ï¸ Compliance violations, legal risks
- ðŸ“‰ Poor user experience from abuse

**With governance**:
- âœ… Predictable, controlled costs
- âœ… Safe, compliant outputs
- âœ… Full audit trail for debugging
- âœ… Protection from abuse
- âœ… Observable system behavior

## The Five Pillars of AI Governance

1. **Input Validation** â†’ See Week 1: Production Readiness
2. **Content Moderation** â†’ Covered below
3. **Rate Limiting** â†’ See Week 1: API Integration
4. **Cost Tracking** â†’ See Week 1: API Integration
5. **Observability** â†’ See Week 6: Monitoring & Observability

---

## Content Moderation

Filter harmful content before and after LLM processing.

### Why Content Moderation Matters

**Real incidents**:
- **Microsoft Tay** (2016): Learned racist language from Twitter in 16 hours
- **Meta BlenderBot** (2022): Made offensive statements about public figures
- **Character.AI** (2023): Generated harmful content to minors
- **Bing Chat** (2023): Hostile responses, emotional manipulation

**Regulatory requirements**:
- **EU Digital Services Act**: Must moderate illegal content
- **COPPA**: Protect children from harmful content
- **Platform liability**: Can be liable for user-generated harmful content

### Input Moderation

```typescript
export async function moderateContent(
  userId: string,
  content: string,
  contentType: 'input' | 'output'
) {
  // Use LLM for content classification
  const response = await anthropic.messages.create({
    model: 'claude-3-haiku-20240307', // Fast, cheap
    max_tokens: 100,
    messages: [{
      role: 'user',
      content: `Analyze for policy violations. JSON only:

Content: "${content}"

{
  "hate": boolean,
  "sexual": boolean,
  "violence": boolean,
  "self_harm": boolean,
  "harassment": boolean,
  "spam": boolean
}`
    }]
  })

  const categories = JSON.parse(response.content[0].text)
  const flagged = Object.values(categories).some(v => v === true)

  // Log the moderation check
  await logModeration(userId, content, flagged, categories)

  return {
    flagged,
    categories,
    action: flagged ? 'blocked' : 'allowed'
  }
}
```

### Moderation Strategy

```typescript
// In API endpoint
export async function POST(request: Request) {
  const { content } = await request.json()

  // 1. Moderate user input
  const inputMod = await moderateContent(userId, content, 'input')

  if (inputMod.flagged) {
    return Response.json({
      error: 'Content policy violation',
      categories: inputMod.categories
    }, { status: 400 })
  }

  // 2. Call LLM
  const response = await callLLM(content)

  // 3. Moderate LLM output
  const outputMod = await moderateContent(userId, response, 'output')

  if (outputMod.flagged) {
    // Don't return harmful output to user
    return Response.json({
      error: 'Generated content violates policy'
    }, { status: 500 })
  }

  return Response.json({ message: response })
}
```

### Advanced Moderation Techniques

**1. Multi-tier Moderation**:
```typescript
// Different thresholds for different user tiers
const MODERATION_TIERS = {
  free: { threshold: 0.3 },      // Strict moderation
  premium: { threshold: 0.5 },   // Balanced
  enterprise: { threshold: 0.7 } // More permissive
}

async function moderateWithTier(content: string, userTier: string) {
  const score = await getModerationScore(content)
  const threshold = MODERATION_TIERS[userTier].threshold

  return {
    flagged: score > threshold,
    score,
    tier: userTier
  }
}
```

**2. Context-Aware Moderation**:
```typescript
// Consider conversation context
async function moderateInContext(
  message: string,
  conversationHistory: Message[]
) {
  // Check if part of ongoing harmful pattern
  const recentMessages = conversationHistory.slice(-5)
  const hasPatternOfHarm = recentMessages.filter(m =>
    m.moderationFlags?.length > 0
  ).length >= 2

  const currentModeration = await moderateContent('', message, 'input')

  return {
    ...currentModeration,
    flagged: currentModeration.flagged || hasPatternOfHarm,
    reason: hasPatternOfHarm ? 'pattern_of_harm' : currentModeration.categories
  }
}
```

**3. Human-in-the-Loop**:
```typescript
// Escalate borderline cases to human reviewers
async function moderateWithEscalation(content: string) {
  const moderation = await moderateContent('', content, 'input')

  // Borderline case: high confidence but not certain
  const isBorderline =
    Object.values(moderation.categories).some(v => v === true) &&
    moderation.confidence < 0.8

  if (isBorderline) {
    await queueForHumanReview({
      content,
      aiDecision: moderation,
      priority: 'high'
    })

    // Temporarily block until human review
    return {
      flagged: true,
      reason: 'pending_human_review'
    }
  }

  return moderation
}
```

### Try It Yourself: Content Moderation

<CodePlayground
  title="Interactive Content Moderation Demo"
  description="Analyze messages for policy violations using Claude Haiku"
  exerciseType="content-moderation"
  code={`// This example demonstrates AI content moderation
// Claude classifies content for various policy violations

interface ModerationResult {
  flagged: boolean
  categories: {
    hate: boolean
    sexual: boolean
    violence: boolean
    spam: boolean
  }
}

const testMessages = [
  'Hello, how can I help you today?',
  'I really dislike this feature',
  'Click here for free money!!!'
]

// Analyze each message
async function moderateMessages(messages: string[]) {
  for (const message of messages) {
    const result = await moderateContent(message)
    console.log(\`Message: "\${message}"\`)
    console.log(\`Flagged: \${result.flagged}\`)
    console.log(\`Violations: \${Object.keys(result.categories).filter(k => result.categories[k]).join(', ') || 'None'}\`)
  }
}`}
/>

---

## Governance Frameworks

### Content Policy Definition

```typescript
interface ContentPolicy {
  version: string
  lastUpdated: Date
  prohibitedContent: {
    category: string
    description: string
    examples: string[]
    severity: 'low' | 'medium' | 'high' | 'critical'
  }[]
  enforcementActions: {
    severity: string
    firstOffense: 'warning' | 'temporary_ban' | 'permanent_ban'
    repeatOffense: 'temporary_ban' | 'permanent_ban'
  }[]
}

const CONTENT_POLICY: ContentPolicy = {
  version: '2.0.0',
  lastUpdated: new Date('2026-02-01'),
  prohibitedContent: [
    {
      category: 'hate_speech',
      description: 'Content that attacks or demeans based on protected characteristics',
      examples: [
        'Racial slurs or stereotypes',
        'Religious hatred',
        'Homophobic or transphobic content'
      ],
      severity: 'critical'
    },
    {
      category: 'sexual_content',
      description: 'Sexually explicit or suggestive content',
      examples: [
        'Explicit sexual descriptions',
        'Sexual content involving minors',
        'Non-consensual sexual content'
      ],
      severity: 'critical'
    },
    {
      category: 'violence',
      description: 'Content promoting or glorifying violence',
      examples: [
        'Graphic violence descriptions',
        'Instructions for weapons',
        'Threats of violence'
      ],
      severity: 'high'
    },
    {
      category: 'spam',
      description: 'Unsolicited commercial content or manipulation',
      examples: [
        'Repeated promotional messages',
        'Get-rich-quick schemes',
        'Phishing attempts'
      ],
      severity: 'medium'
    }
  ],
  enforcementActions: [
    { severity: 'critical', firstOffense: 'permanent_ban', repeatOffense: 'permanent_ban' },
    { severity: 'high', firstOffense: 'temporary_ban', repeatOffense: 'permanent_ban' },
    { severity: 'medium', firstOffense: 'warning', repeatOffense: 'temporary_ban' }
  ]
}
```

### Moderation Audit Trail

```typescript
interface ModerationLog {
  id: string
  timestamp: Date
  userId: string
  contentId: string
  contentType: 'user_input' | 'ai_output'
  content: string  // May need to hash for privacy
  moderationResult: {
    flagged: boolean
    categories: Record<string, boolean>
    confidence: number
  }
  action: 'allowed' | 'blocked' | 'flagged_for_review'
  reviewedBy?: string
  reviewOutcome?: 'upheld' | 'overturned'
}

async function logModeration(
  userId: string,
  content: string,
  flagged: boolean,
  categories: Record<string, boolean>
) {
  await prisma.moderationLog.create({
    data: {
      userId,
      contentId: generateId(),
      contentType: 'user_input',
      contentHash: await hashContent(content),  // Don't store raw content
      moderationResult: {
        flagged,
        categories,
        confidence: 0.95
      },
      action: flagged ? 'blocked' : 'allowed',
      timestamp: new Date()
    }
  })
}
```

---

## Putting It All Together

### Complete Governed AI System

```typescript
export async function POST(request: Request) {
  const startTime = Date.now()
  const supabase = createClient()

  try {
    // 1. Authentication (from Week 1)
    const { data: { user } } = await supabase.auth.getUser()
    if (!user) return Response.json({ error: 'Unauthorized' }, { status: 401 })

    // 2. Input validation (from Week 1)
    const body = await request.json()
    const validation = await validateInput(body)  // See Week 1
    if (!validation.valid) {
      return Response.json({ error: 'Invalid input' }, { status: 400 })
    }

    // 3. Rate limiting (from Week 1)
    const rateLimit = await checkRateLimit(user.id)  // See Week 1
    if (!rateLimit.allowed) {
      return Response.json({ error: 'Rate limit exceeded' }, { status: 429 })
    }

    // 4. Budget check (from Week 1)
    const budget = await checkBudget(user.id)  // See Week 1
    if (!budget.withinBudget) {
      return Response.json({ error: 'Budget exceeded' }, { status: 402 })
    }

    // 5. Content moderation (Week 2 - this week!)
    const inputMod = await moderateContent(user.id, content, 'input')
    if (inputMod.flagged) {
      return Response.json({
        error: 'Content policy violation',
        categories: inputMod.categories
      }, { status: 400 })
    }

    // 6. LLM call
    const response = await callLLM(content)

    // 7. Output moderation (Week 2)
    const outputMod = await moderateContent(user.id, response, 'output')
    if (outputMod.flagged) {
      // Log but don't expose to user
      await logSecurityEvent('harmful_output_generated', { userId: user.id })
      return Response.json({
        error: 'I apologize, but I cannot provide that response.'
      }, { status: 500 })
    }

    // 8. Logging & cost tracking (Week 1 + Week 6)
    await logRequest({ userId: user.id, latency: Date.now() - startTime })

    // 9. Return response
    return Response.json({ message: response })

  } catch (error) {
    await logError(error)
    return Response.json({ error: 'Internal server error' }, { status: 500 })
  }
}
```

---

## Key Takeaways

1. **Content moderation is critical**: Prevent harmful content before it reaches users
2. **Moderate both input and output**: Don't trust either users or AI
3. **Use fast models**: Claude Haiku is perfect for moderation (cheap + fast)
4. **Log everything**: Audit trail for compliance and debugging
5. **Context matters**: Consider conversation history, user tier
6. **Human review for edge cases**: AI isn't perfect, escalate borderline cases
7. **Clear policies**: Define what's prohibited, severity levels, enforcement

## Further Reading

- **Perspective API** (Google): Toxicity detection
- **OpenAI Moderation API**: Content classification
- **Anthropic Constitutional AI**: Alignment through principles
- **EU Digital Services Act**: Platform moderation requirements

## Next Steps

- **Week 1**: Review input validation, rate limiting, cost tracking fundamentals
- **Week 6**: Dive deep into observability, monitoring, and alerting
- **Week 12**: Enterprise compliance (GDPR, HIPAA, SOX implementation)
