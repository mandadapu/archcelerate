# Advanced Chat Architecture

Learn production-grade patterns for building scalable, maintainable chat applications with LLMs.

## Production Architecture Patterns

Building a production chat application requires more than just calling an API. You need to handle:

1. **State Management**: Conversation history, user sessions, context
2. **Persistence**: Database storage for conversations and messages
3. **Real-time Updates**: Streaming responses, typing indicators
4. **Error Handling**: API failures, rate limits, timeouts
5. **Scalability**: Multiple users, concurrent requests, caching

### Conversation State Management

**Three-tier state architecture**:

```typescript
// 1. Client State (UI layer)
interface ClientState {
  messages: DisplayMessage[]
  inputValue: string
  isTyping: boolean
  error: string | null
}

// 2. Server State (API layer)
interface ServerState {
  conversationId: string
  messages: StoredMessage[]
  metadata: ConversationMetadata
}

// 3. Context State (LLM layer)
interface ContextState {
  systemPrompt: string
  messages: APIMessage[]
  temperature: number
  maxTokens: number
}
```

**Why three tiers?**
- **Separation of concerns**: UI logic ≠ Business logic ≠ AI logic
- **Performance**: Don't send UI state to the database
- **Flexibility**: Change UI without affecting data model

## Database Schema Design

Production chat applications need efficient database schemas:

```sql
-- Conversations table
CREATE TABLE conversations (
  id UUID PRIMARY KEY,
  user_id UUID REFERENCES users(id),
  title TEXT,
  created_at TIMESTAMP,
  updated_at TIMESTAMP,
  metadata JSONB  -- flexible storage for app-specific data
);

-- Messages table
CREATE TABLE messages (
  id UUID PRIMARY KEY,
  conversation_id UUID REFERENCES conversations(id),
  role TEXT CHECK (role IN ('user', 'assistant', 'system')),
  content TEXT,
  tokens INTEGER,
  created_at TIMESTAMP,
  metadata JSONB
);

-- Indexes for performance
CREATE INDEX idx_messages_conversation
  ON messages(conversation_id, created_at DESC);
```

**Key design decisions**:
- **Separate tables**: Easier to query, better performance
- **Metadata JSONB**: Flexible for future features
- **Indexes**: Critical for fast message retrieval
- **Role enum**: Ensures data integrity

## Streaming Responses

Streaming improves perceived latency by showing partial responses:

```typescript
// Server-side streaming
export async function POST(request: Request) {
  const stream = await client.messages.stream({
    model: 'claude-3-5-sonnet-20241022',
    max_tokens: 1024,
    messages: conversationHistory
  })

  // Create a readable stream
  const encoder = new TextEncoder()
  const readableStream = new ReadableStream({
    async start(controller) {
      for await (const chunk of stream) {
        if (chunk.type === 'content_block_delta') {
          const text = chunk.delta.text
          controller.enqueue(encoder.encode(text))
        }
      }
      controller.close()
    }
  })

  return new Response(readableStream, {
    headers: {
      'Content-Type': 'text/event-stream',
      'Cache-Control': 'no-cache',
      'Connection': 'keep-alive'
    }
  })
}
```

**Streaming benefits**:
- **User experience**: Show progress immediately
- **Perceived latency**: Feels 3-5x faster
- **Engagement**: Users see value before completion

## Error Handling Strategy

Production systems need comprehensive error handling:

```typescript
// Error classification
enum ErrorType {
  RATE_LIMIT = 'rate_limit',      // Retry with backoff
  BUDGET_EXCEEDED = 'budget',      // User action needed
  VALIDATION = 'validation',       // Client-side fix
  API_ERROR = 'api_error',         // Retry or degrade
  NETWORK = 'network',             // Retry immediately
  MODERATION = 'moderation'        // Block and log
}

// Error handling wrapper
async function handleChatRequest(request) {
  try {
    return await processChat(request)
  } catch (error) {
    const errorType = classifyError(error)

    switch (errorType) {
      case ErrorType.RATE_LIMIT:
        return retryWithBackoff(request, 3)

      case ErrorType.API_ERROR:
        // Try fallback model
        return await useFallbackModel(request)

      case ErrorType.BUDGET_EXCEEDED:
        return { error: 'Budget exceeded', upgrade: true }

      default:
        logError(error)
        return { error: 'Something went wrong' }
    }
  }
}
```

## Conversation History Management

Balancing context quality vs. cost:

```typescript
// Strategy: Keep recent messages + summary of old messages
async function buildContext(conversationId: string) {
  const allMessages = await getMessages(conversationId)

  if (allMessages.length <= 10) {
    return allMessages // All fits in context
  }

  // Keep last 10 messages
  const recentMessages = allMessages.slice(-10)

  // Summarize older messages
  const oldMessages = allMessages.slice(0, -10)
  const summary = await summarizeConversation(oldMessages)

  return [
    { role: 'system', content: `Previous conversation summary: ${summary}` },
    ...recentMessages
  ]
}
```

**Context optimization strategies**:
1. **Rolling window**: Keep last N messages
2. **Summarization**: Compress old messages
3. **Semantic search**: Retrieve relevant past messages
4. **Pruning**: Remove redundant information

## Scalability Considerations

**Handling concurrent users**:
- **Database connection pooling**: Reuse connections
- **Caching**: Cache user metadata, common responses
- **Rate limiting**: Prevent abuse, ensure fair usage
- **Queue-based processing**: Handle spikes in traffic

**Example connection pooling**:
```typescript
// Supabase automatically handles connection pooling
const supabase = createClient(url, key, {
  db: {
    poolSize: 20  // Adjust based on load
  }
})
```

## Key Takeaways

1. **Separate concerns**: UI state, business logic, AI context
2. **Design for scale**: Connection pools, caching, indexes
3. **Stream responses**: Better UX, perceived performance
4. **Handle errors gracefully**: Classify, retry, degrade
5. **Optimize context**: Balance quality vs. cost
6. **Monitor everything**: Latency, errors, costs

## Next Steps

- Build production chat features (persistence, streaming)
- Implement governance from day one
- Deploy and monitor in production
