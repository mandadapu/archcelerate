---
title: 'RAG Production Failure Patterns & Forensic Debugging'
description: 'Real-world case study: diagnosing and fixing context poisoning, stale retrieval, and latency spikes in a production RAG pipeline'
---

# RAG Production Failure Patterns

## The Scenario

A conversational AI built for **patient adherence monitoring** began providing generic, irrelevant advice instead of specific guidance based on sensor signals (ECG, accelerometer). The system had been working in staging, but degraded silently in production over several weeks.

This is a deep dive into what went wrong and how it was fixed.

---

## 1. What Went Wrong

Three failures compounded into a single symptom — **"context poisoning"** — where the LLM received the wrong context and produced plausible but incorrect responses.

### Failure 1: Chunking Strategy (The Root Cause)

The system used **fixed-size chunking** (500 tokens with 50-token overlap). This works for uniform text but breaks on structured clinical data:

```typescript
// The BROKEN approach: fixed-size chunking on clinical data
interface NaiveChunkConfig {
  chunkSize: 500       // tokens
  overlapSize: 50      // tokens
  strategy: 'fixed'    // one-size-fits-all
}

// Problem: HL7/FHIR records and time-series summaries vary wildly in structure.
// A 500-token chunk might split an ECG pattern mid-signal:
//
// Chunk 47: "...heart rate elevated to 112bpm at 14:32. ECG shows ST-segment"
// Chunk 48: "elevation in leads V1-V4, consistent with anterior wall..."
//
// The embedding model loses the semantic meaning of the clinical event.
// Vector search might retrieve Chunk 47 but miss the critical diagnosis in Chunk 48.
```

**Why this matters**: The embedding model encodes each chunk independently. When a clinical event spans two chunks, neither chunk captures the full meaning. The vector similarity score for the correct answer drops below irrelevant but self-contained chunks.

### Failure 2: Retrieval Quality (Stale Context)

The vector search retrieved the **top 5 most similar chunks** without any temporal awareness:

```typescript
// The BROKEN approach: similarity-only retrieval
async function naiveRetrieve(query: string, limit: number = 5) {
  const embedding = await embed(query)

  // Only factor: cosine similarity
  const results = await db.query(`
    SELECT content, metadata
    FROM documents
    ORDER BY embedding <=> $1
    LIMIT $2
  `, [embedding, limit])

  return results.rows
}

// Problem: In a noisy patient history spanning months,
// the top 5 often included outdated data from 6 months ago
// instead of the acute signals from the last 24 hours.
//
// Query: "What are the patient's current cardiac risk indicators?"
// Retrieved: Lab results from June (high similarity to "cardiac")
// Missed: Yesterday's accelerometer anomaly (lower similarity score)
```

### Failure 3: Latency Degradation

As the PostgreSQL database grew past 500K vectors, search times exceeded the **sub-2-second SLA**:

```typescript
// No HNSW index = sequential scan at scale
// 100K vectors: ~200ms  ✅
// 500K vectors: ~1.8s   ⚠️
// 1M vectors:   ~4.2s   ❌ SLA breach

// The compounding effect:
// Slow retrieval → request timeouts → retry storms → cascading failures
```

---

## 2. The Fix: pgvector Hybrid Search with Metadata Filtering

The first fix combined **semantic similarity** with **metadata filtering** to ensure only recent, relevant context reached the LLM.

```typescript
import { Pool } from 'pg'

interface HybridSearchConfig {
  patientId: string
  timeWindowHours: number
  semanticWeight: number
  limit: number
}

async function hybridSearchWithMetadata(
  pool: Pool,
  query: string,
  config: HybridSearchConfig
): Promise<SearchResult[]> {
  const queryEmbedding = await generateEmbedding(query)

  // Hybrid search: semantic similarity + temporal metadata filtering
  const results = await pool.query(`
    WITH recent_docs AS (
      -- Stage 1: Filter by patient and time window FIRST (fast, uses B-tree index)
      SELECT id, content, metadata, embedding
      FROM patient_documents
      WHERE patient_id = $1
        AND created_at > NOW() - INTERVAL '${config.timeWindowHours} hours'
        AND document_type IN ('sensor_summary', 'clinical_note', 'alert')
    )
    SELECT
      id,
      content,
      metadata,
      -- Stage 2: Rank by semantic similarity within filtered set
      1 - (embedding <=> $2) AS similarity_score
    FROM recent_docs
    ORDER BY embedding <=> $2
    LIMIT $3
  `, [config.patientId, queryEmbedding, config.limit])

  return results.rows
}

// Usage:
// Instead of "find 5 most similar chunks from ALL history",
// now: "find 5 most similar chunks from the LAST 24 HOURS for THIS patient"
const context = await hybridSearchWithMetadata(pool, userQuery, {
  patientId: 'patient_abc',
  timeWindowHours: 24,
  semanticWeight: 0.7,
  limit: 5
})
```

**Why this works**: The `WHERE` clause eliminates stale context before vector similarity is even calculated. The LLM only sees recent, patient-specific data — no more "context poisoning" from months-old lab results.

### Adding the HNSW Index

To solve the latency problem, an HNSW index was added:

```sql
-- Create HNSW index for approximate nearest neighbor search
CREATE INDEX idx_patient_docs_embedding
ON patient_documents
USING hnsw (embedding vector_cosine_ops)
WITH (m = 16, ef_construction = 200);

-- Add B-tree index for metadata filtering
CREATE INDEX idx_patient_docs_lookup
ON patient_documents (patient_id, created_at DESC);

-- Result:
-- Before: 1.8s for 500K vectors (sequential scan)
-- After:  35ms for 500K vectors (HNSW + metadata pre-filter)
```

---

## 3. The Fix: Document-Aware Chunking

The second fix replaced fixed-size chunking with **structure-aware chunking** that respects document boundaries.

```typescript
interface ChunkingStrategy {
  type: 'document_aware'
  fhirSplitter: RegExp
  timeSeriesWindow: number // minutes
  maxChunkTokens: number
  preserveHeaders: boolean
}

// Strategy 1: Regex-based splitting for FHIR/HL7 resources
function chunkFhirDocument(document: string): string[] {
  const chunks: string[] = []

  // Split on FHIR resource boundaries — never mid-resource
  const resourcePattern = /(?=resourceType:\s*["']?\w+)/g
  const resources = document.split(resourcePattern).filter(Boolean)

  for (const resource of resources) {
    // Each FHIR resource becomes its own chunk, preserving semantic integrity
    if (countTokens(resource) <= 1000) {
      chunks.push(resource)
    } else {
      // Only split large resources, using section headers as boundaries
      const sections = resource.split(/(?=##?\s)/)
      chunks.push(...sections)
    }
  }

  return chunks
}

// Strategy 2: Sliding window for time-series sensor summaries
function chunkTimeSeries(
  signals: SensorReading[],
  windowMinutes: number = 30
): string[] {
  const chunks: string[] = []
  let windowStart = signals[0].timestamp

  let currentWindow: SensorReading[] = []

  for (const signal of signals) {
    const elapsed = signal.timestamp - windowStart

    if (elapsed > windowMinutes * 60 * 1000) {
      // Close current window — summarize as one semantic chunk
      chunks.push(summarizeWindow(currentWindow))
      currentWindow = [signal]
      windowStart = signal.timestamp
    } else {
      currentWindow.push(signal)
    }
  }

  // Don't forget the last window
  if (currentWindow.length > 0) {
    chunks.push(summarizeWindow(currentWindow))
  }

  return chunks
}

function summarizeWindow(readings: SensorReading[]): string {
  const start = new Date(readings[0].timestamp).toISOString()
  const end = new Date(readings[readings.length - 1].timestamp).toISOString()
  const avgHR = readings.reduce((sum, r) => sum + r.heartRate, 0) / readings.length
  const maxHR = Math.max(...readings.map(r => r.heartRate))
  const anomalies = readings.filter(r => r.flagged)

  return [
    `Sensor Window: ${start} to ${end}`,
    `Readings: ${readings.length} | Avg HR: ${avgHR.toFixed(0)}bpm | Max HR: ${maxHR}bpm`,
    anomalies.length > 0
      ? `ALERTS: ${anomalies.map(a => a.alertType).join(', ')}`
      : 'No anomalies detected',
  ].join('\n')
}

interface SensorReading {
  timestamp: number
  heartRate: number
  flagged: boolean
  alertType?: string
}
```

**Why this works**: Each chunk now represents a complete semantic unit — a full FHIR resource or a coherent time window of sensor data. The embedding model captures the actual meaning of clinical events instead of arbitrary text fragments.

---

## 4. The Fix: LangGraph Self-Correction Loop

The third fix addressed **hallucinations** by introducing a "Critic" node that validates responses against retrieved sources before they reach the patient.

```typescript
import { StateGraph, END } from '@langchain/langgraph'
import { ChatAnthropic } from '@langchain/anthropic'

// State flowing through the graph
interface RAGState {
  query: string
  retrievedDocs: string[]
  response: string
  isGrounded: boolean
  retryCount: number
  maxRetries: number
}

// Node A: Retrieve context via pgvector hybrid search
async function retrieveNode(state: RAGState): Promise<Partial<RAGState>> {
  const docs = await hybridSearchWithMetadata(pool, state.query, {
    patientId: extractPatientId(state.query),
    timeWindowHours: state.retryCount > 0 ? 72 : 24, // Widen window on retry
    semanticWeight: 0.7,
    limit: state.retryCount > 0 ? 10 : 5 // Retrieve more on retry
  })

  return { retrievedDocs: docs.map(d => d.content) }
}

// Node B: Generate response from retrieved context
async function generateNode(state: RAGState): Promise<Partial<RAGState>> {
  const llm = new ChatAnthropic({ model: 'claude-sonnet-4-5-20250929' })

  const response = await llm.invoke([
    {
      role: 'system',
      content: `You are a patient adherence assistant. Answer ONLY based on the provided context.
If the context doesn't contain enough information, say "I don't have recent data for that."
Never fabricate clinical information.

Context:
${state.retrievedDocs.join('\n\n---\n\n')}`
    },
    { role: 'user', content: state.query }
  ])

  return { response: response.content as string }
}

// Node C: The Critic — verify response is grounded in sources
async function criticNode(state: RAGState): Promise<Partial<RAGState>> {
  const llm = new ChatAnthropic({ model: 'claude-sonnet-4-5-20250929' })

  const verdict = await llm.invoke([
    {
      role: 'system',
      content: `You are a clinical accuracy reviewer.
Compare the RESPONSE against the SOURCE DOCUMENTS.
Flag if the response contains ANY claim not supported by the sources.

Respond with JSON: { "grounded": true/false, "issues": ["list of ungrounded claims"] }`
    },
    {
      role: 'user',
      content: `SOURCE DOCUMENTS:\n${state.retrievedDocs.join('\n---\n')}

RESPONSE:\n${state.response}`
    }
  ])

  const result = JSON.parse(verdict.content as string)
  return {
    isGrounded: result.grounded,
    retryCount: state.retryCount + 1
  }
}

// Routing: if ungrounded AND retries remaining, loop back to retrieve
function shouldRetry(state: RAGState): string {
  if (state.isGrounded) return 'deliver'
  if (state.retryCount >= state.maxRetries) return 'deliver_with_warning'
  return 'retry_retrieve'
}

// Build the graph
const graph = new StateGraph<RAGState>({
  channels: {
    query: { value: (a: string, b: string) => b ?? a },
    retrievedDocs: { value: (a: string[], b: string[]) => b ?? a },
    response: { value: (a: string, b: string) => b ?? a },
    isGrounded: { value: (a: boolean, b: boolean) => b ?? a },
    retryCount: { value: (a: number, b: number) => b ?? a },
    maxRetries: { value: (a: number, b: number) => b ?? a },
  }
})
  .addNode('retrieve', retrieveNode)
  .addNode('generate', generateNode)
  .addNode('critic', criticNode)
  .addEdge('__start__', 'retrieve')
  .addEdge('retrieve', 'generate')
  .addEdge('generate', 'critic')
  .addConditionalEdges('critic', shouldRetry, {
    deliver: END,
    deliver_with_warning: END,
    retry_retrieve: 'retrieve'  // Loop back with wider search
  })

const app = graph.compile()

// Execute
const result = await app.invoke({
  query: "What are the patient's cardiac risk indicators from the last 24 hours?",
  retrievedDocs: [],
  response: '',
  isGrounded: false,
  retryCount: 0,
  maxRetries: 2
})
```

### The Self-Correction Flow

```
┌──────────────┐     ┌──────────────┐     ┌──────────────┐
│   Retrieve   │────▶│   Generate   │────▶│    Critic    │
│  (pgvector)  │     │   (Claude)   │     │  (Verifier)  │
└──────────────┘     └──────────────┘     └──────┬───────┘
       ▲                                         │
       │                                         ▼
       │                                  ┌──────────────┐
       │◀─── retry (wider window) ────────│  Grounded?   │
       │                                  │  Yes → END   │
       │                                  │  No → Retry  │
       └──────────────────────────────────└──────────────┘
```

**Why this works**: The Critic node catches ungrounded claims _before_ they reach the patient. On retry, the retrieval window widens (24h → 72h) and fetches more documents (5 → 10), giving the generator richer context. After max retries, the system delivers with an explicit disclaimer rather than hallucinating.

---

## 5. The Result

By shifting to this compliance-first architecture, the system achieved:

| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| **Retrieval Relevance** | ~45% (stale context) | ~92% (time-filtered) | +47% |
| **Response Grounding** | ~70% (frequent hallucinations) | ~97% (critic-verified) | +27% |
| **P95 Latency** | 4.2s (sequential scan) | 180ms (HNSW + metadata) | 23x faster |
| **Hallucination Rate** | ~15% of responses | &lt;1% of responses | 15x reduction |
| **System Uptime** | Intermittent timeouts | 24/7 stable | Reliable |

---

## Architect's Takeaways

**1. Chunking is architecture, not preprocessing.** Fixed-size chunking is the default in tutorials but fails on structured data. Match your chunking strategy to your document structure — FHIR resources, time-series windows, and free-text notes each need different approaches.

**2. Retrieval without metadata filtering is a time bomb.** Pure vector similarity works on static corpora, but any system with temporal data _must_ filter by recency. Yesterday's sensor anomaly matters more than last year's perfect lab result.

**3. Add a verification loop before production.** The Retrieve → Generate → Critic pattern adds latency (~500ms for the critic call) but eliminates the class of errors that matter most in regulated environments. The cost of one hallucinated clinical recommendation far exceeds the cost of an extra LLM call.

**4. Index early, not after the crisis.** HNSW indexes should be created at deployment, not after latency alarms fire. Budget for index maintenance (rebuild schedules, `ef_search` tuning) from day one.

---

## Further Reading

- **Week 3**: RAG Memory Fundamentals — chunking strategies and the "Lost in the Middle" problem
- **Week 5**: Framework Comparison — LangGraph orchestration patterns
- **Week 6**: Enterprise RAG Hardening — production metrics (Recall@10, MRR, TTFT, QPS)
- **Week 7**: Observability Pillars — tracing and monitoring for production RAG systems
