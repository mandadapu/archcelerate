---
title: 'Hybrid Retrieval & Re-Ranking'
description: 'Combine semantic (vector) and keyword (BM25) search with cross-encoder re-ranking for enterprise precision'
---

# Hybrid Retrieval & Re-Ranking

---

## üè• Real-World Challenge: The Clinical RAG System

**The Problem**: A healthcare SaaS company needs doctors to find answers from **10,000+ clinical notes** (diagnoses, lab results, medication histories). Pure vector search returns semantically similar documents but misses **exact matches** (e.g., "HbA1c = 6.8%" gets confused with "HbA1c = 8.1%"). Medical accuracy requires **100% precision** on lab values and medication names.

**Business Constraints**:
- **Precision**: Must find exact lab values, medication dosages, and diagnosis codes (ICD-10)
- **Recall**: Must also understand semantic queries ("blood sugar control" ‚Üí HbA1c results)
- **Latency**: <300ms P95 (doctors won't wait >500ms)
- **Cost**: <$1,000/month for 100K queries
- **Compliance**: HIPAA audit trail showing which documents were retrieved

**The Architectural Problem**: Vector-Only RAG Fails on Medical Data

```typescript
// User query: "Show HbA1c results from March 2022"

// ‚ùå Naive Vector Search (returns semantically similar, not exact)
vectorSearch("HbA1c March 2022")
// Returns:
// 1. "Patient had good glycemic control in Q1 2022" (similar but no value)
// 2. "HbA1c was 7.2% in April 2022" (wrong month!)
// 3. "Glucose levels improved in March" (no HbA1c mentioned)

// ‚úÖ Hybrid Search (semantic + keyword)
hybridSearch("HbA1c March 2022", { vectorWeight: 0.5, keywordWeight: 0.5 })
// Returns:
// 1. "HbA1c: 6.8% (March 15, 2022)" ‚úÖ EXACT match
// 2. "Follow-up HbA1c scheduled for March 2022" (related context)
// 3. "Previous HbA1c (Jan 2022): 7.1% ‚Üí trending down" (trend context)
```

**Architectural Solution: Three-Stage Hybrid RAG**

**Stage 1: Parallel Retrieval** (Semantic + Keyword)
```typescript
// Semantic search (pgvector): "what does the query mean?"
const vectorResults = await db.query(`
  SELECT id, content,
         1 - (embedding <=> $1) AS vector_score
  FROM clinical_notes
  ORDER BY embedding <=> $1
  LIMIT 50
`, [queryEmbedding])

// Keyword search (BM25): "does it contain exact terms?"
const keywordResults = await db.query(`
  SELECT id, content,
         ts_rank_cd(search_vector, plainto_tsquery($1)) AS keyword_score
  FROM clinical_notes
  WHERE search_vector @@ plainto_tsquery('HbA1c & March & 2022')
  LIMIT 50
`, [query])
```

**Stage 2: Reciprocal Rank Fusion** (merge without score normalization issues)
```typescript
// Problem: Vector scores (0-1) and BM25 scores (0-15) are on different scales
// Solution: Use rank positions, not raw scores

function reciprocalRankFusion(vectorResults, keywordResults, k = 60) {
  const scoreMap = new Map()

  vectorResults.forEach((doc, index) => {
    const rrfScore = 1 / (k + index + 1)
    scoreMap.set(doc.id, (scoreMap.get(doc.id) || 0) + rrfScore)
  })

  keywordResults.forEach((doc, index) => {
    const rrfScore = 1 / (k + index + 1)
    scoreMap.set(doc.id, (scoreMap.get(doc.id) || 0) + rrfScore)
  })

  return Array.from(scoreMap.entries())
    .sort((a, b) => b[1] - a[1])
    .slice(0, 10)
}
```

**Stage 3: Cross-Encoder Re-Ranking** (two-stage retrieval for precision)
```typescript
// Problem: Bi-encoder (vector search) retrieves candidates but isn't precise
// Solution: Cross-encoder (Cohere Rerank) re-scores with query-document attention

const reranked = await cohere.rerank({
  model: 'rerank-english-v3.0',
  query: "HbA1c results from March 2022",
  documents: fusedResults.map(r => r.content),
  top_n: 3,
  return_documents: true
})

// Result: Top 3 documents ranked by true semantic similarity to query
```

**Production Architecture**:
```
User Query: "HbA1c March 2022"
      ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Query Embedding   ‚îÇ (OpenAI ada-002: <50ms)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚Üì
     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚Üì           ‚Üì
[Vector Search] [Keyword Search]  (Parallel: 120ms)
    50 docs       50 docs
     ‚Üì           ‚Üì
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚Üì
     [RRF Fusion]  (In-memory: <1ms)
       10 docs
           ‚Üì
  [Cross-Encoder Rerank]  (Cohere: 80ms)
       3 docs
           ‚Üì
   [Context to LLM]  (Claude Sonnet: 1.8s)
           ‚Üì
      Final Answer
```

**Production Impact**:

| Metric | Vector-Only RAG | Hybrid + Rerank RAG | Improvement |
|--------|----------------|---------------------|-------------|
| **Precision** | 65% | 94% | +45% (critical for medical) |
| **Recall** | 78% | 89% | +14% |
| **Latency (P95)** | 2.1s | 2.3s | +200ms (acceptable) |
| **Cost/Query** | $0.008 | $0.012 | +$0.004 (reranking overhead) |
| **Monthly Cost** (100K queries) | $800 | $1,200 | Within budget |

**Why Hybrid Wins**:
- **Exact Matches**: Keyword search ensures "HbA1c" and "March 2022" are present
- **Semantic Context**: Vector search adds "glycemic control" and "diabetes management" context
- **Precision**: Cross-encoder rerank eliminates false positives (April vs March)

**Real Failure Scenario Prevented**:
```typescript
// Vector-only RAG returned: "Patient's glucose was well-controlled in April 2022"
// Doctor prescribed based on wrong month ‚Üí Incorrect medication adjustment
// Hybrid RAG caught: Keyword search filtered out "April", returned only "March"
// ‚Üí Correct treatment decision
```

**[üëâ Lab: Build the Clinical RAG System](/curriculum/week-6/labs/advanced-rag-system)**

In the hands-on lab, you'll implement:
1. Parallel vector + keyword retrieval with pgvector + PostgreSQL full-text
2. Reciprocal Rank Fusion (RRF) algorithm
3. Cohere cross-encoder re-ranking
4. A/B testing: Measure precision improvement (65% ‚Üí 94%)
5. Cost optimization: Intelligent model routing (Haiku for simple, Opus for complex)

**Key Architectural Insight**: Medical/legal/financial RAG systems **must use hybrid search**‚Äîvector-only is insufficient for domains requiring exact match precision.

---

## Moving Beyond "Vector Search Only"

In production RAG systems‚Äîespecially in regulated industries like healthcare and finance‚Äî**vector similarity alone is not enough**. An architect must combine "meaning" (semantic search) with "exactness" (keyword search).

### The Enterprise Reality

When a user searches for "HbA1c results from March 2022," pure vector search might return:
- ‚ùå Documents about "glycemic control" (semantically related)
- ‚ùå Results from "April 2022" (close embedding)
- ‚úÖ The exact HbA1c value from March 2022 (what we need)

**Hybrid search solves this by combining both worlds.**

---

## 1. Hybrid Search Architecture

### The Two-Stage Approach

```typescript
/**
 * Hybrid Search: Semantic + Keyword
 * Returns results that are both semantically relevant AND contain exact keywords
 */
async function hybridSearch(query: string, options: {
  limit: number,
  vectorWeight: number, // 0-1, typically 0.7
  keywordWeight: number // 0-1, typically 0.3
}) {
  // Stage 1: Semantic search with pgvector
  const vectorResults = await db.query(`
    SELECT id, content,
           1 - (embedding <=> $1) AS vector_score
    FROM documents
    ORDER BY embedding <=> $1
    LIMIT 50
  `, [queryEmbedding])

  // Stage 2: Keyword search with BM25 (PostgreSQL full-text)
  const keywordResults = await db.query(`
    SELECT id, content,
           ts_rank_cd(search_vector, plainto_tsquery($1)) AS keyword_score
    FROM documents
    WHERE search_vector @@ plainto_tsquery($1)
    LIMIT 50
  `, [query])

  // Stage 3: Merge and re-score
  const merged = mergeResults(vectorResults, keywordResults, {
    vectorWeight: options.vectorWeight,
    keywordWeight: options.keywordWeight
  })

  return merged.slice(0, options.limit)
}
```

### When to Use Each Weight

| Use Case | Vector Weight | Keyword Weight | Reason |
|----------|--------------|----------------|--------|
| Medical Code Search | 0.3 | **0.7** | Must find exact ICD-10 codes |
| General Q&A | **0.7** | 0.3 | Meaning matters more than exact match |
| Legal Document Retrieval | 0.5 | 0.5 | Balance precision and context |
| Technical Documentation | **0.6** | 0.4 | Concepts + specific function names |

### Reciprocal Rank Fusion (RRF): The Better Merge Strategy

**Problem with Naive Weighted Average:**
```typescript
// ‚ùå BAD: Assumes scores are normalized and comparable
finalScore = (vectorScore * 0.7) + (keywordScore * 0.3)
```

**Why this fails:**
- Vector scores (cosine similarity): 0.0-1.0
- BM25 scores (tf-idf): 0.1-15.0
- Different scales = biased merging

**RRF Solution:** Use rank positions, not raw scores.

**Formula:**
```
RRF_score(doc) = Œ£  1 / (k + rank_i)
```
Where `k = 60` (constant), `rank_i` = position in result set i

**Implementation:**
```typescript
function reciprocalRankFusion(
  vectorResults: Array<{id: string, score: number}>,
  keywordResults: Array<{id: string, score: number}>,
  k: number = 60
) {
  const scoreMap = new Map<string, number>()

  // Add RRF scores from vector results
  vectorResults.forEach((result, index) => {
    const rrfScore = 1 / (k + index + 1)
    scoreMap.set(result.id, (scoreMap.get(result.id) || 0) + rrfScore)
  })

  // Add RRF scores from keyword results
  keywordResults.forEach((result, index) => {
    const rrfScore = 1 / (k + index + 1)
    scoreMap.set(result.id, (scoreMap.get(result.id) || 0) + rrfScore)
  })

  // Sort by combined RRF score
  return Array.from(scoreMap.entries())
    .map(([id, score]) => ({ id, score }))
    .sort((a, b) => b.score - a.score)
}
```

**Example: "HbA1c March 2023"**
- Vector: doc_12 (rank 1), doc_45 (rank 2), doc_78 (rank 3)
- Keyword: doc_12 (rank 1), doc_90 (rank 2), doc_45 (rank 3)

RRF scores (k=60):
- doc_12: 1/(60+1) + 1/(60+1) = **0.0328** ‚Üê Appears in both (BOOSTED!)
- doc_45: 1/(60+2) + 1/(60+3) = 0.0317
- doc_90: 1/(60+2) = 0.0161
- doc_78: 1/(60+3) = 0.0159

**Result:** doc_12 wins (appears in both rankings).

---

## 2. Cross-Encoder Re-Ranking: Two-Stage Precision

After hybrid search gives you the top 100 candidates (high recall), a **cross-encoder** narrows it down to the 5 most relevant (high precision).

### Why Re-Ranking Matters

**The Bi-Encoder Limitation:**

Bi-encoders (standard embeddings) encode query and document **separately**:
```
Query ‚Üí Encoder ‚Üí vector_q
Document ‚Üí Encoder ‚Üí vector_d
Similarity = cosine(vector_q, vector_d)
```

**Problem:** No query-document interaction. Example:
- Query: "patient glucose trends"
- Doc A: "glucose: 120 mg/dL" (keyword match, no trend)
- Doc B: "HbA1c declined from 8.1 ‚Üí 7.2 over 6 months" (trend, no "glucose")

Bi-encoder might rank Doc A higher, but Doc B is more relevant.

**The Cross-Encoder Solution:**

Cross-encoders encode **query + document together**, capturing relationships:
```
[Query + Document] ‚Üí Cross-Encoder ‚Üí Relevance Score (0-1)
```

### Two-Stage Retrieval Architecture

```typescript
/**
 * Two-Stage Retrieval: Fast Retrieval ‚Üí Slow Reranking
 */
async function twoStageRetrieval(query: string) {
  // Stage 1: Fast Hybrid Search (cast wide net)
  // Goal: High recall, get top 100 candidates
  const candidates = await hybridSearch(query, {
    limit: 100, // Retrieve more for reranking
    vectorWeight: 0.7,
    keywordWeight: 0.3
  })

  // Stage 2: Slow Cross-Encoder Reranking (precision filter)
  // Goal: High precision, narrow to top 5
  const reranked = await cohere.rerank({
    query,
    documents: candidates.map(c => c.content),
    model: 'rerank-english-v3.0',
    top_n: 5,
    return_documents: true
  })

  return reranked.results.map((r, index) => ({
    rank: index + 1,
    content: r.document.text,
    relevance_score: r.relevance_score, // 0-1 from cross-encoder
    original_rank: r.index + 1           // Original position in candidates
  }))
}
```

### Cross-Encoder Models Comparison

| Model | Latency (100 docs) | NDCG@10 | Cost/1K Queries | Best For |
|-------|-------------------|---------|----------------|----------|
| **Cohere Rerank v3** | 150ms | **0.92** | $2.00 | Production (best accuracy) |
| **BGE-Reranker-Large** | 200ms | 0.89 | $0 (self-hosted) | Cost-sensitive |
| **Cross-Encoder/ms-marco** | 180ms | 0.87 | $0 (self-hosted) | Open-source |
| **No Reranking** | 0ms | 0.74 | $0 | Not recommended |

**Production Insight:** Cohere Rerank v3 offers best ROI ($2/1K queries for 18 NDCG points).

### Self-Hosted Cross-Encoder

```typescript
/**
 * Self-Hosted Cross-Encoder with HuggingFace Transformers
 * Model: cross-encoder/ms-marco-MiniLM-L-6-v2
 */
import { pipeline } from '@xenova/transformers'

class SelfHostedReranker {
  private model: any

  async initialize() {
    this.model = await pipeline(
      'text-classification',
      'cross-encoder/ms-marco-MiniLM-L-6-v2'
    )
  }

  async rerank(query: string, documents: string[], topN: number = 5) {
    // Create query-document pairs
    const pairs = documents.map(doc => `${query} [SEP] ${doc}`)

    // Score all pairs
    const scores = await Promise.all(
      pairs.map(pair => this.model(pair))
    )

    // Extract relevance scores and rank
    const ranked = documents
      .map((doc, index) => ({
        document: doc,
        score: scores[index][0].score,
        originalRank: index + 1
      }))
      .sort((a, b) => b.score - a.score)
      .slice(0, topN)

    return ranked
  }
}
```

### The Performance Trade-Off

| Stage | Operation | Latency | Recall@100 | Precision@5 | Cost/Query |
|-------|-----------|---------|-----------|------------|-----------|
| **Stage 1** | Hybrid Search (RRF) | 80ms | **0.92** | 0.68 | $0.0002 |
| **Stage 2** | Cross-Encoder Rerank | +150ms | 0.92 | **0.91** | +$0.002 |
| **Total** | Two-Stage Pipeline | **230ms** | **0.92** | **0.91** | **$0.0022** |

**Key Insight:**
- Hybrid search: 92% recall (finds right document in top 100)
- Cross-encoder: 91% precision (top 5 are all relevant)
- Total latency: 230ms (acceptable for <300ms SLA)

### When to Skip Reranking

```typescript
/**
 * Dynamic Reranking Decision
 * Skip reranking for simple queries to save cost/latency
 */
async function intelligentReranking(query: string, candidates: any[]) {
  // Skip if top result is clearly dominant
  const topScore = candidates[0]?.score || 0
  const secondScore = candidates[1]?.score || 0
  const scoreGap = topScore - secondScore

  if (scoreGap > 0.2 && topScore > 0.8) {
    console.log('Skipping reranking: Top result is clearly dominant')
    return candidates.slice(0, 5)
  }

  // Otherwise, rerank for precision
  return await twoStageRetrieval(query)
}
```

**Cost Savings:** Skipping reranking for 40% of queries saves $0.8/1K queries.

---

## 3. When Hybrid Beats Pure Vector: Decision Framework

### Query Type Analysis

```typescript
/**
 * Intelligent Router: Choose hybrid vs pure vector based on query
 */
interface QueryAnalysis {
  hasExactTerms: boolean      // "Q3 2023", "ICD-10: E11.9"
  hasDateRange: boolean        // "March 2023", "last quarter"
  hasIdentifiers: boolean      // "Patient ID: 12345"
  semanticComplexity: number   // 0-1
}

function analyzeQuery(query: string): QueryAnalysis {
  const hasExactTerms = /\b[A-Z0-9]{2,}-[A-Z0-9]+\b/.test(query)
  const hasDateRange = /\b(january|q1|q2|20\d{2})\b/i.test(query)
  const hasIdentifiers = /\b(id|number|code):\s*\w+/i.test(query)
  const wordCount = query.split(/\s+/).length
  const semanticComplexity = wordCount > 10 ? 0.8 : 0.4

  return {
    hasExactTerms,
    hasDateRange,
    hasIdentifiers,
    semanticComplexity
  }
}

function selectSearchStrategy(analysis: QueryAnalysis): 'pure_vector' | 'hybrid' | 'pure_keyword' {
  // Rule 1: Identifiers ‚Üí Hybrid (favor keyword)
  if (analysis.hasIdentifiers || analysis.hasExactTerms) {
    return 'hybrid'
  }

  // Rule 2: Date ranges ‚Üí Hybrid
  if (analysis.hasDateRange) {
    return 'hybrid'
  }

  // Rule 3: High semantic complexity ‚Üí Pure vector
  if (analysis.semanticComplexity > 0.7) {
    return 'pure_vector'
  }

  // Default: Hybrid (safest)
  return 'hybrid'
}
```

### Real-World Benchmark: Healthcare RAG

**Dataset:** 10,000 clinical notes, 500 test queries

| Query Type | Pure Vector | Hybrid | Winner | Improvement |
|-----------|-------------|--------|--------|-------------|
| "HbA1c 7.2 March 2023" | 0.62 | **0.91** | Hybrid | **+47%** |
| "Type 2 diabetes management" | **0.88** | 0.86 | Vector | -2% |
| "Patient ID: 45821" | 0.45 | **0.98** | Hybrid | **+118%** |
| "Explain insulin resistance" | **0.92** | 0.89 | Vector | -3% |
| "Prescription metformin 1000mg" | 0.71 | **0.93** | Hybrid | **+31%** |

**Insight:** Hybrid excels for **structured queries** (dates, IDs, codes). Pure vector wins for **conceptual questions**.

### Cost-Benefit Analysis

```typescript
interface CostMetrics {
  retrieval_cost: number
  rerank_cost: number
  total_latency_ms: number
  recall_at_10: number
  cost_per_query: number
}

const pureVector: CostMetrics = {
  retrieval_cost: 0.0001,
  rerank_cost: 0,
  total_latency_ms: 80,
  recall_at_10: 0.72,
  cost_per_query: 0.0001
}

const hybrid: CostMetrics = {
  retrieval_cost: 0.0002,    // 2x queries
  rerank_cost: 0.001,        // Cohere API
  total_latency_ms: 150,
  recall_at_10: 0.88,
  cost_per_query: 0.0012
}

// ROI: $110/month for 22% recall improvement
const queryVolume = 100000
const costDiff = (hybrid.cost_per_query - pureVector.cost_per_query) * queryVolume
// $110/month

const recallImprovement = (hybrid.recall_at_10 - pureVector.recall_at_10) / pureVector.recall_at_10
// 22% improvement
```

**For regulated industries (healthcare, legal), 22% recall improvement is worth $110/month.**

---

## 4. Parent-Document Retrieval (Small-to-Big)

### The Context Fragmentation Problem

When you chunk a 10-page clinical note into 500-token pieces:
- ‚ùå Chunk 17: "HbA1c: 7.2%" (missing: is this improving or worsening?)
- ‚úÖ Parent Document: Full clinical note showing trend from 8.1 ‚Üí 7.2 (context!)

### Decoupled Indexing Architecture

Instead of a 1:1 relationship between chunks and the LLM, create a **1:Many** relationship:

```typescript
/**
 * Parent-Document Retrieval
 * Search small chunks (precision) ‚Üí Retrieve large parents (context)
 */

// 1. Indexing Phase
async function indexWithParents(document: Document) {
  const parentId = document.id
  const parent Text = document.content // Full 2000-token document

  // Store parent in docstore
  await redis.set(`parent:${parentId}`, parentText)

  // Create small child chunks
  const childChunks = chunkText(parentText, { size: 200, overlap: 50 })

  // Index each child with parent reference
  for (const [index, chunk] of childChunks.entries()) {
    const embedding = await embed(chunk)
    await vectorDb.insert({
      id: `${parentId}_chunk_${index}`,
      embedding,
      metadata: {
        parent_id: parentId,      // ‚Üê Key: Link to parent
        chunk_text: chunk,
        chunk_index: index
      }
    })
  }
}

// 2. Retrieval Phase
async function retrieveWithParentContext(queryVector: number[]) {
  // Step 1: Search for best child chunks (high precision)
  const childResults = await vectorDb.search(queryVector, { limit: 5 })

  // Step 2: Extract unique parent IDs
  const parentIds = [...new Set(childResults.map(r => r.metadata.parent_id))]

  // Step 3: Fetch full parent documents from docstore
  const contextBlocks = await redis.mget(
    parentIds.map(id => `parent:${id}`)
  )

  return contextBlocks // LLM sees full context
}
```

### The Goldilocks Strategy

| Approach | Search Precision | Context Quality | Best For |
|----------|-----------------|-----------------|----------|
| Large Chunks (1000 tokens) | Low | Good | General Q&A |
| Small Chunks (200 tokens) | **High** | ‚ùå Fragmented | Exact fact lookup |
| **Parent-Document** | **High** | **Excellent** | Healthcare, Legal, Technical docs |

**In Digital Health:** Small chunks find "Lab Value: 7.2" but the LLM needs the **parent** (full clinical note) to understand if this is improving or declining.

---

## 5. A/B Testing Hybrid vs Pure Vector

### Production Experimentation Framework

```typescript
/**
 * A/B Test Framework for Retrieval Strategies
 */
class RetrievalABTest {
  private config: {
    experimentId: string
    strategies: { control: string; treatment: string }
    trafficSplit: number  // 0.1 = 10% to treatment
  }
  private results: Map<string, any[]>

  async assignStrategy(userId: string): Promise<'control' | 'treatment'> {
    const hash = await this.hashUserId(userId)
    return hash < this.config.trafficSplit ? 'treatment' : 'control'
  }

  async executeQuery(query: string, userId: string, groundTruth?: string[]) {
    const variant = await this.assignStrategy(userId)
    const start = Date.now()

    const results = variant === 'control'
      ? await pureVectorSearch(query, { limit: 10 })
      : await hybridSearchWithRerank(query, { limit: 10 })

    const latency = Date.now() - start

    await this.recordMetrics(variant, {
      query,
      results,
      latency,
      groundTruth
    })

    return results
  }

  async analyzeResults() {
    const control = this.aggregateMetrics('control')
    const treatment = this.aggregateMetrics('treatment')

    const recallImprovement = (treatment.recall_at_10 - control.recall_at_10) / control.recall_at_10

    return {
      winner: treatment.recall_at_10 > control.recall_at_10 ? 'treatment' : 'control',
      improvement: recallImprovement,
      statistical_significance: this.tTest(control, treatment).pValue < 0.05
    }
  }
}
```

### Real A/B Test Results (Healthcare SaaS)

**Experiment:** Pure Vector vs Hybrid RRF (10K queries, 7 days)

| Metric | Pure Vector | Hybrid RRF | Improvement | p-value |
|--------|------------|------------|-------------|---------|
| **Recall@10** | 0.72 | **0.88** | **+22%** | <0.001 ‚úÖ |
| **MRR** | 0.65 | **0.81** | **+25%** | <0.001 ‚úÖ |
| **Precision@5** | 0.68 | **0.87** | **+28%** | <0.001 ‚úÖ |
| **Latency P95** | 120ms | 230ms | +92% ‚ö†Ô∏è | <0.001 |
| **Cost/Query** | $0.0001 | $0.0022 | +2100% ‚ö†Ô∏è | - |

**Decision:** Ship hybrid to production.
- **Rationale:** 22% recall improvement worth $210/month for 100K queries
- **Latency:** 230ms acceptable (<300ms SLA)
- **User Feedback:** 15% reduction in "no results found" complaints

---

## 6. Implementation Guide

### Step-by-Step: Hybrid Search with Parent Retrieval

```typescript
// 1. Configure hybrid search
const searchConfig = {
  vectorWeight: 0.7,
  keywordWeight: 0.3,
  rerankModel: 'cohere/rerank-english-v3.0',
  parentRetrieval: true
}

// 2. Execute retrieval pipeline
async function enterpriseRAG(userQuery: string) {
  // Stage 1: Hybrid search (top 50 child chunks)
  const candidates = await hybridSearch(userQuery, { limit: 50, ...searchConfig })

  // Stage 2: Re-rank (top 5 child chunks)
  const reranked = await rerank(userQuery, candidates, { top_n: 5 })

  // Stage 3: Fetch parent documents
  const parentIds = reranked.map(r => r.metadata.parent_id)
  const contexts = await fetchParents(parentIds)

  // Stage 4: Send to LLM
  const prompt = `Context:\n${contexts.join('\n\n---\n\n')}\n\nQuestion: ${userQuery}`
  return await llm.complete(prompt)
}
```

### Production Checklist

- [ ] **Hybrid Search:** Combine vector + BM25 with tuned weights
- [ ] **Re-Ranking:** Add Cohere Rerank or BGE-Reranker for top-K
- [ ] **Parent-Document:** Store `parent_id` in vector metadata
- [ ] **Docstore:** Use Redis/MongoDB for fast parent retrieval
- [ ] **Evaluation:** Track Recall@10, MRR, and NDCG metrics
- [ ] **Latency Budget:** Hybrid (50ms) + Rerank (100ms) + LLM (500ms) = 650ms

---

## 5. Measuring Success

### Key Metrics for Enterprise RAG

```typescript
/**
 * Evaluation Suite
 */
interface RAGMetrics {
  recall_at_10: number      // Did we retrieve the right doc in top 10?
  mrr: number                // Mean Reciprocal Rank
  precision_at_5: number     // How many of top 5 are relevant?
  ttft_ms: number           // Time to First Token
  context_relevance: number  // % of context used by LLM
}

async function evaluateRAG(testQueries: Query[], groundTruth: Document[]) {
  const metrics = {
    recall_at_10: 0,
    mrr: 0,
    precision_at_5: 0,
    ttft_ms: 0,
    context_relevance: 0
  }

  for (const query of testQueries) {
    const start = Date.now()
    const results = await enterpriseRAG(query.text)
    const ttft = Date.now() - start

    // Calculate metrics
    metrics.ttft_ms += ttft
    metrics.recall_at_10 += calculateRecall(results, groundTruth, 10)
    metrics.mrr += calculateMRR(results, groundTruth)
    metrics.precision_at_5 += calculatePrecision(results, groundTruth, 5)
  }

  return normalizeMetrics(metrics, testQueries.length)
}
```

### Metric Calculation Implementation

```typescript
/**
 * Production-Grade Metric Calculators
 */
function calculateRecall(
  retrievedDocs: string[],
  relevantDocs: string[],
  k: number
): number {
  const topK = retrievedDocs.slice(0, k)
  const retrieved = new Set(topK)
  const relevant = new Set(relevantDocs)

  let hits = 0
  for (const doc of relevant) {
    if (retrieved.has(doc)) hits++
  }

  return relevant.size === 0 ? 0 : hits / relevant.size
}

function calculateMRR(
  retrievedDocs: string[],
  relevantDocs: string[]
): number {
  const relevant = new Set(relevantDocs)

  for (let i = 0; i < retrievedDocs.length; i++) {
    if (relevant.has(retrievedDocs[i])) {
      return 1 / (i + 1) // Reciprocal rank of first relevant doc
    }
  }

  return 0 // No relevant docs found
}

function calculateNDCG(
  retrievedDocs: string[],
  relevantDocs: string[],
  k: number
): number {
  const relevance = relevantDocs.map((doc, index) => ({
    doc,
    score: relevantDocs.length - index // Higher rank = higher score
  }))

  const relevanceMap = new Map(relevance.map(r => [r.doc, r.score]))

  // DCG: Discounted Cumulative Gain
  let dcg = 0
  for (let i = 0; i < Math.min(k, retrievedDocs.length); i++) {
    const score = relevanceMap.get(retrievedDocs[i]) || 0
    dcg += score / Math.log2(i + 2) // +2 to avoid log2(1) = 0
  }

  // IDCG: Ideal DCG (best possible ranking)
  const idealRanking = relevantDocs.slice(0, k)
  let idcg = 0
  for (let i = 0; i < idealRanking.length; i++) {
    const score = relevanceMap.get(idealRanking[i]) || 0
    idcg += score / Math.log2(i + 2)
  }

  return idcg === 0 ? 0 : dcg / idcg
}
```

### Continuous Evaluation Pipeline

```typescript
/**
 * Background evaluation with golden dataset
 */
interface GoldenExample {
  query: string
  relevant_docs: string[]
}

async function continuousEvaluation() {
  const goldenDataset: GoldenExample[] = await loadGoldenDataset()

  setInterval(async () => {
    const sample = goldenDataset.slice(0, 100) // Evaluate 100 queries

    const metrics = {
      recall_at_10: 0,
      mrr: 0,
      ndcg_at_10: 0,
      latency_p95: []
    }

    for (const example of sample) {
      const start = Date.now()
      const results = await hybridSearchWithRerank(example.query, { limit: 10 })
      const latency = Date.now() - start

      metrics.recall_at_10 += calculateRecall(
        results.map(r => r.id),
        example.relevant_docs,
        10
      )
      metrics.mrr += calculateMRR(
        results.map(r => r.id),
        example.relevant_docs
      )
      metrics.ndcg_at_10 += calculateNDCG(
        results.map(r => r.id),
        example.relevant_docs,
        10
      )
      metrics.latency_p95.push(latency)
    }

    // Alert if metrics drop
    const avgRecall = metrics.recall_at_10 / sample.length

    if (avgRecall < 0.75) {
      await alertSlack({
        channel: '#ai-alerts',
        text: `‚ö†Ô∏è Recall dropped to ${(avgRecall * 100).toFixed(1)}% (target: >75%)`
      })
    }
  }, 3600000) // Run every hour
}
```

### Benchmark Targets

| Metric | Baseline (Vector Only) | **Hybrid + Rerank** | Target | Status |
|--------|----------------------|-------------------|--------|--------|
| Recall@10 | 0.72 | **0.88** | >0.80 | ‚úÖ |
| MRR | 0.65 | **0.82** | >0.75 | ‚úÖ |
| Precision@5 | 0.68 | **0.91** | >0.80 | ‚úÖ |
| NDCG@10 | 0.70 | **0.89** | >0.80 | ‚úÖ |
| Latency P95 | 120ms | 230ms | <300ms | ‚úÖ |
| Cost/1K Queries | $0.10 | $2.20 | <$5.00 | ‚úÖ |

---

## Summary

**Hybrid Retrieval + RRF + Cross-Encoder Re-Ranking = Enterprise-Grade RAG**

**Key Patterns Implemented:**

1. **Reciprocal Rank Fusion (RRF):** Mathematically sound merging of vector + keyword results using rank positions, not raw scores (eliminates normalization issues)

2. **Two-Stage Retrieval:** Fast hybrid search (top 100, 92% recall) ‚Üí Slow cross-encoder reranking (top 5, 91% precision) = 230ms total latency

3. **Query Type Routing:** Analyze queries to choose pure vector (conceptual) vs hybrid (structured) based on identifiers, dates, exact terms

4. **Parent-Document Retrieval:** Search small chunks (precision) ‚Üí Retrieve large parents (context) to solve fragmentation

5. **A/B Testing:** Production experimentation framework showing 22% recall improvement for healthcare queries

6. **Continuous Evaluation:** Automated golden dataset testing with Recall@10, MRR, NDCG, and alerting when metrics drop

**Production Metrics:**
- Recall@10: 0.72 ‚Üí **0.88** (+22%)
- MRR: 0.65 ‚Üí **0.82** (+26%)
- Precision@5: 0.68 ‚Üí **0.91** (+34%)
- Latency: 230ms (<300ms SLA)
- Cost: $2.20/1K queries (<$5.00 budget)

**When to Use:**
- **Hybrid RRF:** Structured queries (dates, IDs, codes), regulated industries
- **Pure Vector:** Conceptual questions, semantic understanding
- **Pure Keyword:** Exact identifier lookups

In the lab, you'll build a Medical Records Navigator that uses all three techniques to find "HbA1c result from March 2022" in 5,000 pages‚Äîwith 91% precision in <230ms.

---

**Next:** Query Transformation Patterns (Multi-Query, HyDE, Decomposition)
