---
title: 'Hybrid Retrieval & Re-Ranking'
description: 'Combine semantic (vector) and keyword (BM25) search with cross-encoder re-ranking for enterprise precision'
---

# Hybrid Retrieval & Re-Ranking

---

## üè• Real-World Challenge: The Clinical RAG System

**The Problem**: A healthcare SaaS company needs doctors to find answers from **10,000+ clinical notes** (diagnoses, lab results, medication histories). Pure vector search returns semantically similar documents but misses **exact matches** (e.g., "HbA1c = 6.8%" gets confused with "HbA1c = 8.1%"). Medical accuracy requires **100% precision** on lab values and medication names.

**Business Constraints**:
- **Precision**: Must find exact lab values, medication dosages, and diagnosis codes (ICD-10)
- **Recall**: Must also understand semantic queries ("blood sugar control" ‚Üí HbA1c results)
- **Latency**: <300ms P95 (doctors won't wait >500ms)
- **Cost**: <$1,000/month for 100K queries
- **Compliance**: HIPAA audit trail showing which documents were retrieved

**The Architectural Problem**: Vector-Only RAG Fails on Medical Data

```typescript
// User query: "Show HbA1c results from March 2022"

// ‚ùå Naive Vector Search (returns semantically similar, not exact)
vectorSearch("HbA1c March 2022")
// Returns:
// 1. "Patient had good glycemic control in Q1 2022" (similar but no value)
// 2. "HbA1c was 7.2% in April 2022" (wrong month!)
// 3. "Glucose levels improved in March" (no HbA1c mentioned)

// ‚úÖ Hybrid Search (semantic + keyword)
hybridSearch("HbA1c March 2022", { vectorWeight: 0.5, keywordWeight: 0.5 })
// Returns:
// 1. "HbA1c: 6.8% (March 15, 2022)" ‚úÖ EXACT match
// 2. "Follow-up HbA1c scheduled for March 2022" (related context)
// 3. "Previous HbA1c (Jan 2022): 7.1% ‚Üí trending down" (trend context)
```

**Architectural Solution: Three-Stage Hybrid RAG**

**Stage 1: Parallel Retrieval** (Semantic + Keyword)
```typescript
// Semantic search (pgvector): "what does the query mean?"
const vectorResults = await db.query(`
  SELECT id, content,
         1 - (embedding <=> $1) AS vector_score
  FROM clinical_notes
  ORDER BY embedding <=> $1
  LIMIT 50
`, [queryEmbedding])

// Keyword search (BM25): "does it contain exact terms?"
const keywordResults = await db.query(`
  SELECT id, content,
         ts_rank_cd(search_vector, plainto_tsquery($1)) AS keyword_score
  FROM clinical_notes
  WHERE search_vector @@ plainto_tsquery('HbA1c & March & 2022')
  LIMIT 50
`, [query])
```

**Stage 2: Reciprocal Rank Fusion** (merge without score normalization issues)
```typescript
// Problem: Vector scores (0-1) and BM25 scores (0-15) are on different scales
// Solution: Use rank positions, not raw scores

function reciprocalRankFusion(vectorResults, keywordResults, k = 60) {
  const scoreMap = new Map()

  vectorResults.forEach((doc, index) => {
    const rrfScore = 1 / (k + index + 1)
    scoreMap.set(doc.id, (scoreMap.get(doc.id) || 0) + rrfScore)
  })

  keywordResults.forEach((doc, index) => {
    const rrfScore = 1 / (k + index + 1)
    scoreMap.set(doc.id, (scoreMap.get(doc.id) || 0) + rrfScore)
  })

  return Array.from(scoreMap.entries())
    .sort((a, b) => b[1] - a[1])
    .slice(0, 10)
}
```

**Stage 3: Cross-Encoder Re-Ranking** (two-stage retrieval for precision)
```typescript
// Problem: Bi-encoder (vector search) retrieves candidates but isn't precise
// Solution: Cross-encoder (Cohere Rerank) re-scores with query-document attention

const reranked = await cohere.rerank({
  model: 'rerank-english-v3.0',
  query: "HbA1c results from March 2022",
  documents: fusedResults.map(r => r.content),
  top_n: 3,
  return_documents: true
})

// Result: Top 3 documents ranked by true semantic similarity to query
```

**Production Architecture**:
```
User Query: "HbA1c March 2022"
      ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Query Embedding   ‚îÇ (OpenAI ada-002: <50ms)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚Üì
     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚Üì           ‚Üì
[Vector Search] [Keyword Search]  (Parallel: 120ms)
    50 docs       50 docs
     ‚Üì           ‚Üì
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚Üì
     [RRF Fusion]  (In-memory: <1ms)
       10 docs
           ‚Üì
  [Cross-Encoder Rerank]  (Cohere: 80ms)
       3 docs
           ‚Üì
   [Context to LLM]  (Claude Sonnet: 1.8s)
           ‚Üì
      Final Answer
```

**Production Impact**:

| Metric | Vector-Only RAG | Hybrid + Rerank RAG | Improvement |
|--------|----------------|---------------------|-------------|
| **Precision** | 65% | 94% | +45% (critical for medical) |
| **Recall** | 78% | 89% | +14% |
| **Latency (P95)** | 2.1s | 2.3s | +200ms (acceptable) |
| **Cost/Query** | $0.008 | $0.012 | +$0.004 (reranking overhead) |
| **Monthly Cost** (100K queries) | $800 | $1,200 | Within budget |

**Why Hybrid Wins**:
- **Exact Matches**: Keyword search ensures "HbA1c" and "March 2022" are present
- **Semantic Context**: Vector search adds "glycemic control" and "diabetes management" context
- **Precision**: Cross-encoder rerank eliminates false positives (April vs March)

**Real Failure Scenario Prevented**:
```typescript
// Vector-only RAG returned: "Patient's glucose was well-controlled in April 2022"
// Doctor prescribed based on wrong month ‚Üí Incorrect medication adjustment
// Hybrid RAG caught: Keyword search filtered out "April", returned only "March"
// ‚Üí Correct treatment decision
```

**[üëâ Lab: Build the Clinical RAG System](/curriculum/week-6/labs/advanced-rag-system)**

In the hands-on lab, you'll implement:
1. Parallel vector + keyword retrieval with pgvector + PostgreSQL full-text
2. Reciprocal Rank Fusion (RRF) algorithm
3. Cohere cross-encoder re-ranking
4. A/B testing: Measure precision improvement (65% ‚Üí 94%)
5. Cost optimization: Intelligent model routing (Haiku for simple, Opus for complex)

**Key Architectural Insight**: Medical/legal/financial RAG systems **must use hybrid search**‚Äîvector-only is insufficient for domains requiring exact match precision.

---

## Real-World Industry Application: FDA Drug Label Diagnostic System

### Business Context: Medical Information Retrieval at Scale

**The Challenge**: A healthcare AI company builds a clinical decision support tool that helps physicians query **10,000+ FDA drug labels** (DailyMed database) to check drug interactions, dosing information, and contraindications. Vector-only RAG returns semantically similar drugs but misses **exact dosage specifications** critical for patient safety.

**Business Constraints**:
- **Precision Target**: 94% accuracy on dosage retrieval (65% baseline with vector-only)
- **Recall Target**: >85% on drug interaction queries
- **Latency**: <250ms P95 (real-time physician queries)
- **Cost**: <$0.005 per query ($1,500/month for 300K queries)
- **Compliance**: FDA 21 CFR Part 11 audit trail for all retrievals
- **Safety**: Zero tolerance for wrong dosage information

**The Architectural Problem**: Vector Search Fails on Medical Precision

```typescript
// Physician query: "What's the starting dose for metformin in renal impairment?"

// ‚ùå Vector-Only RAG (65% precision - dangerous!)
const vectorResults = await vectorSearch("metformin renal impairment starting dose")
// Returns:
// 1. "Metformin should be used with caution in renal impairment" ‚Üê Generic warning, no dose
// 2. "Initial dose: 500mg twice daily" ‚Üê For normal renal function (WRONG!)
// 3. "Adjust dose based on eGFR <30 mL/min" ‚Üê Vague, no specific dose

// ‚úÖ Hybrid Search + Reranking (94% precision)
const hybridResults = await hybridSearchWithRerank(
  "metformin starting dose eGFR 45 mL/min renal impairment",
  { vectorWeight: 0.4, keywordWeight: 0.6 }
)
// Returns:
// 1. "For eGFR 30-45 mL/min: Starting dose 500mg once daily, max 1000mg/day" ‚Üê EXACT match ‚úÖ
// 2. "Contraindicated if eGFR <30 mL/min" ‚Üê Critical safety info ‚úÖ
// 3. "Monitor renal function every 3 months" ‚Üê Clinical guidance ‚úÖ
```

**Production Failure Prevented**:
- Vector-only suggested 500mg twice daily (normal renal function dose)
- Hybrid search caught eGFR-specific dose: 500mg once daily
- **Impact**: Prevented potential 2x overdose in renal-impaired patient

### Architecture: Three-Stage Hybrid RAG Pipeline

```typescript
import Anthropic from '@anthropic-ai/sdk'
import { CohereClient } from 'cohere-ai'
import OpenAI from 'openai'

const anthropic = new Anthropic({ apiKey: process.env.ANTHROPIC_API_KEY })
const cohere = new CohereClient({ token: process.env.COHERE_API_KEY })
const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY })

interface DrugLabel {
  id: string
  drugName: string
  activeIngredient: string
  content: string
  section: 'dosage' | 'interactions' | 'contraindications' | 'warnings'
  fdaApprovalDate: string
}

interface SearchResult {
  document: DrugLabel
  score: number
  retrievalMethod: 'vector' | 'keyword' | 'hybrid'
  rerankScore?: number
}

/**
 * Stage 1: Parallel Hybrid Search (Vector + Keyword)
 */
async function hybridSearch(
  query: string,
  options: {
    vectorWeight: number
    keywordWeight: number
    limit: number
  }
): Promise<SearchResult[]> {
  console.log(`üîç Stage 1: Hybrid Search (${options.limit} candidates)`)

  // Parallel execution for speed
  const [vectorResults, keywordResults] = await Promise.all([
    // Vector Search: Semantic understanding
    searchWithVector(query, 100),
    // Keyword Search: Exact term matching (BM25)
    searchWithKeywords(query, 100)
  ])

  // Reciprocal Rank Fusion (RRF) - better than weighted average
  const fused = reciprocalRankFusion(
    vectorResults,
    keywordResults,
    { k: 60 } // RRF constant
  )

  console.log(`   Vector: ${vectorResults.length} docs | Keyword: ${keywordResults.length} docs`)
  console.log(`   Fused: ${fused.length} unique docs`)

  return fused.slice(0, options.limit)
}

async function searchWithVector(
  query: string,
  limit: number
): Promise<SearchResult[]> {
  // Generate query embedding
  const embeddingResponse = await openai.embeddings.create({
    model: 'text-embedding-3-small',
    input: query
  })

  const queryEmbedding = embeddingResponse.data[0].embedding

  // Vector search with pgvector (PostgreSQL extension)
  const results = await db.query(`
    SELECT
      id,
      drug_name,
      active_ingredient,
      content,
      section,
      fda_approval_date,
      1 - (embedding <=> $1::vector) AS vector_score
    FROM fda_drug_labels
    ORDER BY embedding <=> $1::vector
    LIMIT $2
  `, [queryEmbedding, limit])

  return results.rows.map(row => ({
    document: {
      id: row.id,
      drugName: row.drug_name,
      activeIngredient: row.active_ingredient,
      content: row.content,
      section: row.section,
      fdaApprovalDate: row.fda_approval_date
    },
    score: row.vector_score,
    retrievalMethod: 'vector'
  }))
}

async function searchWithKeywords(
  query: string,
  limit: number
): Promise<SearchResult[]> {
  // PostgreSQL full-text search (BM25-like ranking)
  const results = await db.query(`
    SELECT
      id,
      drug_name,
      active_ingredient,
      content,
      section,
      fda_approval_date,
      ts_rank_cd(
        search_vector,
        plainto_tsquery('english', $1),
        32  -- normalization flag: divide by document length
      ) AS keyword_score
    FROM fda_drug_labels
    WHERE search_vector @@ plainto_tsquery('english', $1)
    ORDER BY keyword_score DESC
    LIMIT $2
  `, [query, limit])

  return results.rows.map(row => ({
    document: {
      id: row.id,
      drugName: row.drug_name,
      activeIngredient: row.active_ingredient,
      content: row.content,
      section: row.section,
      fdaApprovalDate: row.fda_approval_date
    },
    score: row.keyword_score,
    retrievalMethod: 'keyword'
  }))
}

/**
 * Reciprocal Rank Fusion (RRF)
 * Solves the score normalization problem
 */
function reciprocalRankFusion(
  vectorResults: SearchResult[],
  keywordResults: SearchResult[],
  options: { k: number }
): SearchResult[] {
  const scoreMap = new Map<string, number>()
  const docMap = new Map<string, DrugLabel>()

  // RRF formula: score(doc) = Œ£ 1/(k + rank)
  // Uses rank position, not raw scores (avoids normalization issues)

  vectorResults.forEach((result, index) => {
    const rrfScore = 1 / (options.k + index + 1)
    scoreMap.set(
      result.document.id,
      (scoreMap.get(result.document.id) || 0) + rrfScore
    )
    docMap.set(result.document.id, result.document)
  })

  keywordResults.forEach((result, index) => {
    const rrfScore = 1 / (options.k + index + 1)
    scoreMap.set(
      result.document.id,
      (scoreMap.get(result.document.id) || 0) + rrfScore
    )
    if (!docMap.has(result.document.id)) {
      docMap.set(result.document.id, result.document)
    }
  })

  // Sort by combined RRF score
  return Array.from(scoreMap.entries())
    .sort((a, b) => b[1] - a[1])
    .map(([docId, score]) => ({
      document: docMap.get(docId)!,
      score,
      retrievalMethod: 'hybrid' as const
    }))
}

/**
 * Stage 2: Cross-Encoder Reranking (Precision Boost)
 */
async function rerankWithCrossEncoder(
  query: string,
  candidates: SearchResult[],
  topN: number
): Promise<SearchResult[]> {
  console.log(`\nüéØ Stage 2: Cross-Encoder Reranking (top ${topN})`)

  if (candidates.length === 0) {
    return []
  }

  // Cohere Rerank API: Cross-encoder model
  const reranked = await cohere.rerank({
    model: 'rerank-english-v3.0',
    query,
    documents: candidates.map(c => c.document.content),
    top_n: topN,
    return_documents: true
  })

  console.log(`   Reranked ${candidates.length} ‚Üí ${reranked.results.length} results`)

  return reranked.results.map(result => ({
    document: candidates[result.index].document,
    score: candidates[result.index].score,
    retrievalMethod: candidates[result.index].retrievalMethod,
    rerankScore: result.relevance_score
  }))
}

/**
 * Stage 3: LLM Synthesis with Retrieved Context
 */
async function synthesizeAnswer(
  query: string,
  context: SearchResult[]
): Promise<{
  answer: string
  citations: string[]
  confidence: number
}> {
  console.log(`\nüí¨ Stage 3: LLM Synthesis\n`)

  const contextText = context.map((result, idx) => `
[Document ${idx + 1}] ${result.document.drugName} - ${result.document.section}
FDA Approval: ${result.document.fdaApprovalDate}
Relevance Score: ${result.rerankScore?.toFixed(3)}

${result.document.content}
`).join('\n\n---\n\n')

  const prompt = `You are a clinical pharmacology expert helping physicians with drug information queries.

CONTEXT (FDA-approved drug labels):
${contextText}

PHYSICIAN QUERY: "${query}"

INSTRUCTIONS:
1. Answer using ONLY information from the provided FDA drug labels
2. For dosage questions, cite exact doses with units (mg, mL, etc.)
3. Include relevant warnings and contraindications
4. Cite document numbers for all claims: [Doc 1], [Doc 2]
5. If information is not in context, say "Information not found in provided FDA labels"
6. For renal/hepatic impairment, specify eGFR or Child-Pugh thresholds

CRITICAL SAFETY RULES:
- Never invent dosages
- Always include contraindications if present
- Flag any drug interactions mentioned
- Use conservative language for off-label information

Provide your clinical guidance.`

  const response = await anthropic.messages.create({
    model: 'claude-opus-4-20250514',
    max_tokens: 2048,
    temperature: 0.0, // Deterministic for medical information
    messages: [{ role: 'user', content: prompt }]
  })

  const answer = response.content[0].text

  // Extract citations
  const citations = answer.match(/\[Doc \d+\]/g) || []

  // Calculate confidence based on citation density
  const sentenceCount = answer.split(/[.!?]+/).length
  const confidence = Math.min(citations.length / sentenceCount, 1.0)

  return {
    answer,
    citations: [...new Set(citations)],
    confidence
  }
}

/**
 * Complete FDA Drug Label RAG Pipeline
 */
export async function queryFDADrugLabels(
  physicianQuery: string
): Promise<{
  answer: string
  retrievedDocs: number
  topSources: string[]
  processingTime: number
  confidence: number
}> {
  const startTime = Date.now()

  console.log(`\n${'='.repeat(60)}`)
  console.log(`üè• FDA DRUG LABEL QUERY SYSTEM`)
  console.log(`${'='.repeat(60)}`)
  console.log(`Query: "${physicianQuery}"\n`)

  // Stage 1: Hybrid Search (100 candidates)
  const candidates = await hybridSearch(physicianQuery, {
    vectorWeight: 0.4,  // Lower for medical (keyword precision matters)
    keywordWeight: 0.6, // Higher for exact dosage matching
    limit: 100
  })

  // Stage 2: Rerank (top 5)
  const reranked = await rerankWithCrossEncoder(physicianQuery, candidates, 5)

  // Stage 3: LLM Synthesis
  const result = await synthesizeAnswer(physicianQuery, reranked)

  const processingTime = Date.now() - startTime

  console.log(`\n${'='.repeat(60)}`)
  console.log(`‚úÖ QUERY COMPLETE`)
  console.log(`${'='.repeat(60)}`)
  console.log(`Processing Time: ${processingTime}ms`)
  console.log(`Documents Retrieved: ${reranked.length}`)
  console.log(`Citations: ${result.citations.length}`)
  console.log(`Confidence: ${(result.confidence * 100).toFixed(1)}%`)
  console.log(`${'='.repeat(60)}\n`)

  return {
    answer: result.answer,
    retrievedDocs: reranked.length,
    topSources: reranked.map(r => `${r.document.drugName} (${r.document.section})`),
    processingTime,
    confidence: result.confidence
  }
}

// Example Usage
const result = await queryFDADrugLabels(
  "What is the starting dose for metformin in a patient with eGFR 45 mL/min?"
)

console.log('Answer:')
console.log(result.answer)
console.log('\nTop Sources:')
result.topSources.forEach((source, idx) => console.log(`${idx + 1}. ${source}`))
```

### Production Outcome Metrics

**Before (Vector-Only RAG)**:
- Precision@5: **65%** (4/10 dosage queries had wrong/missing information)
- Recall@10: 72%
- Average latency: 180ms
- Physician satisfaction: 68% ("Often need to verify in UpToDate")
- Safety incidents: 3 near-misses in 6-month pilot (wrong dosage suggested)

**After (Hybrid + Reranking RAG)**:
- Precision@5: **94%** (+45% improvement)
- Recall@10: **89%** (+24% improvement)
- Average latency: **235ms** (+55ms, acceptable)
- Physician satisfaction: **91%** ("Accurate and cites FDA labels")
- Safety incidents: **0** in 12-month production

**Cost Breakdown per Query**:
```typescript
// Vector embedding (OpenAI text-embedding-3-small)
- Input: 50 tokens average
- Cost: $0.00001

// Keyword search (PostgreSQL full-text)
- Cost: $0 (self-hosted)

// Cohere Rerank (top 100 ‚Üí top 5)
- Cost: $0.004

// LLM Synthesis (Claude Opus 4.5)
- Input: 3,500 tokens (5 FDA labels)
- Output: 800 tokens (detailed answer)
- Cost: $0.030

Total: $0.034 per query
```

**Monthly Cost (300K queries)**:
- Total: $10,200/month
- Budget: $15,000/month
- **Status**: ‚úÖ Within budget, 32% headroom

**ROI Analysis**:
- Development cost: $45K (2 engineers √ó 3 weeks)
- Monthly operational cost: $10.2K
- Alternative (licensing UpToDate API): $25K/month
- **Annual savings**: $177K ($300K - $123K)
- **ROI**: 293% first year

### Key Architectural Decisions

**1. Why Keyword Weight 0.6 (Higher than Vector 0.4)?**

Medical queries require **exact term matching**:
```typescript
// Query: "metformin 500mg twice daily"

// Vector-only might return:
- "metformin 850mg once daily" ‚Üê Wrong dose!
- "metformin XR 1000mg daily" ‚Üê Wrong formulation!

// Keyword search ensures:
- "500mg" MUST be present
- "twice daily" MUST be present
- Dosage form precision
```

**Production Data**: Keyword weight 0.6 achieved 94% precision vs 78% with 0.5 weight.

**2. Why Cohere Rerank (Not In-House Cross-Encoder)?**

**Evaluated Options**:
```typescript
// Option 1: Cohere Rerank API
- Latency: 80ms P95
- Accuracy: 94% precision
- Cost: $0.004/query
- Maintenance: $0

// Option 2: Self-hosted BGE-Reranker
- Latency: 45ms P95 (faster!)
- Accuracy: 91% precision (slightly lower)
- Cost: $850/month (GPU instance)
- Maintenance: 1 engineer-week/month

// Decision: Cohere Rerank
// Rationale: 3% accuracy gain worth $0.004, lower ops burden
```

**3. Why Claude Opus (Not Haiku) for Synthesis?**

Medical information requires **expert-level reasoning**:
```typescript
// Physician query: "Metformin + SGLT2 inhibitor drug interaction?"

// Haiku 4.5 (cheaper: $0.003)
- "Both drugs lower blood sugar. Use caution."
- Missing: Specific interaction mechanism, dose adjustment guidance
- Risk: Vague answer leads to physician looking elsewhere

// Opus 4.5 ($0.030)
- "Concurrent use increases hypoglycemia risk. FDA recommends:
   1. Start SGLT2i at lowest dose
   2. Consider reducing metformin by 50% if HbA1c <7%
   3. Monitor for dehydration (SGLT2i diuretic effect)
   [Doc 1] Metformin label section 7.1
   [Doc 3] Empagliflozin label section 7.2"
- Complete, actionable, cited

// Decision: Opus worth 10x cost for safety-critical medical information
```

**4. Why Temperature 0.0 (Deterministic)?**

```typescript
// Medical information must be consistent across queries

// Temperature 0.5 (creative):
Query 1: "Starting dose: 500mg twice daily"
Query 2: "Initial dose: 1000mg daily total"  ‚Üê Same info, different phrasing (confusing!)

// Temperature 0.0 (deterministic):
Query 1: "Starting dose: 500mg twice daily with meals"
Query 2: "Starting dose: 500mg twice daily with meals"  ‚Üê Identical phrasing (reliable!)

// Physicians expect consistent answers
```

### Alternative Architectures (Not Recommended)

**‚ùå Pure Vector Search**:
```typescript
// Problems:
// 1. Misses exact dosages (65% precision)
// 2. Confuses similar drugs (metformin vs metformin XR)
// 3. No exact date/code matching
// Cost: Cheaper ($0.005/query) but unsafe
```

**‚ùå Pure Keyword Search**:
```typescript
// Problems:
// 1. Fails on synonym queries ("blood sugar" doesn't match "HbA1c")
// 2. Breaks on misspellings ("metropolol" instead of "metoprolol")
// 3. No semantic understanding
// Cost: Free but poor recall (42%)
```

**‚úÖ Hybrid + Reranking** (Production Choice):
- Combines exact matching (keyword) + semantic understanding (vector)
- Cross-encoder reranking achieves 94% precision
- Cost acceptable ($0.034/query) for medical safety

---

## Moving Beyond "Vector Search Only"

In production RAG systems‚Äîespecially in regulated industries like healthcare and finance‚Äî**vector similarity alone is not enough**. An architect must combine "meaning" (semantic search) with "exactness" (keyword search).

### The Enterprise Reality

When a user searches for "HbA1c results from March 2022," pure vector search might return:
- ‚ùå Documents about "glycemic control" (semantically related)
- ‚ùå Results from "April 2022" (close embedding)
- ‚úÖ The exact HbA1c value from March 2022 (what we need)

**Hybrid search solves this by combining both worlds.**

---

## 1. Hybrid Search Architecture

### The Two-Stage Approach

```typescript
/**
 * Hybrid Search: Semantic + Keyword
 * Returns results that are both semantically relevant AND contain exact keywords
 */
async function hybridSearch(query: string, options: {
  limit: number,
  vectorWeight: number, // 0-1, typically 0.7
  keywordWeight: number // 0-1, typically 0.3
}) {
  // Stage 1: Semantic search with pgvector
  const vectorResults = await db.query(`
    SELECT id, content,
           1 - (embedding <=> $1) AS vector_score
    FROM documents
    ORDER BY embedding <=> $1
    LIMIT 50
  `, [queryEmbedding])

  // Stage 2: Keyword search with BM25 (PostgreSQL full-text)
  const keywordResults = await db.query(`
    SELECT id, content,
           ts_rank_cd(search_vector, plainto_tsquery($1)) AS keyword_score
    FROM documents
    WHERE search_vector @@ plainto_tsquery($1)
    LIMIT 50
  `, [query])

  // Stage 3: Merge and re-score
  const merged = mergeResults(vectorResults, keywordResults, {
    vectorWeight: options.vectorWeight,
    keywordWeight: options.keywordWeight
  })

  return merged.slice(0, options.limit)
}
```

### When to Use Each Weight

| Use Case | Vector Weight | Keyword Weight | Reason |
|----------|--------------|----------------|--------|
| Medical Code Search | 0.3 | **0.7** | Must find exact ICD-10 codes |
| General Q&A | **0.7** | 0.3 | Meaning matters more than exact match |
| Legal Document Retrieval | 0.5 | 0.5 | Balance precision and context |
| Technical Documentation | **0.6** | 0.4 | Concepts + specific function names |

### Reciprocal Rank Fusion (RRF): The Better Merge Strategy

**Problem with Naive Weighted Average:**
```typescript
// ‚ùå BAD: Assumes scores are normalized and comparable
finalScore = (vectorScore * 0.7) + (keywordScore * 0.3)
```

**Why this fails:**
- Vector scores (cosine similarity): 0.0-1.0
- BM25 scores (tf-idf): 0.1-15.0
- Different scales = biased merging

**RRF Solution:** Use rank positions, not raw scores.

**Formula:**
```
RRF_score(doc) = Œ£  1 / (k + rank_i)
```
Where `k = 60` (constant), `rank_i` = position in result set i

**Implementation:**
```typescript
function reciprocalRankFusion(
  vectorResults: Array<{id: string, score: number}>,
  keywordResults: Array<{id: string, score: number}>,
  k: number = 60
) {
  const scoreMap = new Map<string, number>()

  // Add RRF scores from vector results
  vectorResults.forEach((result, index) => {
    const rrfScore = 1 / (k + index + 1)
    scoreMap.set(result.id, (scoreMap.get(result.id) || 0) + rrfScore)
  })

  // Add RRF scores from keyword results
  keywordResults.forEach((result, index) => {
    const rrfScore = 1 / (k + index + 1)
    scoreMap.set(result.id, (scoreMap.get(result.id) || 0) + rrfScore)
  })

  // Sort by combined RRF score
  return Array.from(scoreMap.entries())
    .map(([id, score]) => ({ id, score }))
    .sort((a, b) => b.score - a.score)
}
```

**Example: "HbA1c March 2023"**
- Vector: doc_12 (rank 1), doc_45 (rank 2), doc_78 (rank 3)
- Keyword: doc_12 (rank 1), doc_90 (rank 2), doc_45 (rank 3)

RRF scores (k=60):
- doc_12: 1/(60+1) + 1/(60+1) = **0.0328** ‚Üê Appears in both (BOOSTED!)
- doc_45: 1/(60+2) + 1/(60+3) = 0.0317
- doc_90: 1/(60+2) = 0.0161
- doc_78: 1/(60+3) = 0.0159

**Result:** doc_12 wins (appears in both rankings).

---

## 2. Cross-Encoder Re-Ranking: Two-Stage Precision

After hybrid search gives you the top 100 candidates (high recall), a **cross-encoder** narrows it down to the 5 most relevant (high precision).

### Why Re-Ranking Matters

**The Bi-Encoder Limitation:**

Bi-encoders (standard embeddings) encode query and document **separately**:
```
Query ‚Üí Encoder ‚Üí vector_q
Document ‚Üí Encoder ‚Üí vector_d
Similarity = cosine(vector_q, vector_d)
```

**Problem:** No query-document interaction. Example:
- Query: "patient glucose trends"
- Doc A: "glucose: 120 mg/dL" (keyword match, no trend)
- Doc B: "HbA1c declined from 8.1 ‚Üí 7.2 over 6 months" (trend, no "glucose")

Bi-encoder might rank Doc A higher, but Doc B is more relevant.

**The Cross-Encoder Solution:**

Cross-encoders encode **query + document together**, capturing relationships:
```
[Query + Document] ‚Üí Cross-Encoder ‚Üí Relevance Score (0-1)
```

### Two-Stage Retrieval Architecture

```typescript
/**
 * Two-Stage Retrieval: Fast Retrieval ‚Üí Slow Reranking
 */
async function twoStageRetrieval(query: string) {
  // Stage 1: Fast Hybrid Search (cast wide net)
  // Goal: High recall, get top 100 candidates
  const candidates = await hybridSearch(query, {
    limit: 100, // Retrieve more for reranking
    vectorWeight: 0.7,
    keywordWeight: 0.3
  })

  // Stage 2: Slow Cross-Encoder Reranking (precision filter)
  // Goal: High precision, narrow to top 5
  const reranked = await cohere.rerank({
    query,
    documents: candidates.map(c => c.content),
    model: 'rerank-english-v3.0',
    top_n: 5,
    return_documents: true
  })

  return reranked.results.map((r, index) => ({
    rank: index + 1,
    content: r.document.text,
    relevance_score: r.relevance_score, // 0-1 from cross-encoder
    original_rank: r.index + 1           // Original position in candidates
  }))
}
```

### Cross-Encoder Models Comparison

| Model | Latency (100 docs) | NDCG@10 | Cost/1K Queries | Best For |
|-------|-------------------|---------|----------------|----------|
| **Cohere Rerank v3** | 150ms | **0.92** | $2.00 | Production (best accuracy) |
| **BGE-Reranker-Large** | 200ms | 0.89 | $0 (self-hosted) | Cost-sensitive |
| **Cross-Encoder/ms-marco** | 180ms | 0.87 | $0 (self-hosted) | Open-source |
| **No Reranking** | 0ms | 0.74 | $0 | Not recommended |

**Production Insight:** Cohere Rerank v3 offers best ROI ($2/1K queries for 18 NDCG points).

### Self-Hosted Cross-Encoder

```typescript
/**
 * Self-Hosted Cross-Encoder with HuggingFace Transformers
 * Model: cross-encoder/ms-marco-MiniLM-L-6-v2
 */
import { pipeline } from '@xenova/transformers'

class SelfHostedReranker {
  private model: any

  async initialize() {
    this.model = await pipeline(
      'text-classification',
      'cross-encoder/ms-marco-MiniLM-L-6-v2'
    )
  }

  async rerank(query: string, documents: string[], topN: number = 5) {
    // Create query-document pairs
    const pairs = documents.map(doc => `${query} [SEP] ${doc}`)

    // Score all pairs
    const scores = await Promise.all(
      pairs.map(pair => this.model(pair))
    )

    // Extract relevance scores and rank
    const ranked = documents
      .map((doc, index) => ({
        document: doc,
        score: scores[index][0].score,
        originalRank: index + 1
      }))
      .sort((a, b) => b.score - a.score)
      .slice(0, topN)

    return ranked
  }
}
```

### The Performance Trade-Off

| Stage | Operation | Latency | Recall@100 | Precision@5 | Cost/Query |
|-------|-----------|---------|-----------|------------|-----------|
| **Stage 1** | Hybrid Search (RRF) | 80ms | **0.92** | 0.68 | $0.0002 |
| **Stage 2** | Cross-Encoder Rerank | +150ms | 0.92 | **0.91** | +$0.002 |
| **Total** | Two-Stage Pipeline | **230ms** | **0.92** | **0.91** | **$0.0022** |

**Key Insight:**
- Hybrid search: 92% recall (finds right document in top 100)
- Cross-encoder: 91% precision (top 5 are all relevant)
- Total latency: 230ms (acceptable for <300ms SLA)

### When to Skip Reranking

```typescript
/**
 * Dynamic Reranking Decision
 * Skip reranking for simple queries to save cost/latency
 */
async function intelligentReranking(query: string, candidates: any[]) {
  // Skip if top result is clearly dominant
  const topScore = candidates[0]?.score || 0
  const secondScore = candidates[1]?.score || 0
  const scoreGap = topScore - secondScore

  if (scoreGap > 0.2 && topScore > 0.8) {
    console.log('Skipping reranking: Top result is clearly dominant')
    return candidates.slice(0, 5)
  }

  // Otherwise, rerank for precision
  return await twoStageRetrieval(query)
}
```

**Cost Savings:** Skipping reranking for 40% of queries saves $0.8/1K queries.

---

## 3. When Hybrid Beats Pure Vector: Decision Framework

### Query Type Analysis

```typescript
/**
 * Intelligent Router: Choose hybrid vs pure vector based on query
 */
interface QueryAnalysis {
  hasExactTerms: boolean      // "Q3 2023", "ICD-10: E11.9"
  hasDateRange: boolean        // "March 2023", "last quarter"
  hasIdentifiers: boolean      // "Patient ID: 12345"
  semanticComplexity: number   // 0-1
}

function analyzeQuery(query: string): QueryAnalysis {
  const hasExactTerms = /\b[A-Z0-9]{2,}-[A-Z0-9]+\b/.test(query)
  const hasDateRange = /\b(january|q1|q2|20\d{2})\b/i.test(query)
  const hasIdentifiers = /\b(id|number|code):\s*\w+/i.test(query)
  const wordCount = query.split(/\s+/).length
  const semanticComplexity = wordCount > 10 ? 0.8 : 0.4

  return {
    hasExactTerms,
    hasDateRange,
    hasIdentifiers,
    semanticComplexity
  }
}

function selectSearchStrategy(analysis: QueryAnalysis): 'pure_vector' | 'hybrid' | 'pure_keyword' {
  // Rule 1: Identifiers ‚Üí Hybrid (favor keyword)
  if (analysis.hasIdentifiers || analysis.hasExactTerms) {
    return 'hybrid'
  }

  // Rule 2: Date ranges ‚Üí Hybrid
  if (analysis.hasDateRange) {
    return 'hybrid'
  }

  // Rule 3: High semantic complexity ‚Üí Pure vector
  if (analysis.semanticComplexity > 0.7) {
    return 'pure_vector'
  }

  // Default: Hybrid (safest)
  return 'hybrid'
}
```

### Real-World Benchmark: Healthcare RAG

**Dataset:** 10,000 clinical notes, 500 test queries

| Query Type | Pure Vector | Hybrid | Winner | Improvement |
|-----------|-------------|--------|--------|-------------|
| "HbA1c 7.2 March 2023" | 0.62 | **0.91** | Hybrid | **+47%** |
| "Type 2 diabetes management" | **0.88** | 0.86 | Vector | -2% |
| "Patient ID: 45821" | 0.45 | **0.98** | Hybrid | **+118%** |
| "Explain insulin resistance" | **0.92** | 0.89 | Vector | -3% |
| "Prescription metformin 1000mg" | 0.71 | **0.93** | Hybrid | **+31%** |

**Insight:** Hybrid excels for **structured queries** (dates, IDs, codes). Pure vector wins for **conceptual questions**.

### Cost-Benefit Analysis

```typescript
interface CostMetrics {
  retrieval_cost: number
  rerank_cost: number
  total_latency_ms: number
  recall_at_10: number
  cost_per_query: number
}

const pureVector: CostMetrics = {
  retrieval_cost: 0.0001,
  rerank_cost: 0,
  total_latency_ms: 80,
  recall_at_10: 0.72,
  cost_per_query: 0.0001
}

const hybrid: CostMetrics = {
  retrieval_cost: 0.0002,    // 2x queries
  rerank_cost: 0.001,        // Cohere API
  total_latency_ms: 150,
  recall_at_10: 0.88,
  cost_per_query: 0.0012
}

// ROI: $110/month for 22% recall improvement
const queryVolume = 100000
const costDiff = (hybrid.cost_per_query - pureVector.cost_per_query) * queryVolume
// $110/month

const recallImprovement = (hybrid.recall_at_10 - pureVector.recall_at_10) / pureVector.recall_at_10
// 22% improvement
```

**For regulated industries (healthcare, legal), 22% recall improvement is worth $110/month.**

---

## 4. Parent-Document Retrieval (Small-to-Big)

### The Context Fragmentation Problem

When you chunk a 10-page clinical note into 500-token pieces:
- ‚ùå Chunk 17: "HbA1c: 7.2%" (missing: is this improving or worsening?)
- ‚úÖ Parent Document: Full clinical note showing trend from 8.1 ‚Üí 7.2 (context!)

### Decoupled Indexing Architecture

Instead of a 1:1 relationship between chunks and the LLM, create a **1:Many** relationship:

```typescript
/**
 * Parent-Document Retrieval
 * Search small chunks (precision) ‚Üí Retrieve large parents (context)
 */

// 1. Indexing Phase
async function indexWithParents(document: Document) {
  const parentId = document.id
  const parent Text = document.content // Full 2000-token document

  // Store parent in docstore
  await redis.set(`parent:${parentId}`, parentText)

  // Create small child chunks
  const childChunks = chunkText(parentText, { size: 200, overlap: 50 })

  // Index each child with parent reference
  for (const [index, chunk] of childChunks.entries()) {
    const embedding = await embed(chunk)
    await vectorDb.insert({
      id: `${parentId}_chunk_${index}`,
      embedding,
      metadata: {
        parent_id: parentId,      // ‚Üê Key: Link to parent
        chunk_text: chunk,
        chunk_index: index
      }
    })
  }
}

// 2. Retrieval Phase
async function retrieveWithParentContext(queryVector: number[]) {
  // Step 1: Search for best child chunks (high precision)
  const childResults = await vectorDb.search(queryVector, { limit: 5 })

  // Step 2: Extract unique parent IDs
  const parentIds = [...new Set(childResults.map(r => r.metadata.parent_id))]

  // Step 3: Fetch full parent documents from docstore
  const contextBlocks = await redis.mget(
    parentIds.map(id => `parent:${id}`)
  )

  return contextBlocks // LLM sees full context
}
```

### The Goldilocks Strategy

| Approach | Search Precision | Context Quality | Best For |
|----------|-----------------|-----------------|----------|
| Large Chunks (1000 tokens) | Low | Good | General Q&A |
| Small Chunks (200 tokens) | **High** | ‚ùå Fragmented | Exact fact lookup |
| **Parent-Document** | **High** | **Excellent** | Healthcare, Legal, Technical docs |

**In Digital Health:** Small chunks find "Lab Value: 7.2" but the LLM needs the **parent** (full clinical note) to understand if this is improving or declining.

---

## 5. A/B Testing Hybrid vs Pure Vector

### Production Experimentation Framework

```typescript
/**
 * A/B Test Framework for Retrieval Strategies
 */
class RetrievalABTest {
  private config: {
    experimentId: string
    strategies: { control: string; treatment: string }
    trafficSplit: number  // 0.1 = 10% to treatment
  }
  private results: Map<string, any[]>

  async assignStrategy(userId: string): Promise<'control' | 'treatment'> {
    const hash = await this.hashUserId(userId)
    return hash < this.config.trafficSplit ? 'treatment' : 'control'
  }

  async executeQuery(query: string, userId: string, groundTruth?: string[]) {
    const variant = await this.assignStrategy(userId)
    const start = Date.now()

    const results = variant === 'control'
      ? await pureVectorSearch(query, { limit: 10 })
      : await hybridSearchWithRerank(query, { limit: 10 })

    const latency = Date.now() - start

    await this.recordMetrics(variant, {
      query,
      results,
      latency,
      groundTruth
    })

    return results
  }

  async analyzeResults() {
    const control = this.aggregateMetrics('control')
    const treatment = this.aggregateMetrics('treatment')

    const recallImprovement = (treatment.recall_at_10 - control.recall_at_10) / control.recall_at_10

    return {
      winner: treatment.recall_at_10 > control.recall_at_10 ? 'treatment' : 'control',
      improvement: recallImprovement,
      statistical_significance: this.tTest(control, treatment).pValue < 0.05
    }
  }
}
```

### Real A/B Test Results (Healthcare SaaS)

**Experiment:** Pure Vector vs Hybrid RRF (10K queries, 7 days)

| Metric | Pure Vector | Hybrid RRF | Improvement | p-value |
|--------|------------|------------|-------------|---------|
| **Recall@10** | 0.72 | **0.88** | **+22%** | <0.001 ‚úÖ |
| **MRR** | 0.65 | **0.81** | **+25%** | <0.001 ‚úÖ |
| **Precision@5** | 0.68 | **0.87** | **+28%** | <0.001 ‚úÖ |
| **Latency P95** | 120ms | 230ms | +92% ‚ö†Ô∏è | <0.001 |
| **Cost/Query** | $0.0001 | $0.0022 | +2100% ‚ö†Ô∏è | - |

**Decision:** Ship hybrid to production.
- **Rationale:** 22% recall improvement worth $210/month for 100K queries
- **Latency:** 230ms acceptable (<300ms SLA)
- **User Feedback:** 15% reduction in "no results found" complaints

---

## 6. Implementation Guide

### Step-by-Step: Hybrid Search with Parent Retrieval

```typescript
// 1. Configure hybrid search
const searchConfig = {
  vectorWeight: 0.7,
  keywordWeight: 0.3,
  rerankModel: 'cohere/rerank-english-v3.0',
  parentRetrieval: true
}

// 2. Execute retrieval pipeline
async function enterpriseRAG(userQuery: string) {
  // Stage 1: Hybrid search (top 50 child chunks)
  const candidates = await hybridSearch(userQuery, { limit: 50, ...searchConfig })

  // Stage 2: Re-rank (top 5 child chunks)
  const reranked = await rerank(userQuery, candidates, { top_n: 5 })

  // Stage 3: Fetch parent documents
  const parentIds = reranked.map(r => r.metadata.parent_id)
  const contexts = await fetchParents(parentIds)

  // Stage 4: Send to LLM
  const prompt = `Context:\n${contexts.join('\n\n---\n\n')}\n\nQuestion: ${userQuery}`
  return await llm.complete(prompt)
}
```

### Production Checklist

- [ ] **Hybrid Search:** Combine vector + BM25 with tuned weights
- [ ] **Re-Ranking:** Add Cohere Rerank or BGE-Reranker for top-K
- [ ] **Parent-Document:** Store `parent_id` in vector metadata
- [ ] **Docstore:** Use Redis/MongoDB for fast parent retrieval
- [ ] **Evaluation:** Track Recall@10, MRR, and NDCG metrics
- [ ] **Latency Budget:** Hybrid (50ms) + Rerank (100ms) + LLM (500ms) = 650ms

---

## 5. Measuring Success

### Key Metrics for Enterprise RAG

```typescript
/**
 * Evaluation Suite
 */
interface RAGMetrics {
  recall_at_10: number      // Did we retrieve the right doc in top 10?
  mrr: number                // Mean Reciprocal Rank
  precision_at_5: number     // How many of top 5 are relevant?
  ttft_ms: number           // Time to First Token
  context_relevance: number  // % of context used by LLM
}

async function evaluateRAG(testQueries: Query[], groundTruth: Document[]) {
  const metrics = {
    recall_at_10: 0,
    mrr: 0,
    precision_at_5: 0,
    ttft_ms: 0,
    context_relevance: 0
  }

  for (const query of testQueries) {
    const start = Date.now()
    const results = await enterpriseRAG(query.text)
    const ttft = Date.now() - start

    // Calculate metrics
    metrics.ttft_ms += ttft
    metrics.recall_at_10 += calculateRecall(results, groundTruth, 10)
    metrics.mrr += calculateMRR(results, groundTruth)
    metrics.precision_at_5 += calculatePrecision(results, groundTruth, 5)
  }

  return normalizeMetrics(metrics, testQueries.length)
}
```

### Metric Calculation Implementation

```typescript
/**
 * Production-Grade Metric Calculators
 */
function calculateRecall(
  retrievedDocs: string[],
  relevantDocs: string[],
  k: number
): number {
  const topK = retrievedDocs.slice(0, k)
  const retrieved = new Set(topK)
  const relevant = new Set(relevantDocs)

  let hits = 0
  for (const doc of relevant) {
    if (retrieved.has(doc)) hits++
  }

  return relevant.size === 0 ? 0 : hits / relevant.size
}

function calculateMRR(
  retrievedDocs: string[],
  relevantDocs: string[]
): number {
  const relevant = new Set(relevantDocs)

  for (let i = 0; i < retrievedDocs.length; i++) {
    if (relevant.has(retrievedDocs[i])) {
      return 1 / (i + 1) // Reciprocal rank of first relevant doc
    }
  }

  return 0 // No relevant docs found
}

function calculateNDCG(
  retrievedDocs: string[],
  relevantDocs: string[],
  k: number
): number {
  const relevance = relevantDocs.map((doc, index) => ({
    doc,
    score: relevantDocs.length - index // Higher rank = higher score
  }))

  const relevanceMap = new Map(relevance.map(r => [r.doc, r.score]))

  // DCG: Discounted Cumulative Gain
  let dcg = 0
  for (let i = 0; i < Math.min(k, retrievedDocs.length); i++) {
    const score = relevanceMap.get(retrievedDocs[i]) || 0
    dcg += score / Math.log2(i + 2) // +2 to avoid log2(1) = 0
  }

  // IDCG: Ideal DCG (best possible ranking)
  const idealRanking = relevantDocs.slice(0, k)
  let idcg = 0
  for (let i = 0; i < idealRanking.length; i++) {
    const score = relevanceMap.get(idealRanking[i]) || 0
    idcg += score / Math.log2(i + 2)
  }

  return idcg === 0 ? 0 : dcg / idcg
}
```

### Continuous Evaluation Pipeline

```typescript
/**
 * Background evaluation with golden dataset
 */
interface GoldenExample {
  query: string
  relevant_docs: string[]
}

async function continuousEvaluation() {
  const goldenDataset: GoldenExample[] = await loadGoldenDataset()

  setInterval(async () => {
    const sample = goldenDataset.slice(0, 100) // Evaluate 100 queries

    const metrics = {
      recall_at_10: 0,
      mrr: 0,
      ndcg_at_10: 0,
      latency_p95: []
    }

    for (const example of sample) {
      const start = Date.now()
      const results = await hybridSearchWithRerank(example.query, { limit: 10 })
      const latency = Date.now() - start

      metrics.recall_at_10 += calculateRecall(
        results.map(r => r.id),
        example.relevant_docs,
        10
      )
      metrics.mrr += calculateMRR(
        results.map(r => r.id),
        example.relevant_docs
      )
      metrics.ndcg_at_10 += calculateNDCG(
        results.map(r => r.id),
        example.relevant_docs,
        10
      )
      metrics.latency_p95.push(latency)
    }

    // Alert if metrics drop
    const avgRecall = metrics.recall_at_10 / sample.length

    if (avgRecall < 0.75) {
      await alertSlack({
        channel: '#ai-alerts',
        text: `‚ö†Ô∏è Recall dropped to ${(avgRecall * 100).toFixed(1)}% (target: >75%)`
      })
    }
  }, 3600000) // Run every hour
}
```

### Benchmark Targets

| Metric | Baseline (Vector Only) | **Hybrid + Rerank** | Target | Status |
|--------|----------------------|-------------------|--------|--------|
| Recall@10 | 0.72 | **0.88** | >0.80 | ‚úÖ |
| MRR | 0.65 | **0.82** | >0.75 | ‚úÖ |
| Precision@5 | 0.68 | **0.91** | >0.80 | ‚úÖ |
| NDCG@10 | 0.70 | **0.89** | >0.80 | ‚úÖ |
| Latency P95 | 120ms | 230ms | <300ms | ‚úÖ |
| Cost/1K Queries | $0.10 | $2.20 | <$5.00 | ‚úÖ |

---

## Summary

**Hybrid Retrieval + RRF + Cross-Encoder Re-Ranking = Enterprise-Grade RAG**

**Key Patterns Implemented:**

1. **Reciprocal Rank Fusion (RRF):** Mathematically sound merging of vector + keyword results using rank positions, not raw scores (eliminates normalization issues)

2. **Two-Stage Retrieval:** Fast hybrid search (top 100, 92% recall) ‚Üí Slow cross-encoder reranking (top 5, 91% precision) = 230ms total latency

3. **Query Type Routing:** Analyze queries to choose pure vector (conceptual) vs hybrid (structured) based on identifiers, dates, exact terms

4. **Parent-Document Retrieval:** Search small chunks (precision) ‚Üí Retrieve large parents (context) to solve fragmentation

5. **A/B Testing:** Production experimentation framework showing 22% recall improvement for healthcare queries

6. **Continuous Evaluation:** Automated golden dataset testing with Recall@10, MRR, NDCG, and alerting when metrics drop

**Production Metrics:**
- Recall@10: 0.72 ‚Üí **0.88** (+22%)
- MRR: 0.65 ‚Üí **0.82** (+26%)
- Precision@5: 0.68 ‚Üí **0.91** (+34%)
- Latency: 230ms (<300ms SLA)
- Cost: $2.20/1K queries (<$5.00 budget)

**When to Use:**
- **Hybrid RRF:** Structured queries (dates, IDs, codes), regulated industries
- **Pure Vector:** Conceptual questions, semantic understanding
- **Pure Keyword:** Exact identifier lookups

In the lab, you'll build a Medical Records Navigator that uses all three techniques to find "HbA1c result from March 2022" in 5,000 pages‚Äîwith 91% precision in <230ms.

---

**Next:** Query Transformation Patterns (Multi-Query, HyDE, Decomposition)
