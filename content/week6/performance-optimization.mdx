---
title: "Performance Optimization & Caching"
description: "Optimize AI system performance with caching strategies and best practices"
estimatedMinutes: 40
---

# Performance Optimization & Caching

## Introduction

LLM API calls are slow (1-5 seconds) and expensive. Caching and optimization strategies can dramatically reduce costs and improve user experience.

## Why Caching Matters

**Without Caching:**
- User asks "What is prompt engineering?" → $0.01, 2 seconds
- Another user asks "What is prompt engineering?" → $0.01, 2 seconds
- 100 users = $1, 200 seconds of waiting

**With Caching:**
- First user: Cache miss → $0.01, 2 seconds
- Next 99 users: Cache hit → $0, instant
- 100 users = $0.01, 2 seconds total

## Caching Strategies

### 1. Semantic Caching

Cache based on semantic similarity, not exact matches:

```typescript
import { embed } from '@/lib/embeddings'
import { redis } from '@/lib/redis'

async function semanticCache(
  query: string,
  threshold: number = 0.95
): Promise<string | null> {
  // Get embedding of query
  const queryEmbedding = await embed(query)

  // Search for similar queries in cache
  const similar = await redis.ft.search(
    'idx:cache',
    `*=>[KNN 1 @embedding $blob AS score]`,
    {
      PARAMS: {
        blob: queryEmbedding
      },
      RETURN: ['content', 'score'],
      DIALECT: 2
    }
  )

  if (similar.documents.length > 0) {
    const doc = similar.documents[0]
    if (parseFloat(doc.value.score) >= threshold) {
      return doc.value.content as string
    }
  }

  return null
}

async function setCachedResponse(
  query: string,
  response: string,
  ttl: number = 3600
) {
  const embedding = await embed(query)

  await redis.hSet(`cache:${hashQuery(query)}`, {
    query,
    response,
    embedding: JSON.stringify(embedding),
    timestamp: Date.now()
  })

  await redis.expire(`cache:${hashQuery(query)}`, ttl)
}
```

### 2. Prompt Caching (Anthropic)

Anthropic supports prompt caching natively:

```typescript
const response = await anthropic.messages.create({
  model: "claude-3-sonnet-20240229",
  max_tokens: 1024,
  system: [
    {
      type: "text",
      text: longSystemPrompt,
      cache_control: { type: "ephemeral" } // Cache this!
    }
  ],
  messages: [
    { role: "user", content: userQuery }
  ]
})

// Subsequent requests with same system prompt:
// - 90% cost reduction for cached portion
// - Faster response time
```

### 3. Response Caching by Route

Cache entire responses for common queries:

```typescript
// app/api/chat/route.ts
import { redis } from '@/lib/redis'

export async function POST(req: Request) {
  const { message } = await req.json()

  // Generate cache key
  const cacheKey = `chat:${hashMessage(message)}`

  // Check cache
  const cached = await redis.get(cacheKey)
  if (cached) {
    return Response.json({
      response: cached,
      cached: true,
      cost: 0
    })
  }

  // Call LLM
  const response = await llm.complete(message)

  // Cache response (1 hour TTL)
  await redis.setex(cacheKey, 3600, response)

  return Response.json({
    response,
    cached: false,
    cost: calculateCost(usage)
  })
}
```

### 4. RAG Document Caching

Cache retrieved documents to avoid re-embedding:

```typescript
async function retrieveWithCache(
  query: string,
  topK: number = 5
) {
  const cacheKey = `rag:${hashQuery(query)}:${topK}`

  // Check cache
  const cached = await redis.get(cacheKey)
  if (cached) {
    return JSON.parse(cached)
  }

  // Perform retrieval
  const results = await vectorDB.search(query, topK)

  // Cache for 1 hour
  await redis.setex(cacheKey, 3600, JSON.stringify(results))

  return results
}
```

## Performance Optimization Techniques

### 1. Streaming Responses

Improve perceived performance with streaming:

```typescript
const stream = await anthropic.messages.stream({
  model: 'claude-3-sonnet-20240229',
  max_tokens: 1024,
  messages: [{ role: 'user', content: query }]
})

// Stream to client immediately
return new Response(
  new ReadableStream({
    async start(controller) {
      for await (const chunk of stream) {
        if (chunk.type === 'content_block_delta') {
          controller.enqueue(chunk.delta.text)
        }
      }
      controller.close()
    }
  }),
  {
    headers: {
      'Content-Type': 'text/event-stream',
      'Cache-Control': 'no-cache'
    }
  }
)
```

### 2. Parallel Processing

Run independent operations in parallel:

```typescript
// ❌ Sequential (slow)
const user = await getUser(userId)
const context = await retrieveContext(query)
const history = await getChatHistory(userId)
// Total: 300ms + 500ms + 200ms = 1000ms

// ✅ Parallel (fast)
const [user, context, history] = await Promise.all([
  getUser(userId),
  retrieveContext(query),
  getChatHistory(userId)
])
// Total: max(300, 500, 200) = 500ms
```

### 3. Request Deduplication

Prevent duplicate concurrent requests:

```typescript
const pendingRequests = new Map<string, Promise<any>>()

async function deduplicateRequest<T>(
  key: string,
  fn: () => Promise<T>
): Promise<T> {
  // Check if request is already in flight
  if (pendingRequests.has(key)) {
    return pendingRequests.get(key) as Promise<T>
  }

  // Execute and cache promise
  const promise = fn().finally(() => {
    pendingRequests.delete(key)
  })

  pendingRequests.set(key, promise)
  return promise
}

// Usage
const response = await deduplicateRequest(
  `llm:${userId}:${queryHash}`,
  () => llm.complete(query)
)
```

### 4. Compression

Compress large contexts before sending to LLM:

```typescript
function compressContext(docs: Document[]): string {
  // Remove duplicate information
  const unique = deduplicateContent(docs)

  // Summarize if too long
  if (totalTokens(unique) > 10000) {
    return summarize(unique, maxTokens: 5000)
  }

  return unique.map(d => d.content).join('\n\n')
}
```

## Cache Invalidation Strategies

### Time-Based (TTL)
```typescript
// Cache for 1 hour
await redis.setex(key, 3600, value)
```

### Event-Based
```typescript
// Invalidate when content updates
await prisma.document.update({
  where: { id },
  data: { content: newContent }
})

// Clear related caches
await redis.del(`rag:*:${documentId}`)
```

### Stale-While-Revalidate
```typescript
async function getWithStaleRevalidate(key: string, fn: () => Promise<any>) {
  const cached = await redis.get(key)
  const ttl = await redis.ttl(key)

  // If cached and not stale, return immediately
  if (cached && ttl > 300) {
    return JSON.parse(cached)
  }

  // If stale, return cached but revalidate in background
  if (cached && ttl > 0) {
    // Revalidate asynchronously
    fn().then(fresh => redis.setex(key, 3600, JSON.stringify(fresh)))
    return JSON.parse(cached)
  }

  // If no cache, fetch fresh
  const fresh = await fn()
  await redis.setex(key, 3600, JSON.stringify(fresh))
  return fresh
}
```

## Measuring Cache Effectiveness

```typescript
interface CacheMetrics {
  hits: number
  misses: number
  hitRate: number
  avgSavingsPerHit: number
  totalSavings: number
}

async function getCacheMetrics(period: '1h' | '24h' | '7d'): Promise<CacheMetrics> {
  const metrics = await prisma.cacheEvent.aggregate({
    where: {
      createdAt: {
        gte: getStartTime(period)
      }
    },
    _count: {
      _all: true
    },
    _sum: {
      savedCost: true
    }
  })

  const hits = await prisma.cacheEvent.count({
    where: {
      hit: true,
      createdAt: { gte: getStartTime(period) }
    }
  })

  const total = metrics._count._all

  return {
    hits,
    misses: total - hits,
    hitRate: hits / total,
    avgSavingsPerHit: metrics._sum.savedCost / hits,
    totalSavings: metrics._sum.savedCost
  }
}
```

## Model Routing: Complexity-Based Selection

### The Cost Problem

**Reality**: Not all queries need GPT-4o.

- Simple query: "What is the capital of France?" → Haiku ($0.0003)
- Complex query: "Analyze this 50-page legal contract for compliance issues" → GPT-4o ($0.150)

**Using GPT-4o for everything:** $15,000/month for 100K queries
**Using intelligent routing:** $2,500/month (83% savings!)

### The Router Pattern

```typescript
/**
 * Intelligent Model Router
 * Route to cheap model (Haiku) or expensive model (GPT-4) based on complexity
 */
interface QueryComplexity {
  score: number // 0-1
  factors: {
    wordCount: number
    hasCodeBlocks: boolean
    requiresReasoning: boolean
    domainSpecific: boolean
  }
  recommendedModel: 'haiku' | 'sonnet' | 'opus'
}

async function analyzeComplexity(query: string): Promise<QueryComplexity> {
  const wordCount = query.split(/\s+/).length
  const hasCodeBlocks = /```/.test(query)
  const requiresReasoning = /(analyze|compare|evaluate|explain why)/i.test(query)
  const domainSpecific = /(medical|legal|technical|financial)/i.test(query)

  // Complexity scoring
  let score = 0

  // Factor 1: Length (0-0.3)
  if (wordCount < 20) score += 0.1
  else if (wordCount < 50) score += 0.2
  else score += 0.3

  // Factor 2: Code blocks (0-0.2)
  if (hasCodeBlocks) score += 0.2

  // Factor 3: Reasoning required (0-0.3)
  if (requiresReasoning) score += 0.3

  // Factor 4: Domain-specific (0-0.2)
  if (domainSpecific) score += 0.2

  // Route based on score
  let recommendedModel: 'haiku' | 'sonnet' | 'opus'
  if (score < 0.3) recommendedModel = 'haiku'
  else if (score < 0.7) recommendedModel = 'sonnet'
  else recommendedModel = 'opus'

  return {
    score,
    factors: {
      wordCount,
      hasCodeBlocks,
      requiresReasoning,
      domainSpecific
    },
    recommendedModel
  }
}
```

### Production Router Implementation

```typescript
/**
 * Production Model Router with Fallback
 */
interface ModelConfig {
  model: string
  costPer1MTokens: number
  maxLatency: number
  capabilities: string[]
}

const MODEL_REGISTRY: Record<string, ModelConfig> = {
  haiku: {
    model: 'claude-3-5-haiku-20241022',
    costPer1MTokens: 0.80,
    maxLatency: 800,
    capabilities: ['fast', 'cheap', 'simple']
  },
  sonnet: {
    model: 'claude-3-5-sonnet-20241022',
    costPer1MTokens: 3.00,
    maxLatency: 1500,
    capabilities: ['balanced', 'reasoning', 'code']
  },
  opus: {
    model: 'claude-opus-4-5-20251101',
    costPer1MTokens: 15.00,
    maxLatency: 3000,
    capabilities: ['complex', 'research', 'analysis']
  }
}

async function routeToModel(query: string, options?: {
  maxCost?: number
  maxLatency?: number
}) {
  // Step 1: Analyze query complexity
  const complexity = await analyzeComplexity(query)

  // Step 2: Select model based on complexity and constraints
  let selectedModel = complexity.recommendedModel

  // Step 3: Apply cost/latency constraints
  const modelConfig = MODEL_REGISTRY[selectedModel]

  if (options?.maxCost && modelConfig.costPer1MTokens > options.maxCost) {
    // Downgrade to cheaper model
    selectedModel = 'haiku'
  }

  if (options?.maxLatency && modelConfig.maxLatency > options.maxLatency) {
    // Downgrade to faster model
    selectedModel = 'haiku'
  }

  // Step 4: Execute with selected model
  const response = await anthropic.messages.create({
    model: MODEL_REGISTRY[selectedModel].model,
    max_tokens: 1024,
    messages: [{ role: 'user', content: query }]
  })

  return {
    response: response.content[0].text,
    model_used: selectedModel,
    cost: calculateCost(response.usage, MODEL_REGISTRY[selectedModel].costPer1MTokens),
    latency_ms: response.latency
  }
}
```

### Real-World Routing Metrics

**Dataset:** 100K production queries over 7 days

| Model | Query % | Avg Cost/Query | Avg Latency | Use Case Examples |
|-------|---------|---------------|-------------|-------------------|
| **Haiku** | 65% | $0.0003 | 450ms | "What is X?", "List Y", Simple lookups |
| **Sonnet** | 30% | $0.0015 | 1200ms | "Explain Z", Code generation, Moderate reasoning |
| **Opus** | 5% | $0.0150 | 2500ms | Legal analysis, Research synthesis, Complex reasoning |

**Total Cost:**
- Without routing (all Opus): $1,500/month
- With intelligent routing: $250/month
- **Savings: $1,250/month (83%)**

### Advanced: LLM-as-Judge for Routing

```typescript
/**
 * Use small model to classify complexity for routing
 */
async function llmRoutingClassifier(query: string): Promise<'haiku' | 'sonnet' | 'opus'> {
  const classifierPrompt = `Classify query complexity for model routing:

Query: "${query}"

Return ONLY one word: haiku, sonnet, or opus

Rules:
- haiku: Simple factual questions, <20 words, no reasoning
- sonnet: Moderate complexity, code generation, explanations
- opus: Complex analysis, multi-step reasoning, research

Classification:`

  const response = await anthropic.messages.create({
    model: 'claude-3-5-haiku-20241022', // Use cheap model for classification
    max_tokens: 10,
    messages: [{ role: 'user', content: classifierPrompt }]
  })

  const classification = response.content[0].text.trim().toLowerCase()

  // Validate response
  if (['haiku', 'sonnet', 'opus'].includes(classification)) {
    return classification as 'haiku' | 'sonnet' | 'opus'
  }

  // Fallback to heuristic if LLM returns invalid response
  return (await analyzeComplexity(query)).recommendedModel
}

// Cost: $0.0003 per routing decision vs $0.015 for opus
// ROI: Pays for itself if it prevents 1 opus call per 50 queries
```

### Routing Decision Monitoring

```typescript
/**
 * Track routing decisions and model performance
 */
interface RoutingMetrics {
  model: 'haiku' | 'sonnet' | 'opus'
  queries_routed: number
  avg_cost_per_query: number
  avg_latency_ms: number
  user_satisfaction: number // 0-1 (from feedback)
  accuracy: number          // 0-1 (for queries with ground truth)
}

async function trackRoutingMetrics() {
  const metrics: Record<string, RoutingMetrics> = {
    haiku: {
      model: 'haiku',
      queries_routed: 0,
      avg_cost_per_query: 0,
      avg_latency_ms: 0,
      user_satisfaction: 0,
      accuracy: 0
    },
    sonnet: { /* ... */ },
    opus: { /* ... */ }
  }

  // Track each routed query
  async function recordRoutingDecision(
    query: string,
    model: 'haiku' | 'sonnet' | 'opus',
    response: any,
    userFeedback?: { satisfied: boolean }
  ) {
    metrics[model].queries_routed++
    metrics[model].avg_cost_per_query = (
      (metrics[model].avg_cost_per_query * (metrics[model].queries_routed - 1) +
        response.cost) / metrics[model].queries_routed
    )

    if (userFeedback) {
      const currentSat = metrics[model].user_satisfaction
      metrics[model].user_satisfaction = (
        (currentSat * (metrics[model].queries_routed - 1) +
          (userFeedback.satisfied ? 1 : 0)) / metrics[model].queries_routed
      )
    }

    // Alert if Haiku satisfaction drops below 80%
    if (model === 'haiku' && metrics[model].user_satisfaction < 0.8) {
      await alertSlack({
        channel: '#ai-alerts',
        text: `⚠️ Haiku routing satisfaction dropped to ${(metrics[model].user_satisfaction * 100).toFixed(1)}%`
      })
    }
  }

  return { metrics, recordRoutingDecision }
}
```

---

## Best Practices

1. **Cache Hot Paths** - Identify and cache most frequent queries
2. **Monitor Hit Rates** - Aim for >60% cache hit rate
3. **Use Appropriate TTLs** - Balance freshness vs. cost savings
4. **Invalidate Smartly** - Clear caches when data changes
5. **Measure Everything** - Track cost savings from caching
6. **Route Intelligently** - Use cheap models (Haiku) for 60-70% of queries
7. **Monitor Routing** - Track satisfaction by model to tune routing logic

## Exercise

Implement caching for your AI application:
1. Add Redis caching layer
2. Implement semantic caching for queries
3. Measure cache hit rate
4. Calculate cost savings

## Resources

- [Redis Caching Patterns](https://redis.io/docs/manual/patterns/)
- [Anthropic Prompt Caching](https://docs.anthropic.com/claude/docs/prompt-caching)
- [Helicone Caching](https://docs.helicone.ai/features/advanced-usage/caching)
