---
title: "Task Delegation & Orchestration: Resilient Execution Governance"
week: 11
concept: 2
description: "Build enterprise orchestration with Dynamic Replanning, DAG Execution Engines, Semantic Loop Detection, and Queue-Based Load Balancing for self-healing multi-agent systems"
estimatedMinutes: 70
objectives:
  - Build intelligent task routers with multi-factor agent scoring
  - Implement Dynamic Plan Re-evaluation for self-healing workflows
  - Detect semantic loops using vector similarity to prevent runaway token spend
  - Design DAG-based dependency engines with automatic tier parallelization
  - Deploy queue-based load balancers with fair-share tenant scheduling
---

# Task Delegation & Orchestration

## Why Task Delegation Matters

**Simple Explanation**: Imagine you're organizing a conference. You wouldn't do everything yourself - you'd delegate registration to one person, catering to another, and AV setup to a third. AI agent delegation works the same way: split complex work among specialized agents.

**The Challenge**: How do you know which agent should handle which task? What if a task needs multiple agents? What if they need to work in a specific order?

**The Solution**: Intelligent routing and orchestration systems that automatically assign tasks based on agent capabilities and manage execution flow.

## Core Concepts

### Task Analysis

**Simple Explanation**: Before delegating, you need to understand what the task requires. It's like reading a job description before hiring someone.

```typescript
interface TaskRequirements {
  type: string              // 'research', 'coding', 'writing', etc.
  complexity: 'low' | 'medium' | 'high'
  requiredCapabilities: string[]  // ['web_search', 'code_generation']
  estimatedTime: number     // seconds
  dependencies: string[]    // Other tasks that must complete first
  priority: number          // 1-10, higher = more urgent
}

async function analyzeTask(description: string): Promise<TaskRequirements> {
  const response = await anthropic.messages.create({
    model: 'claude-3-5-sonnet-20241022',
    max_tokens: 500,
    messages: [{
      role: 'user',
      content: `Analyze this task and extract requirements:

Task: "${description}"

Return JSON with:
{
  "type": "research|coding|writing|analysis|creative",
  "complexity": "low|medium|high",
  "requiredCapabilities": ["capability1", "capability2"],
  "estimatedTime": <seconds>,
  "dependencies": [],
  "priority": &lt;1-10>
}`
    }]
  })

  return JSON.parse(response.content[0].text)
}

// Example usage
const requirements = await analyzeTask(
  "Write a technical blog post about RAG systems with code examples"
)

console.log(requirements)
// {
//   type: "writing",
//   complexity: "high",
//   requiredCapabilities: ["research", "writing", "code_generation"],
//   estimatedTime: 300,
//   dependencies: [],
//   priority: 5
// }
```

**Technical Details**:
- **LLM-Based Analysis**: Use an LLM to understand task requirements
- **Capability Matching**: Compare task needs with agent capabilities
- **Complexity Scoring**: Determines if task needs decomposition
- **Cost**: ~$0.001 per task analysis (very cheap!)

---

### Enterprise Pattern: The Dynamic Plan Re-evaluator

**The Problem**: The `analyzeTask` function above produces a static execution plan. But in production, agents fail. Research times out. APIs return errors. An LLM produces a "High Uncertainty" flag on its output. A static plan treats these as terminal failures ‚Äî the entire workflow crashes because one subtask couldn't complete.

**Architect's Insight**: "A plan shouldn't be static. An Architect uses a 'Replanning Loop.' If the Research Agent fails to find information or returns a High Uncertainty flag, the system shouldn't just proceed to the Writer. It should trigger a Plan Mutation: 'Since research failed for Topic A, delegate an alternative search to Specialist B.' This ensures the orchestration is self-healing rather than just a linear sequence of failures."

```typescript
// src/week11/orchestration/dynamic-planner.ts

interface ExecutionState {
  taskId: string
  status: 'pending' | 'running' | 'completed' | 'failed' | 'replanned'
  result?: any
  error?: string
  confidence: number          // 0.0-1.0, how certain the agent is in its output
  attempts: number
  assignedAgent: string
}

interface PlanMutation {
  type: 'retry_same' | 'delegate_alternative' | 'decompose_further' | 'skip_with_fallback'
  originalTaskId: string
  reason: string
  newPlan?: Subtask[]          // Replacement subtasks if decomposing further
  alternativeAgent?: string    // Target agent if delegating
  timestamp: Date
}

/**
 * Dynamic Plan Re-evaluator
 *
 * After each agent completes (or fails), the planner evaluates whether
 * the execution plan is still valid. If not, it mutates the plan in-place
 * without restarting completed work.
 *
 * This is the difference between "orchestration" and "resilient governance."
 */
class DynamicPlanner {
  private states: Map<string, ExecutionState> = new Map()
  private mutations: PlanMutation[] = []
  private maxRetries = 2
  private confidenceThreshold = 0.6

  async evaluateAndReplan(
    completedTask: ExecutionState,
    remainingPlan: Subtask[],
    availableAgents: Agent[]
  ): Promise<{ updatedPlan: Subtask[]; mutation?: PlanMutation }> {
    this.states.set(completedTask.taskId, completedTask)

    // Case 1: Task succeeded with high confidence ‚Äî continue as planned
    if (completedTask.status === 'completed'
        && completedTask.confidence >= this.confidenceThreshold) {
      console.log(`‚úÖ Task ${completedTask.taskId} completed (confidence: ${completedTask.confidence})`)
      return { updatedPlan: remainingPlan }
    }

    // Case 2: Task failed ‚Äî trigger replanning
    if (completedTask.status === 'failed') {
      return this.handleFailure(completedTask, remainingPlan, availableAgents)
    }

    // Case 3: Low confidence ‚Äî agent is uncertain about its output
    if (completedTask.confidence < this.confidenceThreshold) {
      return this.handleLowConfidence(completedTask, remainingPlan, availableAgents)
    }

    return { updatedPlan: remainingPlan }
  }

  private async handleFailure(
    failedTask: ExecutionState,
    remainingPlan: Subtask[],
    availableAgents: Agent[]
  ): Promise<{ updatedPlan: Subtask[]; mutation: PlanMutation }> {
    // Strategy 1: Retry with same agent (transient errors)
    if (failedTask.attempts < this.maxRetries) {
      const mutation: PlanMutation = {
        type: 'retry_same',
        originalTaskId: failedTask.taskId,
        reason: `Transient failure (attempt ${failedTask.attempts}/${this.maxRetries}): ${failedTask.error}`,
        timestamp: new Date()
      }
      this.mutations.push(mutation)
      console.log(`üîÑ Retrying ${failedTask.taskId} (attempt ${failedTask.attempts + 1})`)
      return { updatedPlan: remainingPlan, mutation }
    }

    // Strategy 2: Delegate to alternative agent
    const alternativeAgent = availableAgents.find(
      a => a.id !== failedTask.assignedAgent
        && a.currentLoad < 80
    )

    if (alternativeAgent) {
      const mutation: PlanMutation = {
        type: 'delegate_alternative',
        originalTaskId: failedTask.taskId,
        reason: `Agent ${failedTask.assignedAgent} failed ${this.maxRetries} times. Delegating to ${alternativeAgent.id}`,
        alternativeAgent: alternativeAgent.id,
        timestamp: new Date()
      }
      this.mutations.push(mutation)
      console.log(`üîÄ Delegating ${failedTask.taskId} to ${alternativeAgent.role}`)
      return { updatedPlan: remainingPlan, mutation }
    }

    // Strategy 3: Skip with fallback content
    const mutation: PlanMutation = {
      type: 'skip_with_fallback',
      originalTaskId: failedTask.taskId,
      reason: 'All agents exhausted. Using fallback content for dependent tasks.',
      timestamp: new Date()
    }
    this.mutations.push(mutation)
    console.log(`‚ö†Ô∏è Skipping ${failedTask.taskId} with fallback`)
    return { updatedPlan: remainingPlan, mutation }
  }

  private async handleLowConfidence(
    uncertainTask: ExecutionState,
    remainingPlan: Subtask[],
    availableAgents: Agent[]
  ): Promise<{ updatedPlan: Subtask[]; mutation: PlanMutation }> {
    // Decompose the uncertain task into smaller, more specific subtasks
    const mutation: PlanMutation = {
      type: 'decompose_further',
      originalTaskId: uncertainTask.taskId,
      reason: `Confidence ${uncertainTask.confidence} below threshold ${this.confidenceThreshold}. Breaking into smaller tasks.`,
      newPlan: await this.decomposeUncertainTask(uncertainTask),
      timestamp: new Date()
    }
    this.mutations.push(mutation)

    // Insert new subtasks into the remaining plan
    const updatedPlan = [...mutation.newPlan!, ...remainingPlan]
    console.log(`üîç Decomposed uncertain task into ${mutation.newPlan!.length} subtasks`)
    return { updatedPlan, mutation }
  }

  private async decomposeUncertainTask(task: ExecutionState): Promise<Subtask[]> {
    // Use LLM to break the uncertain task into more specific queries
    const response = await anthropic.messages.create({
      model: 'claude-sonnet-4',
      max_tokens: 1000,
      messages: [{
        role: 'user',
        content: `This task produced uncertain results. Break it into 2-3 more specific subtasks:
Task: ${task.taskId}
Partial result: ${JSON.stringify(task.result)}
Generate focused subtasks that will produce higher-confidence results.`
      }]
    })
    return JSON.parse(response.content[0].text)
  }

  /** Audit trail of all plan mutations for debugging and compliance */
  getMutationLog(): PlanMutation[] {
    return [...this.mutations]
  }
}
```

| Strategy | Static Plan | Dynamic Replanning |
|----------|------------|-------------------|
| **Agent failure** | Entire workflow crashes | Retry ‚Üí Delegate ‚Üí Fallback |
| **Low confidence output** | Passes bad data downstream | Decomposes into focused subtasks |
| **Recovery time** | Full restart (minutes) | Selective retry (seconds) |
| **Token waste on failure** | 100% (all prior work lost) | ~5% (only failed subtask retried) |
| **Success rate** | ~75% (one failure kills all) | ~95% (self-healing) |

**Interview Defense Template**:

> **Interviewer:** "What happens when an agent fails mid-workflow in your orchestration system?"
>
> **You:** "We use a Dynamic Replanning Loop. After each agent completes, the planner evaluates the result's confidence score and status. If an agent fails, it follows a cascade: first retry the same agent for transient errors, then delegate to an alternative agent with available capacity, and finally skip with fallback content if all agents are exhausted. If an agent returns a low-confidence result, the planner decomposes that task into smaller, more specific subtasks. Every mutation is logged for audit. This means a single agent failure costs us seconds and a few cents in retry tokens, not a full workflow restart that burns $5 in duplicate processing."

---

### Operational Safeguard: Semantic Cycle Detection (The Token-Bleed Circuit Breaker)

**Architect's Tip ‚Äî Detecting "Thinking in Circles"**: "Don't just count iterations. A slow agent isn't the only risk ‚Äî a **Looping Agent** can stay under the timeout while draining your entire budget. It repeatedly fails a task and retries with the same faulty logic, generating slightly different but semantically identical outputs each time. Use **Vector Similarity** to compare an agent's current 'Plan' or 'Thought' with its previous three attempts. If the cosine similarity is &gt;0.98, the agent is stuck in a **semantic loop**. The orchestrator must trigger a **Hard-Stop** and escalate to a higher-reasoning 'Manager' model (like Claude Opus) to break the cycle by providing a new strategy."

```typescript
/**
 * Semantic Cycle Detection
 *
 * Problem: An agent tasked with "find the CEO's email for Acme Corp"
 * searches the web, gets no result, rephrases slightly, searches again,
 * gets no result, rephrases again... Each attempt costs $0.03 and takes
 * 8 seconds. After 20 attempts, you've spent $0.60 and 160 seconds
 * with zero progress. The agent stays under the per-request timeout,
 * so the orchestrator thinks it's "working."
 *
 * Solution: Embed each agent attempt's output/plan and compare against
 * the rolling window of previous attempts using cosine similarity.
 * If similarity exceeds the loop threshold (0.98), the agent is stuck.
 * Hard-stop the agent and escalate to a reasoning model that can
 * diagnose WHY the agent is stuck and provide a new approach.
 *
 * Interview Defense: "We embed every agent attempt and compare it
 * against the last 3 outputs using cosine similarity. If any pair
 * exceeds 0.98, we trigger a semantic loop circuit breaker ‚Äî the
 * agent is stopped immediately and the task is escalated to a
 * reasoning model that provides a fresh strategy. This prevents
 * runaway token spend from 'thinking in circles' agents."
 */

interface AgentAttempt {
  attemptNumber: number
  output: string
  embedding: number[]     // Vector embedding of the output
  tokenCost: number
  durationMs: number
  timestamp: Date
}

interface LoopDetectionResult {
  isLooping: boolean
  maxSimilarity: number       // Highest similarity between recent attempts
  similarPair: [number, number] | null  // Which attempts are most similar
  totalTokensWasted: number   // Tokens spent on redundant attempts
  recommendation: 'continue' | 'hard-stop' | 'escalate'
}

class SemanticCycleDetector {
  private attempts: Map<string, AgentAttempt[]> = new Map()
  private loopThreshold: number
  private windowSize: number       // How many recent attempts to compare
  private maxTokenBudget: number   // Hard budget cap per task

  constructor(
    loopThreshold: number = 0.98,
    windowSize: number = 3,
    maxTokenBudget: number = 5000
  ) {
    this.loopThreshold = loopThreshold
    this.windowSize = windowSize
    this.maxTokenBudget = maxTokenBudget
  }

  async recordAttempt(
    taskId: string,
    output: string,
    tokenCost: number,
    durationMs: number
  ): Promise<LoopDetectionResult> {
    // Get or create attempt history for this task
    if (!this.attempts.has(taskId)) {
      this.attempts.set(taskId, [])
    }
    const history = this.attempts.get(taskId)!

    // Embed the current output
    const embedding = await this.embedText(output)

    const attempt: AgentAttempt = {
      attemptNumber: history.length + 1,
      output,
      embedding,
      tokenCost,
      durationMs,
      timestamp: new Date()
    }
    history.push(attempt)

    // Not enough history to detect loops
    if (history.length < 2) {
      return {
        isLooping: false,
        maxSimilarity: 0,
        similarPair: null,
        totalTokensWasted: 0,
        recommendation: 'continue'
      }
    }

    // Compare current attempt against recent window
    const recentAttempts = history.slice(-this.windowSize)
    let maxSimilarity = 0
    let similarPair: [number, number] | null = null

    for (let i = 0; i < recentAttempts.length - 1; i++) {
      const similarity = this.cosineSimilarity(
        recentAttempts[i].embedding,
        attempt.embedding
      )

      if (similarity > maxSimilarity) {
        maxSimilarity = similarity
        similarPair = [recentAttempts[i].attemptNumber, attempt.attemptNumber]
      }
    }

    // Check total token spend
    const totalTokens = history.reduce((sum, a) => sum + a.tokenCost, 0)

    // Determine recommendation
    const isLooping = maxSimilarity >= this.loopThreshold
    const overBudget = totalTokens > this.maxTokenBudget

    let recommendation: 'continue' | 'hard-stop' | 'escalate'
    if (isLooping) {
      recommendation = 'escalate'  // Agent is stuck ‚Äî needs fresh strategy
    } else if (overBudget) {
      recommendation = 'hard-stop'  // Budget exhausted ‚Äî stop entirely
    } else {
      recommendation = 'continue'
    }

    if (isLooping) {
      console.log(
        `üîÑ SEMANTIC LOOP DETECTED on task ${taskId}!\n` +
        `   Attempts ${similarPair![0]} and ${similarPair![1]} ` +
        `are ${(maxSimilarity * 100).toFixed(1)}% similar\n` +
        `   Total tokens wasted: ${totalTokens}\n` +
        `   Action: ${recommendation.toUpperCase()}`
      )
    }

    return {
      isLooping,
      maxSimilarity,
      similarPair: isLooping ? similarPair : null,
      totalTokensWasted: isLooping ? totalTokens : 0,
      recommendation
    }
  }

  // Escalate to a reasoning model to break the loop
  async breakLoop(
    taskId: string,
    originalTask: string,
    failedAttempts: AgentAttempt[]
  ): Promise<{ newStrategy: string; reasoning: string }> {
    console.log(`üß† Escalating task ${taskId} to reasoning model...`)

    const attemptsText = failedAttempts.map(a =>
      `Attempt ${a.attemptNumber}: "${a.output.substring(0, 200)}..."`
    ).join('\n')

    const response = await anthropic.messages.create({
      model: 'claude-opus-4-6',  // High-reasoning model
      max_tokens: 800,
      messages: [{
        role: 'user',
        content: `An AI agent is stuck in a loop on this task. It keeps producing similar outputs without making progress.

TASK: ${originalTask}

FAILED ATTEMPTS (all semantically similar):
${attemptsText}

Diagnose:
1. WHY is the agent stuck? What pattern is it repeating?
2. What FUNDAMENTALLY DIFFERENT approach should it try?
3. What specific instruction would break the loop?

Respond in JSON:
{
  "diagnosis": "why the agent is stuck",
  "newStrategy": "specific new approach the agent should try",
  "reasoning": "why this new approach will work"
}`
      }]
    })

    const result = JSON.parse(response.content[0].text)

    console.log(`üí° New strategy: ${result.newStrategy.substring(0, 100)}...`)

    return {
      newStrategy: result.newStrategy,
      reasoning: result.reasoning
    }
  }

  private async embedText(text: string): Promise<number[]> {
    // Use OpenAI embeddings (or any embedding model)
    const response = await openai.embeddings.create({
      model: 'text-embedding-3-small',
      input: text.substring(0, 8000)  // Truncate to embedding limit
    })
    return response.data[0].embedding
  }

  private cosineSimilarity(a: number[], b: number[]): number {
    let dotProduct = 0
    let normA = 0
    let normB = 0

    for (let i = 0; i < a.length; i++) {
      dotProduct += a[i] * b[i]
      normA += a[i] * a[i]
      normB += b[i] * b[i]
    }

    return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB))
  }

  // Get diagnostic metrics for monitoring
  getMetrics(taskId: string): {
    attempts: number
    totalTokens: number
    totalDuration: number
    avgSimilarity: number
  } {
    const history = this.attempts.get(taskId) || []
    return {
      attempts: history.length,
      totalTokens: history.reduce((s, a) => s + a.tokenCost, 0),
      totalDuration: history.reduce((s, a) => s + a.durationMs, 0),
      avgSimilarity: 0  // Computed on demand
    }
  }
}

// Integration with Dynamic Planner:
//
// const cycleDetector = new SemanticCycleDetector(
//   0.98,    // Loop threshold: 98% cosine similarity
//   3,       // Compare against last 3 attempts
//   5000     // Hard budget: 5,000 tokens per task
// )
//
// async function executeWithLoopDetection(agent, task) {
//   for (let attempt = 1; attempt <= 10; attempt++) {
//     const output = await agent.execute(task)
//
//     const detection = await cycleDetector.recordAttempt(
//       task.id,
//       output.text,
//       output.tokenCost,
//       output.durationMs
//     )
//
//     if (detection.recommendation === 'continue') {
//       return output  // Success ‚Äî agent made progress
//     }
//
//     if (detection.recommendation === 'hard-stop') {
//       throw new Error(`Task ${task.id} exceeded token budget`)
//     }
//
//     if (detection.recommendation === 'escalate') {
//       // Agent is looping ‚Äî get a new strategy from reasoning model
//       const { newStrategy } = await cycleDetector.breakLoop(
//         task.id,
//         task.description,
//         cycleDetector.attempts.get(task.id)!
//       )
//
//       // Retry with the new strategy injected into the prompt
//       task.description = `${task.description}\n\nIMPORTANT: ${newStrategy}`
//       // Loop continues with fresh approach
//     }
//   }
// }
//
// Semantic Loop Detection Performance:
//
// | Metric                      | Without Detection | With Detection |
// |-----------------------------|-------------------|----------------|
// | Avg tokens wasted per loop  | 15,000+           | 2,500          |
// | Avg cost per stuck agent    | $1.20             | $0.20          |
// | Time before detection       | Timeout (60s+)    | 3 attempts (24s) |
// | Loop resolution rate        | 0% (just times out)| 85% (new strategy) |
// | Embedding cost per check    | N/A               | $0.0001        |
```

---

## Intelligent Router

**Simple Explanation**: A router decides which agent is best suited for a task, like a dispatcher assigning emergency calls to the right responders.

```typescript
interface Agent {
  id: string
  role: string
  capabilities: Set<string>
  currentLoad: number      // 0-100, how busy they are
  successRate: number      // 0-1, historical performance
  avgResponseTime: number  // milliseconds
}

class AgentRouter {
  private agents: Map<string, Agent>

  constructor(agents: Agent[]) {
    this.agents = new Map(agents.map(a => [a.id, a]))
  }

  async route(task: Task): Promise<Agent> {
    // Step 1: Analyze task requirements
    const requirements = await this.analyzeTask(task)
    console.log('Task requires:', requirements.requiredCapabilities)

    // Step 2: Filter agents by capability
    const capableAgents = this.filterByCapabilities(
      requirements.requiredCapabilities
    )
    console.log(`${capableAgents.length} capable agents found`)

    if (capableAgents.length === 0) {
      throw new Error(`No agent can handle: ${requirements.requiredCapabilities}`)
    }

    // Step 3: Score each agent
    const scoredAgents = capableAgents.map(agent => ({
      agent,
      score: this.scoreAgent(agent, requirements)
    }))

    // Step 4: Select best agent
    const best = scoredAgents.sort((a, b) => b.score - a.score)[0]
    console.log(`Selected: ${best.agent.role} (score: ${best.score})`)

    return best.agent
  }

  private filterByCapabilities(required: string[]): Agent[] {
    return Array.from(this.agents.values()).filter(agent =>
      required.every(cap => agent.capabilities.has(cap))
    )
  }

  private scoreAgent(agent: Agent, requirements: TaskRequirements): number {
    // Multi-factor scoring
    const capabilityScore = this.getCapabilityScore(agent, requirements)
    const loadScore = 1 - (agent.currentLoad / 100)  // Lower load = higher score
    const performanceScore = agent.successRate
    const speedScore = 1 / (agent.avgResponseTime / 1000)  // Faster = higher score

    // Weighted combination
    const weights = {
      capability: 0.4,
      load: 0.2,
      performance: 0.3,
      speed: 0.1
    }

    return (
      capabilityScore * weights.capability +
      loadScore * weights.load +
      performanceScore * weights.performance +
      speedScore * weights.speed
    )
  }

  private getCapabilityScore(agent: Agent, requirements: TaskRequirements): number {
    const required = new Set(requirements.requiredCapabilities)
    const matched = new Set(
      [...required].filter(cap => agent.capabilities.has(cap))
    )

    // Bonus for extra capabilities
    const extraCapabilities = agent.capabilities.size - matched.size
    const bonusScore = Math.min(extraCapabilities * 0.1, 0.3)

    return (matched.size / required.size) + bonusScore
  }

  private async analyzeTask(task: Task): Promise<TaskRequirements> {
    // Use LLM to understand requirements
    const response = await anthropic.messages.create({
      model: 'claude-3-5-sonnet-20241022',
      max_tokens: 500,
      messages: [{
        role: 'user',
        content: `Analyze task requirements:

Task: ${task.description}

Return JSON: {
  requiredCapabilities: string[],
  complexity: 'low' | 'medium' | 'high',
  estimatedTime: number
}`
      }]
    })

    return JSON.parse(response.content[0].text)
  }
}

// Example usage
const router = new AgentRouter([
  {
    id: 'researcher-1',
    role: 'researcher',
    capabilities: new Set(['web_search', 'document_analysis', 'summarization']),
    currentLoad: 30,
    successRate: 0.92,
    avgResponseTime: 15000
  },
  {
    id: 'coder-1',
    role: 'coder',
    capabilities: new Set(['code_generation', 'debugging', 'testing']),
    currentLoad: 70,
    successRate: 0.88,
    avgResponseTime: 25000
  },
  {
    id: 'researcher-2',
    role: 'researcher',
    capabilities: new Set(['web_search', 'document_analysis']),
    currentLoad: 10,
    successRate: 0.95,
    avgResponseTime: 12000
  }
])

const task = {
  description: 'Find recent papers about transformer architecture improvements'
}

const selectedAgent = await router.route(task)
// Likely selects researcher-2 (low load, high success rate, fast)
```

**Why Multi-Factor Scoring?**:
- **Capability Match**: Can they actually do the task?
- **Current Load**: Don't overwhelm busy agents
- **Success Rate**: Favor agents with good track record
- **Speed**: Faster agents provide better UX

## Task Decomposition

**Simple Explanation**: Breaking a big task into smaller subtasks that can be assigned to different agents. Like breaking "build a house" into "foundation", "framing", "electrical", etc.

```typescript
class TaskDecomposer {
  async decompose(complexTask: ComplexTask): Promise<Subtask[]> {
    const response = await anthropic.messages.create({
      model: 'claude-3-5-sonnet-20241022',
      max_tokens: 2000,
      messages: [{
        role: 'user',
        content: `Decompose this complex task into subtasks:

Task: ${complexTask.description}
Requirements: ${JSON.stringify(complexTask.requirements)}
Available agents: ${this.getAgentRoles().join(', ')}

Return JSON array of subtasks:
[
  {
    "description": "Clear description of subtask",
    "assignTo": "agent_role",
    "requiredCapabilities": ["capability1", "capability2"],
    "dependencies": [0, 1],  // Indices of subtasks that must complete first
    "estimatedTime": <seconds>,
    "priority": &lt;1-10>
  }
]

Rules:
1. Each subtask should be independently completable
2. Minimize dependencies to enable parallelization
3. Assign to most appropriate agent type
4. Order by logical flow`
      }]
    })

    const subtasks = JSON.parse(response.content[0].text)

    // Validate dependencies
    this.validateDependencies(subtasks)

    return subtasks
  }

  private validateDependencies(subtasks: Subtask[]): void {
    subtasks.forEach((task, index) => {
      task.dependencies.forEach(depIndex => {
        if (depIndex &gt;= index) {
          throw new Error(
            `Invalid dependency: Task ${index} depends on later task ${depIndex}`
          )
        }
        if (depIndex &lt; 0 || depIndex &gt;= subtasks.length) {
          throw new Error(`Invalid dependency index: ${depIndex}`)
        }
      })
    })

    // Check for circular dependencies
    const visited = new Set<number>()
    const checkCircular = (taskIndex: number, path: Set<number>) => {
      if (path.has(taskIndex)) {
        throw new Error('Circular dependency detected')
      }
      if (visited.has(taskIndex)) return

      visited.add(taskIndex)
      path.add(taskIndex)

      subtasks[taskIndex].dependencies.forEach(depIndex => {
        checkCircular(depIndex, new Set(path))
      })

      path.delete(taskIndex)
    }

    subtasks.forEach((_, index) => checkCircular(index, new Set()))
  }
}

// Example
const decomposer = new TaskDecomposer()

const complexTask = {
  description: 'Create a comprehensive technical blog post about RAG systems',
  requirements: [
    'research current RAG implementations',
    'provide code examples',
    'explain tradeoffs',
    'include performance metrics'
  ]
}

const subtasks = await decomposer.decompose(complexTask)

console.log(subtasks)
// [
//   {
//     description: "Research RAG architecture patterns and implementations",
//     assignTo: "researcher",
//     requiredCapabilities: ["web_search", "document_analysis"],
//     dependencies: [],
//     estimatedTime: 60,
//     priority: 10
//   },
//   {
//     description: "Write code examples for RAG pipeline",
//     assignTo: "coder",
//     requiredCapabilities: ["code_generation"],
//     dependencies: [0],  // Needs research first
//     estimatedTime: 120,
//     priority: 8
//   },
//   {
//     description: "Gather performance benchmarks",
//     assignTo: "researcher",
//     requiredCapabilities: ["data_analysis"],
//     dependencies: [0],  // Needs research first (can run parallel with task 1)
//     estimatedTime: 45,
//     priority: 7
//   },
//   {
//     description: "Write blog post draft",
//     assignTo: "writer",
//     requiredCapabilities: ["writing", "technical_communication"],
//     dependencies: [0, 1, 2],  // Needs all research and code
//     estimatedTime: 90,
//     priority: 9
//   },
//   {
//     description: "Review and edit",
//     assignTo: "editor",
//     requiredCapabilities: ["editing", "quality_assurance"],
//     dependencies: [3],  // Needs draft
//     estimatedTime: 30,
//     priority: 6
//   }
// ]
```

**Execution Strategy**:
```typescript
// Task 0 ‚Üí runs immediately (no dependencies)
// Tasks 1, 2 ‚Üí run in parallel after task 0 completes
// Task 3 ‚Üí runs after tasks 0, 1, 2 complete
// Task 4 ‚Üí runs after task 3 completes

// Total time: 60 + max(120, 45) + 90 + 30 = 300 seconds
// Sequential time: 60 + 120 + 45 + 90 + 30 = 345 seconds
// Speedup: 15% faster through parallelization
```

---

### Enterprise Pattern: Dependency-Aware DAG Engine

**The Problem**: The execution strategy above uses manual `Promise.all` for each batch. This works for 5 subtasks but breaks down at scale. With 20+ subtasks across a complex workflow, manually grouping batches is error-prone, doesn't handle dynamic dependency changes (from replanning), and can't visualize the execution graph for debugging.

**Architect's Insight**: "Don't manually manage Promise.all. Build a Dependency Graph. The orchestrator identifies which tasks have zero dependencies and fires them off in parallel. As tasks finish, it 'unlocks' the next tier of the graph. This maximizes throughput ‚Äî running Research and Coding simultaneously ‚Äî while ensuring the Synthesis stage never starts with empty inputs."

```typescript
// src/week11/orchestration/dag-engine.ts

interface DAGNode {
  id: string
  task: Subtask
  dependencies: Set<string>    // IDs of nodes that must complete first
  dependents: Set<string>      // IDs of nodes waiting on this one
  status: 'blocked' | 'ready' | 'running' | 'completed' | 'failed'
  result?: any
  tier?: number                // Execution tier (0 = no deps, 1 = depends on tier 0, etc.)
}

/**
 * DAG Executor: Directed Acyclic Graph engine for task orchestration
 *
 * Automatically resolves execution order from dependency declarations.
 * Fires all zero-dependency nodes in parallel, then unlocks downstream
 * nodes as their dependencies complete. Handles dynamic graph mutations
 * from the DynamicPlanner.
 */
class DAGExecutor {
  private nodes: Map<string, DAGNode> = new Map()
  private completedNodes: Set<string> = new Set()

  buildGraph(subtasks: Subtask[]): void {
    // Step 1: Create nodes
    for (const task of subtasks) {
      this.nodes.set(task.id, {
        id: task.id,
        task,
        dependencies: new Set(task.dependencies),
        dependents: new Set(),
        status: 'blocked'
      })
    }

    // Step 2: Wire up reverse dependencies (dependents)
    for (const node of this.nodes.values()) {
      for (const depId of node.dependencies) {
        const depNode = this.nodes.get(depId)
        if (!depNode) throw new Error(`Missing dependency: ${depId}`)
        depNode.dependents.add(node.id)
      }
    }

    // Step 3: Assign execution tiers via topological sort
    this.assignTiers()

    // Step 4: Mark tier-0 nodes as ready
    for (const node of this.nodes.values()) {
      if (node.dependencies.size === 0) {
        node.status = 'ready'
      }
    }
  }

  private assignTiers(): void {
    const inDegree = new Map<string, number>()
    const queue: string[] = []

    // Initialize in-degrees
    for (const [id, node] of this.nodes) {
      inDegree.set(id, node.dependencies.size)
      if (node.dependencies.size === 0) {
        queue.push(id)
        node.tier = 0
      }
    }

    // BFS topological sort
    while (queue.length > 0) {
      const currentId = queue.shift()!
      const currentNode = this.nodes.get(currentId)!

      for (const dependentId of currentNode.dependents) {
        const depNode = this.nodes.get(dependentId)!
        const remaining = (inDegree.get(dependentId) || 0) - 1
        inDegree.set(dependentId, remaining)

        if (remaining === 0) {
          depNode.tier = (currentNode.tier || 0) + 1
          depNode.status = 'blocked' // Will be unlocked during execution
          queue.push(dependentId)
        }
      }
    }

    // Detect cycles (nodes with non-zero in-degree after sort)
    for (const [id, degree] of inDegree) {
      if (degree > 0) {
        throw new Error(`Circular dependency detected involving task: ${id}`)
      }
    }
  }

  async execute(
    runner: (task: Subtask) => Promise<any>
  ): Promise<Map<string, any>> {
    const results = new Map<string, any>()

    while (this.completedNodes.size < this.nodes.size) {
      // Find all ready nodes
      const readyNodes = Array.from(this.nodes.values())
        .filter(n => n.status === 'ready')

      if (readyNodes.length === 0) {
        const blocked = Array.from(this.nodes.values())
          .filter(n => n.status === 'blocked')
        if (blocked.length > 0) {
          throw new Error('Deadlock: blocked nodes with no ready nodes')
        }
        break
      }

      const tierNum = readyNodes[0].tier || 0
      console.log(`‚ö° Executing Tier ${tierNum}: ${readyNodes.length} tasks in parallel`)

      // Execute all ready nodes in parallel
      const tierResults = await Promise.allSettled(
        readyNodes.map(async (node) => {
          node.status = 'running'
          console.log(`  ‚Üí Running: ${node.task.description}`)

          const result = await runner(node.task)

          node.status = 'completed'
          node.result = result
          this.completedNodes.add(node.id)
          results.set(node.id, result)

          // Unlock dependents
          for (const depId of node.dependents) {
            const depNode = this.nodes.get(depId)!
            depNode.dependencies.delete(node.id)
            if (depNode.dependencies.size === 0) {
              depNode.status = 'ready'
            }
          }

          return result
        })
      )

      // Handle failures
      for (let i = 0; i < tierResults.length; i++) {
        if (tierResults[i].status === 'rejected') {
          readyNodes[i].status = 'failed'
          console.log(`  ‚úó Failed: ${readyNodes[i].task.description}`)
        }
      }
    }

    return results
  }

  /** Visualize execution tiers for debugging */
  visualize(): string {
    const tiers = new Map<number, DAGNode[]>()
    for (const node of this.nodes.values()) {
      const tier = node.tier || 0
      if (!tiers.has(tier)) tiers.set(tier, [])
      tiers.get(tier)!.push(node)
    }

    let output = 'DAG Execution Plan:\n'
    for (const [tier, nodes] of Array.from(tiers.entries()).sort((a, b) => a[0] - b[0])) {
      output += `\n  Tier ${tier} (parallel):\n`
      for (const node of nodes) {
        const deps = node.task.dependencies.length > 0
          ? ` ‚Üê [${node.task.dependencies.join(', ')}]`
          : ' (no deps)'
        output += `    ${node.id}: ${node.task.description}${deps}\n`
      }
    }
    return output
  }
}

// Usage
const dag = new DAGExecutor()
dag.buildGraph(subtasks)
console.log(dag.visualize())
// DAG Execution Plan:
//   Tier 0 (parallel):
//     research: Research RAG patterns (no deps)
//   Tier 1 (parallel):
//     code: Write code examples ‚Üê [research]
//     benchmarks: Gather benchmarks ‚Üê [research]
//   Tier 2 (parallel):
//     draft: Write blog post ‚Üê [research, code, benchmarks]
//   Tier 3 (parallel):
//     review: Review and edit ‚Üê [draft]

const results = await dag.execute(async (task) => {
  const agent = await router.route(task)
  return agent.execute(task)
})
```

| Approach | 5 Tasks | 20 Tasks | 50 Tasks | Dynamic Replanning |
|----------|---------|----------|----------|--------------------|
| **Sequential** | 345s | 1,400s | 3,500s | Not supported |
| **Manual Promise.all** | 300s | 900s | 2,100s | Manual rewiring |
| **DAG Engine** | 300s | 650s | 1,200s | Auto-unlocks new nodes |

**Interview Defense Template**:

> **Interviewer:** "How do you handle parallel execution in a multi-agent workflow with complex dependencies?"
>
> **You:** "We use a DAG execution engine. Each subtask declares its dependencies, and the engine builds a directed acyclic graph. It assigns execution tiers via topological sort ‚Äî tier 0 has zero dependencies and fires immediately in parallel. As tier-0 tasks complete, they unlock tier-1 nodes, and so on. This maximizes concurrency automatically without manual Promise.all grouping. At 50 tasks, the DAG engine achieves 65% better throughput than sequential execution because it identifies all parallelizable paths. It also integrates with our Dynamic Planner ‚Äî when a replanning mutation injects new subtasks, the DAG just adds nodes and recalculates tiers."

---

## Hierarchical Delegation (Manager-Worker Pattern)

**Simple Explanation**: One manager agent coordinates multiple worker agents. The manager makes decisions, workers execute tasks.

```typescript
class ManagerAgent {
  private workers: Map<string, Agent>
  private router: AgentRouter
  private decomposer: TaskDecomposer

  constructor(workers: Agent[]) {
    this.workers = new Map(workers.map(w => [w.id, w]))
    this.router = new AgentRouter(workers)
    this.decomposer = new TaskDecomposer()
  }

  async execute(task: ComplexTask): Promise<Result> {
    console.log(`Manager: Received task "${task.description}"`)

    // Step 1: Decompose into subtasks
    console.log('Manager: Decomposing task...')
    const subtasks = await this.decomposer.decompose(task)
    console.log(`Manager: Created ${subtasks.length} subtasks`)

    // Step 2: Create execution plan
    const plan = this.createExecutionPlan(subtasks)
    console.log('Manager: Execution plan ready')
    console.log('  - Parallel batches:', plan.batches.length)
    console.log('  - Total estimated time:', plan.estimatedTime, 'seconds')

    // Step 3: Execute plan
    const results = await this.executePlan(plan)

    // Step 4: Synthesize results
    console.log('Manager: Synthesizing results...')
    const final = await this.synthesizeResults(results, task)

    // Step 5: Validate
    console.log('Manager: Validating output...')
    const validated = await this.validate(final, task.requirements)

    if (!validated.isValid) {
      console.log('Manager: Validation failed, retrying...')
      // Could implement retry logic here
      throw new Error(`Validation failed: ${validated.errors.join(', ')}`)
    }

    console.log('Manager: Task completed successfully')
    return final
  }

  private createExecutionPlan(subtasks: Subtask[]): ExecutionPlan {
    // Group subtasks into batches that can run in parallel
    const batches: Subtask[][] = []
    const completed = new Set<number>()

    while (completed.size < subtasks.length) {
      const batch = subtasks.filter((task, index) =>
        !completed.has(index) &&
        task.dependencies.every(dep => completed.has(dep))
      )

      if (batch.length === 0) {
        throw new Error('Circular dependencies detected')
      }

      batches.push(batch)
      batch.forEach((_, index) => {
        const originalIndex = subtasks.indexOf(batch[index])
        completed.add(originalIndex)
      })
    }

    // Estimate total time (sum of batch times, since batches run sequentially)
    const estimatedTime = batches.reduce((total, batch) => {
      const batchTime = Math.max(...batch.map(t => t.estimatedTime))
      return total + batchTime
    }, 0)

    return { batches, estimatedTime }
  }

  private async executePlan(plan: ExecutionPlan): Promise<Result[]> {
    const results: Result[] = []

    for (let i = 0; i < plan.batches.length; i++) {
      const batch = plan.batches[i]
      console.log(`Manager: Executing batch ${i + 1}/${plan.batches.length}`)
      console.log(`  - ${batch.length} tasks in parallel`)

      // Execute batch in parallel
      const batchResults = await Promise.all(
        batch.map(async (subtask) => {
          // Route to appropriate agent
          const agent = await this.router.route(subtask)
          console.log(`  - Assigned "${subtask.description}" to ${agent.role}`)

          // Execute
          const startTime = Date.now()
          const result = await agent.execute(subtask)
          const duration = Date.now() - startTime

          console.log(`  ‚úì Completed in ${duration}ms`)

          return result
        })
      )

      results.push(...batchResults)
    }

    return results
  }

  private async synthesizeResults(results: Result[], task: ComplexTask): Promise<Result> {
    // Combine all subtask results into final output
    const response = await anthropic.messages.create({
      model: 'claude-3-5-sonnet-20241022',
      max_tokens: 3000,
      messages: [{
        role: 'user',
        content: `Synthesize these results into a coherent final output:

Original task: ${task.description}
Requirements: ${JSON.stringify(task.requirements)}

Results from agents:
${results.map((r, i) => `
Result ${i + 1}:
${JSON.stringify(r, null, 2)}
`).join('\n')}

Create a final output that integrates all results.`
      }]
    })

    return {
      content: response.content[0].text,
      metadata: {
        subtasksCompleted: results.length,
        timestamp: new Date().toISOString()
      }
    }
  }

  private async validate(result: Result, requirements: string[]): Promise<ValidationResult> {
    // Check if result meets all requirements
    const response = await anthropic.messages.create({
      model: 'claude-3-5-sonnet-20241022',
      max_tokens: 500,
      messages: [{
        role: 'user',
        content: `Validate this result against requirements:

Requirements:
${requirements.map((r, i) => `${i + 1}. ${r}`).join('\n')}

Result:
${JSON.stringify(result)}

Return JSON:
{
  "isValid": boolean,
  "errors": string[],  // Empty if valid
  "score": 0-100       // Overall quality score
}`
      }]
    })

    return JSON.parse(response.content[0].text)
  }
}

// Example usage
const manager = new ManagerAgent([
  { id: 'r1', role: 'researcher', capabilities: new Set(['web_search']), currentLoad: 20, successRate: 0.9, avgResponseTime: 15000 },
  { id: 'c1', role: 'coder', capabilities: new Set(['code_generation']), currentLoad: 40, successRate: 0.85, avgResponseTime: 30000 },
  { id: 'w1', role: 'writer', capabilities: new Set(['writing']), currentLoad: 10, successRate: 0.95, avgResponseTime: 20000 },
])

const result = await manager.execute({
  description: 'Create technical documentation for our API',
  requirements: [
    'Include code examples',
    'Explain all endpoints',
    'Provide authentication guide',
    'Add troubleshooting section'
  ]
})

// Manager automatically:
// 1. Decomposes into subtasks
// 2. Routes each subtask to best agent
// 3. Executes in optimal order (parallel where possible)
// 4. Synthesizes results
// 5. Validates output
```

**Performance Benefits**:
- **Automatic Parallelization**: Manager identifies tasks that can run simultaneously
- **Optimal Routing**: Each subtask goes to the most appropriate agent
- **Quality Control**: Built-in validation ensures requirements are met
- **Failure Recovery**: Can retry failed subtasks without restarting entire workflow

---

### Enterprise Pattern: Queue-Based Agent Load Balancer

**The Problem**: The Manager-Worker pattern above pushes tasks directly to agents. In a single-user system, this works fine. In a multi-tenant platform with 50 concurrent users, this is a disaster. One "Power User" running a 50-page document generation monopolizes the Researcher agent, causing every other user's requests to timeout. Direct push has no concept of fairness, priority, or back-pressure.

**Architect's Insight**: "Never push tasks directly to an agent. Implement an Agent Task Queue. If your Code Reviewer agent is busy processing a massive file, new tasks should sit in a queue with a Priority Weight. This allows you to 'Fair-share' compute across multiple users, ensuring one Power User doesn't block the agents for everyone else on the platform."

```typescript
// src/week11/orchestration/load-balancer.ts

interface QueuedTask {
  id: string
  task: Subtask
  tenantId: string
  priority: number             // 1-10, higher = more urgent
  enqueuedAt: Date
  deadline?: Date              // Optional SLA deadline
}

interface AgentPool {
  agentId: string
  role: string
  maxConcurrency: number       // Max parallel tasks per agent
  activeCount: number
  queue: QueuedTask[]
}

/**
 * Agent Load Balancer: Queue-based task distribution with fair-share scheduling
 *
 * Each agent has a bounded task queue. Tasks are enqueued with tenant ID
 * and priority. The scheduler uses weighted fair-share to prevent any
 * single tenant from monopolizing agent capacity.
 */
class AgentLoadBalancer {
  private pools: Map<string, AgentPool> = new Map()
  private tenantUsage: Map<string, number> = new Map() // Tasks processed per tenant

  registerAgent(agentId: string, role: string, maxConcurrency: number): void {
    this.pools.set(agentId, {
      agentId,
      role,
      maxConcurrency,
      activeCount: 0,
      queue: []
    })
  }

  /**
   * Enqueue a task with fair-share priority adjustment
   * Power users get their priority reduced to prevent starvation
   */
  enqueue(task: Subtask, tenantId: string, basePriority: number): {
    queued: boolean
    position: number
    estimatedWait: number
  } {
    // Find eligible agent pools for this task's required capabilities
    const eligiblePools = Array.from(this.pools.values())
      .filter(pool => this.canHandle(pool, task))

    if (eligiblePools.length === 0) {
      throw new Error(`No agent pool can handle task: ${task.description}`)
    }

    // Select pool with shortest queue
    const bestPool = eligiblePools.sort(
      (a, b) => a.queue.length - b.queue.length
    )[0]

    // Apply fair-share priority adjustment
    const tenantTaskCount = this.tenantUsage.get(tenantId) || 0
    const fairSharePenalty = Math.min(tenantTaskCount * 0.5, 5) // Max -5 priority
    const adjustedPriority = Math.max(1, basePriority - fairSharePenalty)

    const queuedTask: QueuedTask = {
      id: `${tenantId}-${Date.now()}`,
      task,
      tenantId,
      priority: adjustedPriority,
      enqueuedAt: new Date()
    }

    // Insert in priority order (higher priority first)
    const insertIndex = bestPool.queue.findIndex(
      t => t.priority < adjustedPriority
    )
    if (insertIndex === -1) {
      bestPool.queue.push(queuedTask)
    } else {
      bestPool.queue.splice(insertIndex, 0, queuedTask)
    }

    // Track tenant usage for fair-share
    this.tenantUsage.set(tenantId, tenantTaskCount + 1)

    const estimatedWait = this.estimateWaitTime(bestPool, bestPool.queue.length - 1)

    return {
      queued: true,
      position: insertIndex === -1 ? bestPool.queue.length : insertIndex + 1,
      estimatedWait
    }
  }

  /**
   * Process next task from an agent's queue
   * Called when an agent becomes available
   */
  async dequeue(agentId: string): Promise<QueuedTask | null> {
    const pool = this.pools.get(agentId)
    if (!pool || pool.queue.length === 0) return null
    if (pool.activeCount >= pool.maxConcurrency) return null

    const next = pool.queue.shift()!
    pool.activeCount++

    return next
  }

  /** Called when an agent finishes a task */
  release(agentId: string): void {
    const pool = this.pools.get(agentId)
    if (pool && pool.activeCount > 0) {
      pool.activeCount--
    }
  }

  private canHandle(pool: AgentPool, task: Subtask): boolean {
    // Check if this pool's agent role matches task requirements
    return task.requiredCapabilities.some(cap =>
      pool.role.includes(cap) || cap.includes(pool.role)
    )
  }

  private estimateWaitTime(pool: AgentPool, position: number): number {
    const avgTaskDuration = 30000 // 30s average
    const slotsAvailable = pool.maxConcurrency - pool.activeCount
    const batchesAhead = Math.ceil(position / Math.max(slotsAvailable, 1))
    return batchesAhead * avgTaskDuration
  }

  /** Dashboard metrics for monitoring */
  getMetrics(): {
    pools: Array<{ agentId: string; queueDepth: number; activeCount: number; utilization: number }>
    tenantFairness: Array<{ tenantId: string; tasksProcessed: number; currentPriority: number }>
  } {
    return {
      pools: Array.from(this.pools.values()).map(pool => ({
        agentId: pool.agentId,
        queueDepth: pool.queue.length,
        activeCount: pool.activeCount,
        utilization: pool.activeCount / pool.maxConcurrency
      })),
      tenantFairness: Array.from(this.tenantUsage.entries()).map(([tenantId, count]) => ({
        tenantId,
        tasksProcessed: count,
        currentPriority: Math.max(1, 10 - Math.min(count * 0.5, 5))
      }))
    }
  }
}

// Production usage
const balancer = new AgentLoadBalancer()
balancer.registerAgent('researcher-1', 'researcher', 3)  // Max 3 concurrent
balancer.registerAgent('researcher-2', 'researcher', 3)
balancer.registerAgent('coder-1', 'coder', 2)            // Max 2 concurrent

// Tenant A (power user) submits 10 tasks
for (let i = 0; i < 10; i++) {
  const result = balancer.enqueue(researchTask, 'tenant-A', 8)
  console.log(`Tenant A task ${i}: position ${result.position}, wait ${result.estimatedWait}ms`)
}
// Tenant A's later tasks get lower priority (fair-share penalty)

// Tenant B submits 1 task ‚Äî gets served ahead of Tenant A's backlog
const urgentResult = balancer.enqueue(researchTask, 'tenant-B', 8)
console.log(`Tenant B: position ${urgentResult.position}`) // Position 1-3 (ahead of penalized tasks)
```

| Metric | Direct Push | Queue-Based Load Balancer |
|--------|-------------|--------------------------|
| **Multi-tenant fairness** | None (first-come, first-served) | Weighted fair-share per tenant |
| **Power user blocking** | Blocks all other users | Penalized priority after threshold |
| **Agent utilization** | Uneven (best agent overloaded) | Balanced across pool |
| **P99 latency (light users)** | 60s+ (stuck behind power user) | &lt; 5s (fair-share priority) |
| **Observability** | None | Queue depth, utilization, per-tenant metrics |
| **Back-pressure** | Agent crash on overload | Bounded queues with wait estimates |

**Interview Defense Template**:

> **Interviewer:** "How do you prevent one tenant from monopolizing your multi-agent system?"
>
> **You:** "We use a Queue-Based Agent Load Balancer with fair-share scheduling. Instead of pushing tasks directly to agents, every task goes into a priority queue associated with the target agent pool. The scheduler applies a fair-share penalty ‚Äî the more tasks a tenant has already processed in the current window, the lower their effective priority for new tasks. This means a power user submitting 50 research tasks won't block a casual user who just submitted one. Each agent pool has a bounded concurrency limit and back-pressure mechanism. We expose queue depth, utilization, and per-tenant metrics on our ops dashboard so we can spot imbalances in real-time."

---

## Key Takeaways

1. **Task Analysis First**: Use LLM-based analysis to understand requirements before routing
2. **Multi-Factor Routing**: Score agents on capability, load, performance, and speed ‚Äî not just capability alone
3. **Dependency Validation**: Always check for circular dependencies before executing a decomposed plan
4. **Manager-Worker Pattern**: One coordinator, many specialists ‚Äî decompose, route, execute, synthesize, validate
5. **Start Simple**: Don't over-engineer delegation for simple tasks; 3-7 meaningful subtasks is the sweet spot
6. **Monitor Agent Load**: Distribute work evenly to prevent bottlenecks
7. **Dynamic Replanning**: Plans must be mutable ‚Äî agent failures trigger selective retry, delegation to alternatives, or task decomposition, not full restarts
8. **Semantic Loop Detection**: Embed agent outputs and compare cosine similarity across attempts ‚Äî similarity &gt;0.98 means the agent is "thinking in circles" and must be escalated to a reasoning model with a fresh strategy
9. **DAG Execution**: Build dependency graphs with topological sort to maximize concurrent throughput automatically, rather than manually grouping Promise.all batches
10. **Queue-Based Load Balancing**: Fair-share scheduling with priority penalties prevents power users from monopolizing multi-tenant agent pools
11. **Granular Resilience**: Design for selective subtask recovery ‚Äî one agent failure should cost seconds and cents, not minutes and dollars

---

## üéØ Architect Challenge: The "Orchestration Crisis" COO Triage

**Scenario**: Your multi-agent system is writing a 50-page technical manual for a Fortune 500 client. The workflow has 12 subtasks across 4 agents: Researcher, Coder, Image Generator, and Writer. After 3 minutes of execution, the Researcher agent times out on subtask #4 ("Gather competitive analysis data") after 60 seconds. The Coder has completed 2 subtasks. The Image Generator is halfway through generating diagrams. The Writer is idle, waiting for research to complete. The client's deadline is in 30 minutes. Your total spend so far is $2.50 in tokens.

**What is your Architectural recovery strategy?**

**A)** Restart the entire 50-page workflow from scratch. Re-run all 12 subtasks to ensure consistency.

**B)** Implement Partial State Resumption. Because you have a Task Orchestrator with a DAG engine, you can see that the Researcher was the only failure. You trigger a Selective Retry only for subtask #4 ‚Äî first retrying the same Researcher, then delegating to Researcher-2 if it fails again. Meanwhile, the Coder and Image Generator continue their work uninterrupted. The Writer remains blocked only on the research subtask, not on the entire workflow. This prevents a single agent failure from burning $2.50 in duplicate tokens and saves 3 minutes of already-completed processing.

**C)** Tell the user the system is broken and ask them to try again later.

**D)** Increase the Researcher agent's timeout from 60 seconds to 5 minutes and hope it completes.

<details>
<summary>Correct Answer</summary>

**B ‚Äî Partial State Resumption (Granular Resilience)**

An Architect designs **Granular Resilience** to protect both the budget and the user's time. Here's why:

**Why B is correct**: The DAG execution engine tracks the status of every node independently. The Researcher's failure on subtask #4 is isolated ‚Äî it doesn't invalidate the Coder's completed work or the Image Generator's in-progress work. The Dynamic Planner follows its cascade: retry the same agent (transient network timeout), then delegate to an alternative Researcher with available capacity. The Writer's dependency on subtask #4 is the only blocked path. Total recovery cost: ~$0.10 in retry tokens and ~30 seconds of additional time.

```typescript
// What Granular Resilience looks like in production
const recoveryResult = await dynamicPlanner.evaluateAndReplan(
  {
    taskId: 'subtask-4',
    status: 'failed',
    error: 'Timeout after 60000ms',
    confidence: 0,
    attempts: 1,
    assignedAgent: 'researcher-1'
  },
  remainingPlan,
  availableAgents
)

// Result: { mutation: { type: 'retry_same', ... } }
// If retry fails: { mutation: { type: 'delegate_alternative', alternativeAgent: 'researcher-2' } }
// Coder and Image Generator never stopped working
```

**Why the other answers are wrong**:

- **A (Full Restart)**: This is the most expensive possible response. You've already spent $2.50 and 3 minutes of processing. Restarting burns another $2.50 and 3 minutes ‚Äî for a total of $5.00 and 6 minutes wasted. The Coder's completed subtasks are perfectly valid and don't need re-execution. This is the hallmark of a system with no state management.

- **C (Tell User It's Broken)**: This is the non-architect response. The system isn't broken ‚Äî one subtask out of twelve timed out. Exposing internal failures to users as "the system is broken" destroys trust and makes the platform look unreliable. An architect's system handles partial failures gracefully without user-visible disruption.

- **D (Increase Timeout)**: This treats the symptom, not the cause. If the Researcher timed out at 60 seconds, waiting 5 minutes doesn't guarantee success ‚Äî the external API it's calling might be down entirely. Meanwhile, every other task is blocked for 5 minutes instead of 60 seconds, making the overall failure *worse*. Timeouts exist to fail fast and trigger recovery, not to delay the inevitable.

**The Architect's Principle**: In a well-designed orchestration system, the blast radius of any single agent failure is exactly one subtask. The DAG engine isolates failures. The Dynamic Planner triggers recovery. The Load Balancer finds alternative capacity. Together, they transform a catastrophic workflow failure into a 30-second retry.

</details>

## Resources
- [AutoGPT Architecture](https://github.com/Significant-Gravitas/AutoGPT)
- [Task Decomposition Strategies](https://arxiv.org/abs/2305.14930)
- [LangChain AgentExecutor](https://python.langchain.com/docs/modules/agents/)
