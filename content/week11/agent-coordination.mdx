---
title: "Multi-Agent Coordination Patterns"
description: "Coordinate multiple AI agents effectively with idempotent hand-offs, atomic state checkpointing, and resilient pipeline execution"
estimatedMinutes: 50
objectives:
  - Design sequential, parallel, and hierarchical multi-agent workflows
  - Implement idempotent agent hand-offs with persistent state checkpointing
  - Choose between Supervisor and Choreography orchestration models
  - Build Egress Hand-offs to reduce inter-agent context bloat by 87%
  - Validate workflow DAGs to prevent circular dependency deadlocks
---

# Multi-Agent Coordination Patterns

## Why Multi-Agent Systems?

**Simple Explanation**: Just like a company needs different specialists (marketing, engineering, sales), complex AI tasks often need different "specialist" agents working together. A single AI agent trying to do everything is like asking one person to run an entire company.

**When You Need Multiple Agents**:
1. **Task Complexity**: The task requires multiple distinct skills (research + writing + coding)
2. **Parallel Processing**: Different parts can be done simultaneously to save time
3. **Specialization**: Each agent optimized for specific capabilities
4. **Reliability**: If one agent fails, others can continue or compensate

**Real Example**: Building a blog post about a technical topic
- **Researcher Agent**: Finds latest information and sources
- **Writer Agent**: Creates engaging content from research
- **Fact Checker Agent**: Verifies claims and statistics
- **SEO Agent**: Optimizes for search engines

## Core Concepts

### Agent Roles & Capabilities

**Simple Explanation**: Think of agents as specialists with specific skills. A researcher agent knows how to search and analyze, while a writer agent knows how to create compelling content.

```typescript
interface Agent {
  role: string          // What the agent does
  capabilities: string[] // Skills/tools it can use
  execute: (task: Task) => Promise<Result>
}

// Example: Research specialist
const researcher: Agent = {
  role: 'researcher',
  capabilities: ['web_search', 'document_analysis', 'data_extraction'],
  execute: async (task) => {
    // Specialized research logic
    const sources = await webSearch(task.query)
    const analysis = await analyzeDocuments(sources)
    return {
      findings: analysis,
      sources: sources,
      confidence: 0.85
    }
  }
}

// Example: Writer specialist
const writer: Agent = {
  role: 'writer',
  capabilities: ['content_generation', 'summarization', 'editing'],
  execute: async (task) => {
    const prompt = `Write a ${task.style} article about:
    ${task.topic}

    Using this research:
    ${task.context}

    Target audience: ${task.audience}`

    const response = await anthropic.messages.create({
      model: 'claude-3-5-sonnet-20241022',
      max_tokens: 2000,
      messages: [{ role: 'user', content: prompt }]
    })

    return {
      content: response.content[0].text,
      wordCount: response.content[0].text.split(' ').length
    }
  }
}

// Example: Coder specialist
const coder: Agent = {
  role: 'coder',
  capabilities: ['code_generation', 'debugging', 'testing'],
  execute: async (task) => {
    const code = await generateCode(task.requirements)
    const tests = await generateTests(code)
    const bugs = await analyzeCode(code)

    return {
      code,
      tests,
      issues: bugs,
      tested: bugs.length === 0
    }
  }
}
```

**Technical Details**:
- **Role Definition**: Clear responsibilities prevent overlap
- **Capability Registry**: Allows dynamic agent selection based on task requirements
- **Stateless vs Stateful**: Stateless agents are easier to scale, stateful remember context
- **Cost**: Each agent call costs separately - plan carefully!

**Architect's Tip ‚Äî Supervisor vs. Choreography (The Orchestration Decision)**: "Every multi-agent system faces the same architectural fork: **Supervisor** (Hub-and-Spoke) or **Choreography** (Event-Driven). A Supervisor has one coordinator that dispatches tasks and collects results ‚Äî it's easy to reason about but creates a single point of failure and a bottleneck. Choreography has agents react to events from each other ‚Äî it scales better but is harder to debug because there's no central log of 'who did what.' An Architect picks Supervisor for **transactional workflows** where you need rollback capability (refund processing, compliance audits), and Choreography for **streaming pipelines** where throughput matters more than consistency (content generation, monitoring)."

```typescript
/**
 * Supervisor vs. Choreography: The Orchestration Decision
 *
 * SUPERVISOR (Hub-and-Spoke):
 * ‚úÖ Central control ‚Äî easy to debug, audit, and rollback
 * ‚úÖ Single orchestrator knows full workflow state
 * ‚úÖ Natural fit for transactional workflows
 * ‚ùå Single point of failure (supervisor crash = full stop)
 * ‚ùå Bottleneck at high concurrency (all agents report to one node)
 *
 * CHOREOGRAPHY (Event-Driven):
 * ‚úÖ No single point of failure ‚Äî agents react independently
 * ‚úÖ Scales horizontally (add agents without changing coordinator)
 * ‚úÖ Natural fit for streaming/pipeline workloads
 * ‚ùå Hard to debug (no central "who did what" log)
 * ‚ùå Circular event loops can emerge silently
 *
 * Interview Defense: "We use Supervisor for compliance-sensitive
 * workflows where we need audit trails and rollback. We use
 * Choreography for content pipelines where throughput matters
 * more than transactional guarantees."
 */

type OrchestrationModel = 'supervisor' | 'choreography'

interface OrchestrationDecision {
  workflow: string
  needsRollback: boolean
  needsAuditTrail: boolean
  agentCount: number
  throughputCritical: boolean
  concurrentUsers: number
}

function selectOrchestrationModel(
  config: OrchestrationDecision
): { model: OrchestrationModel; reasoning: string } {
  // Rule 1: Transactional workflows always use Supervisor
  if (config.needsRollback || config.needsAuditTrail) {
    return {
      model: 'supervisor',
      reasoning: `Workflow "${config.workflow}" requires ${
        config.needsRollback ? 'rollback' : 'audit trail'
      } ‚Äî Supervisor provides central state tracking`
    }
  }

  // Rule 2: High concurrency + throughput ‚Üí Choreography
  if (config.throughputCritical && config.concurrentUsers > 100) {
    return {
      model: 'choreography',
      reasoning: `${config.concurrentUsers} concurrent users with throughput priority ‚Äî Choreography avoids supervisor bottleneck`
    }
  }

  // Rule 3: Small agent count ‚Üí Supervisor (simpler)
  if (config.agentCount <= 4) {
    return {
      model: 'supervisor',
      reasoning: `Only ${config.agentCount} agents ‚Äî Supervisor overhead is minimal, debugging is easier`
    }
  }

  // Default: Choreography for large, non-transactional systems
  return {
    model: 'choreography',
    reasoning: `${config.agentCount} agents without transactional requirements ‚Äî Choreography scales better`
  }
}

// Decision matrix:
//
// | Workflow              | Rollback? | Audit? | Agents | Model         |
// |-----------------------|-----------|--------|--------|---------------|
// | Refund processing     | Yes       | Yes    | 3      | Supervisor    |
// | Content generation    | No        | No     | 6      | Choreography  |
// | Compliance audit      | No        | Yes    | 4      | Supervisor    |
// | Real-time monitoring  | No        | No     | 8      | Choreography  |
// | Invoice approval      | Yes       | Yes    | 3      | Supervisor    |
```

## Orchestration Patterns

### Pattern 1: Sequential Workflow (Pipeline)

**Simple Explanation**: Agents work one after another, like an assembly line. The output of one agent becomes the input for the next.

**When to Use**:
- Output of one agent is needed as input for the next
- Tasks have dependencies (can't write before researching)
- Need to maintain quality through staged review

```typescript
async function sequentialWorkflow(topic: string) {
  console.log('Starting sequential workflow for:', topic)

  // Stage 1: Research (0-30 seconds)
  console.log('Stage 1: Research...')
  const research = await agents.researcher.execute({
    type: 'research',
    query: topic,
    depth: 'comprehensive'
  })
  console.log(`‚úì Found ${research.sources.length} sources`)

  // Stage 2: Outline (5-10 seconds)
  console.log('Stage 2: Outline...')
  const outline = await agents.writer.execute({
    type: 'outline',
    context: research.findings,
    format: 'blog_post'
  })
  console.log(`‚úì Created ${outline.sections.length} sections`)

  // Stage 3: Write (20-40 seconds)
  console.log('Stage 3: Writing...')
  const draft = await agents.writer.execute({
    type: 'write',
    outline: outline,
    context: research.findings,
    style: 'professional'
  })
  console.log(`‚úì Wrote ${draft.wordCount} words`)

  // Stage 4: Review (10-15 seconds)
  console.log('Stage 4: Review...')
  const review = await agents.reviewer.execute({
    type: 'review',
    content: draft.content,
    criteria: ['accuracy', 'clarity', 'engagement']
  })

  // Stage 5: Revise if needed (15-30 seconds)
  if (review.needsRevision) {
    console.log('Stage 5: Revising...')
    const final = await agents.writer.execute({
      type: 'revise',
      content: draft.content,
      feedback: review.suggestions
    })
    return final
  }

  return draft
}

// Usage
const blogPost = await sequentialWorkflow('AI Agent Coordination Patterns')
// Total time: 50-125 seconds
// Total cost: ~$0.15-0.30 (depending on content length)
```

**Performance Metrics**:
- **Total Time**: 50-125 seconds (sum of all stages)
- **Parallelization**: None (must be sequential)
- **Cost**: 5 agent calls √ó $0.03-0.06 = $0.15-0.30 per workflow
- **Quality**: Highest (each stage reviews previous)

### Pattern 2: Parallel Workflow (Fan-Out/Fan-In)

**Simple Explanation**: Multiple agents work at the same time on different parts, then combine results. Like having multiple researchers each investigating different aspects simultaneously.

**When to Use**:
- Tasks can be split into independent parts
- Speed is critical (want to minimize total time)
- Different perspectives improve quality

```typescript
async function parallelWorkflow(task: string) {
  console.log('Starting parallel workflow for:', task)

  // Fan-Out: Multiple agents work simultaneously
  const startTime = Date.now()

  const [research, examples, references, competitors] = await Promise.all([
    // Agent 1: General research (30s)
    agents.researcher.execute({
      type: 'research',
      query: task,
      focus: 'general_overview'
    }),

    // Agent 2: Code examples (25s)
    agents.coder.execute({
      type: 'examples',
      query: task,
      language: 'typescript'
    }),

    // Agent 3: Academic references (35s)
    agents.librarian.execute({
      type: 'references',
      query: task,
      sources: ['arxiv', 'papers_with_code']
    }),

    // Agent 4: Competitor analysis (40s)
    agents.analyst.execute({
      type: 'competitor_analysis',
      topic: task
    })
  ])

  const fanOutTime = Date.now() - startTime
  console.log(`‚úì Parallel execution completed in ${fanOutTime}ms`)

  // Fan-In: Coordinator synthesizes all results
  const synthesis = await agents.coordinator.execute({
    type: 'synthesize',
    inputs: {
      research: research.findings,
      codeExamples: examples.code,
      academicSources: references.papers,
      marketAnalysis: competitors.insights
    }
  })

  const totalTime = Date.now() - startTime
  console.log(`‚úì Total workflow: ${totalTime}ms`)

  return {
    content: synthesis,
    metadata: {
      parallelTime: fanOutTime,
      totalTime: totalTime,
      speedup: '3.5x compared to sequential'
    }
  }
}

// Performance comparison
// Sequential: 30 + 25 + 35 + 40 = 130 seconds
// Parallel: max(30, 25, 35, 40) = 40 seconds + 5s synthesis = 45 seconds
// Speedup: 2.9x faster!
```

**Performance Metrics**:
- **Total Time**: 45 seconds (longest agent + synthesis)
- **Speedup**: 2-4x compared to sequential
- **Cost**: Same as sequential (same number of calls)
- **Tradeoff**: Less quality control between stages

### Pattern 3: Hierarchical Coordination (Manager-Worker)

**Simple Explanation**: One "manager" agent breaks down complex tasks and delegates to "worker" agents, then combines their results. Like a project manager coordinating a team.

```typescript
class ManagerAgent {
  private workers: Map<string, Agent>

  constructor(workers: Agent[]) {
    this.workers = new Map(workers.map(w => [w.role, w]))
  }

  async delegateTask(complexTask: ComplexTask) {
    // Step 1: Analyze and decompose the task
    const decomposition = await this.decomposeTask(complexTask)
    console.log(`Decomposed into ${decomposition.subtasks.length} subtasks`)

    // Step 2: Create execution plan
    const plan = this.createExecutionPlan(decomposition)
    console.log(`Execution plan:`, plan)

    // Step 3: Assign subtasks to appropriate workers
    const assignments = plan.subtasks.map(subtask => ({
      subtask,
      agent: this.selectBestAgent(subtask.requirements)
    }))

    // Step 4: Execute with proper coordination
    const results = []
    for (const assignment of assignments) {
      if (assignment.subtask.dependencies.length === 0) {
        // No dependencies, can run immediately
        results.push(assignment.agent.execute(assignment.subtask))
      } else {
        // Has dependencies, wait for them
        const dependencyResults = results.filter((_, i) =>
          assignment.subtask.dependencies.includes(i)
        )
        await Promise.all(dependencyResults)
        results.push(assignment.agent.execute(assignment.subtask))
      }
    }

    const completedResults = await Promise.all(results)

    // Step 5: Synthesize and validate
    const synthesized = await this.synthesize(completedResults)
    const validated = await this.validate(synthesized, complexTask.requirements)

    return validated
  }

  private async decomposeTask(task: ComplexTask) {
    // Use LLM to intelligently break down the task
    const response = await anthropic.messages.create({
      model: 'claude-3-5-sonnet-20241022',
      max_tokens: 1500,
      messages: [{
        role: 'user',
        content: `Decompose this complex task into subtasks:

Task: ${task.description}
Requirements: ${JSON.stringify(task.requirements)}
Available agents: ${Array.from(this.workers.keys()).join(', ')}

Return JSON array of subtasks with:
- description: what needs to be done
- assignTo: which agent should do it
- dependencies: array of subtask indices this depends on
- estimatedTime: seconds`
      }]
    })

    return JSON.parse(response.content[0].text)
  }

  private selectBestAgent(requirements: string[]): Agent {
    // Score each agent based on capability match
    const scores = Array.from(this.workers.entries()).map(([role, agent]) => {
      const matchScore = requirements.filter(req =>
        agent.capabilities.includes(req)
      ).length
      return { agent, score: matchScore }
    })

    // Return agent with highest score
    return scores.sort((a, b) => b.score - a.score)[0].agent
  }

  private async synthesize(results: Result[]) {
    // Combine all results into coherent output
    const combined = await anthropic.messages.create({
      model: 'claude-3-5-sonnet-20241022',
      max_tokens: 2000,
      messages: [{
        role: 'user',
        content: `Synthesize these agent results into a coherent output:

${results.map((r, i) => `Result ${i + 1}: ${JSON.stringify(r)}`).join('\n\n')}`
      }]
    })

    return combined.content[0].text
  }
}

// Example usage
const manager = new ManagerAgent([
  researcher,
  writer,
  coder,
  reviewer,
  analyst
])

const result = await manager.delegateTask({
  description: 'Create a comprehensive guide on building AI agents with code examples',
  requirements: [
    'research current best practices',
    'provide working code examples',
    'explain tradeoffs',
    'include performance metrics'
  ]
})
```

**Why This Works**:
- **Intelligent Decomposition**: LLM breaks down complex tasks appropriately
- **Dynamic Assignment**: Best agent chosen based on capabilities
- **Dependency Management**: Ensures proper execution order
- **Quality Control**: Manager validates final output

## Communication Patterns

### Shared Memory

```typescript
class SharedMemory {
  private store = new Map<string, any>()

  async write(key: string, value: any, metadata?: any) {
    this.store.set(key, {
      value,
      metadata,
      timestamp: Date.now(),
      version: (this.store.get(key)?.version || 0) + 1
    })
  }

  async read(key: string) {
    return this.store.get(key)?.value
  }

  async subscribe(key: string, callback: (value: any) => void) {
    // Notify callback when key changes
    // Implementation depends on your event system
  }
}

// Agents share findings through memory
const memory = new SharedMemory()

// Agent 1 writes findings
await memory.write('research_findings', researchResults)

// Agent 2 reads and builds on it
const findings = await memory.read('research_findings')
const article = await writeArticle(findings)
```

### Message Passing

```typescript
class MessageBus {
  private subscribers = new Map<string, Function[]>()

  subscribe(topic: string, handler: Function) {
    if (!this.subscribers.has(topic)) {
      this.subscribers.set(topic, [])
    }
    this.subscribers.get(topic)!.push(handler)
  }

  async publish(topic: string, message: any) {
    const handlers = this.subscribers.get(topic) || []
    await Promise.all(handlers.map(h => h(message)))
  }
}

// Usage
const bus = new MessageBus()

// Agent subscribes to events
bus.subscribe('research_complete', async (data) => {
  console.log('Starting writing based on research...')
  await writer.execute({ type: 'write', context: data })
})

// Another agent publishes event
await bus.publish('research_complete', researchResults)
```

**Architect's Tip ‚Äî Selective Context Summarization (The Egress Hand-off)**: "The #1 hidden cost in multi-agent systems is **context bloat**. When Agent A passes its full 8,000-token output to Agent B, and Agent B passes its 10,000-token output to Agent C, you're paying for 18,000 tokens of context that Agent C doesn't need. An Architect enforces an **Egress Hand-off** ‚Äî each agent summarizes its output into a **500-token structured handoff** before passing to the next agent. This cuts inter-agent token costs by 87% and actually improves downstream quality because the receiving agent isn't distracted by irrelevant details."

```typescript
/**
 * Selective Context Summarization (Egress Hand-off)
 *
 * Problem: Agent A produces 8,000 tokens. Agent B receives all 8,000
 * plus its own prompt (2,000 tokens). Agent C receives A's 8,000 +
 * B's 10,000 + its own prompt. By Agent D, you're at 35,000+ tokens
 * of context ‚Äî most of which is irrelevant noise.
 *
 * Cost: At $0.003/1K input tokens, a 4-agent chain costs $0.174
 * per request in context alone. At 100K requests/month = $17,400.
 *
 * Solution: Each agent produces a structured 500-token "Egress Summary"
 * before handing off. Downstream agents receive ONLY the summary,
 * not the full working context.
 *
 * Result: 4-agent chain drops from 58,000 to 7,500 input tokens.
 * Monthly cost: $2,250 (87% reduction). Quality improves because
 * downstream agents focus on relevant signals.
 *
 * Interview Defense: "We enforce Egress Hand-offs between agents.
 * Each agent summarizes its output into 500 tokens before passing
 * downstream. This cut our inter-agent token costs by 87% and
 * actually improved task accuracy by 12% because downstream
 * agents weren't distracted by irrelevant context."
 */

interface EgressSummary {
  agentId: string
  taskCompleted: string        // What this agent did (1 sentence)
  keyFindings: string[]        // Top 3-5 findings (bullet points)
  confidence: number           // 0-1 confidence score
  handoffContext: string       // Structured context for next agent
  tokenCount: number           // Must be <= 500
  warnings: string[]           // Issues the next agent should know about
}

async function createEgressSummary(
  agentId: string,
  fullOutput: string,
  nextAgentRole: string
): Promise<EgressSummary> {
  const response = await anthropic.messages.create({
    model: 'claude-haiku-4-5-20251001',  // Cheap model for summarization
    max_tokens: 600,
    messages: [{
      role: 'user',
      content: `You are creating a handoff summary for the "${nextAgentRole}" agent.

Summarize the following output in EXACTLY this JSON format:
{
  "taskCompleted": "one sentence of what was done",
  "keyFindings": ["finding 1", "finding 2", "finding 3"],
  "confidence": 0.0-1.0,
  "handoffContext": "structured context the next agent needs (max 300 tokens)",
  "warnings": ["any issues or gaps"]
}

RULES:
- Total output must be under 500 tokens
- Include ONLY information relevant to "${nextAgentRole}"
- Omit internal reasoning and intermediate steps

Output to summarize:
${fullOutput}`
    }]
  })

  const parsed = JSON.parse(response.content[0].text)

  return {
    agentId,
    ...parsed,
    tokenCount: response.usage.output_tokens
  }
}

// Cost comparison for a 4-agent chain:
//
// WITHOUT Egress Hand-off (full context forwarding):
//   Agent A output: 8,000 tokens
//   Agent B input:  8,000 + 2,000 prompt = 10,000 tokens ‚Üí output: 10,000
//   Agent C input: 10,000 + 8,000 + 2,000 = 20,000 tokens ‚Üí output: 6,000
//   Agent D input:  6,000 + 20,000 + 2,000 = 28,000 tokens
//   Total input tokens: 10,000 + 20,000 + 28,000 = 58,000
//   Cost at $0.003/1K: $0.174 per request
//   Monthly (100K requests): $17,400
//
// WITH Egress Hand-off (500-token summaries):
//   Agent A output: 8,000 tokens ‚Üí Egress: 500 tokens
//   Agent B input:  500 + 2,000 = 2,500 tokens ‚Üí Egress: 500 tokens
//   Agent C input:  500 + 2,000 = 2,500 tokens ‚Üí Egress: 500 tokens
//   Agent D input:  500 + 2,000 = 2,500 tokens
//   Total input tokens: 2,500 + 2,500 + 2,500 = 7,500
//   Cost at $0.003/1K: $0.023 per request
//   Monthly (100K requests): $2,250
//
// Savings: $15,150/month (87% reduction)
// Quality: +12% accuracy (less noise in context)
```

### Idempotent Agent Hand-offs: Atomic State Checkpointing

**Architect's Tip ‚Äî The Immutable Artifact Contract**: "In a multi-agent system, every agent output must be treated as an **Immutable Artifact** with a unique version ID. Use a persistent state store (like Redis or Postgres) to 'checkpoint' the exact output of Agent A before notifying Agent B. If the system crashes, the orchestrator resumes from the last completed artifact rather than re-triggering the entire chain. This ensures **Atomic Consistency** across the team. Without checkpointing, a crash after the Researcher succeeds but before the Writer starts means you re-run the Researcher ‚Äî wasting tokens and potentially getting different results."

```typescript
/**
 * Idempotent Agent Hand-offs
 *
 * Problem: In a sequential pipeline (Researcher ‚Üí Writer ‚Üí Reviewer),
 * if the Writer crashes after the Researcher succeeds, a naive system
 * restarts the ENTIRE pipeline. The Researcher runs again, wasting
 * $0.35 in tokens, and may produce DIFFERENT results (LLM outputs
 * are non-deterministic). The Writer then receives different context
 * than the original plan intended.
 *
 * Solution: Checkpoint every agent's output as an immutable artifact
 * with a version ID in a persistent store. On restart, the orchestrator
 * checks the checkpoint store and resumes from the last completed
 * artifact ‚Äî skipping already-completed agents entirely.
 *
 * Interview Defense: "Every agent output is checkpointed as an immutable
 * artifact with a content hash and version ID. On crash recovery, the
 * orchestrator queries the checkpoint store and resumes from the last
 * completed stage. This makes our pipelines idempotent ‚Äî re-running
 * the same workflow ID produces the same result without duplicate work
 * or token waste."
 */

interface AgentArtifact {
  artifactId: string          // Unique ID: `${workflowId}-${agentId}-${version}`
  workflowId: string          // Parent workflow execution ID
  agentId: string             // Which agent produced this
  stage: number               // Pipeline stage (0 = first agent, 1 = second, etc.)
  contentHash: string         // SHA-256 of the output (for deduplication)
  output: any                 // The actual agent output
  inputHash: string           // Hash of the input this agent received
  model: string               // Model used (for reproducibility)
  tokenCost: number           // Tokens consumed
  createdAt: Date
  status: 'completed' | 'failed'
}

interface CheckpointStore {
  save(artifact: AgentArtifact): Promise<void>
  getLatestForStage(workflowId: string, stage: number): Promise<AgentArtifact | null>
  getWorkflowState(workflowId: string): Promise<Map<number, AgentArtifact>>
}

// Redis-backed checkpoint store for production
class RedisCheckpointStore implements CheckpointStore {
  private redis: any  // Redis client

  constructor(redisClient: any) {
    this.redis = redisClient
  }

  async save(artifact: AgentArtifact): Promise<void> {
    const key = `checkpoint:${artifact.workflowId}:stage:${artifact.stage}`
    await this.redis.set(key, JSON.stringify(artifact))

    // Also store by artifact ID for direct lookup
    await this.redis.set(`artifact:${artifact.artifactId}`, JSON.stringify(artifact))

    // Set TTL (24 hours ‚Äî artifacts are ephemeral, not permanent storage)
    await this.redis.expire(key, 86400)

    console.log(
      `üìå Checkpointed: ${artifact.agentId} (stage ${artifact.stage}) ‚Üí ` +
      `${artifact.artifactId} [${artifact.tokenCost} tokens]`
    )
  }

  async getLatestForStage(
    workflowId: string,
    stage: number
  ): Promise<AgentArtifact | null> {
    const key = `checkpoint:${workflowId}:stage:${stage}`
    const data = await this.redis.get(key)
    return data ? JSON.parse(data) : null
  }

  async getWorkflowState(
    workflowId: string
  ): Promise<Map<number, AgentArtifact>> {
    const state = new Map<number, AgentArtifact>()
    // Scan for all stages of this workflow
    const keys = await this.redis.keys(`checkpoint:${workflowId}:stage:*`)

    for (const key of keys) {
      const data = await this.redis.get(key)
      if (data) {
        const artifact = JSON.parse(data)
        state.set(artifact.stage, artifact)
      }
    }
    return state
  }
}

class IdempotentPipelineExecutor {
  private checkpoints: CheckpointStore
  private agents: Array<{ agent: Agent; stage: number }>

  constructor(checkpoints: CheckpointStore, agents: Agent[]) {
    this.checkpoints = checkpoints
    this.agents = agents.map((agent, i) => ({ agent, stage: i }))
  }

  async execute(workflowId: string, initialInput: any): Promise<any> {
    console.log(`\n=== Idempotent Pipeline: ${workflowId} ===`)

    // Step 1: Check existing checkpoints (crash recovery)
    const existingState = await this.checkpoints.getWorkflowState(workflowId)
    const resumeFromStage = this.findResumePoint(existingState)

    if (resumeFromStage > 0) {
      console.log(
        `‚ôªÔ∏è  Resuming from stage ${resumeFromStage} ` +
        `(${resumeFromStage} stages already checkpointed)`
      )
    }

    // Step 2: Execute pipeline, skipping completed stages
    let currentInput = initialInput

    for (const { agent, stage } of this.agents) {
      // Check if this stage is already completed
      const existing = existingState.get(stage)
      if (existing && existing.status === 'completed') {
        console.log(`‚è≠Ô∏è  Stage ${stage} (${agent.role}): SKIPPED (checkpointed)`)
        currentInput = existing.output  // Use cached output as next input
        continue
      }

      // Execute this stage
      console.log(`‚ñ∂Ô∏è  Stage ${stage} (${agent.role}): EXECUTING...`)
      const startTime = Date.now()

      const output = await agent.execute({
        ...currentInput,
        _workflowId: workflowId,
        _stage: stage
      })

      const duration = Date.now() - startTime

      // Checkpoint the output BEFORE proceeding to next stage
      const artifact: AgentArtifact = {
        artifactId: `${workflowId}-${agent.role}-${Date.now()}`,
        workflowId,
        agentId: agent.role,
        stage,
        contentHash: this.hashContent(output),
        output,
        inputHash: this.hashContent(currentInput),
        model: 'claude-sonnet-4-5-20250929',
        tokenCost: output._tokenCost || 0,
        createdAt: new Date(),
        status: 'completed'
      }

      await this.checkpoints.save(artifact)

      console.log(
        `‚úÖ Stage ${stage} (${agent.role}): COMPLETED ` +
        `(${duration}ms, ${artifact.tokenCost} tokens)`
      )

      currentInput = output  // Pass to next stage
    }

    console.log(`\nüèÅ Pipeline ${workflowId} complete`)
    return currentInput
  }

  private findResumePoint(
    existingState: Map<number, AgentArtifact>
  ): number {
    // Find the highest completed stage
    let maxCompleted = -1
    for (const [stage, artifact] of existingState) {
      if (artifact.status === 'completed' && stage > maxCompleted) {
        maxCompleted = stage
      }
    }
    return maxCompleted + 1  // Resume from the NEXT stage
  }

  private hashContent(content: any): string {
    // Simple hash for deduplication (use crypto.createHash in production)
    return Buffer.from(JSON.stringify(content)).toString('base64').slice(0, 32)
  }
}

// Crash Recovery Example:
//
// FIRST RUN (crashes after stage 1):
//   ‚ñ∂Ô∏è  Stage 0 (researcher): EXECUTING...
//   üìå Checkpointed: researcher (stage 0) ‚Üí wf-123-researcher-001
//   ‚úÖ Stage 0 (researcher): COMPLETED (15,000ms, 2,400 tokens)
//   ‚ñ∂Ô∏è  Stage 1 (writer): EXECUTING...
//   üìå Checkpointed: writer (stage 1) ‚Üí wf-123-writer-001
//   ‚úÖ Stage 1 (writer): COMPLETED (22,000ms, 3,100 tokens)
//   ‚ñ∂Ô∏è  Stage 2 (reviewer): EXECUTING...
//   üí• CRASH ‚Äî process killed (OOM, timeout, network failure)
//
// SECOND RUN (resumes from stage 2):
//   ‚ôªÔ∏è  Resuming from stage 2 (2 stages already checkpointed)
//   ‚è≠Ô∏è  Stage 0 (researcher): SKIPPED (checkpointed)
//   ‚è≠Ô∏è  Stage 1 (writer): SKIPPED (checkpointed)
//   ‚ñ∂Ô∏è  Stage 2 (reviewer): EXECUTING...
//   üìå Checkpointed: reviewer (stage 2) ‚Üí wf-123-reviewer-001
//   ‚úÖ Stage 2 (reviewer): COMPLETED (8,000ms, 1,200 tokens)
//   üèÅ Pipeline wf-123 complete
//
// WITHOUT checkpointing: Re-run all 3 stages ‚Üí $0.67 wasted, 45s lost
// WITH checkpointing:    Resume from stage 2 ‚Üí $0.00 wasted, 0s lost
//
// | Metric               | Without Checkpointing | With Checkpointing |
// |----------------------|-----------------------|--------------------|
// | Tokens wasted        | 5,500 (stages 0+1)    | 0                  |
// | Cost wasted          | $0.45                  | $0.00              |
// | Time wasted          | 37 seconds             | 0 seconds          |
// | Result consistency   | Different (LLM non-determinism) | Identical (cached) |
// | Redis overhead       | N/A                    | ~2ms per checkpoint |
```

---

## Best Practices

1. **Clear Role Definition**: Each agent should have a specific, well-defined purpose
2. **Avoid Over-Coordination**: Too much coordination overhead defeats the purpose
3. **Handle Failures Gracefully**: One agent failure shouldn't break the entire system
4. **Checkpoint Agent Outputs**: Every stage should be an immutable artifact in a persistent store for crash recovery
5. **Monitor Costs**: Multi-agent systems can get expensive quickly
6. **Use Caching**: Many agents may need the same information
7. **Set Timeouts**: Prevent one slow agent from blocking everything

## Common Pitfalls

1. **Too Many Agents**: More agents ‚â† better results. Start simple.
   - **Bad**: 10 agents for a simple task ‚Üí $5 in API calls
   - **Good**: 2-3 agents for complex task ‚Üí $0.30 in API calls

2. **Circular Dependencies**: Agent A waits for B, B waits for A
   - **Fix**: Use dependency graphs, validate before execution

**Architect's Tip ‚Äî DAG-Based Workflow Validation (The Cycle Detector)**: "Never execute a multi-agent workflow without first parsing it into a **Directed Acyclic Graph (DAG)** and validating it. A circular dependency between agents is a **silent deadlock** ‚Äî Agent A waits for B, B waits for C, C waits for A, and your system hangs forever with no error message. An Architect validates the DAG before the first agent fires. If a cycle is detected, it's a **hard stop** ‚Äî no execution, immediate error to the developer. This costs 0.1ms and prevents hours of debugging deadlocked production systems."

```typescript
/**
 * DAG-Based Workflow Validation
 *
 * Problem: Circular dependencies between agents cause silent deadlocks.
 * Agent A ‚Üí B ‚Üí C ‚Üí A creates an infinite wait with no error.
 * In production, this manifests as "the system just hangs" with
 * no logs, no errors, and confused users.
 *
 * Solution: Parse agent dependencies into a DAG. Run topological
 * sort before execution. If a cycle exists, reject the workflow
 * immediately with a clear error showing the cycle path.
 *
 * Interview Defense: "We validate every workflow as a DAG before
 * execution. Cycle detection costs 0.1ms and prevents silent
 * deadlocks that would otherwise take hours to debug in production."
 */

interface WorkflowNode {
  agentId: string
  dependsOn: string[]  // Agent IDs this node waits for
}

interface ValidationResult {
  valid: boolean
  executionOrder: string[] | null
  cycle: string[] | null
  parallelGroups: string[][] | null
}

function validateWorkflowDAG(nodes: WorkflowNode[]): ValidationResult {
  const graph = new Map<string, string[]>()
  const inDegree = new Map<string, number>()

  // Build adjacency list and in-degree count
  for (const node of nodes) {
    if (!graph.has(node.agentId)) graph.set(node.agentId, [])
    if (!inDegree.has(node.agentId)) inDegree.set(node.agentId, 0)

    for (const dep of node.dependsOn) {
      if (!graph.has(dep)) graph.set(dep, [])
      graph.get(dep)!.push(node.agentId)
      inDegree.set(node.agentId, (inDegree.get(node.agentId) || 0) + 1)
    }
  }

  // Kahn's algorithm for topological sort
  const queue: string[] = []
  const executionOrder: string[] = []
  const parallelGroups: string[][] = []

  // Start with nodes that have no dependencies
  for (const [id, degree] of inDegree) {
    if (degree === 0) queue.push(id)
  }

  while (queue.length > 0) {
    // All nodes in current queue can run in parallel
    const currentGroup = [...queue]
    parallelGroups.push(currentGroup)

    const nextQueue: string[] = []
    for (const current of queue) {
      executionOrder.push(current)
      for (const neighbor of graph.get(current) || []) {
        const newDegree = inDegree.get(neighbor)! - 1
        inDegree.set(neighbor, newDegree)
        if (newDegree === 0) nextQueue.push(neighbor)
      }
    }
    queue.length = 0
    queue.push(...nextQueue)
  }

  // If not all nodes are in execution order, there's a cycle
  if (executionOrder.length !== nodes.length) {
    const cycleNodes = nodes
      .filter(n => !executionOrder.includes(n.agentId))
      .map(n => n.agentId)
    return {
      valid: false,
      executionOrder: null,
      cycle: cycleNodes,
      parallelGroups: null
    }
  }

  return {
    valid: true,
    executionOrder,
    cycle: null,
    parallelGroups
  }
}

// Example: Valid workflow
//
// const valid = validateWorkflowDAG([
//   { agentId: 'researcher',  dependsOn: [] },
//   { agentId: 'coder',       dependsOn: [] },
//   { agentId: 'writer',      dependsOn: ['researcher', 'coder'] },
//   { agentId: 'reviewer',    dependsOn: ['writer'] }
// ])
//
// Result: {
//   valid: true,
//   executionOrder: ['researcher', 'coder', 'writer', 'reviewer'],
//   parallelGroups: [['researcher', 'coder'], ['writer'], ['reviewer']]
// }
//
// ‚Üí researcher and coder run in parallel (Group 1)
// ‚Üí writer runs after both complete (Group 2)
// ‚Üí reviewer runs last (Group 3)

// Example: Circular dependency (HARD STOP)
//
// const invalid = validateWorkflowDAG([
//   { agentId: 'researcher',  dependsOn: ['reviewer'] },  // ‚Üê cycle!
//   { agentId: 'writer',      dependsOn: ['researcher'] },
//   { agentId: 'reviewer',    dependsOn: ['writer'] }
// ])
//
// Result: {
//   valid: false,
//   cycle: ['researcher', 'writer', 'reviewer'],
//   executionOrder: null
// }
//
// ‚Üí HARD STOP: "Circular dependency detected:
//     researcher ‚Üí writer ‚Üí reviewer ‚Üí researcher"
// ‚Üí Fix the workflow before any agent fires
```

3. **No Conflict Resolution**: What happens when agents disagree?
   - **Fix**: Implement arbitration (see Week 11, Conflict Resolution)

4. **Ignoring Latency**: Sequential can be too slow for users
   - **Fix**: Use parallel where possible, show progress indicators

## Real-World Example

**Task**: Generate a technical blog post with code examples

```typescript
async function createTechnicalBlogPost(topic: string) {
  const manager = new ManagerAgent([researcher, coder, writer, reviewer])

  // Manager automatically:
  // 1. Has researcher find information (30s)
  // 2. Has coder create examples (parallel, 25s)
  // 3. Has writer create draft with research + code (20s)
  // 4. Has reviewer check quality (15s)
  // 5. Has writer revise if needed (15s)

  const post = await manager.delegateTask({
    description: `Technical blog post about ${topic}`,
    requirements: ['accurate', 'code examples', 'beginner friendly']
  })

  return post
}

// Total time: ~90 seconds (with parallelization)
// Total cost: ~$0.40
// Quality: High (multiple review stages)
```

---

## Architect Challenge: The Specialist vs. Generalist Quiz

**You are designing a multi-agent system for an investment firm. The CEO wants an "AI Analyst" that produces weekly investment reports.**

**The Situation:**

Each report requires: (1) **Market Research** ‚Äî scanning 50+ sources for sector trends, (2) **Financial Modeling** ‚Äî running DCF valuations on 10 stocks, (3) **Risk Assessment** ‚Äî evaluating geopolitical and regulatory risks, and (4) **Report Writing** ‚Äî producing a polished 20-page PDF. The current single-agent approach takes **45 minutes** and costs **$2.80 per report**. The CFO wants to scale from 10 reports/week to 200 reports/week. The current architecture won't survive.

**Your options:**

**A)** Use a **single powerful agent** (Opus) with a massive prompt that includes all instructions for research, modeling, risk, and writing. Give it web search, calculator, and PDF tools. Scale by running 200 sequential requests.

**B)** Deploy a **Supervisor-Specialist Pattern**. One lightweight Supervisor agent (Haiku) decomposes each report request into 4 sub-tasks, dispatches them to 4 specialized agents (Research-Sonnet, Finance-Sonnet, Risk-Haiku, Writer-Sonnet), validates the DAG for dependency cycles, collects results via Egress Hand-offs (500-token summaries), and synthesizes the final report.

**C)** Use **Choreography** with event-driven agents. Each agent listens for events and publishes results. No central coordinator ‚Äî agents self-organize through a message bus.

**D)** Fine-tune a single model on 500 example reports so it can do everything in one call without multiple agents.

<details>
<summary><strong>Click to reveal the correct answer</strong></summary>

### Correct Answer: B ‚Äî Supervisor-Specialist Pattern

An Architect uses **Supervisor-Specialist** for transactional, multi-skill workflows where quality and auditability matter.

**The Math:**

```typescript
// Current: Single agent (Opus)
//   Time per report: 45 minutes
//   Cost per report: $2.80
//   200 reports/week sequential: 150 hours (impossible)
//   200 reports/week parallel: 200 √ó $2.80 = $560/week

// Supervisor-Specialist Pattern:
//   Supervisor (Haiku): $0.02 per dispatch
//   Research Agent (Sonnet): $0.35 (parallel, 8 min)
//   Finance Agent (Sonnet): $0.40 (parallel, 10 min)
//   Risk Agent (Haiku): $0.08 (parallel, 3 min)
//   Writer Agent (Sonnet): $0.30 (sequential after others, 5 min)
//   Egress summaries: 4 √ó $0.01 = $0.04
//
//   Total per report: $1.19
//   Time per report: max(8, 10, 3) + 5 = 15 minutes (67% faster)
//   200 reports/week: 200 √ó $1.19 = $238/week (58% cheaper)
//   Annual savings: (560 - 238) √ó 52 = $16,744
```

**Why other answers fail:**

- **A) Single powerful agent** ‚Äî Opus at $2.80/report √ó 200 = $560/week. No parallelism within each report (45 min each). At 200 reports, you need 150 hours of sequential processing per week. Even with parallel instances, you're paying 2.4x more than the Specialist pattern and getting no quality improvement from specialization.

- **C) Choreography** ‚Äî Investment reports are **transactional workflows** that need audit trails. The CFO needs to know exactly which agent produced which section for compliance. Choreography makes this nearly impossible to trace. When the Risk Agent produces a wrong assessment, you need the Supervisor's central log to identify and fix it. Choreography is for streaming pipelines, not auditable financial documents.

- **D) Fine-tune a single model** ‚Äî Violates the **Behavior vs. Memory** rule. Market data changes daily. A fine-tuned model would confidently report last month's stock prices. Fine-tuning teaches behavior (format, tone), not facts (market data). The report would look polished but contain stale, hallucinated numbers ‚Äî the worst possible outcome for an investment firm.

**The Architect's Principle:** "Supervisor-Specialist is the default for **business-critical, multi-skill workflows**. The Supervisor provides auditability (every sub-task is logged), rollback (re-run just the failed agent), and cost control (use cheap models for simple sub-tasks, expensive models only where quality demands it). The 500-token Egress Hand-off between agents prevents context bloat. The DAG validation prevents deadlocks. This is not just a pattern ‚Äî it's an **operating system for AI workflows**."

</details>

---

## Resources
- [Multi-Agent Systems](https://lilianweng.github.io/posts/2023-06-23-agent/)
- [AutoGPT Architecture](https://github.com/Significant-Gravitas/AutoGPT)
- [LangChain Multi-Agent](https://python.langchain.com/docs/modules/agents/multi_agent)
