---
title: "LoRA and Parameter-Efficient Fine-Tuning"
description: "Master LoRA, QLoRA, and PEFT techniques for fine-tuning LLMs with 95% memory reduction and 10x cost savings"
estimatedMinutes: 55
week: 10
concept: 2
difficulty: advanced
objectives:
  - Understand LoRA architecture and low-rank adaptation mathematics
  - Implement QLoRA for 4-bit quantized training with 95% memory savings
  - Compare PEFT techniques (LoRA, Prefix Tuning, Adapters, IA3)
  - Deploy LoRA adapters with HuggingFace and production infrastructure
---

import { CodePlayground } from '@/components/curriculum/CodePlayground'

# LoRA and Parameter-Efficient Fine-Tuning

Master parameter-efficient fine-tuning (PEFT) techniques that enable fine-tuning billion-parameter models on consumer GPUs with 95% memory reduction.

> **Note**: LoRA revolutionized fine-tuning in 2021. Today it's the industry standard for adapting LLMs - used by Stable Diffusion, ChatGPT plugins, and enterprise AI deployments.

## What is LoRA?

**Simple Explanation**: LoRA (Low-Rank Adaptation) freezes the original model weights and trains small "adapter" matrices that modify the model's behavior. Instead of updating 7 billion parameters, you update 4 million. It's like adding specialized skills to a general expert without retraining everything they know.

**The Problem with Full Fine-Tuning**:
```
Llama 3 8B Model:
- Parameters: 8 billion
- Memory for training: 120 GB (FP16)
- GPU required: 8x A100 (80GB each) = $24/hour
- Training 1 epoch (10K examples): 4 hours = $96

Most teams can't afford this!
```

**LoRA Solution**:
```
Llama 3 8B with LoRA:
- Trainable parameters: 4 million (0.05% of model)
- Memory for training: 12 GB with 4-bit quantization
- GPU required: 1x RTX 4090 (24GB) = $1/hour or free (Colab)
- Training 1 epoch: 4 hours = $4
- Adapter size: 100 MB (vs 16 GB full model)

96% cost reduction!
```

## LoRA Architecture

**How LoRA Works**:

In a transformer layer, the attention mechanism uses weight matrices W (e.g., W_q, W_k, W_v):

```
Standard fine-tuning:
W_new = W_pretrained + Î”W  (update all parameters)

LoRA:
W_new = W_pretrained + BA  (B and A are low-rank matrices)

Where:
- W_pretrained: Frozen original weights (d Ã— d)
- B: Trainable matrix (d Ã— r)
- A: Trainable matrix (r Ã— d)
- r: Rank (typically 4, 8, 16, or 32)
- d: Model dimension (4096 for Llama 3 8B)

Parameters:
- Standard: d Ã— d = 4096 Ã— 4096 = 16.7M parameters
- LoRA: d Ã— r + r Ã— d = (4096 Ã— 8) + (8 Ã— 4096) = 65K parameters
- Reduction: 99.6%!
```

<CodePlayground
  title="LoRA Mathematics Visualization"
  description="See how low-rank decomposition works. Try different rank values to see memory tradeoffs!"
  exerciseType="lora-math"
  code={`interface LoRAConfig {
  d: number // Model dimension
  r: number // LoRA rank
  alpha: number // Scaling factor
}

function calculateLoRAParameters(config: LoRAConfig): {
  originalParams: number
  loraParams: number
  reduction: number
  memoryMB: number
} {
  const { d, r } = config

  // Original weight matrix parameters
  const originalParams = d * d

  // LoRA parameters: two matrices B (dÃ—r) and A (rÃ—d)
  const loraParams = (d * r) + (r * d)

  // Reduction percentage
  const reduction = ((originalParams - loraParams) / originalParams) * 100

  // Memory usage (FP16 = 2 bytes per parameter)
  const memoryMB = (loraParams * 2) / (1024 * 1024)

  return {
    originalParams,
    loraParams,
    reduction,
    memoryMB
  }
}

// Example configurations
const configs: LoRAConfig[] = [
  { d: 4096, r: 4, alpha: 16 },
  { d: 4096, r: 8, alpha: 16 },
  { d: 4096, r: 16, alpha: 32 },
  { d: 4096, r: 32, alpha: 64 }
]

console.log('LoRA Parameter Analysis\\n')
console.log('Model dimension (d): 4096 (Llama 3 8B)\\n')
console.log('â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”')
console.log('â”‚ Rank â”‚ Original Params â”‚ LoRA Params  â”‚ Reduction  â”‚ Memory   â”‚')
console.log('â”œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤')

configs.forEach(config => {
  const result = calculateLoRAParameters(config)

  console.log(
    \`â”‚ r=\${config.r.toString().padEnd(2)} â”‚ \${result.originalParams.toLocaleString().padEnd(15)} â”‚ \${result.loraParams.toLocaleString().padEnd(12)} â”‚ \${result.reduction.toFixed(2).padStart(5)}%   â”‚ \${result.memoryMB.toFixed(1).padStart(5)} MB â”‚\`
  )
})

console.log('â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜')

console.log('\\nKey Insights:')
console.log('  â€¢ Higher rank (r) = more expressiveness but more memory')
console.log('  â€¢ r=8 is sweet spot for most tasks (99.5% reduction)')
console.log('  â€¢ r=16 for complex domain adaptation')
console.log('  â€¢ r=4 sufficient for simple style/formatting tasks')

// Calculate for full model
const llama3_8b_layers = 32 // 32 transformer layers
const attention_matrices = 4 // Q, K, V, O projections

const totalLoRAParams = configs[1].loraParams * llama3_8b_layers * attention_matrices
const totalOriginalParams = configs[1].originalParams * llama3_8b_layers * attention_matrices

console.log(\`\\nFull Llama 3 8B Model (r=8):\`)
console.log(\`  Original parameters: \${totalOriginalParams.toLocaleString()}\`)
console.log(\`  LoRA parameters: \${totalLoRAParams.toLocaleString()}\`)
console.log(\`  Trainable: \${((totalLoRAParams / 8_000_000_000) * 100).toFixed(3)}% of 8B model\`)
`}
/>

## QLoRA: 4-bit Quantization for Memory Efficiency

**QLoRA** combines LoRA with 4-bit quantization to enable fine-tuning on consumer GPUs.

**Memory Breakdown**:
```
Llama 3 8B in different precisions:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Precision       â”‚ Size     â”‚ GPU Required â”‚ Use Caseâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ FP32 (32-bit)   â”‚ 32 GB    â”‚ 4x A100      â”‚ Trainingâ”‚
â”‚ FP16 (16-bit)   â”‚ 16 GB    â”‚ 2x A100      â”‚ Trainingâ”‚
â”‚ Int8 (8-bit)    â”‚ 8 GB     â”‚ 1x A100      â”‚ Inferenceâ”‚
â”‚ NF4 (4-bit)     â”‚ 4 GB     â”‚ 1x RTX 4090  â”‚ QLoRA   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

With QLoRA:
- Base model: 4 GB (4-bit quantized, frozen)
- LoRA adapters: 100 MB (FP16, trainable)
- Optimizer states: 8 GB
- Total: ~12 GB (fits on consumer GPU!)
```

<CodePlayground
  title="QLoRA Fine-Tuning Implementation"
  description="Complete QLoRA fine-tuning pipeline for Llama 3. Production-ready code with memory optimization!"
  exerciseType="qlora-training"
  code={`// This is conceptual TypeScript showing the process.
// Actual implementation uses Python with transformers + PEFT libraries.

interface QLoRAConfig {
  baseModel: string
  loraRank: number
  loraAlpha: number
  loraDropout: number
  targetModules: string[]
  quantization: '4bit' | '8bit'
}

interface TrainingArgs {
  outputDir: string
  numEpochs: number
  batchSize: number
  learningRate: number
  warmupSteps: number
  loggingSteps: number
}

// Python code for reference (this is what you'd actually run)
const pythonQLoRACode = \`
import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments
)
from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training
)
from trl import SFTTrainer
from datasets import load_dataset

# Step 1: Configure 4-bit quantization
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",  # Normal Float 4-bit
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True  # Nested quantization for more savings
)

# Step 2: Load base model with quantization
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Meta-Llama-3-8B",
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Meta-Llama-3-8B")
tokenizer.pad_token = tokenizer.eos_token

# Step 3: Prepare model for training
model = prepare_model_for_kbit_training(model)

# Step 4: Configure LoRA
lora_config = LoraConfig(
    r=16,  # Rank
    lora_alpha=32,  # Scaling factor
    target_modules=[
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj"
    ],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

# Step 5: Apply LoRA adapters
model = get_peft_model(model, lora_config)

print("Trainable parameters:")
model.print_trainable_parameters()
# Output: trainable params: 41,943,040 || all params: 8,030,261,248 || trainable%: 0.52%

# Step 6: Load and format dataset
dataset = load_dataset("json", data_files="training_data.jsonl")

def format_instruction(example):
    return f"""### Instruction:
{example['instruction']}

### Input:
{example['input']}

### Response:
{example['output']}"""

# Step 7: Training arguments
training_args = TrainingArguments(
    output_dir="./llama3-qlora-customer-support",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    gradient_checkpointing=True,  # Save memory
    optim="paged_adamw_8bit",  # 8-bit optimizer
    learning_rate=2e-4,
    warmup_steps=100,
    logging_steps=10,
    save_strategy="epoch",
    fp16=False,
    bf16=True,  # Use bfloat16 for better stability
    max_grad_norm=0.3,
    group_by_length=True  # Group similar lengths for efficiency
)

# Step 8: Initialize trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    tokenizer=tokenizer,
    formatting_func=format_instruction,
    max_seq_length=512
)

# Step 9: Train!
print("Starting training...")
trainer.train()

# Step 10: Save LoRA adapters
model.save_pretrained("./llama3-customer-support-lora")
tokenizer.save_pretrained("./llama3-customer-support-lora")

print("Training complete! LoRA adapters saved.")
\`

// Deployment code (TypeScript/Node.js)
async function deployQLoRAModel() {
  console.log('QLoRA Training Pipeline\\n')
  console.log('=' .repeat(80))
  console.log()

  console.log('Hardware Requirements:')
  console.log('  Minimum: RTX 4090 (24GB VRAM) or A10G')
  console.log('  Recommended: A100 (40GB) for faster training')
  console.log('  Budget option: Google Colab Pro ($10/month)\\n')

  console.log('Memory Breakdown:')
  console.log('  Base model (4-bit): 4.5 GB')
  console.log('  LoRA adapters (FP16): 0.1 GB')
  console.log('  Optimizer states: 6 GB')
  console.log('  Gradients & activations: 3 GB')
  console.log('  Buffer: 1 GB')
  console.log('  TOTAL: ~14.6 GB\\n')

  console.log('Training Performance:')
  console.log('  Dataset: 10,000 examples')
  console.log('  Batch size: 4 (with gradient accumulation)')
  console.log('  Effective batch size: 16')
  console.log('  Training time: ~4 hours (RTX 4090)')
  console.log('  Training time: ~2 hours (A100)\\n')

  console.log('Cost Analysis:')
  console.log('  Cloud GPU (A100): $2/hour Ã— 2 hours = $4')
  console.log('  vs Full fine-tuning (8x A100): $24/hour Ã— 4 hours = $96')
  console.log('  Savings: $92 (96% reduction)\\n')

  console.log('Output:')
  console.log('  LoRA adapter size: ~100 MB')
  console.log('  Base model: 4.5 GB (shared across all adapters)')
  console.log('  Can train 100 adapters for storage cost of 1 full model!\\n')

  console.log('Python Training Script:')
  console.log(pythonQLoRACode)
}

deployQLoRAModel()
`}
/>

## PEFT Techniques Comparison

LoRA isn't the only parameter-efficient fine-tuning method. Here's how it compares:

<CodePlayground
  title="PEFT Techniques Comparison"
  description="Compare different parameter-efficient fine-tuning methods. See tradeoffs in memory, speed, and quality!"
  exerciseType="peft-comparison"
  code={`interface PEFTMethod {
  name: string
  trainableParams: number // as % of model
  memoryOverhead: string
  inferenceLatency: string // vs base model
  accuracy: string // typical performance
  bestFor: string[]
  limitations: string[]
}

const peftMethods: PEFTMethod[] = [
  {
    name: 'LoRA',
    trainableParams: 0.5,
    memoryOverhead: '~100 MB',
    inferenceLatency: '+0-2%',
    accuracy: 'Excellent (95-98% of full fine-tuning)',
    bestFor: [
      'General-purpose fine-tuning',
      'Classification & generation tasks',
      'Multiple task-specific adapters',
      'Production deployments'
    ],
    limitations: [
      'Requires choosing target modules',
      'Rank (r) is a hyperparameter to tune',
      'May underperform on very complex tasks'
    ]
  },
  {
    name: 'QLoRA',
    trainableParams: 0.5,
    memoryOverhead: '~100 MB + quantization',
    inferenceLatency: '+5-10%',
    accuracy: 'Excellent (94-97% of full fine-tuning)',
    bestFor: [
      'Consumer GPU training (24GB)',
      'Cost-sensitive fine-tuning',
      'Rapid experimentation',
      'Large model adaptation (70B on 1 GPU)'
    ],
    limitations: [
      '4-bit quantization slightly degrades quality',
      'Slower inference than FP16',
      'Requires bitsandbytes library'
    ]
  },
  {
    name: 'Prefix Tuning',
    trainableParams: 0.1,
    memoryOverhead: '~10 MB',
    inferenceLatency: '+15-20%',
    accuracy: 'Good (85-90% of full fine-tuning)',
    bestFor: [
      'Extremely low memory budgets',
      'Task-specific prompting',
      'Multi-task learning with shared model'
    ],
    limitations: [
      'Lower accuracy than LoRA',
      'Adds latency (virtual tokens)',
      'Harder to optimize (training instability)'
    ]
  },
  {
    name: 'Adapters',
    trainableParams: 1.0,
    memoryOverhead: '~200 MB',
    inferenceLatency: '+10-15%',
    accuracy: 'Very Good (92-95% of full fine-tuning)',
    bestFor: [
      'Transfer learning from related tasks',
      'Incremental learning scenarios',
      'When inference latency acceptable'
    ],
    limitations: [
      'Higher memory than LoRA',
      'Adds layers (slower inference)',
      'More parameters to train'
    ]
  },
  {
    name: 'IA3 (Infused Adapter)',
    trainableParams: 0.01,
    memoryOverhead: '~10 MB',
    inferenceLatency: '+0-1%',
    accuracy: 'Good (88-92% of full fine-tuning)',
    bestFor: [
      'Minimal memory footprint',
      'Fast inference required',
      'Simple task adaptation'
    ],
    limitations: [
      'Lower expressiveness',
      'Best for simpler tasks',
      'Less research/community support'
    ]
  }
]

console.log('Parameter-Efficient Fine-Tuning (PEFT) Methods\\n')
console.log('Base Model: Llama 3 8B (8 billion parameters)\\n')
console.log('='.repeat(100))

peftMethods.forEach(method => {
  console.log(\`\\n\${method.name}\`)
  console.log('-'.repeat(50))
  console.log(\`Trainable Parameters: \${method.trainableParams}% (\${(8_000_000_000 * method.trainableParams / 100).toLocaleString()} params)\`)
  console.log(\`Memory Overhead: \${method.memoryOverhead}\`)
  console.log(\`Inference Latency: \${method.inferenceLatency}\`)
  console.log(\`Accuracy: \${method.accuracy}\`)

  console.log(\`\\nBest For:\`)
  method.bestFor.forEach(use => console.log(\`  âœ“ \${use}\`))

  console.log(\`\\nLimitations:\`)
  method.limitations.forEach(limit => console.log(\`  âœ— \${limit}\`))
})

console.log('\\n' + '='.repeat(100))
console.log('\\nRecommendation by Use Case:\\n')

const recommendations = [
  {
    useCase: 'General fine-tuning (production)',
    recommended: 'LoRA',
    reason: 'Best balance of accuracy, speed, and memory'
  },
  {
    useCase: 'Consumer GPU training (RTX 4090)',
    recommended: 'QLoRA',
    reason: 'Enables training 70B models on 24GB GPU'
  },
  {
    useCase: 'Minimal memory budget',
    recommended: 'IA3',
    reason: 'Only 0.01% trainable parameters'
  },
  {
    useCase: 'Multiple task adapters',
    recommended: 'LoRA',
    reason: 'Easy to swap adapters, minimal storage'
  },
  {
    useCase: 'Real-time inference critical',
    recommended: 'LoRA or IA3',
    reason: 'Minimal latency overhead'
  }
]

recommendations.forEach(rec => {
  console.log(\`\${rec.useCase}:\`)
  console.log(\`  â†’ \${rec.recommended}\`)
  console.log(\`  Reason: \${rec.reason}\\n\`)
})
`}
/>

---

## âš–ï¸ Real-World Challenge: The Legal Language Specialist

**The Problem**: A global law firm (1,200+ attorneys) needs an AI assistant for **high-volume contract review** - analyzing 5,000+ contracts per month. The model must speak in high-fidelity "Legalese," follow strict formatting standards, and understand complex legal precedents.

**Current State**: GPT-5 / Claude Opus 4.5 provide excellent quality but are **too expensive** at scale:
- Cost per contract: $2.40 (8,000 tokens @ $0.0003/token)
- Monthly cost: 5,000 contracts Ã— $2.40 = **$12,000/month**
- Annual: **$144,000** just for API costs

**Business Constraint**: Budget limit is $2,000/month ($0.40 per contract).

### Architectural Solution: QLoRA Fine-Tuning on Legal Dataset

**Strategy**: Take an open-source model (Llama 3 70B) and fine-tune it on a curated dataset of 10,000 "golden" legal contracts using QLoRA. Achieve frontier-model accuracy for legal tasks while reducing inference costs by 70%.

#### Step 1: Dataset Curation (The Foundation)

```typescript
interface LegalContract {
  id: string
  contract_type: 'NDA' | 'MSA' | 'Employment' | 'M&A' | 'IP_License' | 'Real_Estate'
  jurisdiction: string
  parties: string[]
  key_terms: Record<string, any>
  full_text: string
  legal_analysis: string  // Human attorney review
  precedents_cited: string[]
  risk_assessment: {
    score: number  // 1-10
    concerns: string[]
    recommendations: string[]
  }
}

// Curate 10,000 high-quality legal contracts
const goldenDataset: LegalContract[] = [
  // Example 1: NDA with standard terms
  {
    id: 'NDA-2024-0001',
    contract_type: 'NDA',
    jurisdiction: 'Delaware',
    parties: ['TechCorp Inc.', 'Innovate LLC'],
    key_terms: {
      effective_date: '2024-01-15',
      duration: '5 years',
      confidential_info_scope: 'Technical specifications, business plans, customer data'
    },
    full_text: `MUTUAL NON-DISCLOSURE AGREEMENT

This Mutual Non-Disclosure Agreement ("Agreement") is entered into as of January 15, 2024 ("Effective Date")...`,
    legal_analysis: `This is a mutual NDA with balanced protections for both parties. Key observations:

1. **Scope of Confidential Information**: Broadly defined but includes necessary carve-outs for (a) publicly available information, (b) independently developed information, (c) rightfully received from third parties.

2. **Duration**: 5-year confidentiality term is standard for technical information. Consider perpetual for trade secrets.

3. **Remedies**: Includes equitable relief provision, essential given inadequacy of monetary damages for breach.

4. **Notable Omissions**: No explicit data destruction provision post-term. Recommend adding: "Upon termination, receiving party shall destroy or return all Confidential Information within 30 days."

5. **Risk Assessment**: Low risk. Standard protective terms. No unusual indemnification or liability clauses.`,
    precedents_cited: [
      'Warner-Lambert Pharm. Co. v. John J. Reynolds, Inc., 178 F. Supp. 655 (S.D.N.Y. 1959)',
      'E.I. du Pont de Nemours & Co. v. Christopher, 431 F.2d 1012 (5th Cir. 1970)'
    ],
    risk_assessment: {
      score: 2,
      concerns: ['Missing data destruction clause'],
      recommendations: ['Add explicit data destruction timeline', 'Include audit rights for compliance verification']
    }
  }
  // ... 9,999 more contracts with attorney analysis
]

// Convert to fine-tuning format
interface FineTuningExample {
  messages: Array<{
    role: 'system' | 'user' | 'assistant'
    content: string
  }>
}

function convertToFineTuningFormat(contract: LegalContract): FineTuningExample {
  return {
    messages: [
      {
        role: 'system',
        content: `You are a senior legal associate at a top-tier law firm. Analyze contracts with precision, cite relevant precedents, and provide actionable risk assessments. Use formal legal terminology and proper citation format (Bluebook).`
      },
      {
        role: 'user',
        content: `Please analyze this ${contract.contract_type} contract:

${contract.full_text}

Provide:
1. Summary of key terms
2. Legal analysis (risks, protections, notable clauses)
3. Precedents (if applicable)
4. Risk score (1-10) with specific concerns
5. Recommendations for negotiation`
      },
      {
        role: 'assistant',
        content: contract.legal_analysis
      }
    ]
  }
}

// Export to JSONL for fine-tuning
import fs from 'fs'

const fineTuningData = goldenDataset.map(convertToFineTuningFormat)

fs.writeFileSync(
  'legal-contracts-training.jsonl',
  fineTuningData.map(example => JSON.stringify(example)).join('\n')
)

console.log(`âœ… Curated ${goldenDataset.length} legal contracts`)
console.log(`ğŸ“ Training examples: ${fineTuningData.length}`)
console.log(`ğŸ“Š Distribution:`)
console.log(`   - NDAs: ${goldenDataset.filter(c => c.contract_type === 'NDA').length}`)
console.log(`   - MSAs: ${goldenDataset.filter(c => c.contract_type === 'MSA').length}`)
console.log(`   - Employment: ${goldenDataset.filter(c => c.contract_type === 'Employment').length}`)
console.log(`   - M&A: ${goldenDataset.filter(c => c.contract_type === 'M&A').length}`)
```

**Dataset Quality Standards**:
- All contracts reviewed by licensed attorneys (JD + 5+ years experience)
- Mix of contract types (balanced distribution)
- Include both standard and edge-case contracts
- Precedent citations verified (Bluebook format)
- Risk assessments calibrated across reviewers

#### Step 2: QLoRA Fine-Tuning (Python Implementation)

```python
# fine_tune_legal_model.py
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from datasets import load_dataset
import torch

# Configuration
model_name = "meta-llama/Meta-Llama-3-70B"  # 70B for legal precision
output_dir = "./legal-llama-70b-qlora"

# QLoRA configuration: 4-bit quantization
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,  # Nested quantization for 33% memory savings
    bnb_4bit_quant_type="nf4",       # NormalFloat4 (optimal for fine-tuning)
    bnb_4bit_compute_dtype=torch.bfloat16
)

# Load base model in 4-bit
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# LoRA configuration
lora_config = LoraConfig(
    r=16,  # Rank (higher for legal precision)
    lora_alpha=32,  # Scaling factor (2x rank)
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj",  # Attention
        "gate_proj", "up_proj", "down_proj"      # MLP
    ],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

# Prepare model for training
model = prepare_model_for_kbit_training(model)
model = get_peft_model(model, lora_config)

# Print trainable parameters
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
total_params = sum(p.numel() for p in model.parameters())

print(f"Trainable parameters: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)")
# Output: Trainable parameters: 335,544,320 (0.48% of 70B)

# Load dataset
dataset = load_dataset('json', data_files='legal-contracts-training.jsonl', split='train')
dataset = dataset.train_test_split(test_size=0.1)

# Training arguments
from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(
    output_dir=output_dir,
    num_train_epochs=3,
    per_device_train_batch_size=1,  # Fits in 24 GB
    gradient_accumulation_steps=16,  # Effective batch size = 16
    gradient_checkpointing=True,
    learning_rate=2e-4,
    lr_scheduler_type="cosine",
    warmup_ratio=0.03,
    logging_steps=10,
    save_strategy="epoch",
    evaluation_strategy="epoch",
    fp16=True,
    optim="paged_adamw_32bit",  # Memory-efficient optimizer
    report_to="wandb",  # Track training metrics
    run_name="legal-llama-70b-qlora"
)

# Start training
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset['train'],
    eval_dataset=dataset['test'],
    tokenizer=tokenizer
)

print("ğŸš€ Starting QLoRA fine-tuning...")
trainer.train()

# Save LoRA adapters (only 100 MB!)
model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)

print(f"âœ… Training complete! LoRA adapters saved to {output_dir}")
```

**Training Metrics**:
- Hardware: 1x A100 (80 GB) or 2x RTX 4090 (24 GB each)
- Training time: 18 hours for 3 epochs on 10K examples
- Cost: $72 (18 hours Ã— $4/hour on cloud GPU)
- Adapter size: 128 MB (vs 140 GB for full model)

#### Step 3: Inference and Deployment

```typescript
// deploy-legal-model.ts
import { HfInference } from '@huggingface/inference'

const hf = new HfInference(process.env.HUGGINGFACE_TOKEN)

interface LegalAnalysisRequest {
  contract_text: string
  contract_type: string
  analysis_depth: 'summary' | 'full' | 'risk_only'
}

interface LegalAnalysisResponse {
  summary: string
  key_terms: Record<string, any>
  risk_score: number
  concerns: string[]
  recommendations: string[]
  precedents: string[]
  processing_time_ms: number
  cost: number
}

async function analyzeLegalContract(
  request: LegalAnalysisRequest
): Promise<LegalAnalysisResponse> {
  const startTime = Date.now()

  const prompt = `Analyze this ${request.contract_type} contract:

${request.contract_text}

Provide:
1. Summary of key terms
2. Legal analysis (risks, protections, notable clauses)
3. Precedents (if applicable)
4. Risk score (1-10) with specific concerns
5. Recommendations for negotiation`

  // Inference on fine-tuned model
  const response = await hf.textGeneration({
    model: 'your-org/legal-llama-70b-qlora',  // Deployed on HuggingFace
    inputs: prompt,
    parameters: {
      max_new_tokens: 2000,
      temperature: 0.1,  // Low temperature for factual legal analysis
      top_p: 0.9,
      do_sample: true
    }
  })

  const analysis = response.generated_text

  // Parse structured output (would use JSON mode in production)
  const processingTime = Date.now() - startTime

  // Calculate cost (self-hosted or HuggingFace Inference API)
  const inputTokens = Math.ceil(request.contract_text.length / 4)
  const outputTokens = Math.ceil(analysis.length / 4)
  const cost = (inputTokens * 0.00006 + outputTokens * 0.00012)  // ~70% cheaper than GPT-5

  return {
    summary: extractSection(analysis, 'Summary'),
    key_terms: parseKeyTerms(analysis),
    risk_score: extractRiskScore(analysis),
    concerns: extractConcerns(analysis),
    recommendations: extractRecommendations(analysis),
    precedents: extractPrecedents(analysis),
    processing_time_ms: processingTime,
    cost
  }
}

// Helper functions (simplified)
function extractSection(text: string, section: string): string {
  const regex = new RegExp(`${section}:?\\s*([\\s\\S]*?)(?=\\n\\d+\\.|$)`, 'i')
  return text.match(regex)?.[1].trim() || ''
}

function extractRiskScore(text: string): number {
  const match = text.match(/risk score:?\s*(\d+)/i)
  return match ? parseInt(match[1]) : 5
}

function extractConcerns(text: string): string[] {
  const concernsSection = extractSection(text, 'Concerns|Risks')
  return concernsSection.split('\n').filter(line => line.trim().startsWith('-')).map(line => line.trim())
}

function extractRecommendations(text: string): string[] {
  const recsSection = extractSection(text, 'Recommendations')
  return recsSection.split('\n').filter(line => line.trim().startsWith('-')).map(line => line.trim())
}

function extractPrecedents(text: string): string[] {
  const precedentsSection = extractSection(text, 'Precedents')
  return precedentsSection.split('\n').filter(line => line.includes('v.') || line.includes('F.'))
}

function parseKeyTerms(text: string): Record<string, any> {
  // In production, use structured outputs or JSON mode
  return {
    effective_date: 'Extracted date',
    parties: ['Party A', 'Party B'],
    term: 'Extracted duration'
  }
}
```

### Production Metrics: Legal Language Specialist

**Before Fine-Tuning (GPT-5 Baseline)**:
- Cost per contract: $2.40 (8,000 tokens)
- Monthly cost (5,000 contracts): $12,000
- Annual cost: $144,000
- Quality: 94% attorney approval rate
- Latency: 4.2s average

**After QLoRA Fine-Tuning (Legal Llama 70B)**:
- Cost per contract: $0.72 (70% reduction)
- Monthly cost (5,000 contracts): $3,600
- Annual cost: $43,200 (savings: **$100,800/year**)
- Quality: 93% attorney approval rate (1% drop, acceptable)
- Latency: 3.8s average (10% faster)

**ROI Calculation**:
- Fine-tuning cost: $72 (one-time)
- First month savings: $12,000 - $3,600 = $8,400
- **Payback period**: 0.3 days (instant ROI)
- Annual ROI: 1,400x (!!!)

### Key Architectural Decisions

**1. Why 70B over 8B?**
```typescript
// Legal domain requires high precision
const modelComparison = [
  { model: 'Llama 3 8B', accuracy: 78%, cost_per_contract: 0.24 },  // Too many errors
  { model: 'Llama 3 70B', accuracy: 93%, cost_per_contract: 0.72 },  // Sweet spot âœ…
  { model: 'GPT-5', accuracy: 94%, cost_per_contract: 2.40 }  // Gold standard but expensive
]

// Verdict: 70B hits quality bar at 70% cost savings
```

**2. Why QLoRA over Full Fine-Tuning?**
- Full fine-tuning 70B: Requires 8x A100 (640 GB VRAM) = $192/hour
- QLoRA: Requires 1x A100 (80 GB VRAM) = $4/hour
- **Cost savings: 98%**

**3. Dataset Size: Why 10,000 Examples?**
```typescript
const datasetScaling = [
  { examples: 1_000, accuracy: 85%, overfitting_risk: 'high' },
  { examples: 10_000, accuracy: 93%, overfitting_risk: 'low' },    // âœ… Sweet spot
  { examples: 100_000, accuracy: 94%, overfitting_risk: 'low' }   // Diminishing returns
]

// 10K provides 93% accuracy at reasonable curation cost ($50K attorney time)
// 100K would cost $500K to curate for 1% accuracy gain
```

**4. Rank Selection (r=16 vs r=8)**:
- Legal domain is complex (precedents, formal language, multi-hop reasoning)
- r=8: 91% accuracy
- r=16: 93% accuracy (worth 2Ã— memory overhead)
- r=32: 93.2% accuracy (diminishing returns)

### Comparison: Fine-Tuning vs Prompt Engineering

| Approach | Monthly Cost | Quality | Latency | When to Use |
|----------|--------------|---------|---------|-------------|
| **Prompt Engineering (GPT-5)** | $12,000 | 94% | 4.2s | Low volume (<500/month) |
| **Fine-Tuned Open-Source (QLoRA)** | $3,600 | 93% | 3.8s | High volume (5,000+/month) âœ… |
| **RAG + Prompt Engineering** | $8,000 | 92% | 5.1s | Need real-time case law updates |

**Verdict**: Fine-tuning wins for high-volume, specialized domains with stable requirements.

### Real-World Impact: Contract Review Automation

**Business Metrics**:
- **Cost savings**: $100,800/year (70% reduction)
- **Throughput**: 5,000 contracts/month â†’ 15,000/month (3x capacity with same budget)
- **Attorney time freed**: 200 hours/month (redirected to high-value negotiations)
- **Error rate**: <1% false positives (flagged for human review)
- **Client satisfaction**: 4.8/5 (same as human-only review)

**Strategic Advantage**:
- Law firm can now offer **flat-fee contract review** (predictable AI costs vs hourly attorney rates)
- Competitive differentiation: 48-hour turnaround vs industry standard 2 weeks
- Upsell opportunity: "Premium human review" tier at higher margin

---

## Production Deployment

### Multi-Adapter Management

One powerful feature of LoRA: **swap adapters at runtime** without reloading the base model.

<CodePlayground
  title="Multi-Adapter Deployment"
  description="Deploy multiple LoRA adapters with a single base model. Watch memory efficiency in action!"
  exerciseType="multi-adapter"
  code={`// Conceptual TypeScript showing adapter management
// Actual implementation uses HuggingFace PEFT library

interface LoRAAdapter {
  id: string
  name: string
  task: string
  filePath: string
  accuracy: number
  lastUpdated: Date
}

class MultiAdapterManager {
  private baseModel: any // Base Llama 3 model (loaded once)
  private adapters: Map<string, LoRAAdapter> = new Map()
  private currentAdapter: string | null = null

  constructor() {
    console.log('Loading base model: Llama 3 8B (4-bit)...')
    console.log('Memory: 4.5 GB\\n')
    // In production: this.baseModel = loadQuantizedModel('llama-3-8b')
  }

  registerAdapter(adapter: LoRAAdapter): void {
    this.adapters.set(adapter.id, adapter)
    console.log(\`Registered adapter: \${adapter.name}\`)
    console.log(\`  Task: \${adapter.task}\`)
    console.log(\`  Size: ~100 MB\`)
    console.log(\`  Total adapters: \${this.adapters.size}\\n\`)
  }

  async switchAdapter(adapterId: string): Promise<void> {
    if (!this.adapters.has(adapterId)) {
      throw new Error(\`Adapter \${adapterId} not found\`)
    }

    const adapter = this.adapters.get(adapterId)!

    console.log(\`Switching to adapter: \${adapter.name}\`)
    console.log(\`  Loading LoRA weights: ~100 MB\`)
    console.log(\`  Switch time: 50-100ms\`)

    // Simulated switch
    await new Promise(resolve => setTimeout(resolve, 75))

    this.currentAdapter = adapterId
    console.log(\`  âœ“ Active adapter: \${adapter.name}\\n\`)
  }

  async inference(input: string): Promise<string> {
    if (!this.currentAdapter) {
      throw new Error('No adapter loaded')
    }

    const adapter = this.adapters.get(this.currentAdapter)!
    console.log(\`Inference with \${adapter.name}:\`)
    console.log(\`  Input: "\${input}"\`)

    // Simulated inference
    const output = \`[Processed by \${adapter.task} adapter]\`
    console.log(\`  Output: "\${output}"\\n\`)

    return output
  }

  getMemoryUsage(): {
    baseModel: number
    adapters: number
    total: number
  } {
    return {
      baseModel: 4500, // MB
      adapters: this.adapters.size * 100, // MB
      total: 4500 + (this.adapters.size * 100)
    }
  }
}

// Example usage
async function demonstrateMultiAdapter() {
  const manager = new MultiAdapterManager()

  // Register multiple task-specific adapters
  const adapters: LoRAAdapter[] = [
    {
      id: 'email-classifier',
      name: 'Email Classification',
      task: 'Classify customer emails into categories',
      filePath: './adapters/email-classifier',
      accuracy: 0.96,
      lastUpdated: new Date('2025-02-01')
    },
    {
      id: 'sql-generator',
      name: 'SQL Generation',
      task: 'Convert natural language to SQL',
      filePath: './adapters/sql-generator',
      accuracy: 0.92,
      lastUpdated: new Date('2025-02-03')
    },
    {
      id: 'code-reviewer',
      name: 'Code Review',
      task: 'Review code and suggest improvements',
      filePath: './adapters/code-reviewer',
      accuracy: 0.89,
      lastUpdated: new Date('2025-02-05')
    }
  ]

  adapters.forEach(adapter => manager.registerAdapter(adapter))

  // Use different adapters for different requests
  await manager.switchAdapter('email-classifier')
  await manager.inference('I need help with my billing')

  await manager.switchAdapter('sql-generator')
  await manager.inference('Show me all orders from last week')

  await manager.switchAdapter('code-reviewer')
  await manager.inference('Review: function foo() { var x = 1; return x }')

  // Memory usage report
  const memory = manager.getMemoryUsage()
  console.log('Memory Usage Report:')
  console.log(\`  Base model: \${memory.baseModel} MB\`)
  console.log(\`  LoRA adapters: \${memory.adapters} MB (\${adapters.length} adapters)\`)
  console.log(\`  Total: \${memory.total} MB\\n\`)

  console.log('vs Full Fine-Tuning:')
  console.log(\`  3 separate models: \${3 * 16000} MB (16 GB each)\`)
  console.log(\`  Savings: \${((1 - memory.total / (3 * 16000)) * 100).toFixed(1)}% memory reduction\`)
}

demonstrateMultiAdapter()
`}
/>

### HuggingFace Deployment

Deploy LoRA models to HuggingFace Inference Endpoints for production:

```typescript
// 1. Push adapter to HuggingFace Hub
// (Done via huggingface-cli in terminal)
// huggingface-cli upload your-username/llama3-email-classifier ./lora-adapter

// 2. Create inference endpoint
interface InferenceEndpoint {
  name: string
  model: string
  adapter: string
  hardware: 'cpu' | 'gpu-small' | 'gpu-large'
  scaling: {
    min: number
    max: number
  }
}

const endpoint: InferenceEndpoint = {
  name: 'email-classifier-prod',
  model: 'meta-llama/Meta-Llama-3-8B',
  adapter: 'your-username/llama3-email-classifier',
  hardware: 'gpu-small', // $0.60/hour
  scaling: {
    min: 1, // Always-on
    max: 5  // Scale to 5 instances under load
  }
}

// 3. Call inference endpoint
async function callInferenceEndpoint(text: string): Promise<string> {
  const response = await fetch(
    'https://api-inference.huggingface.co/models/your-username/llama3-email-classifier',
    {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${process.env.HF_TOKEN}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        inputs: text,
        parameters: {
          max_new_tokens: 50,
          temperature: 0.3
        }
      })
    }
  )

  const data = await response.json()
  return data[0].generated_text
}
```

## Production Metrics

### Training Performance

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Method             â”‚ Training Timeâ”‚ GPU Memory   â”‚ Cost/Epoch   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Full Fine-Tuning   â”‚ 4 hours      â”‚ 120 GB       â”‚ $96          â”‚
â”‚ LoRA (FP16)        â”‚ 4 hours      â”‚ 24 GB        â”‚ $4           â”‚
â”‚ QLoRA (4-bit)      â”‚ 4 hours      â”‚ 14 GB        â”‚ $2           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Dataset: 10,000 examples, Llama 3 8B
GPU: A100 ($24/hour), RTX 4090 ($1/hour)
```

### Inference Performance

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Configuration      â”‚ Latency      â”‚ Throughput   â”‚ Cost/1K Tok  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Base Model (FP16)  â”‚ 45 ms        â”‚ 22 tok/sec   â”‚ $0.002       â”‚
â”‚ LoRA (FP16)        â”‚ 46 ms (+2%)  â”‚ 21 tok/sec   â”‚ $0.002       â”‚
â”‚ QLoRA (4-bit)      â”‚ 52 ms (+15%) â”‚ 19 tok/sec   â”‚ $0.001       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Batch size: 1, Sequence length: 512 tokens
```

## Common Pitfalls

### 1. Wrong Target Modules
**Problem**: Not adapting the right layers
```python
# âŒ Bad: Only adapt output layer
target_modules = ["lm_head"]  # Insufficient for most tasks

# âœ… Good: Adapt all attention projections
target_modules = ["q_proj", "k_proj", "v_proj", "o_proj"]
```

### 2. Rank Too Low or Too High
**Problem**: Rank doesn't match task complexity
```python
# âŒ Bad: r=4 for complex generation task
lora_config = LoraConfig(r=4)  # Underfits

# âŒ Bad: r=128 for simple classification
lora_config = LoraConfig(r=128)  # Overfits, wastes memory

# âœ… Good: Match rank to task
# Classification: r=8
# Generation: r=16
# Complex domain adaptation: r=32
```

### 3. Forgetting to Merge Adapters
**Problem**: Shipping base model + adapter separately
```python
# For deployment, merge adapter into base model:
from peft import PeftModel

base_model = AutoModelForCausalLM.from_pretrained("meta-llama/Meta-Llama-3-8B")
model = PeftModel.from_pretrained(base_model, "lora-adapter")

# Merge and save as single model
merged_model = model.merge_and_unload()
merged_model.save_pretrained("llama3-merged")
```

### 4. Not Using Gradient Checkpointing
**Problem**: Running out of memory during training
```python
# âœ… Always enable for large models
training_args = TrainingArguments(
    gradient_checkpointing=True,  # Trades compute for memory
    # Saves ~40% memory at ~15% slower training
)
```

## Key Takeaways

### LoRA Fundamentals
- **Core Idea**: Train low-rank adapter matrices instead of full model
- **Memory Savings**: 95-99% fewer parameters to update
- **Cost Reduction**: 96% cheaper training ($4 vs $96 per epoch)
- **Flexibility**: Multiple task-specific adapters per base model

### QLoRA Advantages
- **Enables consumer GPU training**: 70B models on 24GB RTX 4090
- **4-bit quantization**: Reduces base model memory by 75%
- **Accuracy**: 94-97% of full fine-tuning performance
- **Production ready**: Used by Meta, Stability AI, HuggingFace

### PEFT Method Selection
- **LoRA**: General-purpose, best accuracy/memory tradeoff
- **QLoRA**: When GPU memory is limited (<40GB)
- **Prefix Tuning**: Extremely low memory (<10MB overhead)
- **IA3**: Fastest inference, minimal parameters (0.01%)

### Production Deployment
- **Multi-adapter pattern**: 1 base model + N adapters (100MB each)
- **Adapter swapping**: 50-100ms to switch tasks
- **HuggingFace Endpoints**: $0.60/hour for GPU inference
- **Inference latency**: +0-2% vs base model (LoRA), +5-10% (QLoRA)

### Best Practices
- Use r=8 for most tasks, r=16 for complex domains
- Target all attention layers (q/k/v/o projections)
- Enable gradient checkpointing for memory savings
- Merge adapters for deployment (single model)
- Monitor for catastrophic forgetting on base capabilities

## Further Reading

- [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685) - Original paper (Microsoft 2021)
- [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314) - University of Washington 2023
- [HuggingFace PEFT Documentation](https://huggingface.co/docs/peft) - Official library docs
- [Practical Guide to LoRA](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms) - Sebastian Raschka
- [LoRA vs Full Fine-Tuning](https://www.anyscale.com/blog/fine-tuning-llms-lora-or-full-parameter-an-in-depth-analysis-with) - Anyscale benchmarks
