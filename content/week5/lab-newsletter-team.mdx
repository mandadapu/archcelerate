---
title: "Lab: Auto-Research & Newsletter Team"
week: 5
lab: true
description: "Build a multi-agent newsroom using Supervisor pattern with state management, error routing, and self-healing"
estimatedMinutes: 180
objectives:
  - Implement three specialized agents (Hunter, Fact-Checker, Writer)
  - Build Supervisor with conditional routing and error handling
  - Add state management with checkpointing
  - Implement "zero-waste" execution with self-healing prompts
---

# Lab: The Auto-Research & Newsletter Team

Build a multi-agent newsroom that produces high-quality technical newsletters with **zero human intervention** while staying under budget.

## Lab Overview

Instead of a simple chatbot, you'll build a **Multi-Agent Newsroom** with three specialists:

1. **Agent 1 (The Hunter)**: Searches for the top 3 trending stories in AI Architecture
2. **Agent 2 (The Fact-Checker)**: Verifies the sources and filters out "hype" vs. "reality"
3. **Agent 3 (The Writer)**: Takes the verified data and writes a professional technical summary
4. **The Orchestrator**: Uses a Supervisor Pattern to manage handoffs between these three

### Success Criteria

‚úÖ Produces newsletter from topic in under 10 API calls
‚úÖ Never publishes unverified information
‚úÖ Self-corrects when Fact-Checker finds issues
‚úÖ Stays under $0.20 per newsletter
‚úÖ Full audit trail of all agent decisions

---

## Part 1: State Schema (The Foundation)

The State is the "Single Source of Truth" for the entire workflow.

### Exercise 1.1: Define the State Schema

```typescript
// src/lab5/types.ts
export interface NewsletterState {
  // Input
  topic: string

  // Hunter outputs
  raw_articles: Array<{
    title: string
    source: string
    summary: string
    url?: string
  }>
  hunter_iteration: number

  // Fact-Checker outputs
  fact_check_score: number
  verified_articles: Array<{
    title: string
    source: string
    summary: string
    credibility: 'high' | 'medium' | 'low'
  }>
  rejected_articles: string[]
  correction_prompt?: string

  // Writer outputs
  draft_newsletter: string
  final_newsletter: string

  // Orchestration
  iteration_count: number
  max_iterations: number
  total_cost: number
  audit_trail: Array<{
    step: string
    agent: string
    timestamp: Date
    tokens_used: number
  }>
}

export const createInitialState = (topic: string): NewsletterState => ({
  topic,
  raw_articles: [],
  hunter_iteration: 0,
  fact_check_score: 0,
  verified_articles: [],
  rejected_articles: [],
  draft_newsletter: '',
  final_newsletter: '',
  iteration_count: 0,
  max_iterations: 3,
  total_cost: 0,
  audit_trail: []
})
```

**‚úÖ Checkpoint**: Your state schema should have:
- Clear separation between agent outputs
- Iteration counters for cycle detection
- Cost tracking
- Audit trail

---

## Part 2: The Hunter Agent (The Researcher)

### Exercise 2.1: Implement the Hunter

```typescript
// src/lab5/agents/hunter.ts
import Anthropic from '@anthropic-ai/sdk'

const anthropic = new Anthropic({ apiKey: process.env.ANTHROPIC_API_KEY })

export async function hunterAgent(
  state: NewsletterState
): Promise<Partial<NewsletterState>> {
  console.log(`\nüîç Hunter: Searching for articles on "${state.topic}"`)
  console.log(`Hunter iteration: ${state.hunter_iteration + 1}`)

  // Build prompt with feedback if this is a retry
  const prompt = state.correction_prompt
    ? `Find articles on "${state.topic}".

FEEDBACK from Fact-Checker:
${state.correction_prompt}

Adjust your search strategy based on this feedback. Find 3 high-quality sources.`
    : `Find the top 3 trending articles on "${state.topic}" in AI Architecture.

Focus on:
- Recent publications (2025-2026)
- Technical depth (not just marketing)
- Credible sources (academic, established tech companies, respected engineers)

Output JSON:
[
  {
    "title": "Article Title",
    "source": "Source Name",
    "summary": "Brief summary",
    "url": "https://..."
  }
]`

  const response = await anthropic.messages.create({
    model: 'claude-3-5-sonnet-20240620',
    max_tokens: 2048,
    messages: [{
      role: 'user',
      content: prompt
    }]
  })

  // Parse response
  const text = response.content[0].text
  const jsonMatch = text.match(/\[[\s\S]*\]/)
  const articles = jsonMatch ? JSON.parse(jsonMatch[0]) : []

  // Track cost
  const tokensUsed = response.usage.input_tokens + response.usage.output_tokens
  const cost = (response.usage.input_tokens * 0.000003) +
               (response.usage.output_tokens * 0.000015)

  console.log(`  Found ${articles.length} articles`)
  console.log(`  Cost: $${cost.toFixed(4)} (${tokensUsed} tokens)`)

  return {
    raw_articles: articles,
    hunter_iteration: state.hunter_iteration + 1,
    total_cost: state.total_cost + cost,
    audit_trail: [
      ...state.audit_trail,
      {
        step: 'hunter',
        agent: 'Hunter',
        timestamp: new Date(),
        tokens_used: tokensUsed
      }
    ],
    correction_prompt: undefined  // Clear feedback after using it
  }
}
```

**Key Features**:
- ‚úÖ Uses `correction_prompt` for self-healing retries
- ‚úÖ Tracks token usage and cost
- ‚úÖ Appends to audit trail
- ‚úÖ Clears correction prompt after use

**Test Your Hunter**:

```typescript
// src/lab5/test-hunter.ts
import { createInitialState } from './types'
import { hunterAgent } from './agents/hunter'

const state = createInitialState('RAG architecture patterns for production')
const result = await hunterAgent(state)

console.log('Raw Articles:', result.raw_articles)
console.log('Total Cost:', result.total_cost)
```

---

## Part 3: The Fact-Checker Agent (The Verifier)

### Exercise 3.1: Implement Quality Scoring

```typescript
// src/lab5/agents/fact-checker.ts
import Anthropic from '@anthropic-ai/sdk'

const anthropic = new Anthropic({ apiKey: process.env.ANTHROPIC_API_KEY })

interface FactCheckResult {
  verified: any[]
  rejected: string[]
  score: number
  feedback: string
}

export async function factCheckerAgent(
  state: NewsletterState
): Promise<Partial<NewsletterState>> {
  console.log(`\n‚úÖ Fact-Checker: Verifying ${state.raw_articles.length} articles`)

  const prompt = `You are a technical fact-checker. Verify these articles for quality and credibility:

${JSON.stringify(state.raw_articles, null, 2)}

For each article, check:
1. Source credibility (is it a known, reputable source?)
2. Technical accuracy (does the summary make sense?)
3. Recency (is it relevant to 2025-2026?)
4. Hype detection (is it marketing fluff or real technical content?)

Output JSON:
{
  "verified": [
    {
      "title": "...",
      "source": "...",
      "summary": "...",
      "credibility": "high" | "medium" | "low"
    }
  ],
  "rejected": ["Title of rejected article"],
  "score": 0.0-1.0,
  "feedback": "Specific feedback on what was wrong with rejected articles"
}`

  const response = await anthropic.messages.create({
    model: 'claude-3-5-sonnet-20240620',
    max_tokens: 2048,
    messages: [{ role: 'user', content: prompt }]
  })

  const text = response.content[0].text
  const jsonMatch = text.match(/\{[\s\S]*\}/)
  const result: FactCheckResult = jsonMatch
    ? JSON.parse(jsonMatch[0])
    : { verified: [], rejected: [], score: 0, feedback: '' }

  // Track cost
  const tokensUsed = response.usage.input_tokens + response.usage.output_tokens
  const cost = (response.usage.input_tokens * 0.000003) +
               (response.usage.output_tokens * 0.000015)

  console.log(`  Verified: ${result.verified.length} articles`)
  console.log(`  Rejected: ${result.rejected.length} articles`)
  console.log(`  Quality Score: ${result.score}`)
  console.log(`  Cost: $${cost.toFixed(4)}`)

  return {
    fact_check_score: result.score,
    verified_articles: result.verified,
    rejected_articles: result.rejected,
    correction_prompt: result.score < 0.8 ? result.feedback : undefined,
    total_cost: state.total_cost + cost,
    audit_trail: [
      ...state.audit_trail,
      {
        step: 'fact_check',
        agent: 'Fact-Checker',
        timestamp: new Date(),
        tokens_used: tokensUsed
      }
    ]
  }
}
```

**Key Features**:
- ‚úÖ Scores article quality (0.0-1.0)
- ‚úÖ Provides specific feedback for rejected articles
- ‚úÖ Sets `correction_prompt` if score is low (< 0.8)

---

## Part 4: The Writer Agent (The Creator)

### Exercise 4.1: Token-Optimized Newsletter Generation

```typescript
// src/lab5/agents/writer.ts
import Anthropic from '@anthropic-ai/sdk'

const anthropic = new Anthropic({ apiKey: process.env.ANTHROPIC_API_KEY })

export async function writerAgent(
  state: NewsletterState
): Promise<Partial<NewsletterState>> {
  console.log(`\n‚úçÔ∏è Writer: Creating newsletter from ${state.verified_articles.length} articles`)

  // ‚úÖ Token Trimming: Only pass verified articles, NOT raw_articles
  const articlesText = state.verified_articles
    .map((a, i) => `${i + 1}. ${a.title}\nSource: ${a.source}\n${a.summary}`)
    .join('\n\n')

  const prompt = `Write a professional technical newsletter for AI architects based on these verified articles:

${articlesText}

Requirements:
- 400-500 words
- Markdown format
- Technical depth (not marketing)
- Clear section headers
- Actionable insights

Title the newsletter: "${state.topic} - This Week"`

  const response = await anthropic.messages.create({
    model: 'claude-3-5-sonnet-20240620',
    max_tokens: 2048,
    messages: [{ role: 'user', content: prompt }]
  })

  const newsletter = response.content[0].text

  // Track cost
  const tokensUsed = response.usage.input_tokens + response.usage.output_tokens
  const cost = (response.usage.input_tokens * 0.000003) +
               (response.usage.output_tokens * 0.000015)

  console.log(`  Newsletter: ${newsletter.length} chars`)
  console.log(`  Cost: $${cost.toFixed(4)}`)

  return {
    final_newsletter: newsletter,
    total_cost: state.total_cost + cost,
    audit_trail: [
      ...state.audit_trail,
      {
        step: 'writer',
        agent: 'Writer',
        timestamp: new Date(),
        tokens_used: tokensUsed
      }
    ]
  }
}
```

**Token Optimization**:
```typescript
// ‚ùå DON'T DO THIS (wastes 3,000+ tokens)
const prompt = `
Raw articles: ${JSON.stringify(state.raw_articles)}
Verified articles: ${JSON.stringify(state.verified_articles)}
`

// ‚úÖ DO THIS (saves 60% tokens)
const prompt = `
Verified articles: ${state.verified_articles.map(a => a.summary).join('\n')}
`
```

---

## Part 5: The Supervisor (The Router)

### Exercise 5.1: Error Routing Table

```typescript
// src/lab5/supervisor.ts

type RoutingDecision =
  | 'to_hunter'
  | 'to_fact_checker'
  | 'to_writer'
  | 'to_human_escalation'
  | 'complete'

interface ErrorRoute {
  category: 'transient' | 'logic_quality' | 'budget_safety' | 'input_error'
  action: RoutingDecision
  reason: string
}

export function routeAfterHunter(state: NewsletterState): RoutingDecision {
  // Check if Hunter found articles
  if (state.raw_articles.length === 0) {
    if (state.hunter_iteration >= 2) {
      return 'to_human_escalation'  // Can't find articles after 2 tries
    }
    return 'to_hunter'  // Retry
  }

  // Proceed to fact-checking
  return 'to_fact_checker'
}

export function routeAfterFactCheck(state: NewsletterState): RoutingDecision {
  console.log(`\nüö¶ Supervisor: Routing decision`)
  console.log(`  Quality Score: ${state.fact_check_score}`)
  console.log(`  Iteration: ${state.iteration_count}`)

  // BUDGET/SAFETY: Max iterations reached
  if (state.iteration_count >= state.max_iterations) {
    console.log(`  ‚ùå Max iterations (${state.max_iterations}) reached`)
    return 'to_human_escalation'
  }

  // BUDGET/SAFETY: Cost exceeded
  if (state.total_cost > 0.20) {
    console.log(`  ‚ùå Budget exceeded ($${state.total_cost.toFixed(4)})`)
    return 'to_human_escalation'
  }

  // LOGIC/QUALITY: Score too low - State Reset needed
  if (state.fact_check_score < 0.8) {
    console.log(`  ‚ö†Ô∏è Low quality score - routing back to Hunter with feedback`)
    console.log(`  Feedback: ${state.correction_prompt}`)
    return 'to_hunter'  // ‚úÖ Self-healing: Retry with feedback
  }

  // SUCCESS: Proceed to Writer
  console.log(`  ‚úÖ High quality - proceeding to Writer`)
  return 'to_writer'
}

export function routeAfterWriter(state: NewsletterState): RoutingDecision {
  return 'complete'
}
```

### Exercise 5.2: The "Self-Healing" Prompt

**Key Pattern**: When routing back for retry, the Supervisor provides **specific feedback** via `correction_prompt`.

```typescript
// Example of correction_prompt from Fact-Checker:
{
  correction_prompt: "The previous articles were biased. Article 2 is from a marketing blog, not a technical source. Please find peer-reviewed or engineering blog sources instead."
}

// Hunter receives this and adjusts strategy:
// "Find articles on RAG architecture..."
// "FEEDBACK from Fact-Checker: The previous articles were biased..."
```

---

## Part 6: The Complete Orchestrator

### Exercise 6.1: Implement the Supervisor Workflow

```typescript
// src/lab5/orchestrator.ts
import { NewsletterState, createInitialState } from './types'
import { hunterAgent } from './agents/hunter'
import { factCheckerAgent } from './agents/fact-checker'
import { writerAgent } from './agents/writer'
import { routeAfterHunter, routeAfterFactCheck, routeAfterWriter } from './supervisor'

export async function runNewsletterWorkflow(topic: string): Promise<NewsletterState> {
  let state = createInitialState(topic)
  let currentStep: 'hunter' | 'fact_checker' | 'writer' | 'complete' = 'hunter'

  console.log(`\nüöÄ Starting Newsletter Workflow`)
  console.log(`Topic: ${topic}`)
  console.log(`Max Iterations: ${state.max_iterations}`)
  console.log(`Budget: $0.20`)
  console.log('=' .repeat(60))

  while (currentStep !== 'complete') {
    // Increment iteration counter
    if (currentStep === 'hunter') {
      state.iteration_count++
      console.log(`\nüìç Iteration ${state.iteration_count}/${state.max_iterations}`)
    }

    // Execute current step
    switch (currentStep) {
      case 'hunter': {
        const update = await hunterAgent(state)
        state = { ...state, ...update }

        const next = routeAfterHunter(state)
        if (next === 'to_human_escalation') {
          throw new Error('Hunter failed to find articles after retries')
        }
        currentStep = next === 'to_fact_checker' ? 'fact_checker' : 'hunter'
        break
      }

      case 'fact_checker': {
        const update = await factCheckerAgent(state)
        state = { ...state, ...update }

        const next = routeAfterFactCheck(state)
        if (next === 'to_human_escalation') {
          console.log('\n‚ö†Ô∏è Escalating to human:')
          console.log(`  Iterations: ${state.iteration_count}`)
          console.log(`  Cost: $${state.total_cost.toFixed(4)}`)
          console.log(`  Quality Score: ${state.fact_check_score}`)
          throw new Error('Workflow escalated to human')
        }

        if (next === 'to_hunter') {
          // ‚úÖ Self-Healing: Go back to Hunter with feedback
          currentStep = 'hunter'
        } else if (next === 'to_writer') {
          currentStep = 'writer'
        }
        break
      }

      case 'writer': {
        const update = await writerAgent(state)
        state = { ...state, ...update }

        currentStep = 'complete'
        break
      }
    }
  }

  console.log('\n' + '='.repeat(60))
  console.log('‚úÖ Workflow Complete!')
  console.log(`Total Cost: $${state.total_cost.toFixed(4)}`)
  console.log(`Total Iterations: ${state.iteration_count}`)
  console.log(`API Calls: ${state.audit_trail.length}`)

  return state
}
```

---

## Part 7: Checkpointing (State Persistence)

### Exercise 7.1: Add Database Checkpoints

```typescript
// src/lab5/checkpoint.ts
import { prisma } from '@/lib/db'
import { NewsletterState } from './types'

export async function saveCheckpoint(
  workflowId: string,
  step: string,
  state: NewsletterState
) {
  await prisma.workflowCheckpoint.create({
    data: {
      workflowId,
      step,
      state: state as any,  // Store as JSON
      timestamp: new Date()
    }
  })
  console.log(`üíæ Checkpoint saved: ${step}`)
}

export async function resumeWorkflow(workflowId: string): Promise<NewsletterState | null> {
  const checkpoint = await prisma.workflowCheckpoint.findFirst({
    where: { workflowId },
    orderBy: { timestamp: 'desc' }
  })

  if (!checkpoint) {
    return null
  }

  console.log(`üìÇ Resuming from checkpoint: ${checkpoint.step}`)
  return checkpoint.state as NewsletterState
}
```

### Exercise 7.2: Update Orchestrator with Checkpoints

```typescript
// In orchestrator.ts
import { saveCheckpoint } from './checkpoint'

export async function runNewsletterWorkflow(
  topic: string,
  workflowId: string = crypto.randomUUID()
): Promise<NewsletterState> {
  let state = createInitialState(topic)

  while (currentStep !== 'complete') {
    // ... execute step ...

    // Save checkpoint after each step
    await saveCheckpoint(workflowId, currentStep, state)
  }

  return state
}
```

---

## Part 8: Testing the Full System

### Exercise 8.1: Run End-to-End Test

```typescript
// src/lab5/test-workflow.ts
import { runNewsletterWorkflow } from './orchestrator'

async function main() {
  try {
    const result = await runNewsletterWorkflow(
      'Vector databases for production RAG systems'
    )

    console.log('\nüì∞ FINAL NEWSLETTER:')
    console.log('='.repeat(60))
    console.log(result.final_newsletter)
    console.log('='.repeat(60))

    console.log('\nüìä METRICS:')
    console.log(`Cost: $${result.total_cost.toFixed(4)}`)
    console.log(`Iterations: ${result.iteration_count}`)
    console.log(`API Calls: ${result.audit_trail.length}`)

    console.log('\nüîç AUDIT TRAIL:')
    result.audit_trail.forEach((entry, i) => {
      console.log(`${i + 1}. ${entry.agent} - ${entry.step} (${entry.tokens_used} tokens)`)
    })

  } catch (error) {
    console.error('‚ùå Workflow failed:', error.message)
  }
}

main()
```

### Expected Output (Success Case)

```
üöÄ Starting Newsletter Workflow
Topic: Vector databases for production RAG systems
Max Iterations: 3
Budget: $0.20
============================================================

üìç Iteration 1/3

üîç Hunter: Searching for articles on "Vector databases for production RAG systems"
Hunter iteration: 1
  Found 3 articles
  Cost: $0.0456 (1520 tokens)

‚úÖ Fact-Checker: Verifying 3 articles
  Verified: 3 articles
  Rejected: 0 articles
  Quality Score: 0.95
  Cost: $0.0412

üö¶ Supervisor: Routing decision
  Quality Score: 0.95
  Iteration: 1
  ‚úÖ High quality - proceeding to Writer

‚úçÔ∏è Writer: Creating newsletter from 3 articles
  Newsletter: 2456 chars
  Cost: $0.0523

============================================================
‚úÖ Workflow Complete!
Total Cost: $0.1391
Total Iterations: 1
API Calls: 3

üì∞ FINAL NEWSLETTER:
============================================================
# Vector Databases for Production RAG Systems - This Week
...
============================================================
```

### Expected Output (Self-Healing Case)

```
üìç Iteration 1/3

üîç Hunter: Searching for articles...
  Found 3 articles
  Cost: $0.0456

‚úÖ Fact-Checker: Verifying 3 articles
  Verified: 1 articles
  Rejected: 2 articles
  Quality Score: 0.45
  Cost: $0.0412

üö¶ Supervisor: Routing decision
  Quality Score: 0.45
  Iteration: 1
  ‚ö†Ô∏è Low quality score - routing back to Hunter with feedback
  Feedback: Articles 2 and 3 are from marketing blogs, not technical sources

üìç Iteration 2/3

üîç Hunter: Searching for articles on "Vector databases..."
Hunter iteration: 2

FEEDBACK from Fact-Checker:
Articles 2 and 3 are from marketing blogs, not technical sources

  Found 3 articles
  Cost: $0.0489

‚úÖ Fact-Checker: Verifying 3 articles
  Verified: 3 articles
  Rejected: 0 articles
  Quality Score: 0.88
  Cost: $0.0421

üö¶ Supervisor: Routing decision
  ‚úÖ High quality - proceeding to Writer

‚úçÔ∏è Writer: Creating newsletter...
  Cost: $0.0512

============================================================
‚úÖ Workflow Complete!
Total Cost: $0.1790
Total Iterations: 2
API Calls: 5
```

---

## Part 9: Lab Rubric (100 Points)

### 1. State Management & Architecture (25 points)

- [ ] **State schema includes all required fields** (5pts)
- [ ] **Atomic updates (agents only modify their fields)** (5pts)
- [ ] **Iteration counter and max_iterations** (5pts)
- [ ] **Cost tracking per agent** (5pts)
- [ ] **Audit trail with timestamps and tokens** (5pts)

### 2. Agent Implementation (30 points)

- [ ] **Hunter agent finds articles and handles feedback** (10pts)
- [ ] **Fact-Checker scores quality and provides specific feedback** (10pts)
- [ ] **Writer uses token-trimmed input (only verified articles)** (10pts)

### 3. Supervisor & Routing (25 points)

- [ ] **Error routing table with 4 categories** (5pts)
- [ ] **Self-healing: Routes back to Hunter with correction_prompt** (10pts)
- [ ] **Budget enforcement (max cost and max iterations)** (5pts)
- [ ] **Proper escalation to human when needed** (5pts)

### 4. Production Features (20 points)

- [ ] **Checkpointing to database** (5pts)
- [ ] **Token trimming (60%+ reduction)** (5pts)
- [ ] **Cycle detection prevents infinite loops** (5pts)
- [ ] **Complete audit trail** (5pts)

### Bonus Points (10 points)

- [ ] **Implement reflection on final newsletter** (+5pts)
- [ ] **Add forking to test multiple Writer styles** (+3pts)
- [ ] **Human-in-the-loop approval before publishing** (+2pts)

---

## Key Achievements ("Zero-Waste" Execution)

By the end of this lab, your system should be able to:

1. ‚úÖ **Identify a hallucination** (Fact-Checker detects low-quality sources)
2. ‚úÖ **Reset the state** to the last "Known Good" point (keep raw_articles, reset verified_data)
3. ‚úÖ **Correct the search strategy** (provide specific feedback to Hunter)
4. ‚úÖ **Produce the final result** without human intervention
5. ‚úÖ **Stay under budget** ($0.20 and 3 iterations max)

### Metrics to Track

| Metric | Target | Why It Matters |
|--------|--------|---------------|
| **Cost per newsletter** | < $0.20 | Production viability |
| **API calls** | < 10 | Efficiency |
| **Iterations** | 1-2 avg | Self-healing effectiveness |
| **Quality score** | > 0.8 | Content credibility |
| **Success rate** | > 90% | Reliability |

---

## Next Steps

After completing this lab:

1. **Week 5 Project**: Build a full-stack app development team (Frontend, Backend, Database specialists)
2. **Production deployment**: Add Redis checkpointing and horizontal scaling
3. **Advanced patterns**: Implement forking and A/B testing for Writer styles

---

## Troubleshooting

### Issue: Fact-Checker always rejects articles

**Diagnosis**: Prompt is too strict or Hunter is searching wrong sources

**Fix**: Adjust Fact-Checker prompt to be more lenient (score > 0.6 instead of 0.8)

### Issue: Infinite loop between Hunter and Fact-Checker

**Diagnosis**: `max_iterations` not enforced or correction_prompt not being used

**Fix**: Verify routing logic checks `iteration_count >= max_iterations`

### Issue: Cost exceeds $0.20

**Diagnosis**: Not using token trimming or too many iterations

**Fix**:
- Verify Writer only receives `verified_articles`, not `raw_articles`
- Reduce `max_iterations` to 2

### Issue: Hunter doesn't improve on retry

**Diagnosis**: `correction_prompt` not being passed or cleared too early

**Fix**: Verify Hunter prompt includes `state.correction_prompt` when present

---

## üè• Alternative Track: Autonomous Medical Research Swarm

> **For learners interested in Digital Health applications**: This alternative lab applies the same multi-agent patterns to clinical research automation.

### The Challenge

A pharmaceutical research team needs to stay current with **500+ new medical papers published daily** across PubMed, ArXiv, and clinical trial databases. Manual research takes **6 hours per topic**, costing **$48K/month** in researcher time, with inconsistent quality and missed critical studies.

### The Solution: Hierarchical Research Swarm

Build a 3-agent clinical research system that produces publication-ready research briefs in **15 minutes** (96% time reduction).

**Agent Architecture**:
1. **Searcher Agent**: Queries PubMed/ArXiv APIs for relevant papers
2. **Critic Agent**: Validates methodology, sample size, statistical significance
3. **Writer Agent**: Synthesizes findings into clinical brief format
4. **Supervisor**: Orchestrates with quality gates and HITL approval

### Adapted State Schema for Medical Research

```typescript
// src/lab5-alt/types-medical.ts
export interface MedicalResearchState {
  // Input
  research_question: string
  pubmed_query: string

  // Searcher outputs
  raw_papers: Array<{
    pmid: string
    title: string
    authors: string[]
    journal: string
    publication_date: string
    abstract: string
    doi?: string
    citation_count: number
  }>
  searcher_iteration: number

  // Critic outputs
  quality_score: number
  validated_papers: Array<{
    pmid: string
    title: string
    methodology_quality: 'high' | 'medium' | 'low'
    sample_size: number
    statistical_significance: boolean
    bias_assessment: string
    clinical_relevance: number  // 1-10 scale
  }>
  rejected_papers: Array<{
    pmid: string
    reason: string
  }>
  correction_feedback?: string

  // Writer outputs
  draft_brief: string
  final_brief: string
  clinical_recommendations: string[]

  // Orchestration
  iteration_count: number
  max_iterations: number
  total_cost: number
  requires_human_review: boolean
  audit_trail: Array<{
    step: string
    agent: string
    timestamp: Date
    tokens_used: number
  }>
}

export const createMedicalInitialState = (question: string): MedicalResearchState => ({
  research_question: question,
  pubmed_query: '',
  raw_papers: [],
  searcher_iteration: 0,
  quality_score: 0,
  validated_papers: [],
  rejected_papers: [],
  draft_brief: '',
  final_brief: '',
  clinical_recommendations: [],
  iteration_count: 0,
  max_iterations: 3,
  total_cost: 0,
  requires_human_review: false,
  audit_trail: []
})
```

### Searcher Agent (PubMed Integration)

```typescript
// src/lab5-alt/agents/searcher.ts
import Anthropic from '@anthropic-ai/sdk'
import axios from 'axios'

const anthropic = new Anthropic({ apiKey: process.env.ANTHROPIC_API_KEY })
const PUBMED_API = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils'

export async function searcherAgent(
  state: MedicalResearchState
): Promise<Partial<MedicalResearchState>> {
  console.log(`\nüî¨ Searcher: Searching PubMed for "${state.research_question}"`)
  console.log(`Searcher iteration: ${state.searcher_iteration + 1}`)

  // Step 1: Generate PubMed query using Claude
  const queryPrompt = state.correction_feedback
    ? `Generate a PubMed query for: "${state.research_question}"

FEEDBACK from Critic:
${state.correction_feedback}

Adjust your search strategy to address this feedback. Include MeSH terms and filters.`
    : `Generate an optimized PubMed query for: "${state.research_question}"

Include:
- MeSH terms for precision
- Publication date filter (last 5 years)
- Study type filters (clinical trials, meta-analyses, systematic reviews)

Output ONLY the query string (no explanation).`

  const queryResponse = await anthropic.messages.create({
    model: 'claude-haiku-4-5-20250703',
    max_tokens: 200,
    messages: [{ role: 'user', content: queryPrompt }]
  })

  const pubmedQuery = queryResponse.content[0].type === 'text'
    ? queryResponse.content[0].text.trim()
    : ''

  console.log(`  PubMed Query: ${pubmedQuery}`)

  // Step 2: Search PubMed API
  try {
    // Search for PMIDs
    const searchUrl = `${PUBMED_API}/esearch.fcgi?db=pubmed&term=${encodeURIComponent(pubmedQuery)}&retmax=10&retmode=json&sort=relevance`
    const searchResponse = await axios.get(searchUrl)
    const pmids = searchResponse.data.esearchresult.idlist

    if (pmids.length === 0) {
      console.log('  ‚ö†Ô∏è No papers found')
      return {
        raw_papers: [],
        searcher_iteration: state.searcher_iteration + 1,
        pubmed_query: pubmedQuery
      }
    }

    console.log(`  Found ${pmids.length} papers`)

    // Fetch paper details
    const fetchUrl = `${PUBMED_API}/efetch.fcgi?db=pubmed&id=${pmids.join(',')}&retmode=xml`
    const fetchResponse = await axios.get(fetchUrl)

    // Parse XML (simplified - use xml2js in production)
    const papers = await parsePubMedXML(fetchResponse.data)

    return {
      raw_papers: papers.slice(0, 5),  // Top 5 papers
      searcher_iteration: state.searcher_iteration + 1,
      pubmed_query: pubmedQuery,
      total_cost: state.total_cost + calculateCost(queryResponse.usage)
    }

  } catch (error) {
    console.error('  ‚ùå PubMed API error:', error.message)
    return {
      raw_papers: [],
      searcher_iteration: state.searcher_iteration + 1
    }
  }
}

// Helper: Parse PubMed XML response
async function parsePubMedXML(xml: string) {
  // In production, use xml2js or similar
  // This is a simplified example
  const papers = [
    {
      pmid: '38123456',
      title: 'Example Paper on CAR-T Therapy Outcomes',
      authors: ['Smith J', 'Doe A'],
      journal: 'Nature Medicine',
      publication_date: '2025-01-15',
      abstract: 'This study examines long-term outcomes of CAR-T therapy...',
      doi: '10.1038/nm.2025.001',
      citation_count: 42
    }
    // ... parse actual XML
  ]
  return papers
}

function calculateCost(usage: any): number {
  // Haiku 4.5 pricing: $0.0025/1K input, $0.0125/1K output
  return (usage.input_tokens * 0.0025 + usage.output_tokens * 0.0125) / 1000
}
```

### Critic Agent (Methodology Validation)

```typescript
// src/lab5-alt/agents/critic.ts
export async function criticAgent(
  state: MedicalResearchState
): Promise<Partial<MedicalResearchState>> {
  console.log(`\nüî¨ Critic: Validating ${state.raw_papers.length} papers`)

  if (state.raw_papers.length === 0) {
    return {
      quality_score: 0,
      correction_feedback: 'No papers found. Broaden search criteria or adjust query.'
    }
  }

  const prompt = `You are a clinical research methodologist. Evaluate these papers for inclusion in a research brief.

RESEARCH QUESTION: "${state.research_question}"

PAPERS TO EVALUATE:
${state.raw_papers.map((p, i) => `
${i + 1}. ${p.title}
   Journal: ${p.journal}
   Authors: ${p.authors.join(', ')}
   Abstract: ${p.abstract}
   PMID: ${p.pmid}
`).join('\n')}

For EACH paper, assess:
1. **Methodology Quality**: high/medium/low (study design, controls, blinding)
2. **Sample Size**: Actual number of participants
3. **Statistical Significance**: Are p-values and confidence intervals reported?
4. **Bias Assessment**: Selection, publication, or funding bias risks
5. **Clinical Relevance**: 1-10 scale (how applicable to real-world practice)

Output JSON array:
[
  {
    "pmid": "38123456",
    "methodology_quality": "high",
    "sample_size": 234,
    "statistical_significance": true,
    "bias_assessment": "Low risk - randomized controlled trial with pre-registration",
    "clinical_relevance": 9,
    "include": true,
    "reason": "High-quality RCT with adequate sample size"
  }
]

**Quality Threshold**: Include papers with methodology_quality="high" OR (methodology_quality="medium" AND sample_size > 100 AND clinical_relevance >= 7)`

  const response = await anthropic.messages.create({
    model: 'claude-sonnet-4-5-20250929',
    max_tokens: 2000,
    messages: [{ role: 'user', content: prompt }]
  })

  const evaluations = JSON.parse(response.content[0].text)

  const validated = evaluations.filter(e => e.include).map(e => ({
    pmid: e.pmid,
    title: state.raw_papers.find(p => p.pmid === e.pmid)!.title,
    methodology_quality: e.methodology_quality,
    sample_size: e.sample_size,
    statistical_significance: e.statistical_significance,
    bias_assessment: e.bias_assessment,
    clinical_relevance: e.clinical_relevance
  }))

  const rejected = evaluations.filter(e => !e.include).map(e => ({
    pmid: e.pmid,
    reason: e.reason
  }))

  const qualityScore = validated.length / state.raw_papers.length

  console.log(`  Validated: ${validated.length} papers`)
  console.log(`  Rejected: ${rejected.length} papers`)
  console.log(`  Quality Score: ${(qualityScore * 100).toFixed(1)}%`)

  // Determine if search needs refinement
  let correctionFeedback = undefined
  if (qualityScore < 0.4) {
    correctionFeedback = `Quality too low (${(qualityScore * 100).toFixed(0)}% acceptance). Issues found:
${rejected.map(r => `- ${r.reason}`).join('\n')}

Search for: higher-impact journals, larger sample sizes, more recent meta-analyses.`
  }

  return {
    quality_score: qualityScore,
    validated_papers: validated,
    rejected_papers: rejected,
    correction_feedback: correctionFeedback,
    total_cost: state.total_cost + calculateCost(response.usage)
  }
}
```

### Writer Agent (Clinical Brief Generation)

```typescript
// src/lab5-alt/agents/writer.ts
export async function writerAgent(
  state: MedicalResearchState
): Promise<Partial<MedicalResearchState>> {
  console.log(`\n‚úçÔ∏è Writer: Generating clinical brief from ${state.validated_papers.length} papers`)

  const prompt = `Generate a publication-ready clinical research brief.

RESEARCH QUESTION: "${state.research_question}"

VALIDATED STUDIES (n=${state.validated_papers.length}):
${state.validated_papers.map((p, i) => `
${i + 1}. ${p.title}
   - Sample Size: ${p.sample_size}
   - Statistical Significance: ${p.statistical_significance ? 'Yes' : 'No'}
   - Clinical Relevance: ${p.clinical_relevance}/10
   - Bias Assessment: ${p.bias_assessment}
`).join('\n')}

Format your brief as:

# Research Brief: [Topic]

## Executive Summary
[2-3 sentences for busy clinicians]

## Key Findings
- **Finding 1**: [Evidence-based statement] (Study: PMID, n=X, p<0.05)
- **Finding 2**: ...
- **Finding 3**: ...

## Methodology Overview
[Brief assessment of evidence quality]

## Clinical Implications
1. [Actionable recommendation]
2. [Actionable recommendation]
3. [Actionable recommendation]

## Limitations
- [Study limitation 1]
- [Study limitation 2]

## References
[List of PMIDs with links]

**Writing Standards**:
- Use precise medical terminology
- Include statistical metrics (p-values, confidence intervals, effect sizes)
- Avoid sensationalism or overstatement
- Flag any contradictory findings
- Note when evidence is insufficient for clinical recommendations`

  const response = await anthropic.messages.create({
    model: 'claude-sonnet-4-5-20250929',
    max_tokens: 3000,
    messages: [{ role: 'user', content: prompt }]
  })

  const brief = response.content[0].type === 'text' ? response.content[0].text : ''

  console.log(`  Brief: ${brief.length} characters`)
  console.log(`  Cost: $${calculateCost(response.usage).toFixed(4)}`)

  // Extract clinical recommendations
  const recommendations = brief.match(/## Clinical Implications\n([\s\S]*?)\n\n##/)?.[1]
    .split('\n')
    .filter(line => line.trim().startsWith('-') || /^\d+\./.test(line.trim()))
    .map(line => line.replace(/^[-\d.]+\s*/, '').trim())
    || []

  return {
    final_brief: brief,
    clinical_recommendations: recommendations,
    total_cost: state.total_cost + calculateCost(response.usage)
  }
}
```

### Medical Research Supervisor

```typescript
// src/lab5-alt/orchestrator-medical.ts
export async function runMedicalResearchWorkflow(
  researchQuestion: string
): Promise<MedicalResearchState> {
  let state = createMedicalInitialState(researchQuestion)

  console.log('\nüöÄ Starting Medical Research Workflow')
  console.log(`Research Question: ${researchQuestion}`)
  console.log(`Max Iterations: ${state.max_iterations}`)
  console.log(`Budget: $0.50`)
  console.log('='.repeat(60))

  while (state.iteration_count < state.max_iterations) {
    console.log(`\nüìç Iteration ${state.iteration_count + 1}/${state.max_iterations}`)

    // Step 1: Searcher
    const searchResult = await searcherAgent(state)
    state = { ...state, ...searchResult }

    // Step 2: Critic
    const criticResult = await criticAgent(state)
    state = { ...state, ...criticResult }

    // Step 3: Routing Decision
    if (state.quality_score >= 0.4) {
      console.log('  ‚úÖ Quality sufficient - proceeding to Writer')

      // Step 4: Writer
      const writerResult = await writerAgent(state)
      state = { ...state, ...writerResult }

      // Success!
      console.log('\n‚úÖ Medical Research Workflow Complete!')
      break

    } else if (state.correction_feedback) {
      console.log('  üîÑ Quality insufficient - refining search')
      state.iteration_count++
      continue

    } else {
      console.log('  ‚ùå Unable to find sufficient evidence')
      state.requires_human_review = true
      break
    }
  }

  if (state.iteration_count >= state.max_iterations) {
    console.log('  ‚ö†Ô∏è Max iterations reached - escalating to human')
    state.requires_human_review = true
  }

  return state
}
```

### Production Metrics (Medical Research Track)

| Metric | Before (Manual) | After (Automated) | Improvement |
|--------|-----------------|-------------------|-------------|
| **Time per research topic** | 6 hours | 15 minutes | 96% reduction |
| **Papers reviewed** | 10-15 | 50+ (filtered to 5 best) | 3-5x coverage |
| **Cost per brief** | $300 (researcher time) | $0.35 (API costs) | 99.9% cost reduction |
| **Quality consistency** | Variable (researcher-dependent) | Standardized rubric | High consistency |
| **Monthly capacity** | 40 topics | 1,000+ topics | 25x scale |

**Annual Business Impact**:
- Cost savings: $48K ‚Üí $3.6K/month = **$533K/year saved**
- Research capacity: 40 ‚Üí 1,000 topics/month = **25x throughput**
- Time to insight: 6 hours ‚Üí 15 minutes = **Clinical decision velocity**

### Alternative Lab Rubric (Same 100 Points)

Replace the newsletter agents with medical research equivalents:

**1. State Management (25 points)**
- Medical research state schema with PubMed-specific fields
- Paper validation and rejection tracking
- Clinical recommendations extraction

**2. Agent Implementation (30 points)**
- Searcher with PubMed API integration (10pts)
- Critic with methodology validation (10pts)
- Writer with clinical brief format (10pts)

**3. Supervisor & Quality Gates (25 points)**
- Quality threshold enforcement (quality_score >= 0.4)
- Human-in-the-loop for insufficient evidence
- Budget and iteration limits

**4. Production Features (20 points)**
- PubMed API error handling
- Statistical significance validation
- Bias assessment in paper selection

### Real-World Application: Oncology Drug Safety

**Scenario**: FDA requests rapid review of CAR-T therapy adverse events

**Traditional Process**:
- Manual PubMed search: 2 hours
- Paper screening: 3 hours
- Analysis and writing: 4 hours
- **Total: 9 hours per topic**

**Automated Swarm**:
- PubMed search (Searcher): 2 minutes
- Quality validation (Critic): 3 minutes
- Clinical brief (Writer): 10 minutes
- **Total: 15 minutes per topic** (96% time reduction)

**Impact**: Critical safety signals identified in hours instead of weeks, potentially preventing patient harm.

---
