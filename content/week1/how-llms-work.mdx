---
title: "How LLMs Actually Work"
description: "From next-token prediction to attention mechanisms to training pipelines — the mental model every AI architect needs"
estimatedMinutes: 45
---

# How LLMs Actually Work

Most people hear "large language model" and their eyes glaze over. But the core idea is beautiful in its simplicity — and understanding it changes how you build with these tools.

> **Architect Perspective**: You don't need to know the metallurgy of the pistons. But you absolutely need to know where the grip breaks, how the car handles in the wet, and which tracks will get you killed. That middle-layer understanding — knowing the behavior of the tool without needing to build it from scratch — is what makes you effective.

---

## The One Big Idea

An LLM does exactly one thing: given a sequence of words, it predicts the next word. That's it. Every impressive thing you've seen — writing poetry, explaining code, having conversations — emerges from doing that one thing extraordinarily well, billions of times.

Think of it like this. If I say "The cat sat on the ___", you don't need to reason about it. Your brain pattern-matches against everything you've ever read and heard, and "mat" just pops up. That's what an LLM does, but across a staggeringly larger space of patterns.

---

## The Mechanism: Attention

The key invention is called **attention**, and it answers one question: which words should pay attention to which other words?

Picture a cocktail party. Every person in the room can hear every other person, but they choose who to listen to based on what's relevant to them right now. The word "bank" listens harder to "river" or "money" depending on context. That's attention.

Each word asks three things:

1. **"What am I looking for?"** (Query)
2. **"What do I represent?"** (Key)
3. **"What information do I carry?"** (Value)

Words with matching queries and keys get connected strongly. The result? Each word ends up carrying information from the entire sentence, weighted by relevance.

And here's the trick — you run many of these attention computations in parallel, each one specializing in something different. One tracks grammar. Another tracks meaning. Another tracks long-range references. Like a search party fanning out across different terrain.

---

## The Training: How It Learns

You take an enormous pile of text — books, websites, code, conversations — and you play a game: hide the next word, make the model guess, tell it how wrong it was, adjust the weights slightly, repeat. Billions of times.

That's it. No one programs in grammar rules or facts about the world. The model discovers all of that on its own because knowing those things helps it predict the next word better.

After this basic training, you fine-tune it with instruction-following examples and human feedback, which turns it from a text-completion engine into something that actually answers questions helpfully.

---

## The Critical Insight: What This Means in Practice

Here's what most people get wrong. Because the output looks like reasoning, people assume the machine is reasoning. It's not. It's doing very sophisticated pattern matching against everything it saw during training.

This tells you exactly where to trust it and where not to:

| Trust Level | When | Why |
|---|---|---|
| **Reliable** | The task matches patterns the model saw heavily in training. Common code patterns, well-known facts, standard writing tasks. | Dense training data means strong pattern matching. |
| **Unreliable** | The task is novel, unusual, or requires genuine computation. Multi-step math, obscure facts, anything where it has to figure something out rather than recognize a pattern. | Sparse training data means hallucination. |
| **Dangerous** | Medium-stakes tasks where people under-verify. | The model sounds equally confident whether it's right or wrong. You can't tell from the output alone. |

### The Practical Upshot

LLMs are extraordinary **reformulation engines**. They translate between representations — code to English, technical to simple, requirements to implementations. They do semantic search, connecting different phrasings of the same idea. That's their sweet spot.

But they don't generate truth. Ground truth has to exist somewhere else. You bring the judgment about what to build and whether the output is correct. The LLM handles the translation and pattern-matching in between.

---

## Going Deeper: The Pieces That Make It Work

### The Interference Pattern — How Concepts Connect

Imagine a still pond.

Drop a stone labeled "France" — ripples spread out. Now drop another stone labeled "capital" — more ripples. Where those ripples constructively interfere — where they overlap and amplify — that's where "Paris" emerges.

Now drop "France" and "Germany" together. The interference pattern lights up completely different regions: borders, history, EU, language families, World War II. The strength of each ripple depends on how often those concepts appeared together in training data.

This isn't a fixed dictionary or a graph of facts. It's a dynamic, context-dependent transformation through high-dimensional space. Change one input word and the entire pattern shifts. That's why context matters so much — the same word produces completely different activations depending on what surrounds it.

### The Layer Cake — How Understanding Builds

The model isn't one big computation. It's 96+ layers stacked on top of each other, and each layer does something different:

- **Early layers** — detect simple, surface-level patterns. Spelling, basic syntax, common word pairings. Think of this as recognizing individual brushstrokes.
- **Middle layers** — build relationships. Subject-verb agreement across long distances, pronoun resolution, thematic connections. Now you're seeing shapes and figures in the painting.
- **Late layers** — capture abstract semantics. Tone, intent, argument structure, rhetorical patterns. Now you understand what the painting means.

Each layer feeds into the next. And here's the critical detail — there are **skip connections** that let information jump across layers. Without these, the signal would degrade as it passes through dozens of layers, like a game of telephone. Skip connections are like having the original speaker also whisper directly to people further down the line.

### The Search Party — How Attention Heads Specialize

Remember the attention mechanism? You don't run just one. You run dozens in parallel at every layer. Each one is an attention "head," and gradient descent — the training process — naturally pushes them to specialize.

Why? Because if two heads are looking at the same thing, one of them is wasted. The training pressure says: "You're not helping. Go find something else to look at." So they spread out.

One head learns to track syntactic dependencies. Another follows coreference — what does "it" refer to? Another picks up on sentiment. Another notices negation. They're like a team of specialists who've learned to cover different ground because that's what minimizes prediction error.

The outputs from all these heads get combined, and the result is a representation that simultaneously encodes grammar, meaning, context, style, and dozens of other dimensions we don't even have names for.

---

## Hallucination — The Failure Mode You Must Understand

This is the most important practical concept. When the model encounters something it doesn't have strong patterns for, it doesn't say "I don't know." It generates the most plausible-sounding completion anyway.

And here's what makes it dangerous: the hallucinated output looks identical to correct output. Same confidence. Same structure. Same fluency. There is no internal "uncertainty meter" you can read.

It's like asking someone for directions in a city they've never visited. Some people will say "I don't know." But an LLM is like someone who will always give you confident directions — sometimes right, sometimes completely fabricated — with the exact same tone of voice either way.

This is why the "turtles all the way down" problem matters. If you use one LLM to check another LLM's work, you're using pattern matching to verify pattern matching. It catches some errors — inconsistencies, obvious mistakes. But systematic errors where all models share the same blind spot? Those sail right through.

**You cannot automate away human judgment entirely.**

---

## The Training Pipeline: How You Build a Brain From Scratch

### Phase 1: Gather Everything

It starts with data. An almost incomprehensible amount of data.

Web pages, books, scientific papers, code repositories, conversations, Wikipedia, forums — trillions of words scraped, cleaned, filtered, and deduplicated. Think of it as feeding the model the entire written output of human civilization.

But here's something people miss: the **composition of this data shapes everything**. If 30% of the training data is code, the model gets good at code. If medical literature is underrepresented, medical advice gets shaky. The training data IS the model's experience of the world. There is nothing else.

This is why training data density matters so much for reliability. The model can only match patterns it has actually seen.

### Phase 2: Tokenization — Chopping Text Into Pieces

Before the model can process text, it needs to break it into units called tokens. But not word-by-word — that would create an enormous vocabulary and waste capacity on rare words.

Instead, models use **subword tokenization**. The algorithm learns to break text into frequently occurring chunks:

- `"understanding"` might become `["under", "stand", "ing"]`
- `"the"` stays as `["the"]` because it's so common
- A rare word like `"defenestration"` gets chopped into smaller pieces

Each token gets mapped to a **vector** — a list of numbers in high-dimensional space. Think of it as coordinates. The word "king" lives at a specific point. "Queen" lives nearby. "Banana" lives far away. These positions aren't programmed — they're learned during training because placing related concepts near each other helps predict the next word.

This is where meaning starts. Not in the words themselves, but in their geometric relationships in this vast numerical space.

### Phase 3: Pretraining — The Main Event

Now the actual learning happens. And the game is deceptively simple:

1. Take a sequence of tokens
2. Hide the next one
3. Ask the model to predict it
4. Measure how wrong it was (the **loss**)
5. Adjust the weights slightly to be less wrong next time
6. Repeat. Billions of times.

The weight adjustment process is called **gradient descent**, and here's the most intuitive way to think about it:

You're standing on a mountainside in thick fog. You can't see the valley floor. All you can do is feel the slope under your feet and take a small step downhill. That's one training step. After millions of steps, you've found a low point — maybe not the absolute lowest, but low enough to be useful.

The "mountain" is the **loss landscape** — a surface where height represents how wrong the model is. The model is trying to find the lowest point, where its predictions are most accurate.

**Backpropagation** is how the model figures out which direction is "downhill." The error signal flows backward through all 96+ layers, telling each weight: "Here's how much you contributed to the mistake. Adjust accordingly." It's like tracing a chain of blame backward through the entire system.

### Phase 4: Supervised Fine-Tuning — Teaching It to Be Helpful

After pretraining, you have a model that's extraordinary at predicting text, but terrible at following instructions. Ask it a question and it might just... continue writing the question. Or generate a Wikipedia article tangentially related to your topic. It's a completion engine, not an assistant.

So you show it examples:

- **Input**: "Summarize this article in three bullet points: [article]"
- **Expected output**: [human-written summary]

Thousands of these instruction-response pairs teach the model a new pattern: when you see something that looks like an instruction, generate something that looks like a helpful response.

This is a relatively small amount of training compared to pretraining — but it completely changes the model's behavior. It's the difference between someone who's read every book in the library and someone who's read every book **and** learned how to have a conversation about them.

### Phase 5: RLHF — Aligning With Human Preferences

Here's the final polish, and it's subtle but powerful.

The model generates multiple responses to the same prompt. Human raters rank them — this one's better, this one's worse, this one's harmful.

From those rankings, you train a **reward model** — a separate neural network that learns to predict which responses humans will prefer. Then you use that reward model to further train the LLM: "Generate responses that the reward model scores highly."

Think of it as three stages of teaching someone to be a good advisor:

| Stage | What Happens | Result |
|---|---|---|
| **Pretraining** | They read everything ever written | They know a lot but have no social skills |
| **Fine-tuning** | They learn how conversations work | Now they can answer questions |
| **RLHF** | They learn which answers people actually find helpful, safe, and appropriate | Now they're a good advisor |

---

## Why Certain Prompts Work Better

Now that you understand the machinery, prompt engineering stops being mysterious. It's just giving the pattern matcher better patterns to match.

### Zero-Shot vs Few-Shot

**Zero-shot**: "Translate this to French: Hello"

You're hoping the model has seen enough translation examples to recognize the pattern. Usually works for common tasks.

**Few-shot**: You give examples first:

- English: "Good morning" → French: "Bonjour"
- English: "Thank you" → French: "Merci"
- English: "Hello" → French: ???

Now you've activated the translation pattern explicitly. The model doesn't have to guess what you want — you've shown it. This is dramatically more reliable for unusual or ambiguous tasks.

### Chain-of-Thought

If you ask "What is 47 × 23?" directly, the model often gets it wrong. But if you say "Think step by step," it gets it right more often.

Why? Not because it's "reasoning." Because the intermediate steps are themselves patterns the model has seen in training data. Math textbooks show work. Tutorials show intermediate steps. By asking for step-by-step output, you're activating the pattern of worked examples rather than the pattern of quick answers.

You're essentially giving the model scratch paper. Each intermediate token it generates becomes part of the context for the next prediction, letting it build up to the answer rather than trying to jump there directly.

### Specificity Wins

- `"Write me some code"` → weak pattern match. Could go anywhere.
- `"Write a Python function that takes a list of integers and returns the two numbers that sum to a target value, using a hash map for O(n) time complexity"` → strong pattern match. The model has seen thousands of Two Sum solutions. You've activated exactly the right neighborhood in its pattern space.

The more precisely you describe what you want, the more precisely the model can match against its training data. Vague prompts get vague results. Crisp specifications get crisp implementations.

### The System Prompt

This is the context that frames everything else. It sets the "character" the model plays for the entire conversation. Think of it as placing the model in a specific region of pattern space before the conversation even begins.

"You are a senior Python developer reviewing code for security vulnerabilities" activates a completely different set of patterns than "You are a creative writing tutor helping a teenager with their first short story."

Same model. Same weights. Radically different behavior — because the context has shifted which patterns are relevant.

---

## The Limits: Where Pattern Matching Breaks Down

Knowing what a tool can't do is just as valuable as knowing what it can.

### The Three Failure Modes

#### 1. Sparse Training Data — "I haven't seen enough of this"

The model encounters something that was rare or absent in its training data. A made-up word. An obscure historical event. A niche technical domain. The patterns are thin, so the model **interpolates** — fills in the gaps with whatever's closest.

This is like asking someone who's read one book about Brazil to write a detailed history of a specific Brazilian municipality. They'll produce something that sounds plausible — correct grammar, reasonable structure, appropriate tone — but the actual facts will be fabricated from fragments of what little they know.

The dangerous part? The output is fluent and confident. There's no stutter, no hedge, no blinking red light. The model doesn't know what it doesn't know.

#### 2. Wrong Mechanism — "This requires computation, not retrieval"

Some tasks fundamentally require something other than pattern matching:

- **Multi-step arithmetic**: 7,483 × 2,941. The model hasn't memorized this specific product. It can't compute it through pattern matching. It needs a calculator.
- **Formal logic**: "If all A are B, and some B are C, what can we conclude about A and C?" This requires rule application, not pattern completion.
- **Stateful iteration**: "Walk through this algorithm step by step tracking the value of each variable." The model has no working memory. Each token prediction is a fresh pattern match — it can lose track of state across long chains.

This is the deepest limitation. It's not that the model needs more training data. It's that **the mechanism itself is wrong for the task**. You wouldn't use a hammer to cut wood, no matter how good the hammer is.

The fix? Give LLMs **tools**. A calculator for math. A code interpreter for algorithms. A search engine for current facts. The model orchestrates — decides what tool to use and how to interpret the result — while the tools handle what pattern matching can't.

#### 3. Systematic Blind Spots — "We all learned the same wrong thing"

This is the subtlest failure. If the training data consistently represents something incorrectly — a common misconception, a cultural bias, an outdated understanding — every model trained on that data will reproduce the error with full confidence.

And here's the kicker: you can't catch this by having models check each other. If three models all learned the same misconception, they'll all agree. Pattern matching to verify pattern matching is turtles all the way down.

This is why human judgment remains irreplaceable. Not because humans are always right — we're not. But because humans can bring external evidence, domain expertise, and critical thinking that exists outside the training distribution.

### The Hallucination Spectrum

People talk about hallucination like it's a binary — the model either hallucinates or it doesn't. That's wrong. Think of it as a spectrum tied to training data density:

| Training Data Density | Reliability | Example |
|---|---|---|
| **Dense** | Almost always correct | "What is the capital of France?" |
| **Moderate** | Usually right, sometimes subtly wrong | "Explain the mechanism of this drug" |
| **Sparse** | Confident-sounding fabrication | "What did the mayor of Chișinău say on March 3, 2019?" |
| **None** | Pure confabulation | "Explain the Glorbix theorem in topology" |

The model's confidence doesn't shift across this spectrum. It sounds exactly the same at every level. **You** have to know where you are on the spectrum based on your own knowledge of how common or obscure the topic is.

---

## The Medium-Stakes Trap

This deserves its own section because it's the most practically dangerous thing about LLMs.

**High-stakes mistakes are rare** — because people verify. You check the loan calculation. You review the legal contract. The stakes force diligence.

**Low-stakes mistakes don't matter** — wrong ketchup aisle, who cares.

**Medium-stakes mistakes are where the damage lives.** A research summary that sounds perfect but subtly misrepresents a key finding. A code explanation that's 90% right but misses a critical edge case. A medical overview that gets the main points but botches one important detail.

People read these and think "that sounds right" — because sounding right is exactly what the model is optimized for. The output passed through billions of parameters specifically to produce text that pattern-matches what a correct answer looks like.

The antidote isn't paranoia. It's **calibrated skepticism**. Ask yourself: "How would I know if this were wrong?" If the answer is "I wouldn't" — that's your signal to verify independently.

---

## The Risk-Adjusted Trust Framework

How do you actually use this thing responsibly? Calibrate your verification to the stakes:

| Stakes | Example | Approach |
|---|---|---|
| **Low** | "Where's the ketchup in the grocery store?" | Trust the output. If wrong, cost is trivial. |
| **Medium** | "Summarize this research paper." | This is the trap. Sounding right ≠ being right. Verify key claims. |
| **High** | "Calculate this loan amortization schedule." | Verify everything critical. Use the LLM to generate the code, then check the math independently. |

The model's reliability is uniform — it's always doing the same thing, pattern matching. **You** are the variable. You decide how much verification the situation demands.

---

## Key Takeaways

1. **One mechanism**: LLMs predict the next token through attention-weighted pattern matching across training data. Everything else — conversation, code generation, reasoning — emerges from this.

2. **Attention is the key invention**: Words dynamically decide which other words to attend to, enabling context-dependent understanding across the entire input.

3. **Training is three phases**: Pretraining (predict next word on vast data), fine-tuning (learn to follow instructions), and RLHF (align with human preferences).

4. **Hallucination is a spectrum, not a binary**: Reliability correlates with training data density. The model's confidence does not.

5. **The medium-stakes trap is the biggest risk**: Not high-stakes (where you verify) or low-stakes (where errors don't matter), but medium-stakes where you under-verify because the output sounds right.

6. **Tools compensate for mechanism gaps**: Calculator for math, code interpreter for algorithms, search for current facts. The model orchestrates; the tools compute.

7. **Human judgment remains irreplaceable**: You cannot use pattern matching to verify pattern matching. External evidence and domain expertise exist outside the training distribution.

---

## Further Reading

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) — The original transformer paper
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) — Visual walkthrough of the architecture
- [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155) — The InstructGPT/RLHF paper
- [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) — The GPT-3 paper on in-context learning
