---
title: "How LLMs Actually Work"
description: "From next-token prediction to attention mechanisms to training pipelines — the mental model every AI architect needs"
estimatedMinutes: 45
---

# How LLMs Actually Work

You already use a tiny language model every day.

Open your phone. Start typing a text message. After a few words, your keyboard suggests the next word. Type "Running late, be there" and it suggests "soon." Type "Hey, want to grab" and it suggests "lunch" or "coffee."

That's it. That's the core of what a large language model does. Given a sequence of words, predict the next one. Your phone does it with a small model trained on your texting habits. ChatGPT and Claude do it with a massive model trained on a significant fraction of everything humans have ever written.

Every impressive thing you've seen — writing poetry, explaining code, having conversations, passing bar exams — emerges from doing that one thing extraordinarily well, billions of times.

> **A note on mental models**: Throughout this piece, we'll build several visual models — ponds with ripples, cocktail parties, search parties, mountains in fog. These are maps, not territory. Like Feynman diagrams in physics, they're useful fictions that let you reason correctly about the system without needing to understand every equation. If a model helps you predict what the system will do, it's a good model. That's the bar.

---

## Ripples in a Pond — How Concepts Connect

Before we get into any machinery, let's build the most important mental model you'll use as an AI architect.

Imagine a perfectly still pond. You drop a stone labeled **"France"** — ripples spread outward. Now you drop a second stone labeled **"capital"** — more ripples. Where those ripples overlap and amplify each other, a concept emerges: **Paris**.

Now pick up those stones and drop **"France"** and **"Germany"** instead. Completely different interference pattern. The ripples now amplify in the regions of borders, history, EU, language families, World War II. The strength of each ripple depends on how often those concepts appeared together in training data.

This is not a dictionary lookup. It's not a fixed web of facts. It's a dynamic, context-dependent pattern that shifts entirely when you change a single word. Drop **"France"** and **"wine"** — now you're in Bordeaux, terroir, appellations. Same first stone, completely different pattern.

<Callout type="info">
**Run this in your head**: Drop "Python" and "error" into the pond. What ripples emerge? Now swap "error" for "snake." Feel how the entire pattern shifts — same first word, completely different activation. That shift is the core of how LLMs work.
</Callout>

This is what happens inside the model with every single prediction. It's not looking up answers. It's computing which concepts resonate with the current context — and the "next word" is whatever emerges most strongly from the interference pattern.

---

## The Cocktail Party — How Words Read Each Other

Now let's zoom into the mechanism that creates those ripple patterns. It's called **attention**, and here's the most intuitive way to think about it.

You're at a cocktail party. Every person in the room can hear every other person. But they don't listen equally to everyone — they tune in to whoever is most relevant to them right now.

The word **"bank"** is standing in the middle of the room. When **"river"** walks in, "bank" immediately tunes in — they have something to talk about. When **"money"** walks in instead, "bank" shifts its attention completely. Same word, different context, different connections.

Each word at this party does three things:

1. **Asks a question** — "What should I pay attention to?" (this is called the Query)
2. **Wears a name tag** — "Here's what I represent." (the Key)
3. **Carries a briefcase** — "Here's the information I have." (the Value)

Words whose questions match other words' name tags form strong connections. The result: after this cocktail party, every word walks away carrying a weighted summary of the entire room's information, with the most relevant conversations weighted most heavily.

<Callout type="info">
**Run this in your head**: The sentence is "The bank by the river was steep." The word "bank" asks its question. Whose name tags match? "River" and "steep" — both strongly connected. Now change the sentence to "The bank approved the loan." Ask again. Now "approved" and "loan" light up. Same word, different party, different connections.
</Callout>

---

## The Search Party — Why Parallel Matters

You don't run just one cocktail party. You run dozens simultaneously, at every layer of the model. Each one is called an attention **head**, and here's why that matters.

Imagine you're searching for a lost hiker. You send out one search team and they cover one area. Fine. But send out twelve teams, each assigned to different terrain — one checks rivers, another checks ridgelines, another checks shelters, another follows footprints — and you cover vastly more ground.

That's what attention heads do. They run in parallel, and the training process naturally pushes them to specialize. Why? Because if two heads are looking at the same thing, one is wasted. The training pressure says: *"You're not helping. Go find something else."* So they spread out.

One head learns grammar. Another tracks what "it" refers to three sentences back. Another detects sentiment. Another follows negation. Their combined output is a representation that simultaneously encodes grammar, meaning, context, style, and dimensions we don't even have names for.

<Callout type="info">
**Run this in your head**: Take the sentence "She said she didn't enjoy the movie, but she was lying." One head tracks the chain of "she" references. Another catches the negation "didn't enjoy." Another detects the reversal "but she was lying." Each one sees a different dimension of the same sentence. Together, they capture the full meaning.
</Callout>

---

## The Layer Cake — How Understanding Builds

The model isn't one big computation. It's many layers stacked on top of each other — modern models have 96 or more. Each layer does something different, building on the last:

- **Early layers** detect surface patterns. Spelling, basic syntax, common word pairings. You're recognizing individual brushstrokes on a canvas.
- **Middle layers** build relationships. Subject-verb agreement across long distances, pronoun resolution, thematic connections. Now you're seeing shapes and figures in the painting.
- **Late layers** capture abstract meaning. Tone, intent, argument structure, rhetorical patterns. Now you understand what the painting is about.

Each layer feeds into the next. But there's a critical trick: **skip connections** let information jump directly across layers. Without these, the signal would degrade through dozens of layers like a game of telephone. Skip connections are like having the original speaker also whisper directly to people further down the line.

<Callout type="info">
**Run this in your head**: Imagine removing all the middle layers. You'd have a model that recognizes words and can capture abstract meaning — but can't connect "she" to the right person, can't track agreement across a paragraph, can't resolve ambiguity. The painting has brushstrokes and a vague theme, but no figures. That's why depth matters.
</Callout>

---

## How It Learned — The Three Stages

The training process is surprisingly simple to understand if you take it in stages. Think of teaching someone to be a good advisor:

| Stage | What Happens | Analogy |
|---|---|---|
| **Pretraining** | Read trillions of words. For each sequence, guess the next word. Get told how wrong you were. Adjust. Repeat billions of times. | Someone who's read every book in the library. Knows a lot, but no social skills. |
| **Fine-tuning** | See thousands of instruction-response pairs: "Summarize this article" → [human-written summary]. Learn that instructions expect helpful responses. | Now they've learned how conversations work. They can answer questions. |
| **RLHF** | Generate multiple responses. Humans rank which are better. Train a reward model on those preferences. Optimize for high-reward responses. | Now they know which answers people actually find helpful, safe, and appropriate. Good advisor. |

### The Key Details Worth Understanding

**Training data is everything.** If 30% of the data is code, the model gets good at code. If medical literature is underrepresented, medical advice gets shaky. The training data IS the model's experience of the world. There is nothing else.

**Gradient descent is hiking in fog.** The model stands on a mountainside where height represents "how wrong I am." It can't see the valley floor — only the slope under its feet. It takes one small step downhill. After millions of steps, it's found a low point. Maybe not the absolute lowest, but low enough. The "error signal" flows backward through every layer, telling each weight: "Here's how much you contributed to the mistake. Adjust." That backward flow is called backpropagation.

**Words become coordinates.** Before processing, text gets chopped into tokens (roughly word-pieces) and each token is mapped to a point in high-dimensional space. "King" lives at one point. "Queen" lives nearby. "Banana" lives far away. These positions aren't programmed — they're learned because placing related concepts near each other helps predict the next word better. Meaning starts here: not in the words themselves, but in their geometric relationships.

---

## Hallucination — The Failure Mode You Must Understand

When the model encounters something it doesn't have strong patterns for, it doesn't say "I don't know." It generates the most plausible-sounding completion anyway.

And here's what makes it dangerous: the hallucinated output looks identical to correct output. Same confidence. Same structure. Same fluency.

It's like asking someone for directions in a city they've never visited. Some people will say "I don't know." An LLM will always give you confident directions — sometimes right, sometimes completely fabricated — with the exact same tone of voice either way.

Think of hallucination as a spectrum tied to training data density:

```
Training Data Density → Reliability
    Dense   ████████████ "Capital of France?"     → Almost always correct
    Medium  ██████       "Mechanism of this drug"  → Usually right, sometimes subtly wrong
    Sparse  ██           "Mayor of Chișinău, 2019" → Confident-sounding fabrication
    None    ░            "The Glorbix theorem"      → Pure confabulation
```

The model's confidence doesn't shift across this spectrum. It sounds exactly the same at every level. **You** have to know where you are based on your own knowledge of how common or obscure the topic is.

And the "turtles all the way down" problem: if you use one LLM to check another, you're using pattern matching to verify pattern matching. It catches inconsistencies and obvious mistakes. But systematic errors where all models share the same blind spot? Those sail right through.

**You cannot automate away human judgment entirely.**

---

## The Medium-Stakes Trap

This deserves its own section because it's the most practically dangerous thing about LLMs.

**High-stakes mistakes are rare** — because people verify. You check the loan calculation. You review the legal contract. The stakes force diligence.

**Low-stakes mistakes don't matter** — wrong ketchup aisle, who cares.

**Medium-stakes mistakes are where the damage lives.** A research summary that sounds perfect but subtly misrepresents a key finding. A code explanation that's 90% right but misses a critical edge case. A medical overview that gets the main points but botches one important detail.

People read these and think "that sounds right" — because sounding right is exactly what the model is optimized for. The output passed through billions of parameters specifically to produce text that pattern-matches what a correct answer looks like.

The antidote isn't paranoia. It's **calibrated skepticism**. Ask yourself: "How would I know if this were wrong?" If the answer is "I wouldn't" — that's your signal to verify independently.

---

## Why Certain Prompts Work Better

Now that you understand the machinery — the pond, the cocktail party, the layer cake — prompt engineering stops being mysterious. It's just giving the pattern matcher better patterns to match.

**Few-shot examples** explicitly activate the right pattern. Instead of hoping the model recognizes what you want, you show it:

- English: "Good morning" → French: "Bonjour"
- English: "Thank you" → French: "Merci"
- English: "Hello" → ???

Now the translation pattern is lit up in the pond. The model doesn't have to guess.

**Chain-of-thought** gives the model scratch paper. "Think step by step" activates the pattern of worked examples from textbooks — which contain intermediate steps. Each intermediate token becomes context for the next prediction, letting the model build to an answer instead of jumping there directly. It's not reasoning. It's activating a more detailed pattern.

**Specificity narrows the search**. "Write me some code" is a weak ripple that could activate anything. "Write a Python function that takes a list of integers and returns two numbers that sum to a target, using a hash map for O(n)" drops a very precise stone. Thousands of Two Sum solutions exist in the training data. You've activated exactly the right neighborhood.

**System prompts shift the starting point**. "You are a senior security engineer" and "You are a creative writing tutor" place the model in completely different regions of pattern space before the conversation begins. Same weights. Radically different behavior.

<Callout type="info">
**Run this in your head**: Imagine the pond starts with the "security engineer" stone already dropped, ripples already spreading. Now every subsequent stone you drop interferes with those existing ripples. The entire conversation plays out in a different part of the pond than if you'd started with "creative writing tutor."
</Callout>

---

## Where Pattern Matching Breaks Down

Knowing what a tool can't do is as valuable as knowing what it can. There are three distinct failure modes, and each requires a different response:

**1. Sparse training data** — "I haven't seen enough of this." The model encounters something rare. A niche domain. An obscure fact. It interpolates — fills gaps with whatever's closest. Like someone who read one book about Brazil writing a detailed history of a specific municipality. Plausible grammar, fabricated facts. The fix: verify anything outside the mainstream.

**2. Wrong mechanism** — "This requires computation, not retrieval." Multi-step arithmetic (7,483 x 2,941). Formal logic. Stateful iteration. The mechanism itself is wrong for the task — you wouldn't use a hammer to cut wood. The fix: give LLMs **tools**. A calculator for math. A code interpreter for algorithms. A search engine for current facts. The model orchestrates; the tools compute.

**3. Systematic blind spots** — "We all learned the same wrong thing." If the training data consistently represents something incorrectly, every model reproduces the error with full confidence. Three models checking each other all agree — because they all learned the same misconception. The fix: human judgment from outside the training distribution. Not because humans are always right, but because we can bring external evidence.

---

## The Risk-Adjusted Trust Framework

How do you use this thing responsibly? Match your verification to the stakes:

| Stakes | Example | Approach |
|---|---|---|
| **Low** | "Where's the ketchup?" | Trust the output. Wrong answer costs nothing. |
| **Medium** | "Summarize this research paper." | The trap zone. Sounding right is not being right. Verify key claims. |
| **High** | "Calculate this loan schedule." | Verify everything critical. Use the LLM to generate code, then check the math independently. |

The model's reliability is uniform — it's always doing the same thing, pattern matching. **You** are the variable. You decide how much verification the situation demands.

---

## Test Your Understanding

If you've built a working mental model, you should be able to answer these three questions smoothly — not from memory, but by running a simulation in your head:

**1. What is actually doing something here?**

Can you describe the mechanism without using the words "attention," "transformer," or "neural network"? In plain language: words arrive, each one reaches out to every other word to figure out who's relevant, the relevant ones share their information, this happens across dozens of parallel specialists and dozens of stacked layers, and the result is a prediction of the most likely next word. If you can run that in your head, you have the mechanism.

**2. If I change this, what changes?**

What happens if you double the training data in one domain? (The model gets more reliable there — denser ripples.) What if you remove the middle layers? (Relationships break — grammar works, abstract meaning works, but nothing connects them.) What if you give a vague prompt instead of a specific one? (Weak ripple — the interference pattern is diffuse, so the prediction could go anywhere.)

**3. Can I hold a picture in my mind that shows me why?**

Close your eyes. Drop two stones in the pond. See the ripples. See where they overlap. That overlap is the prediction. If the picture is clear and you can manipulate it — change the words, watch the pattern shift — you understand how LLMs work. Not the math. Not the code. The mechanism. And that's what makes you effective.

> *"What I cannot create, I do not understand."* — Richard Feynman

---

## Further Reading

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) — The original transformer paper
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) — Visual walkthrough of the architecture
- [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155) — The InstructGPT/RLHF paper
- [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) — The GPT-3 paper on in-context learning
