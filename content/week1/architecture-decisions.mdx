# AI Architecture & Design Patterns

Learn to make informed architectural decisions when building production AI systems.

## Architecture Patterns for AI Applications

### Serverless vs Long-Running

Choose based on your use case, traffic patterns, and cost constraints.

| Factor | Serverless | Long-Running |
|--------|-----------|--------------|
| **Best For** | Intermittent traffic, event-driven | Constant traffic, stateful apps |
| **Cold Start** | 1-5 seconds | None (always warm) |
| **Scaling** | Automatic, instant | Manual configuration |
| **Cost** | Pay per request | Pay for uptime |
| **State** | Stateless (by design) | Can maintain state |
| **Examples** | Chatbots, API endpoints | Real-time collaboration, agents |

**Serverless Architecture** (AWS Lambda, Vercel, Cloud Run):

```typescript
// Next.js API Route (serverless)
export async function POST(request: Request) {
  const { message } = await request.json()

  // Each request is independent
  const anthropic = new Anthropic({
    apiKey: process.env.ANTHROPIC_API_KEY
  })

  const response = await anthropic.messages.create({
    model: 'claude-3-5-sonnet-20241022',
    max_tokens: 1024,
    messages: [{ role: 'user', content: message }]
  })

  return Response.json({ reply: response.content[0].text })
}
```

**Pros:**
- ✅ Zero infrastructure management
- ✅ Automatic scaling to zero (no idle costs)
- ✅ Massive parallelism (1000s of concurrent requests)
- ✅ Fast deployment and iteration

**Cons:**
- ❌ Cold starts (1-5s latency)
- ❌ Stateless (need external storage for state)
- ❌ Execution time limits (30s-15min depending on platform)
- ❌ Higher cost at very high volume

**Long-Running Architecture** (Express, FastAPI, Docker containers):

```typescript
// Express server with persistent connections
import express from 'express'
import Anthropic from '@anthropic-ai/sdk'

const app = express()
const anthropic = new Anthropic() // Connection pool persists

// In-memory session cache
const sessions = new Map<string, Message[]>()

app.post('/chat', async (req, res) => {
  const { sessionId, message } = req.body

  // Retrieve conversation history from memory
  const history = sessions.get(sessionId) || []
  history.push({ role: 'user', content: message })

  const response = await anthropic.messages.create({
    model: 'claude-3-5-sonnet-20241022',
    max_tokens: 1024,
    messages: history
  })

  history.push({ role: 'assistant', content: response.content[0].text })
  sessions.set(sessionId, history)

  res.json({ reply: response.content[0].text })
})

app.listen(3000)
```

**Pros:**
- ✅ No cold starts (always warm)
- ✅ Can maintain in-memory state
- ✅ WebSocket support for real-time
- ✅ Lower cost at sustained high volume

**Cons:**
- ❌ Must manage servers, scaling, health checks
- ❌ Idle capacity costs money
- ❌ More complex deployment
- ❌ Harder to scale automatically

**Decision Framework:**

```
Use Serverless if:
- Traffic is spiky or unpredictable
- You want zero ops overhead
- Cold start latency is acceptable (>1s)
- Each request is independent

Use Long-Running if:
- You need <100ms latency consistently
- WebSocket/SSE connections required
- Heavy in-memory caching needed
- Sustained high traffic (>1M requests/day)
```

### Synchronous vs Streaming Responses

**Synchronous (wait for complete response):**

```typescript
async function generateSync(prompt: string): Promise<string> {
  const response = await anthropic.messages.create({
    model: 'claude-3-5-sonnet-20241022',
    max_tokens: 1024,
    messages: [{ role: 'user', content: prompt }]
  })

  return response.content[0].text // All at once
}

// Usage
const result = await generateSync("Explain quantum computing")
console.log(result) // Waits 2-5 seconds, then shows full text
```

**Use when:**
- Processing batch operations
- Response time doesn't matter
- Need to validate/process full response before showing

**Streaming (token-by-token):**

```typescript
async function* generateStream(prompt: string) {
  const stream = await anthropic.messages.stream({
    model: 'claude-3-5-sonnet-20241022',
    max_tokens: 1024,
    messages: [{ role: 'user', content: prompt }]
  })

  for await (const chunk of stream) {
    if (chunk.type === 'content_block_delta') {
      yield chunk.delta.text // Token by token
    }
  }
}

// Usage
for await (const token of generateStream("Explain quantum computing")) {
  process.stdout.write(token) // Shows tokens as they arrive
}
```

**Use when:**
- User-facing chat interfaces
- Long responses (>500 tokens)
- Want to reduce perceived latency
- Mobile apps (show progress immediately)

**Hybrid Approach:**

```typescript
// Stream to user, but collect full response for logging
async function streamAndLog(prompt: string, userId: string) {
  let fullResponse = ''

  const stream = generateStream(prompt)

  for await (const token of stream) {
    fullResponse += token
    sendToUser(token) // Stream to user
  }

  // Log complete response after streaming finishes
  await logConversation(userId, prompt, fullResponse)
}
```

## State Management Patterns

### Pattern 1: Stateless (Serverless)

**Every request includes full context:**

```typescript
export async function POST(request: Request) {
  const { messages } = await request.json() // Client sends full history

  const response = await anthropic.messages.create({
    model: 'claude-3-5-sonnet-20241022',
    max_tokens: 1024,
    messages: messages // Use what client sends
  })

  return Response.json({ reply: response.content[0].text })
}
```

**Pros:** Simple, no server state, scales infinitely
**Cons:** Client manages history, higher token costs (resends full context)

### Pattern 2: Database-Backed State

**Store conversations in database:**

```typescript
import { prisma } from '@/lib/db'

export async function POST(request: Request) {
  const { conversationId, message } = await request.json()

  // Fetch conversation history
  const conversation = await prisma.conversation.findUnique({
    where: { id: conversationId },
    include: { messages: { orderBy: { createdAt: 'asc' } } }
  })

  // Build message array
  const messages = [
    ...conversation.messages.map(m => ({
      role: m.role,
      content: m.content
    })),
    { role: 'user', content: message }
  ]

  const response = await anthropic.messages.create({
    model: 'claude-3-5-sonnet-20241022',
    max_tokens: 1024,
    messages: messages
  })

  // Save both user message and assistant response
  await prisma.message.createMany({
    data: [
      { conversationId, role: 'user', content: message },
      { conversationId, role: 'assistant', content: response.content[0].text }
    ]
  })

  return Response.json({ reply: response.content[0].text })
}
```

**Pros:** Persistent history, works with serverless, audit trail
**Cons:** Database latency, cost of storage, need to manage cleanup

### Pattern 3: Distributed Cache (Redis)

**Fast session storage with TTL:**

```typescript
import Redis from 'ioredis'

const redis = new Redis(process.env.REDIS_URL)

export async function POST(request: Request) {
  const { sessionId, message } = await request.json()

  // Get conversation from cache
  const cached = await redis.get(`conversation:${sessionId}`)
  const messages = cached ? JSON.parse(cached) : []

  // Add new message
  messages.push({ role: 'user', content: message })

  const response = await anthropic.messages.create({
    model: 'claude-3-5-sonnet-20241022',
    max_tokens: 1024,
    messages: messages
  })

  // Update cache with TTL (24 hours)
  messages.push({ role: 'assistant', content: response.content[0].text })
  await redis.setex(
    `conversation:${sessionId}`,
    86400, // 24 hours
    JSON.stringify(messages)
  )

  return Response.json({ reply: response.content[0].text })
}
```

**Pros:** Fast (< 5ms), auto-cleanup with TTL, cheap
**Cons:** Not durable (cache can evict), need Redis infrastructure

### Pattern 4: In-Memory (Long-Running Only)

```typescript
// Only works in long-running servers, not serverless
const sessions = new Map<string, Message[]>()

// Cleanup old sessions every hour
setInterval(() => {
  const oneHourAgo = Date.now() - 3600000
  for (const [id, data] of sessions.entries()) {
    if (data.lastAccess < oneHourAgo) {
      sessions.delete(id)
    }
  }
}, 3600000)
```

**Pros:** Fastest possible, no external dependencies
**Cons:** Lost on restart, not horizontally scalable, memory limits

### Decision Matrix

| Pattern | Latency | Durability | Cost | Best For |
|---------|---------|------------|------|----------|
| Stateless | Low | N/A | High | Simple APIs, low volume |
| Database | Medium | High | Medium | Audit requirements, multi-user |
| Redis Cache | Very Low | Medium | Low | High-traffic chat, sessions |
| In-Memory | Instant | None | Free | Single-server, temporary |

## Model Tier Selection

Choose the right model based on task complexity, not by default.

### Cost-First Approach

```typescript
// Cascading model selection
async function generateWithFallback(prompt: string, complexity: 'simple' | 'complex') {
  try {
    // Try cheap model first for simple tasks
    if (complexity === 'simple') {
      return await anthropic.messages.create({
        model: 'claude-3-haiku-20240307', // $0.25/$1.25 per MTok
        max_tokens: 512,
        messages: [{ role: 'user', content: prompt }]
      })
    }
  } catch (error) {
    console.log('Haiku failed, trying Sonnet...')
  }

  // Fall back to more capable model
  return await anthropic.messages.create({
    model: 'claude-3-5-sonnet-20241022', // $3/$15 per MTok
    max_tokens: 1024,
    messages: [{ role: 'user', content: prompt }]
  })
}
```

### Task-Based Routing

```typescript
function selectModel(taskType: string): string {
  const modelMap: Record<string, string> = {
    // Simple, high-volume tasks
    'classification': 'claude-3-haiku-20240307',
    'sentiment': 'claude-3-haiku-20240307',
    'extraction': 'claude-3-haiku-20240307',
    'translation': 'claude-3-haiku-20240307',

    // Medium complexity
    'chat': 'claude-3-5-sonnet-20241022',
    'summarization': 'claude-3-5-sonnet-20241022',
    'qa': 'claude-3-5-sonnet-20241022',

    // High complexity
    'code': 'claude-3-5-sonnet-20241022',
    'reasoning': 'claude-opus-4-5-20251101',
    'research': 'claude-opus-4-5-20251101'
  }

  return modelMap[taskType] || 'claude-3-5-sonnet-20241022'
}
```

## Rate Limiting Strategies

Prevent abuse and control costs with multi-layer rate limiting.

### Layer 1: User-Based Limits

```typescript
import { Ratelimit } from '@upstash/ratelimit'
import { Redis } from '@upstash/redis'

const ratelimit = new Ratelimit({
  redis: Redis.fromEnv(),
  limiter: Ratelimit.slidingWindow(10, '1 m'), // 10 requests per minute
})

export async function POST(request: Request) {
  const userId = request.headers.get('x-user-id')

  const { success, remaining } = await ratelimit.limit(userId)

  if (!success) {
    return Response.json(
      { error: 'Rate limit exceeded', retryAfter: 60 },
      { status: 429 }
    )
  }

  // Process request...
  return Response.json({ remaining })
}
```

### Layer 2: Token-Based Budget

```typescript
interface TokenBudget {
  daily: number
  used: number
  resetAt: Date
}

async function checkTokenBudget(userId: string, estimatedTokens: number): Promise<boolean> {
  const budget = await redis.get<TokenBudget>(`budget:${userId}`)

  if (!budget) {
    // Initialize new budget
    await redis.set(`budget:${userId}`, {
      daily: 100000, // 100k tokens per day
      used: 0,
      resetAt: new Date(Date.now() + 86400000)
    })
    return true
  }

  // Check if reset needed
  if (new Date() > budget.resetAt) {
    budget.used = 0
    budget.resetAt = new Date(Date.now() + 86400000)
  }

  // Check if over budget
  if (budget.used + estimatedTokens > budget.daily) {
    return false
  }

  // Update usage
  budget.used += estimatedTokens
  await redis.set(`budget:${userId}`, budget)

  return true
}
```

### Layer 3: Concurrent Request Limits

```typescript
class ConcurrencyLimiter {
  private active = new Map<string, number>()
  private maxConcurrent = 5

  async acquire(userId: string): Promise<boolean> {
    const current = this.active.get(userId) || 0

    if (current >= this.maxConcurrent) {
      return false
    }

    this.active.set(userId, current + 1)
    return true
  }

  release(userId: string) {
    const current = this.active.get(userId) || 0
    this.active.set(userId, Math.max(0, current - 1))
  }
}

const limiter = new ConcurrencyLimiter()

export async function POST(request: Request) {
  const userId = request.headers.get('x-user-id')

  if (!await limiter.acquire(userId)) {
    return Response.json(
      { error: 'Too many concurrent requests' },
      { status: 429 }
    )
  }

  try {
    // Process request...
    return Response.json({ result })
  } finally {
    limiter.release(userId)
  }
}
```

## Architecture Decision Checklist

When building a new AI feature, answer these questions:

**Deployment:**
- [ ] Expected request volume? (<100/min → serverless, >1000/min → long-running)
- [ ] Need for WebSockets? (Yes → long-running, No → serverless)
- [ ] Cold start tolerance? (<1s required → long-running)

**State Management:**
- [ ] Need conversation history? (Yes → database/Redis, No → stateless)
- [ ] Audit requirements? (Yes → database, No → Redis/memory)
- [ ] Session duration? (<1 hour → Redis, >1 day → database)

**Model Selection:**
- [ ] Task complexity? (Simple → Haiku, Complex → Sonnet/Opus)
- [ ] Cost priority? (High → start with cheapest, test quality)
- [ ] Latency requirements? (<2s → Haiku, <5s → Sonnet)

**Rate Limiting:**
- [ ] Need user-level limits? (Public API → yes)
- [ ] Need budget controls? (Paid tiers → yes)
- [ ] Concurrent request limits? (Expensive operations → yes)

## Key Takeaways

**Architecture Selection:**
- Use **serverless** for event-driven, unpredictable traffic
- Use **long-running** for real-time, stateful applications
- Use **streaming** for user-facing chat (better UX)

**State Management:**
- **Stateless**: Simplest, works everywhere, but expensive tokens
- **Database**: Durable, auditable, but slower
- **Redis**: Fast, cheap, but not durable
- **In-memory**: Fastest, but only for long-running servers

**Model Selection:**
- Start with **cheapest model** (Haiku) and upgrade only if needed
- Route by task type: classification → Haiku, reasoning → Opus
- Test quality vs cost tradeoffs for your specific use case

**Rate Limiting:**
- Implement **multiple layers**: per-user, token budget, concurrency
- Prevent abuse and control costs from day one
- Make limits visible to users (show remaining quota)

**The Principle: Start Simple, Scale When Needed**

Don't over-architect. Begin with:
1. Serverless deployment
2. Stateless or Redis for state
3. Haiku for simple tasks, Sonnet for complex
4. Basic rate limiting

Optimize when you have real traffic data, not before.
