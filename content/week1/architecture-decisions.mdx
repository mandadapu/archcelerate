# Architectural ROI: Unit Economics & Model Selection Matrix

Engineering for **cost-performance efficiency** at scale‚Äîbecause every token has a price, and every model choice has an ROI.

> **Architect Perspective**: AI architecture isn't about "which is best"‚Äîit's about **maximizing value per dollar** while meeting quality thresholds. The right model is the **cheapest one that passes your quality bar**.

## The Unit Economics Problem

**Reality Check**: AI costs scale linearly with usage
- Production chatbot: 10K users √ó 100 msgs/day √ó 500 tokens/msg = **500M tokens/day**
- At Opus pricing ($15/MTok input): **$7,500/day** = **$225K/month**
- At Sonnet pricing ($3/MTok input): **$1,500/day** = **$45K/month**
- At Haiku pricing ($0.25/MTok input): **$125/day** = **$3.75K/month**

**Architectural Mandate**: Your model selection directly determines whether your AI product is **profitable or a cost center**.

## The Efficiency Frontier: 2026 Model Pricing Reality

**The Physics of Cost**: Every model sits on a cost-performance curve. Your job is to find the **minimum viable quality** at the **lowest cost**.

### Claude 4.5 Series: The 2026 Price-Performance Landscape

| Model | Input $/MTok | Output $/MTok | Latency (p95) | Quality Tier | Best For |
|-------|-------------|--------------|---------------|--------------|----------|
| **Opus 4.5** | $15 | $75 | 4-8s | Tier 1 (Frontier) | Complex reasoning, research, code generation |
| **Sonnet 4.5** | $3 | $15 | 1-3s | Tier 1 (Production) | General chat, summarization, most tasks |
| **Haiku 4.5** | $0.25 | $1.25 | 0.3-0.8s | Tier 2 (Fast) | Classification, extraction, high-volume |

**The Cost Multiplier Effect**:
- Opus is **60x more expensive** than Haiku
- Sonnet is **12x more expensive** than Haiku
- At 500M tokens/day, the difference between Haiku and Opus is **$221K/month**

### The Model Selection Matrix: ROI-Driven Decisions

**Framework**: Start with the cheapest model, upgrade only when quality metrics fail.

```typescript
interface ModelTier {
  name: string
  costPerMTok: { input: number; output: number }
  latency: number
  qualityScore: number  // 0-100, measured on your eval set
}

const TIER_SYSTEM: ModelTier[] = [
  {
    name: 'claude-haiku-4.5',
    costPerMTok: { input: 0.25, output: 1.25 },
    latency: 500,      // ms p95
    qualityScore: 75   // Your evals
  },
  {
    name: 'claude-sonnet-4.5',
    costPerMTok: { input: 3, output: 15 },
    latency: 2000,
    qualityScore: 92
  },
  {
    name: 'claude-opus-4.5',
    costPerMTok: { input: 15, output: 75 },
    latency: 6000,
    qualityScore: 98
  }
]

// Decision Logic: Cost-First Selection
function selectModel(
  taskType: string,
  qualityThreshold: number = 85
): string {
  // Try tiers in cost order
  for (const tier of TIER_SYSTEM) {
    const taskQuality = getTaskQuality(taskType, tier.name)

    if (taskQuality >= qualityThreshold) {
      console.log(`Selected ${tier.name} (quality: ${taskQuality}, cost: $${tier.costPerMTok.input}/MTok)`)
      return tier.name
    }
  }

  // Fallback to highest quality if nothing passes
  return TIER_SYSTEM[TIER_SYSTEM.length - 1].name
}

// Example: Task-based quality mapping
function getTaskQuality(task: string, model: string): number {
  const qualityMatrix: Record<string, Record<string, number>> = {
    'classification': {
      'claude-haiku-4.5': 95,   // ‚úÖ Haiku excels at this
      'claude-sonnet-4.5': 97,
      'claude-opus-4.5': 98
    },
    'summarization': {
      'claude-haiku-4.5': 78,   // ‚ö†Ô∏è  Below threshold for critical use
      'claude-sonnet-4.5': 93,  // ‚úÖ Sonnet hits 85%+ threshold
      'claude-opus-4.5': 97
    },
    'code-generation': {
      'claude-haiku-4.5': 65,   // ‚ùå Fails threshold
      'claude-sonnet-4.5': 88,  // ‚úÖ Acceptable
      'claude-opus-4.5': 96     // ‚úÖ Best, but is it worth 5x cost?
    }
  }

  return qualityMatrix[task]?.[model] ?? 0
}
```

**Result**:
- Classification: **Always use Haiku** (95% quality, $0.25/MTok)
- Summarization: **Use Sonnet** (93% quality, $3/MTok) - Haiku fails threshold
- Code Gen: **Use Sonnet** (88% quality, $3/MTok) - Opus's 8-point improvement doesn't justify 5x cost

### The Dynamic Router Pattern: Automated Complexity-Aware Routing

**Architect's Principle**: Don't hardcode your model selection. Build a **Dynamic Router** that automatically chooses the optimal model based on real-time complexity analysis.

**The Problem with Static Selection**: In production, you don't manually decide which model to use for each request. You need an intelligent routing layer that:
1. Analyzes incoming request complexity in **&lt;50ms**
2. Routes to the cheapest capable model
3. Tracks performance metrics for continuous optimization

#### Production Implementation: The Complexity Router

```typescript
interface ComplexityScore {
  score: number          // 0-100, where 100 = most complex
  reasoning: string
  recommendedModel: string
  estimatedCost: number
}

interface RouterMetrics {
  totalRequests: number
  routingBreakdown: Record<string, number>
  costSavings: number
  avgComplexityScore: number
}

export class ComplexityAwareRouter {
  private metrics: RouterMetrics = {
    totalRequests: 0,
    routingBreakdown: {},
    costSavings: 0,
    avgComplexityScore: 0
  }

  /**
   * Fast complexity scoring using lightweight heuristics
   * Production requirement: &lt;50ms latency
   */
  private async scoreComplexity(prompt: string): Promise<ComplexityScore> {
    // OPTIMIZATION: Use cheap heuristics first, only call LLM if uncertain

    // Rule 1: Length-based heuristic (0ms)
    const wordCount = prompt.split(/\s+/).length
    if (wordCount &lt; 20) {
      return {
        score: 15,
        reasoning: 'Short query, likely classification or simple lookup',
        recommendedModel: 'claude-haiku-4.5',
        estimatedCost: 0.0005
      }
    }

    // Rule 2: Keyword detection (0ms)
    const complexKeywords = ['analyze', 'compare', 'explain why', 'reasoning', 'strategy', 'architecture']
    const simpleKeywords = ['classify', 'extract', 'summarize', 'list']

    const hasComplexKeywords = complexKeywords.some(kw => prompt.toLowerCase().includes(kw))
    const hasSimpleKeywords = simpleKeywords.some(kw => prompt.toLowerCase().includes(kw))

    if (hasSimpleKeywords && !hasComplexKeywords) {
      return {
        score: 25,
        reasoning: 'Simple extraction/classification task',
        recommendedModel: 'claude-haiku-4.5',
        estimatedCost: 0.0006
      }
    }

    if (hasComplexKeywords) {
      return {
        score: 85,
        reasoning: 'Complex reasoning required',
        recommendedModel: 'claude-opus-4.5',
        estimatedCost: 0.045
      }
    }

    // Rule 3: Fallback to 1-token LLM classifier (only if heuristics uncertain)
    // Use Haiku for fast classification: ~30ms, $0.0001 cost
    const classificationResponse = await anthropic.messages.create({
      model: 'claude-haiku-4.5',
      max_tokens: 10,  // 1-token response: just the score
      temperature: 0,
      messages: [{
        role: 'user',
        content: `Rate this query's complexity 0-100 (0=trivial, 100=deep reasoning). Return only the number.\n\nQuery: "${prompt}"`
      }]
    })

    const score = parseInt(classificationResponse.content[0].text.trim())

    // Route based on score thresholds
    let model: string
    let cost: number
    if (score &lt; 40) {
      model = 'claude-haiku-4.5'
      cost = 0.0006
    } else if (score &lt; 75) {
      model = 'claude-sonnet-4.5'
      cost = 0.009
    } else {
      model = 'claude-opus-4.5'
      cost = 0.045
    }

    return {
      score,
      reasoning: `Complexity score ${score} via Haiku classifier`,
      recommendedModel: model,
      estimatedCost: cost
    }
  }

  /**
   * Main routing function - automatically selects optimal model
   */
  async route(prompt: string): Promise<{
    response: string
    model: string
    cost: number
    complexityScore: number
  }> {
    const startTime = Date.now()

    // Step 1: Score complexity (&lt;50ms)
    const complexity = await this.scoreComplexity(prompt)

    // Step 2: Route to recommended model
    const response = await anthropic.messages.create({
      model: complexity.recommendedModel,
      max_tokens: 1024,
      messages: [{ role: 'user', content: prompt }]
    })

    const actualCost = this.calculateActualCost(
      complexity.recommendedModel,
      response.usage.input_tokens,
      response.usage.output_tokens
    )

    // Step 3: Track metrics
    this.updateMetrics(complexity.recommendedModel, complexity.score, actualCost)

    const latency = Date.now() - startTime
    console.log(`‚úÖ Routed to ${complexity.recommendedModel} (score: ${complexity.score}, cost: $${actualCost.toFixed(4)}, latency: ${latency}ms)`)

    return {
      response: response.content[0].text,
      model: complexity.recommendedModel,
      cost: actualCost,
      complexityScore: complexity.score
    }
  }

  private calculateActualCost(model: string, inputTokens: number, outputTokens: number): number {
    const pricing: Record<string, { input: number; output: number }> = {
      'claude-haiku-4.5': { input: 0.25, output: 1.25 },
      'claude-sonnet-4.5': { input: 3, output: 15 },
      'claude-opus-4.5': { input: 15, output: 75 }
    }

    const price = pricing[model]
    return (inputTokens / 1_000_000) * price.input + (outputTokens / 1_000_000) * price.output
  }

  private updateMetrics(model: string, score: number, cost: number) {
    this.metrics.totalRequests++
    this.metrics.routingBreakdown[model] = (this.metrics.routingBreakdown[model] || 0) + 1

    // Track cost savings vs. "always use Opus"
    const opusCost = 0.045  // Estimated
    this.metrics.costSavings += (opusCost - cost)

    // Update rolling average complexity
    this.metrics.avgComplexityScore =
      (this.metrics.avgComplexityScore * (this.metrics.totalRequests - 1) + score) /
      this.metrics.totalRequests
  }

  /**
   * Production analytics - understand routing patterns
   */
  getRouterReport(): string {
    const total = this.metrics.totalRequests
    const breakdown = Object.entries(this.metrics.routingBreakdown)
      .map(([model, count]) => `  ${model}: ${((count / total) * 100).toFixed(1)}% (${count} requests)`)
      .join('\n')

    return `
üéØ Dynamic Router Report
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
Total Requests: ${total.toLocaleString()}
Cost Savings: $${this.metrics.costSavings.toFixed(2)} (vs always-Opus)
Avg Complexity: ${this.metrics.avgComplexityScore.toFixed(1)}/100

Routing Breakdown:
${breakdown}

Cost Efficiency: ${((this.metrics.costSavings / (total * 0.045)) * 100).toFixed(1)}% savings
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
    `
  }
}
```

#### Architect's Implementation Tips

**1. Don't Pay for Classification**
> "The router's complexity scoring should cost &lt;1% of the query cost. Use lightweight heuristics (word count, keyword matching) before calling an LLM. When you do need an LLM classifier, use Haiku with max_tokens=10 for a 1-token score."

**2. The 80/20 Rule**
> "If 80% of your queries are 'simple' (score &lt;40), route them to Haiku. This is how you achieve that **80% cost reduction** without losing quality. Track your actual routing distribution and adjust thresholds."

**3. Continuous Optimization**
> "The router should learn from production traffic. If Haiku consistently handles score=45 queries well, raise your Haiku threshold from 40‚Üí50. This incremental tuning compounds into massive savings."

#### Production Results: Real-World Impact

**Scenario**: Customer support chatbot processing 100K requests/day

**Before Dynamic Router** (all requests ‚Üí Sonnet):
- Cost: 100K √ó $0.009 = $900/day = **$27,000/month**
- Quality: Sonnet consistently over-provisioned for simple queries

**After Dynamic Router**:
```typescript
// Actual routing distribution from 30 days of production traffic
Routing Breakdown:
  claude-haiku-4.5:  78.2% (78,200 requests/day)
  claude-sonnet-4.5: 18.5% (18,500 requests/day)
  claude-opus-4.5:    3.3% (3,300 requests/day)

// Cost calculation
Daily cost:
  Haiku:  78,200 √ó $0.0006 = $46.92
  Sonnet: 18,500 √ó $0.009  = $166.50
  Opus:   3,300  √ó $0.045  = $148.50
  Total: $361.92/day = $10,858/month

Savings: $27,000 - $10,858 = $16,142/month (60% reduction)
Quality: Maintained (95%+ satisfaction across all tiers)
```

**The ROI**:
- Implementation: 8 engineering hours = $1,600
- Monthly savings: $16,142
- Payback period: **3 days**

### ROI Calculation: The Cost-Per-Conversation Economics

**Framework**: Calculate unit economics to justify model selection.

```typescript
interface ConversationCost {
  model: string
  avgInputTokens: number
  avgOutputTokens: number
  conversationsPerDay: number
  monthlyCost: number
  costPerConversation: number
}

function calculateCosts(
  model: string,
  inputPrice: number,
  outputPrice: number,
  avgInputTokens: number,
  avgOutputTokens: number,
  dailyConversations: number
): ConversationCost {
  // Cost per single conversation
  const inputCost = (avgInputTokens / 1_000_000) * inputPrice
  const outputCost = (avgOutputTokens / 1_000_000) * outputPrice
  const costPerConvo = inputCost + outputCost

  // Monthly cost at scale
  const monthlyCost = costPerConvo * dailyConversations * 30

  return {
    model,
    avgInputTokens,
    avgOutputTokens,
    conversationsPerDay: dailyConversations,
    monthlyCost,
    costPerConversation: costPerConvo
  }
}

// Example: Customer support chatbot
const DAILY_CONVERSATIONS = 50_000
const AVG_INPUT_TOKENS = 2_000   // System prompt + history + question
const AVG_OUTPUT_TOKENS = 500    // Response length

const scenarios = [
  calculateCosts('Opus 4.5', 15, 75, AVG_INPUT_TOKENS, AVG_OUTPUT_TOKENS, DAILY_CONVERSATIONS),
  calculateCosts('Sonnet 4.5', 3, 15, AVG_INPUT_TOKENS, AVG_OUTPUT_TOKENS, DAILY_CONVERSATIONS),
  calculateCosts('Haiku 4.5', 0.25, 1.25, AVG_INPUT_TOKENS, AVG_OUTPUT_TOKENS, DAILY_CONVERSATIONS)
]

/* Results:
Opus 4.5:   $0.0675/conversation √ó 50K/day = $101,250/month
Sonnet 4.5: $0.0135/conversation √ó 50K/day = $20,250/month
Haiku 4.5:  $0.00113/conversation √ó 50K/day = $1,695/month

ROI Analysis:
- Opus costs 60x more than Haiku
- Sonnet costs 12x more than Haiku
- If Haiku quality is 85% and Sonnet is 92%:
  ‚Üí Is 7% quality improvement worth $18,555/month?
  ‚Üí Only if customer retention impact > $18,555/month
*/
```

**Decision Framework**:
```typescript
// The ROI Formula
const qualityGain = sonnetQuality - haikuQuality  // e.g., 92 - 85 = 7%
const costDelta = sonnetCost - haikuCost          // e.g., $20,250 - $1,695 = $18,555
const breakEvenLTV = costDelta / (conversionRate * qualityGain)

// Example: If 7% quality improvement increases conversions by 3%
const additionalRevenue = dailyConversations * 0.03 * avgOrderValue
// Use Sonnet only if additionalRevenue > $18,555/month
```

### The Value-per-Token (VPT) Metric: Director-Level Cost Analysis

**Architect's Principle**: In a Director-level review, you don't say "Opus is better." You say "The VPT for this specific clinical reasoning task justifies the 3x premium because it reduces diagnostic error rates by 12%."

**The Problem with Simple Cost Comparison**: Saying "Opus costs 5x more than Sonnet" is incomplete. The real question is: **Does the quality improvement generate 5x more value?**

#### The VPT Formula

**Value-per-Token (VPT)** = (Revenue Impact - Cost Delta) / Token Delta

This metric tells you whether upgrading to a more expensive model generates positive ROI.

```typescript
interface VPTAnalysis {
  baselineModel: string
  upgradedModel: string
  costDelta: number          // Additional cost per month
  qualityDelta: number       // Quality improvement (0-100)
  revenueImpact: number      // Estimated revenue increase per month
  vpt: number                // Value per token
  recommendation: 'UPGRADE' | 'STAY' | 'TEST'
}

/**
 * Calculate Value-per-Token for model upgrade decision
 */
function calculateVPT(
  task: string,
  baselineModel: string,
  upgradedModel: string,
  dailyVolume: number,
  avgOrderValue: number,
  conversionRate: number
): VPTAnalysis {
  // Get quality scores from your eval set
  const baselineQuality = getTaskQuality(task, baselineModel)  // e.g., 90%
  const upgradedQuality = getTaskQuality(task, upgradedModel)  // e.g., 96%
  const qualityDelta = upgradedQuality - baselineQuality       // e.g., 6%

  // Get cost delta
  const baselineCost = estimateRequestCost(baselineModel)      // e.g., $0.009 (Sonnet)
  const upgradedCost = estimateRequestCost(upgradedModel)      // e.g., $0.045 (Opus)
  const costDelta = upgradedCost - baselineCost                // e.g., $0.036

  // Estimate revenue impact
  // Assumption: Quality improvement correlates with conversion lift
  const conversionLift = qualityDelta * 0.5  // Conservative: 6% quality ‚Üí 3% conversion lift
  const additionalConversions = dailyVolume * (conversionLift / 100)
  const dailyRevenueImpact = additionalConversions * avgOrderValue * conversionRate

  // Monthly impact
  const monthlyCostDelta = costDelta * dailyVolume * 30
  const monthlyRevenueImpact = dailyRevenueImpact * 30

  // VPT calculation
  const netValue = monthlyRevenueImpact - monthlyCostDelta
  const tokenDelta = estimateTokenDelta(baselineModel, upgradedModel)
  const vpt = netValue / tokenDelta

  // Decision logic
  let recommendation: 'UPGRADE' | 'STAY' | 'TEST'
  if (vpt > 0 && monthlyRevenueImpact > monthlyCostDelta * 2) {
    recommendation = 'UPGRADE'  // Clear positive ROI
  } else if (vpt &lt; 0) {
    recommendation = 'STAY'     // Negative ROI
  } else {
    recommendation = 'TEST'     // Marginal - run A/B test
  }

  return {
    baselineModel,
    upgradedModel,
    costDelta: monthlyCostDelta,
    qualityDelta,
    revenueImpact: monthlyRevenueImpact,
    vpt,
    recommendation
  }
}

// Helper functions
function estimateRequestCost(model: string): number {
  const avgTokens = 2000  // input + output
  const pricing: Record<string, number> = {
    'claude-haiku-4.5': 0.0006,
    'claude-sonnet-4.5': 0.009,
    'claude-opus-4.5': 0.045
  }
  return pricing[model]
}

function estimateTokenDelta(baseline: string, upgraded: string): number {
  // Opus typically generates slightly longer responses
  const tokenMultipliers: Record<string, number> = {
    'claude-haiku-4.5': 1.0,
    'claude-sonnet-4.5': 1.1,
    'claude-opus-4.5': 1.15
  }
  return (tokenMultipliers[upgraded] - tokenMultipliers[baseline]) * 1_000_000
}
```

#### Real-World Example: E-Commerce Product Recommendations

**Context**:
- **Feature**: AI-powered product recommendations
- **Volume**: 50,000 users/day browsing product pages
- **Current model**: Sonnet 4.5
- **Consideration**: Upgrade to Opus 4.5 for "better" recommendations

**Baseline Metrics** (Sonnet 4.5):
- Quality Score: 88% (measured via click-through rate)
- Cost: $0.009/request
- Monthly cost: 50K √ó 30 √ó $0.009 = **$13,500**
- Current conversion rate: 2.5%
- Avg order value: $75

**Upgraded Metrics** (Opus 4.5):
- Quality Score: 94% (6% improvement via A/B test)
- Cost: $0.045/request
- Monthly cost: 50K √ó 30 √ó $0.045 = **$67,500**
- Expected conversion rate: 2.58% (3% relative lift from quality improvement)
- Avg order value: $75 (unchanged)

**VPT Analysis**:

```typescript
const analysis = calculateVPT(
  'product-recommendations',
  'claude-sonnet-4.5',
  'claude-opus-4.5',
  50_000,  // daily volume
  75,      // avg order value
  0.025    // baseline conversion rate
)

/* Results:
{
  baselineModel: 'claude-sonnet-4.5',
  upgradedModel: 'claude-opus-4.5',
  costDelta: $54,000/month,
  qualityDelta: 6%,
  revenueImpact: $16,875/month,
  vpt: -$0.37,  // NEGATIVE!
  recommendation: 'STAY'
}

Breakdown:
- Cost increase: $67,500 - $13,500 = $54,000/month
- Revenue increase: 50K √ó 30 √ó 0.03 √ó 0.025 √ó $75 = $16,875/month
- Net impact: $16,875 - $54,000 = -$37,125/month loss

VERDICT: DO NOT UPGRADE. Opus's 6% quality improvement does NOT justify 5x cost increase.
*/
```

**The Architect's Decision**: **Stay on Sonnet.** While Opus improves quality by 6%, the incremental revenue ($16,875) is far below the cost increase ($54,000). **VPT is negative** (-$0.37), meaning every additional token costs you money.

#### Counterexample: When Opus Wins on VPT

**Context**: Clinical diagnosis assistant for rare diseases

**Baseline Metrics** (Sonnet 4.5):
- Diagnostic accuracy: 82%
- Cost: $0.009/diagnosis
- Monthly volume: 5,000 diagnoses
- Monthly cost: 5K √ó $0.009 = **$45**
- Misdiagnosis cost: Avg $15,000 in wasted tests/treatments + liability risk

**Upgraded Metrics** (Opus 4.5):
- Diagnostic accuracy: 94% (**12% improvement**)
- Cost: $0.045/diagnosis
- Monthly cost: 5K √ó $0.045 = **$225**
- Misdiagnosis reduction: 12% √ó 5,000 = 600 fewer errors/month

**VPT Analysis**:

```typescript
const medicalVPT = {
  costDelta: $225 - $45 = $180/month,
  qualityDelta: 12%,
  errorReduction: 600 misdiagnoses/month,
  costSavings: 600 √ó $15,000 = $9,000,000/month (avoided wasted treatments),
  vpt: $9,000,000 / 150_000 tokens = $60/token,  // MASSIVELY POSITIVE
  recommendation: 'UPGRADE IMMEDIATELY'
}

VERDICT: UPGRADE TO OPUS. The 12% accuracy improvement saves $9M/month in avoided
misdiagnosis costs. Spending an extra $180/month for $9M in savings is a no-brainer.
VPT is +$60/token.
```

**The Principle**: **VPT justifies Opus when quality improvements have high-value downstream impact** (clinical safety, legal compliance, enterprise sales). VPT rejects Opus when quality gains don't drive proportional revenue (casual recommendations, content generation).

#### Production Pattern: VPT-Driven Model Selection

```typescript
export class VPTOptimizedRouter {
  private vptCache = new Map<string, VPTAnalysis>()

  async routeWithVPT(
    task: string,
    userContext: {
      orderValue?: number
      riskLevel?: 'low' | 'medium' | 'high'
    }
  ): Promise<{ model: string; reasoning: string }> {
    // High-risk tasks: Always use highest quality (VPT analysis already done)
    if (userContext.riskLevel === 'high') {
      return {
        model: 'claude-opus-4.5',
        reasoning: 'High-risk task - quality takes precedence over cost'
      }
    }

    // High-value transactions: Calculate VPT
    if (userContext.orderValue && userContext.orderValue > 500) {
      // For high AOV, quality improvements have higher revenue impact
      // VPT likely positive for Sonnet/Opus upgrade
      return {
        model: 'claude-sonnet-4.5',
        reasoning: `High AOV ($${userContext.orderValue}) justifies Sonnet for quality`
      }
    }

    // Low-value, low-risk: Optimize for cost
    return {
      model: 'claude-haiku-4.5',
      reasoning: 'Low-risk, cost-optimized routing'
    }
  }

  /**
   * Run VPT analysis for a new feature/task
   */
  analyzeFeatureVPT(
    featureName: string,
    dailyVolume: number,
    revenuePerConversion: number,
    baselineConversionRate: number
  ): void {
    console.log(`\nüìä VPT Analysis: ${featureName}`)
    console.log('‚îÅ'.repeat(50))

    // Compare all model pairs
    const pairs = [
      ['claude-haiku-4.5', 'claude-sonnet-4.5'],
      ['claude-sonnet-4.5', 'claude-opus-4.5'],
      ['claude-haiku-4.5', 'claude-opus-4.5']
    ]

    pairs.forEach(([baseline, upgraded]) => {
      const analysis = calculateVPT(
        featureName,
        baseline,
        upgraded,
        dailyVolume,
        revenuePerConversion,
        baselineConversionRate
      )

      console.log(`\n${baseline} ‚Üí ${upgraded}:`)
      console.log(`  Quality Œî: +${analysis.qualityDelta}%`)
      console.log(`  Cost Œî: $${analysis.costDelta.toLocaleString()}/month`)
      console.log(`  Revenue Œî: $${analysis.revenueImpact.toLocaleString()}/month`)
      console.log(`  VPT: $${analysis.vpt.toFixed(2)}/token`)
      console.log(`  üìã Recommendation: ${analysis.recommendation}`)
    })
  }
}
```

#### Architect's Implementation Tips

**1. VPT is Task-Specific**
> "A model upgrade might have positive VPT for medical diagnosis but negative VPT for casual chatbots. Always calculate VPT per feature, not globally."

**2. Quality Metrics Must Be Measured**
> "Don't guess quality improvements. Run A/B tests with real traffic. 'Opus feels better' is not a VPT input. 'Opus increased CTR from 4.2% ‚Üí 4.9%' is."

**3. VPT Compounds at Scale**
> "At 10K requests/day, negative VPT of -$0.10/token = -$30K/month. At 1M requests/day, that's -$3M/month. Small VPT differences become existential at scale."

## Optimization Strategies: Input Token Cost Management

**The Input Token Problem**: In conversational AI, input tokens (context) grow unbounded, but **you pay for every token on every request**.

### Cost Growth Pattern

```typescript
// Conversation turn costs
interface TurnCost {
  turn: number
  inputTokens: number
  cost: number
}

// ‚ùå Naive approach: Send full history every time
function calculateNaiveCosts(): TurnCost[] {
  const SYSTEM_PROMPT = 1_000  // tokens
  const TURN_SIZE = 500        // tokens per turn (user + assistant)

  const costs: TurnCost[] = []
  let cumulativeTokens = SYSTEM_PROMPT

  for (let turn = 1; turn <= 20; turn++) {
    cumulativeTokens += TURN_SIZE
    costs.push({
      turn,
      inputTokens: cumulativeTokens,
      cost: (cumulativeTokens / 1_000_000) * 3  // Sonnet pricing
    })
  }

  return costs
}

/* Result: Exponential cost growth
Turn 1:  1,500 tokens  = $0.0045
Turn 5:  3,500 tokens  = $0.0105
Turn 10: 6,000 tokens  = $0.0180
Turn 20: 11,000 tokens = $0.0330

20-turn conversation cost: 20 turns √ó avg $0.0195 = $0.39
At 50K conversations/day: $585K/month (unsustainable!)
*/
```

### Strategy 1: Truncate (Sliding Window)

**Pattern**: Keep only the last N turns, discard old context.

```typescript
// ‚úÖ Truncation: Fixed token budget
const MAX_HISTORY_TURNS = 10  // Last 10 turns only

function truncateHistory(
  messages: Message[],
  maxTurns: number = MAX_HISTORY_TURNS
): Message[] {
  // Always keep system prompt
  const systemMessage = messages.find(m => m.role === 'system')

  // Take last N user/assistant turns
  const conversationTurns = messages.filter(m => m.role !== 'system')
  const recentTurns = conversationTurns.slice(-maxTurns * 2)  // *2 for user+assistant pairs

  return systemMessage
    ? [systemMessage, ...recentTurns]
    : recentTurns
}

// Cost impact: Caps at ~6K tokens regardless of conversation length
// Turn 20 cost: $0.018 (instead of $0.033)
// Monthly savings: $585K ‚Üí $270K (54% reduction)
```

**Pros:**
- ‚úÖ Simple to implement
- ‚úÖ Predictable costs
- ‚úÖ No additional API calls

**Cons:**
- ‚ùå Loses context from early conversation
- ‚ùå Poor for long-running sessions (e.g., multi-day support tickets)

### Strategy 2: Summarize (Context Compression)

**Pattern**: Summarize old turns into a condensed context block.

```typescript
// ‚úÖ Summarization: Compress old context
async function summarizeAndCompress(
  messages: Message[],
  turnThreshold: number = 10
): Promise<Message[]> {
  if (messages.length <= turnThreshold * 2) {
    return messages  // Not long enough to summarize
  }

  const systemMessage = messages.find(m => m.role === 'system')
  const conversationTurns = messages.filter(m => m.role !== 'system')

  // Split into old (to summarize) and recent (keep verbatim)
  const oldTurns = conversationTurns.slice(0, -turnThreshold * 2)
  const recentTurns = conversationTurns.slice(-turnThreshold * 2)

  // Summarize old turns using Haiku (cheap summarization)
  const summary = await anthropic.messages.create({
    model: 'claude-haiku-4.5',  // $0.25/MTok for summarization
    max_tokens: 500,              // Cap summary length
    messages: [
      {
        role: 'user',
        content: `Summarize this conversation history into key points (max 500 tokens):

${oldTurns.map(m => `${m.role}: ${m.content}`).join('\n\n')}`
      }
    ]
  })

  // Build compressed context
  return [
    systemMessage!,
    { role: 'system', content: `Previous context: ${summary.content[0].text}` },
    ...recentTurns
  ]
}

/* Cost Analysis:
Original 20-turn conversation: 11,000 input tokens
After summarization:
  - System prompt: 1,000 tokens
  - Summary of turns 1-10: 500 tokens (compressed from 5,000)
  - Recent turns 11-20: 5,000 tokens
  - Total: 6,500 tokens (vs. 11,000 original)

Savings: 41% input token reduction
Summarization cost: ~2,500 tokens @ $0.25/MTok = $0.000625 (negligible)
*/
```

**Pros:**
- ‚úÖ Preserves context from entire conversation
- ‚úÖ Suitable for long-running sessions
- ‚úÖ Quality degradation minimal

**Cons:**
- ‚ùå Additional API call for summarization
- ‚ùå Adds latency (~500ms for summary)
- ‚ùå More complex implementation

### Decision Matrix: Truncate vs. Summarize

| Factor | Truncate | Summarize |
|--------|----------|-----------|
| **Cost Savings** | 54% | 41% + summary cost |
| **Context Retention** | Poor (loses history) | Good (compresses history) |
| **Complexity** | Low | Medium |
| **Latency** | None | +500ms per summarization |
| **Best For** | Short sessions (&lt;10 turns), FAQ bots | Long sessions, customer support, research |

**Production Pattern: Hybrid Approach**

```typescript
// ‚úÖ Best of both worlds
async function optimizeContext(
  messages: Message[],
  sessionType: 'short' | 'long'
): Promise<Message[]> {
  const turnCount = messages.filter(m => m.role !== 'system').length / 2

  if (sessionType === 'short' || turnCount <= 15) {
    // Short sessions: Truncate (simpler, faster)
    return truncateHistory(messages, 10)
  } else {
    // Long sessions: Summarize (preserve context)
    return await summarizeAndCompress(messages, 10)
  }
}
```

## Production Cost Optimization Patterns

### Pattern 1: Tiered Quality System

**Strategy**: Route tasks to different model tiers based on complexity and user tier.

```typescript
interface UserTier {
  tier: 'free' | 'pro' | 'enterprise'
  model: string
  maxTokens: number
  monthlyCost: number
}

const USER_TIER_CONFIG: Record<string, UserTier> = {
  free: {
    tier: 'free',
    model: 'claude-haiku-4.5',
    maxTokens: 500,
    monthlyCost: 0  // Target: break even with ads/upsell
  },
  pro: {
    tier: 'pro',
    model: 'claude-sonnet-4.5',
    maxTokens: 2000,
    monthlyCost: 20  // $20/month subscription
  },
  enterprise: {
    tier: 'enterprise',
    model: 'claude-opus-4.5',
    maxTokens: 4000,
    monthlyCost: 200  // $200/month subscription
  }
}

async function routeByUserTier(
  userTier: string,
  message: string
): Promise<string> {
  const config = USER_TIER_CONFIG[userTier]

  const response = await anthropic.messages.create({
    model: config.model,
    max_tokens: config.maxTokens,
    messages: [{ role: 'user', content: message }]
  })

  return response.content[0].text
}

/* Unit Economics by Tier:
Free tier:
  - Haiku @ $0.25/MTok input, ~2K tokens/request = $0.0005/request
  - Target: 40K requests/month breakeven (via ads/upsell)

Pro tier ($20/month):
  - Sonnet @ $3/MTok input, ~2K tokens/request = $0.006/request
  - Breakeven: 3,333 requests/month
  - Average user: ~500 requests/month ‚Üí $17 margin/user

Enterprise tier ($200/month):
  - Opus @ $15/MTok input, ~2K tokens/request = $0.030/request
  - Breakeven: 6,667 requests/month
  - Average user: ~2,000 requests/month ‚Üí $140 margin/user
*/
```

### Pattern 2: Intelligent Caching

**Strategy**: Cache common responses to avoid redundant API calls.

```typescript
import { createHash } from 'crypto'

interface CachedResponse {
  response: string
  timestamp: number
  hits: number
}

const responseCache = new Map<string, CachedResponse>()

function getCacheKey(prompt: string): string {
  return createHash('sha256').update(prompt).digest('hex')
}

async function getCachedOrGenerate(
  prompt: string,
  cacheTTL: number = 3600000  // 1 hour
): Promise<{ response: string; cached: boolean }> {
  const cacheKey = getCacheKey(prompt)
  const cached = responseCache.get(cacheKey)

  // Check cache
  if (cached && Date.now() - cached.timestamp < cacheTTL) {
    cached.hits++
    console.log(`Cache hit (saved $${calculateSavings(prompt)})`)
    return { response: cached.response, cached: true }
  }

  // Cache miss - call API
  const response = await anthropic.messages.create({
    model: 'claude-sonnet-4.5',
    max_tokens: 1024,
    messages: [{ role: 'user', content: prompt }]
  })

  const responseText = response.content[0].text

  // Cache response
  responseCache.set(cacheKey, {
    response: responseText,
    timestamp: Date.now(),
    hits: 1
  })

  return { response: responseText, cached: false }
}

/* ROI Analysis:
FAQ bot with 100K requests/day:
  - 20% of requests are duplicates (20K/day)
  - Avg request: 2K tokens input + 500 tokens output
  - Cost per request: (2000/1M √ó $3) + (500/1M √ó $15) = $0.0135
  - Savings: 20K requests √ó $0.0135 = $270/day = $8,100/month

Redis cache cost: ~$50/month
Net savings: $8,050/month (99.4% savings on cached requests)
*/
```

### Pattern 3: Batching and Asynchronous Processing

**Strategy**: Batch non-urgent requests to optimize throughput.

```typescript
interface BatchRequest {
  id: string
  prompt: string
  priority: 'urgent' | 'normal' | 'low'
  callback: (response: string) => void
}

class BatchProcessor {
  private queue: BatchRequest[] = []
  private processing = false

  async add(request: BatchRequest) {
    this.queue.push(request)

    // Urgent requests process immediately
    if (request.priority === 'urgent') {
      return this.processNow(request)
    }

    // Batch normal/low priority requests
    if (!this.processing && this.queue.length >= 10) {
      this.processBatch()
    }
  }

  private async processBatch() {
    this.processing = true
    const batch = this.queue.splice(0, 10)

    // Process in parallel (respect rate limits)
    const results = await Promise.allSettled(
      batch.map(req => this.callAPI(req.prompt))
    )

    results.forEach((result, i) => {
      if (result.status === 'fulfilled') {
        batch[i].callback(result.value)
      }
    })

    this.processing = false

    // Process next batch if queue has items
    if (this.queue.length >= 10) {
      this.processBatch()
    }
  }

  private async callAPI(prompt: string): Promise<string> {
    const response = await anthropic.messages.create({
      model: 'claude-haiku-4.5',  // Use cheaper model for batched tasks
      max_tokens: 512,
      messages: [{ role: 'user', content: prompt }]
    })
    return response.content[0].text
  }
}

/* Cost Optimization:
Batch processing allows:
  - Using cheaper models (Haiku) for non-urgent tasks
  - Rate limit optimization (fewer API calls during peak)
  - Better utilization of concurrent request limits

Example: Email classification system
  - Urgent emails: Sonnet in real-time (~$0.006/email)
  - Normal emails: Haiku in 30s batches (~$0.0005/email)
  - Savings: 92% on 80% of traffic = 73% overall cost reduction
*/
```

## Cost Attribution and Monitoring

**Architect Perspective**: You can't optimize what you don't measure. Track costs per user, per feature, per endpoint.

### Pattern 1: Per-Request Cost Tracking

```typescript
interface RequestCost {
  userId: string
  endpoint: string
  model: string
  inputTokens: number
  outputTokens: number
  cost: number
  timestamp: Date
}

async function trackCost(
  userId: string,
  endpoint: string,
  usage: { input_tokens: number; output_tokens: number },
  model: string
) {
  const pricing = {
    'claude-haiku-4.5': { input: 0.25, output: 1.25 },
    'claude-sonnet-4.5': { input: 3, output: 15 },
    'claude-opus-4.5': { input: 15, output: 75 }
  }

  const price = pricing[model]
  const cost =
    (usage.input_tokens / 1_000_000) * price.input +
    (usage.output_tokens / 1_000_000) * price.output

  await prisma.aiCost.create({
    data: {
      userId,
      endpoint,
      model,
      inputTokens: usage.input_tokens,
      outputTokens: usage.output_tokens,
      cost,
      timestamp: new Date()
    }
  })

  return cost
}

// Usage in API route
export async function POST(request: Request) {
  const { message } = await request.json()
  const userId = request.headers.get('x-user-id')

  const response = await anthropic.messages.create({
    model: 'claude-sonnet-4.5',
    max_tokens: 1024,
    messages: [{ role: 'user', content: message }]
  })

  // Track cost
  await trackCost(
    userId!,
    '/api/chat',
    response.usage,
    'claude-sonnet-4.5'
  )

  return Response.json({ reply: response.content[0].text })
}
```

### Pattern 2: Cost Anomaly Detection

```typescript
// Detect users/features with abnormally high costs
async function detectCostAnomalies() {
  const anomalies = await prisma.$queryRaw`
    SELECT
      userId,
      endpoint,
      SUM(cost) as total_cost,
      COUNT(*) as request_count,
      AVG(cost) as avg_cost
    FROM ai_costs
    WHERE timestamp > NOW() - INTERVAL '24 hours'
    GROUP BY userId, endpoint
    HAVING SUM(cost) > 100  -- Alert if user costs > $100/day
    ORDER BY total_cost DESC
  `

  return anomalies
}

/* Use cases:
1. Infinite loop bugs (user making 10K requests/min)
2. Abuse detection (free tier user exceeding quota)
3. Feature optimization (which endpoints are most expensive?)

Example output:
[
  { userId: 'user123', endpoint: '/api/code-review', total_cost: 250, request_count: 5000 },
  { userId: 'user456', endpoint: '/api/chat', total_cost: 180, request_count: 12000 }
]

Actions:
- Rate limit user123's code-review requests
- Investigate why user456 is making 12K chat requests/day
*/
```

### Pattern 3: Financial Circuit Breaker with Token Quotas

**Architect's Principle**: Cost governance isn't just a monthly bill check. You must implement **Hard Caps at the UserID level** to prevent infinite loops, automated "wallet-draining" attacks, and runaway costs.

**The Problem**: Without real-time enforcement, a single user can:
- Run infinite loops that cost thousands in minutes
- Execute automated prompt injection attacks
- Accidentally drain your entire budget before you notice

**The Solution**: Implement a **Financial Circuit Breaker** that trips in real-time when cost thresholds are exceeded.

```typescript
interface UserBudget {
  userId: string
  tier: 'free' | 'pro' | 'enterprise'

  // Hard limits
  hourlyHardCap: number      // Trip circuit if exceeded
  dailyHardCap: number
  monthlyHardCap: number

  // Soft warnings (alert but don't block)
  hourlySoftWarn: number
  dailySoftWarn: number

  // Current spend tracking
  currentHourlySpend: number
  currentDailySpend: number
  currentMonthlySpend: number

  // Circuit breaker state
  circuitState: 'CLOSED' | 'OPEN'  // OPEN = blocked
  circuitTrippedAt?: Date
  circuitResetAt?: Date

  // Rate limiting
  requestsThisHour: number
  requestsThisDay: number
  hourlyRequestLimit: number
  dailyRequestLimit: number
}

interface BudgetCheckResult {
  allowed: boolean
  reason?: string
  currentSpend: {
    hourly: number
    daily: number
    monthly: number
  }
  limitsRemaining: {
    hourly: number
    daily: number
    monthly: number
  }
  warning?: string
}

export class FinancialCircuitBreaker {
  private readonly TIER_CONFIGS = {
    free: {
      hourlyHardCap: 2.00,      // $2/hour max
      dailyHardCap: 10.00,      // $10/day max
      monthlyHardCap: 100.00,   // $100/month max
      hourlySoftWarn: 1.00,     // Warn at 50% of hard cap
      dailySoftWarn: 5.00,
      hourlyRequestLimit: 100,
      dailyRequestLimit: 1000
    },
    pro: {
      hourlyHardCap: 20.00,
      dailyHardCap: 100.00,
      monthlyHardCap: 1000.00,
      hourlySoftWarn: 10.00,
      dailySoftWarn: 50.00,
      hourlyRequestLimit: 1000,
      dailyRequestLimit: 10000
    },
    enterprise: {
      hourlyHardCap: 500.00,
      dailyHardCap: 5000.00,
      monthlyHardCap: 50000.00,
      hourlySoftWarn: 250.00,
      dailySoftWarn: 2500.00,
      hourlyRequestLimit: 10000,
      dailyRequestLimit: 100000
    }
  }

  /**
   * Check if request is allowed under budget constraints
   * CRITICAL: This must be called BEFORE processing any LLM request
   */
  async checkBudget(userId: string, estimatedCost: number = 0.01): Promise<BudgetCheckResult> {
    const budget = await this.getBudget(userId)

    // HARD STOP: Circuit breaker is open (user blocked)
    if (budget.circuitState === 'OPEN') {
      // Check if reset time has passed
      if (budget.circuitResetAt && new Date() > budget.circuitResetAt) {
        await this.resetCircuit(userId)
      } else {
        return {
          allowed: false,
          reason: `Financial circuit breaker OPEN. Exceeded ${budget.tier} tier limits. Resets at ${budget.circuitResetAt?.toISOString()}`,
          currentSpend: {
            hourly: budget.currentHourlySpend,
            daily: budget.currentDailySpend,
            monthly: budget.currentMonthlySpend
          },
          limitsRemaining: { hourly: 0, daily: 0, monthly: 0 }
        }
      }
    }

    const config = this.TIER_CONFIGS[budget.tier]

    // Check 1: Hourly hard cap
    if (budget.currentHourlySpend + estimatedCost > config.hourlyHardCap) {
      await this.tripCircuit(userId, 'HOURLY_HARD_CAP', config.hourlyHardCap)
      return {
        allowed: false,
        reason: `Hourly hard cap exceeded: $${budget.currentHourlySpend.toFixed(2)}/$${config.hourlyHardCap} (${budget.tier} tier)`,
        currentSpend: {
          hourly: budget.currentHourlySpend,
          daily: budget.currentDailySpend,
          monthly: budget.currentMonthlySpend
        },
        limitsRemaining: { hourly: 0, daily: 0, monthly: 0 }
      }
    }

    // Check 2: Daily hard cap
    if (budget.currentDailySpend + estimatedCost > config.dailyHardCap) {
      await this.tripCircuit(userId, 'DAILY_HARD_CAP', config.dailyHardCap)
      return {
        allowed: false,
        reason: `Daily hard cap exceeded: $${budget.currentDailySpend.toFixed(2)}/$${config.dailyHardCap}`,
        currentSpend: {
          hourly: budget.currentHourlySpend,
          daily: budget.currentDailySpend,
          monthly: budget.currentMonthlySpend
        },
        limitsRemaining: { hourly: 0, daily: 0, monthly: 0 }
      }
    }

    // Check 3: Monthly hard cap
    if (budget.currentMonthlySpend + estimatedCost > config.monthlyHardCap) {
      await this.tripCircuit(userId, 'MONTHLY_HARD_CAP', config.monthlyHardCap)
      return {
        allowed: false,
        reason: `Monthly hard cap exceeded: $${budget.currentMonthlySpend.toFixed(2)}/$${config.monthlyHardCap}`,
        currentSpend: {
          hourly: budget.currentHourlySpend,
          daily: budget.currentDailySpend,
          monthly: budget.currentMonthlySpend
        },
        limitsRemaining: { hourly: 0, daily: 0, monthly: 0 }
      }
    }

    // Check 4: Rate limiting (prevent infinite loops)
    if (budget.requestsThisHour >= config.hourlyRequestLimit) {
      return {
        allowed: false,
        reason: `Hourly request limit exceeded: ${budget.requestsThisHour}/${config.hourlyRequestLimit}`,
        currentSpend: {
          hourly: budget.currentHourlySpend,
          daily: budget.currentDailySpend,
          monthly: budget.currentMonthlySpend
        },
        limitsRemaining: { hourly: 0, daily: 0, monthly: 0 }
      }
    }

    // Soft warnings (alert but don't block)
    let warning: string | undefined
    if (budget.currentHourlySpend > config.hourlySoftWarn) {
      warning = `‚ö†Ô∏è  Approaching hourly limit: $${budget.currentHourlySpend.toFixed(2)}/$${config.hourlyHardCap}`
    }

    // Request allowed
    return {
      allowed: true,
      currentSpend: {
        hourly: budget.currentHourlySpend,
        daily: budget.currentDailySpend,
        monthly: budget.currentMonthlySpend
      },
      limitsRemaining: {
        hourly: config.hourlyHardCap - budget.currentHourlySpend,
        daily: config.dailyHardCap - budget.currentDailySpend,
        monthly: config.monthlyHardCap - budget.currentMonthlySpend
      },
      warning
    }
  }

  /**
   * Trip the circuit breaker - BLOCKS all requests until reset
   */
  private async tripCircuit(userId: string, reason: string, limit: number) {
    const resetTime = new Date(Date.now() + 60 * 60 * 1000)  // Reset in 1 hour

    await redis.hset(`budget:${userId}`, {
      circuitState: 'OPEN',
      circuitTrippedAt: new Date().toISOString(),
      circuitResetAt: resetTime.toISOString()
    })

    // Alert ops team
    await this.sendAlert({
      severity: 'CRITICAL',
      userId,
      reason: `Financial circuit breaker TRIPPED: ${reason}`,
      limit,
      resetTime
    })

    console.error(`üö® CIRCUIT BREAKER TRIPPED: User ${userId} - ${reason} (limit: $${limit})`)
  }

  /**
   * Reset circuit breaker after cooldown period
   */
  private async resetCircuit(userId: string) {
    await redis.hset(`budget:${userId}`, {
      circuitState: 'CLOSED',
      circuitTrippedAt: null,
      circuitResetAt: null,
      // Reset hourly counters
      currentHourlySpend: 0,
      requestsThisHour: 0
    })

    console.log(`‚úÖ Circuit breaker RESET: User ${userId}`)
  }

  /**
   * Increment budget after successful request
   * CRITICAL: Must be called AFTER request completes to track actual cost
   */
  async recordCost(userId: string, actualCost: number) {
    const budget = await this.getBudget(userId)

    // Atomic increment
    await redis.hincrby(`budget:${userId}`, 'currentHourlySpend', actualCost)
    await redis.hincrby(`budget:${userId}`, 'currentDailySpend', actualCost)
    await redis.hincrby(`budget:${userId}`, 'currentMonthlySpend', actualCost)
    await redis.hincrby(`budget:${userId}`, 'requestsThisHour', 1)
    await redis.hincrby(`budget:${userId}`, 'requestsThisDay', 1)

    // Check if we've crossed soft warning threshold (post-request)
    const newHourlySpend = budget.currentHourlySpend + actualCost
    const config = this.TIER_CONFIGS[budget.tier]

    if (newHourlySpend > config.hourlySoftWarn && budget.currentHourlySpend <= config.hourlySoftWarn) {
      await this.sendAlert({
        severity: 'WARNING',
        userId,
        reason: `Soft warning: Hourly spend $${newHourlySpend.toFixed(2)} exceeded $${config.hourlySoftWarn}`,
        limit: config.hourlyHardCap
      })
    }
  }

  private async getBudget(userId: string): Promise<UserBudget> {
    const budget = await redis.hgetall(`budget:${userId}`)

    if (!budget || Object.keys(budget).length === 0) {
      // Initialize new user
      const defaultBudget: UserBudget = {
        userId,
        tier: 'free',
        ...this.TIER_CONFIGS.free,
        currentHourlySpend: 0,
        currentDailySpend: 0,
        currentMonthlySpend: 0,
        circuitState: 'CLOSED',
        requestsThisHour: 0,
        requestsThisDay: 0
      }
      await redis.hset(`budget:${userId}`, defaultBudget as any)
      return defaultBudget
    }

    return budget as unknown as UserBudget
  }

  private async sendAlert(alert: {
    severity: 'WARNING' | 'CRITICAL'
    userId: string
    reason: string
    limit: number
    resetTime?: Date
  }) {
    // Integration with monitoring (Datadog, PagerDuty, etc.)
    console.log(`[${alert.severity}] ${alert.reason}`)

    // In production: Send to Slack, PagerDuty, etc.
    // await slack.send({ channel: '#ai-alerts', text: alert.reason })
  }
}
```

#### Production Usage: API Route with Circuit Breaker

```typescript
const circuitBreaker = new FinancialCircuitBreaker()

export async function POST(request: Request) {
  const userId = request.headers.get('x-user-id')!
  const { message } = await request.json()

  // STEP 1: Check budget BEFORE calling LLM
  const budgetCheck = await circuitBreaker.checkBudget(userId, 0.01)  // Estimated cost

  if (!budgetCheck.allowed) {
    return Response.json(
      {
        error: budgetCheck.reason,
        limitsRemaining: budgetCheck.limitsRemaining
      },
      { status: 429 }  // 429 Too Many Requests
    )
  }

  // Log soft warnings
  if (budgetCheck.warning) {
    console.warn(budgetCheck.warning)
  }

  // STEP 2: Call LLM (budget check passed)
  const response = await anthropic.messages.create({
    model: 'claude-sonnet-4.5',
    max_tokens: 1024,
    messages: [{ role: 'user', content: message }]
  })

  // STEP 3: Record actual cost
  const actualCost = calculateCost(response.usage.input_tokens, response.usage.output_tokens)
  await circuitBreaker.recordCost(userId, actualCost)

  return Response.json({
    reply: response.content[0].text,
    cost: actualCost,
    limitsRemaining: budgetCheck.limitsRemaining
  })
}
```

#### Architect's Implementation Tips

**1. Hard Caps are Non-Negotiable**
> "If a user's session cost exceeds $2.00 in an hour, the Safety Proxy should trip a 'Financial Circuit Breaker' to prevent infinite loops or automated 'wallet-draining' attacks. This is not optional‚Äîit's architectural self-defense."

**2. Hourly Limits Matter More Than Daily**
> "A runaway script can drain $10K in 10 minutes. Daily limits won't save you. Implement **hourly hard caps** with circuit breaker logic that trips immediately. Free tier: $2/hour. Pro: $20/hour. Enterprise: $500/hour."

**3. Rate Limiting Complements Cost Limits**
> "Even if cost is within limits, 1000 requests/minute signals a bug or attack. Enforce **request-per-hour limits** (free: 100/hr, pro: 1000/hr) to catch infinite loops before they cost you money."

#### Real-World Impact

**Before Financial Circuit Breaker**:
- Incident: Free tier user ran infinite loop
- Duration: 22 minutes before manual intervention
- Cost: **$8,247** in Opus API calls
- Recovery: Manual account block, no automated protection

**After Financial Circuit Breaker**:
- Same scenario: Infinite loop triggered
- Circuit tripped: After $2.00 hourly cap (3.8 minutes)
- Cost: **$2.00** (99.98% damage prevention)
- Recovery: Automatic 1-hour block, user notified, service protected

**The ROI**: Single implementation prevents catastrophic cost incidents. The Financial Circuit Breaker is the difference between a $10 monthly bill and a $100K surprise.
```

## The Cost Optimization Decision Tree

Use this framework to make ROI-driven architectural decisions:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ New AI Feature Request              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
               v
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ What's the volume? ‚îÇ
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ               ‚îÇ
       v               v
   &lt;1K req/day    >10K req/day
       ‚îÇ               ‚îÇ
       ‚îÇ               v
       ‚îÇ      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ      ‚îÇ Run cost analysis:   ‚îÇ
       ‚îÇ      ‚îÇ - Tokens per request ‚îÇ
       ‚îÇ      ‚îÇ - Model needed       ‚îÇ
       ‚îÇ      ‚îÇ - Monthly projection ‚îÇ
       ‚îÇ      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ                 ‚îÇ
       ‚îÇ                 v
       ‚îÇ        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ        ‚îÇ Cost &gt; $10K/month? ‚îÇ
       ‚îÇ        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ              ‚îÇ
       ‚îÇ      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ      ‚îÇ               ‚îÇ
       ‚îÇ     YES              NO
       ‚îÇ      ‚îÇ               ‚îÇ
       v      v               v
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ Start simple: ‚îÇ  ‚îÇ Optimize strategy:  ‚îÇ
   ‚îÇ - Use Sonnet  ‚îÇ  ‚îÇ - Tier system       ‚îÇ
   ‚îÇ - No caching  ‚îÇ  ‚îÇ - Input compression ‚îÇ
   ‚îÇ - Optimize    ‚îÇ  ‚îÇ - Caching           ‚îÇ
   ‚îÇ   if needed   ‚îÇ  ‚îÇ - Cheaper models    ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Decision Framework by Feature Type

| Feature Type | Volume | Strategy | Expected Cost | Model Choice |
|--------------|--------|----------|---------------|--------------|
| **FAQ Bot** | High (100K+/day) | Cache + Haiku | $100-500/mo | Haiku 4.5 |
| **Customer Support** | Medium (10K/day) | Summarize history + Sonnet | $500-2K/mo | Sonnet 4.5 |
| **Code Review** | Low (1K/day) | Quality-first + Opus | $500-1K/mo | Opus 4.5 |
| **Content Generation** | Medium (5K/day) | Tier by user + Sonnet/Haiku | $200-1K/mo | Mixed |
| **Data Extraction** | Very High (1M+/day) | Batch + Haiku | $250-500/mo | Haiku 4.5 |

---

## Real-World Industry Application

### The Multi-Tier Triage System: Digital Health Platform

**The Challenge**: A global telehealth platform receives **50,000 patient queries per day**. Some are simple ("How do I reset my password?"), others are complex medical symptom questions requiring careful analysis. Using Opus 4.5 for all queries costs **$225K/month**‚Äîmaking the product unprofitable.

**Business Context:**
- **Volume**: 50,000 queries/day (1.5M/month)
- **Quality Requirement**: 100% accuracy on medical safety queries (no hallucinations on drug interactions, symptoms)
- **Cost Target**: &lt;$30K/month to maintain 70% gross margin
- **Latency SLA**: &lt;3 seconds p95 (user abandonment threshold)

**The Architectural Decision**: Implement an intelligent **multi-tier triage system** that routes queries to the cheapest model capable of handling them safely.

#### Solution Architecture

```typescript
interface TriageResult {
  classification: 'simple' | 'complex' | 'critical'
  confidence: number          // 0-1, confidence in classification
  routedTo: 'haiku' | 'opus' | 'human'
  response?: string
  cost: number                // Cost in dollars for this query
  latency: number             // Response time in ms
}

interface QueryMetrics {
  totalCost: number
  avgLatency: number
  routingBreakdown: {
    haiku: number            // % routed to Haiku
    opus: number             // % routed to Opus
    human: number            // % routed to human review
  }
}

async function triageQuery(userQuery: string): Promise<TriageResult> {
  const startTime = Date.now()

  // TIER 1: Fast classifier (Haiku 4.5) - determines complexity
  const classification = await classifyQueryComplexity(userQuery)

  if (classification.type === 'simple' && classification.confidence > 0.9) {
    // Handle with Haiku 4.5: password resets, appointment booking, basic FAQs
    const response = await callHaiku(userQuery)

    return {
      classification: 'simple',
      confidence: classification.confidence,
      routedTo: 'haiku',
      response,
      cost: calculateCost('haiku', userQuery, response),
      latency: Date.now() - startTime
    }
  }

  if (classification.type === 'critical' || classification.confidence &lt; 0.7) {
    // Safety-critical or uncertain: escalate to human
    await notifyHumanReviewer(userQuery, classification)

    return {
      classification: 'critical',
      confidence: classification.confidence,
      routedTo: 'human',
      cost: 0,  // Human review is separate cost center
      latency: Date.now() - startTime
    }
  }

  // TIER 2: Complex medical query - route to Opus 4.5
  const response = await callOpus(userQuery, {
    systemPrompt: MEDICAL_SAFETY_PROMPT,
    temperature: 0.0,  // Deterministic for medical advice
    maxTokens: 2048
  })

  // Validate medical response for safety
  const safetyCheck = await validateMedicalResponse(response)

  if (!safetyCheck.passed) {
    // Failed safety check - escalate to human
    await notifyHumanReviewer(userQuery, { issue: safetyCheck.reason })

    return {
      classification: 'complex',
      confidence: classification.confidence,
      routedTo: 'human',
      cost: calculateCost('opus', userQuery, response),
      latency: Date.now() - startTime
    }
  }

  return {
    classification: 'complex',
    confidence: classification.confidence,
    routedTo: 'opus',
    response,
    cost: calculateCost('opus', userQuery, response),
    latency: Date.now() - startTime
  }
}

// Classifier: Fast Haiku call to determine query complexity
async function classifyQueryComplexity(query: string): Promise<{
  type: 'simple' | 'complex' | 'critical'
  confidence: number
  reasoning: string
}> {
  const prompt = `You are a medical query classifier. Analyze this patient query and classify it:

SIMPLE: Password resets, appointment scheduling, contact info requests
COMPLEX: Medical symptoms, drug interactions, treatment questions
CRITICAL: Emergency symptoms (chest pain, difficulty breathing), suicidal ideation

Query: "${query}"

Return JSON with: type (simple/complex/critical), confidence (0-1), reasoning.`

  const response = await anthropic.messages.create({
    model: 'claude-haiku-4.5',
    max_tokens: 256,
    temperature: 0.0,
    messages: [{ role: 'user', content: prompt }]
  })

  return JSON.parse(response.content[0].text)
}

// Cost calculation helper
function calculateCost(
  model: 'haiku' | 'opus',
  input: string,
  output: string
): number {
  const inputTokens = estimateTokens(input)
  const outputTokens = estimateTokens(output)

  const pricing = {
    haiku: { input: 0.25, output: 1.25 },
    opus: { input: 15, output: 75 }
  }

  const cost =
    (inputTokens / 1_000_000) * pricing[model].input +
    (outputTokens / 1_000_000) * pricing[model].output

  return cost
}
```

#### Production Implementation: Metrics Tracking

```typescript
class TriageMonitor {
  private metrics: QueryMetrics = {
    totalCost: 0,
    avgLatency: 0,
    routingBreakdown: { haiku: 0, opus: 0, human: 0 }
  }

  async processQueryWithTracking(query: string): Promise<TriageResult> {
    const result = await triageQuery(query)

    // Update cost metrics
    this.metrics.totalCost += result.cost

    // Update routing breakdown
    this.metrics.routingBreakdown[result.routedTo]++

    // Update latency (rolling average)
    const totalQueries = Object.values(this.metrics.routingBreakdown)
      .reduce((a, b) => a + b, 0)
    this.metrics.avgLatency =
      (this.metrics.avgLatency * (totalQueries - 1) + result.latency) / totalQueries

    // Alert on anomalies
    if (result.cost > 0.50) {
      console.warn(`‚ö†Ô∏è  High-cost query: $${result.cost.toFixed(4)} - "${query.substring(0, 50)}..."`)
    }

    if (result.latency > 5000) {
      console.warn(`‚ö†Ô∏è  Slow query: ${result.latency}ms - "${query.substring(0, 50)}..."`)
    }

    return result
  }

  getDailyReport(): string {
    const { haiku, opus, human } = this.metrics.routingBreakdown
    const total = haiku + opus + human

    return `
üìä Daily Triage Report
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
Total Queries: ${total.toLocaleString()}
Total Cost: $${this.metrics.totalCost.toFixed(2)}
Avg Latency: ${this.metrics.avgLatency.toFixed(0)}ms

Routing Breakdown:
  üü¢ Haiku:  ${((haiku/total) * 100).toFixed(1)}% (${haiku.toLocaleString()} queries)
  üü° Opus:   ${((opus/total) * 100).toFixed(1)}% (${opus.toLocaleString()} queries)
  üî¥ Human:  ${((human/total) * 100).toFixed(1)}% (${human.toLocaleString()} queries)

Cost per Query: $${(this.metrics.totalCost / total).toFixed(4)}
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
    `
  }
}
```

#### Outcome Metrics

**Before Triage System** (all queries ‚Üí Opus 4.5):
- **Cost**: $225,000/month (1.5M queries √ó 3K avg tokens √ó $15/MTok)
- **Latency**: 4.2s p95
- **Quality**: 98% accuracy

**After Triage System** (intelligent routing):
- **Cost**: $28,500/month (80% reduction)
  - 80% simple queries ‚Üí Haiku ($0.01/query)
  - 15% complex queries ‚Üí Opus ($0.15/query)
  - 5% critical queries ‚Üí Human review ($0/direct API cost)
- **Latency**: 1.8s p95 (57% improvement)
  - Haiku queries: 0.6s avg
  - Opus queries: 4.5s avg
  - Weighted average: 1.8s
- **Quality**: 100% on safety-critical queries (maintained)

**Cost Breakdown**:
```typescript
// 50,000 queries/day √ó 30 days = 1,500,000 queries/month

// Simple queries (80% = 1,200,000)
const simpleCost = 1_200_000 * 0.01 = $12,000

// Complex queries (15% = 225,000)
const complexCost = 225_000 * 0.15 = $33,750

// Critical queries (5% = 75,000) - human review
const criticalCost = 0  // Separate budget

// Total API Cost
const totalCost = $12,000 + $33,750 = $45,750/month
```

**Actual Results** (after optimization):
- Implemented caching for FAQs (30% of simple queries): **$3,600/month savings**
- Fine-tuned Haiku confidence threshold (0.9 ‚Üí 0.85): **10% more queries to Haiku**
- **Final Cost**: **$28,500/month** (87% reduction from original)

**Business Impact**:
- **Gross Margin**: Increased from 30% ‚Üí 78%
- **SLA Compliance**: 99.2% of queries under 3s (vs 94% before)
- **Safety**: Zero medical accuracy incidents (100% HITL on critical)
- **ROI**: 3-month implementation, $196K/month savings = **22-day payback period**

#### Key Architectural Decisions

**Why Haiku 4.5 as Classifier**:
- **Speed**: 0.3-0.8s latency (vs 4-8s for Opus)
- **Cost**: $0.25/MTok (60x cheaper than Opus)
- **Accuracy**: 95% classification accuracy on test set (sufficient for routing)

**Why Confidence Threshold of 0.9**:
- **Safety**: Err on side of escalation (better to use Opus unnecessarily than miss critical symptoms)
- **Measured Impact**: Lowering to 0.85 increased Haiku routing by 10% but added 2% false negatives (unacceptable for medical)

**Why Human Escalation for &lt; 0.7 Confidence**:
- **Edge Cases**: Ambiguous queries (e.g., "I feel weird") need human judgment
- **Compliance**: HIPAA requires human review for uncertain medical advice
- **Cost**: Human review is expensive but necessary for 5% of queries

**The ROI Formula in Practice**:
```typescript
// Original approach
const opusCost = 1_500_000 * 0.15 = $225_000/month
const opusLatency = 4.2

// Triage approach
const triageCost = $28_500/month
const triageLatency = 1.8

// Savings
const monthlySavings = $225_000 - $28_500 = $196_500
const latencyImprovement = ((4.2 - 1.8) / 4.2) * 100 = 57%

// Implementation cost
const engineeringCost = 40 hours √ó $200/hr = $8,000
const paybackPeriod = $8,000 / $196_500 * 30 = 1.2 days
```

**The Architect's Principle**: Don't route everything to the best model. Build a **quality-aware routing system** that uses the cheapest model capable of meeting your quality threshold for each query type.

---

## Architect's Design Challenge: Unit Economics Case Study

**Context**: You are the AI Architect at a SaaS platform with **10,000 free-tier users**. You currently use **Sonnet 4.5** for the "Smart Summaries" feature, costing you **$2,500/month**. Your conversion rate to the paid tier is **1%** (100 paid users/month).

**Current Metrics**:
- Model: Claude Sonnet 4.5
- Monthly cost: $2,500 (10K users √ó 25 summaries/month √ó $0.01/summary)
- Summary quality: 90% user satisfaction
- Free ‚Üí Paid conversion: 1% (100 conversions/month)
- Paid tier revenue: $20/month √ó 100 users = $2,000/month
- **Current margin: -$500/month (losing money on free tier)**

**Alternative Scenarios**:

**Option A: Stay on Sonnet to keep quality high**
- Quality: 90% (unchanged)
- Cost: $2,500/month (unchanged)
- Conversion rate: 1% (unchanged)
- Net margin: -$500/month

**Option B: Switch to Haiku immediately to save 90% in costs**
- Quality: 82% (-8% drop from Sonnet)
- Cost: $250/month (90% reduction)
- Conversion rate: Unknown (quality drop might impact conversions)
- Estimated margin: +$1,750/month (if conversion holds)

**Option C: Run an A/B test to measure quality impact on conversion**
- Allocate 50% of traffic to Haiku, 50% to Sonnet for 30 days
- Measure: Does the 8% quality drop impact the 1% conversion rate?
- Test cost: $1,375/month (mixed) for 1 month
- If Haiku conversion = Sonnet conversion: Switch to Haiku (saves $2,250/month going forward)
- If Haiku conversion drops below breakeven: Stay on Sonnet or optimize differently

**Option D: Use Opus to try and increase the 1% conversion rate**
- Quality: 96% (+6% improvement over Sonnet)
- Cost: $12,500/month (5x increase)
- Estimated conversion lift: +20% relative (1.0% ‚Üí 1.2%)
- Additional revenue: 20 extra conversions √ó $20 = $400/month
- Net margin: -$10,900/month (catastrophic loss)

---

### Question: What is your architectural decision?

**A)** Stay on Sonnet to keep quality high
- *Reasoning*: Quality is paramount, cost is secondary
- *Risk*: Losing $500/month indefinitely

**B)** Switch to Haiku immediately to save 90% in costs
- *Reasoning*: Haiku's 82% quality is "good enough," instant profitability
- *Risk*: Quality drop might kill conversions (unknown impact)

**C)** Run an A/B test to measure Haiku's impact on conversion rate before committing
- *Reasoning*: Data-driven decision. If 82% quality maintains 1% conversion, Haiku wins. If not, we have data to decide.
- *Risk*: 1 month of mixed costs, but limited downside

**D)** Use Opus to try and increase the 1% conversion rate
- *Reasoning*: Better quality drives more conversions
- *Risk*: 5x cost increase for only 20% conversion lift = negative ROI

---

### The Correct Answer: **C** ‚Äî Run an A/B test

**Why C is the Architect's answer**:

**The Decision Logic**:
```typescript
// Option B (Haiku) breakeven analysis
const haikuCost = 250            // $250/month
const sonnetCost = 2500          // $2,500/month
const savings = sonnetCost - haikuCost  // $2,250/month

// For Haiku to be worse than Sonnet:
// Revenue loss must exceed cost savings
const currentRevenue = 100 * 20  // $2,000/month
const allowableRevenueLoss = savings  // $2,250/month

// Breakeven conversion rate:
// (currentConversionRate - newConversionRate) * 10K users * $20 = $2,250
// newConversionRate = 1.0% - (2250 / (10000 * 20)) = 1.0% - 1.125% = NEGATIVE

// This means Haiku would need to KILL conversions entirely (drop below -0.125%) to be worse than Sonnet
// Highly unlikely that 8% quality drop ‚Üí 100%+ conversion drop
```

**The A/B Test Design**:
```typescript
interface ABTestConfig {
  duration: number       // 30 days
  trafficSplit: number   // 50% control (Sonnet), 50% variant (Haiku)
  successMetric: string  // Free ‚Üí Paid conversion rate
  costThreshold: number  // If Haiku conversion &lt; 0.5%, abort
}

// Expected outcomes:
// Scenario 1 (80% probability): Haiku conversion = 0.95%-1.05%
//   ‚Üí Quality drop doesn't impact conversion
//   ‚Üí Switch to Haiku: Save $2,250/month
//   ‚Üí ROI: $27K/year savings

// Scenario 2 (15% probability): Haiku conversion = 0.70%-0.94%
//   ‚Üí Quality drop causes 6-30% conversion loss
//   ‚Üí Calculate breakeven: Is cost savings > revenue loss?
//   ‚Üí If yes: Switch. If no: Stay on Sonnet or optimize further.

// Scenario 3 (5% probability): Haiku conversion &lt; 0.70%
//   ‚Üí Quality drop causes >30% conversion loss
//   ‚Üí Stay on Sonnet, investigate other optimizations (caching, tier system)
```

**Why A is wrong**: Staying on Sonnet assumes quality improvement is worth the cost, but you haven't measured if 90% quality drives more conversions than 82% quality. You're losing $500/month on an assumption.

**Why B is wrong**: Switching immediately without testing is reckless. While Haiku likely maintains conversions, you're risking your entire paid tier revenue ($2K/month) to save $2,250. Test first, then commit.

**Why D is wrong**: Opus costs 5x more ($12,500) for a 6% quality improvement. Even if Opus increases conversions by 20% (1.0% ‚Üí 1.2%), that's only +$400/month revenue vs. +$10,000 cost. **VPT is massively negative.**

---

### The Architect's Principle: Always Data-Driven Optimization

**What makes C the architectural answer**:

1. **Risk-bounded**: 1-month test costs $1,375 (mixed). If it fails, you're out $625 vs. Sonnet baseline. If it succeeds, you save $2,250/month forever.

2. **Quantified decision criteria**: "If Haiku maintains >0.9% conversion, switch. Otherwise, stay."

3. **Scales learning**: The A/B test data informs future decisions (e.g., if Haiku drops conversion by 10%, you know the quality ‚Üí conversion elasticity for this feature).

4. **Production discipline**: Directors don't make $27K/year decisions ($2,250 √ó 12 months) based on gut feel. They run experiments.

**Post-Test Actions** (after 30 days):

```typescript
interface ABTestResults {
  sonnetGroup: {
    conversions: 50       // 1.0% of 5K users
    revenue: 1000         // 50 √ó $20
    cost: 1250            // 50% of $2,500
  }
  haikuGroup: {
    conversions: 49       // 0.98% of 5K users (2% drop, not significant)
    revenue: 980          // 49 √ó $20
    cost: 125             // 50% of $250
  }
}

// Decision logic:
const revenueImpact = 980 - 1000            // -$20/month (on 50% of traffic)
const costSavings = 1250 - 125              // +$1,125/month (on 50% of traffic)
const netBenefit = costSavings - revenueImpact  // +$1,145/month on 50% traffic

// Extrapolate to 100% Haiku traffic:
const fullHaikuBenefit = netBenefit * 2     // +$2,290/month
const conclusion = 'SWITCH TO HAIKU'

// Result: Save $2,290/month with &lt;2% conversion impact (statistically insignificant)
```

---

### Key Lesson: The Architect's Answer is Always Evidence-Based

In a Director-level review, you don't say:
- ‚ùå "Haiku is cheaper, let's use it"
- ‚ùå "Sonnet has better quality, worth the cost"
- ‚ùå "Opus will increase conversions"

You say:
- ‚úÖ "I ran a 30-day A/B test with 10K users. Haiku reduced cost by 90% ($2,250/month) with statistically insignificant conversion impact (&lt;2% drop). **VPT is +$2.29/token**. Recommendation: Switch to Haiku for this feature, monitor conversion for 90 days."

**This is what separates a developer from an architect.**

## Key Takeaways

**The Unit Economics Mindset:**
- Every architectural decision has a **cost multiplier**
- Opus costs **60x more** than Haiku‚Äîjustify the difference with revenue impact
- Input tokens grow unbounded‚Äî**truncate or summarize** to cap costs

**The Efficiency Frontier:**
- **Quality threshold** > **Minimum viable model**
- Haiku at 85% quality beats Sonnet at 92% if the 7% gain doesn't drive $18K/month in value
- Run **evals** to map task quality to models, then choose the cheapest model above threshold

**Cost Optimization Patterns:**
1. **Model Selection Matrix**: Route by task complexity, not by default
2. **Truncate vs. Summarize**: Truncate for short sessions, summarize for long ones
3. **Tiered Quality**: Free users get Haiku, paid users get Sonnet
4. **Caching**: 20% duplicate requests = $8K/month savings
5. **Batching**: Non-urgent tasks use cheaper models

**Cost Attribution:**
- **Track costs per user/endpoint/feature**‚Äîyou can't optimize what you don't measure
- **Anomaly detection**: Alert when users exceed $100/day (infinite loops, abuse)
- **Budget enforcement**: Cap free tier at $1/day to prevent runaway costs

**The ROI Formula:**
```typescript
// For any architectural choice:
const qualityGain = newApproach.quality - currentApproach.quality
const costDelta = newApproach.cost - currentApproach.cost
const revenueImpact = qualityGain * conversionRate * avgOrderValue

// Only upgrade if:
revenueImpact > costDelta
```

**The Principle: Cheapest Model Above Threshold**

Don't default to the best model. Start with the **cheapest model that passes your quality bar**, then upgrade only when revenue impact justifies the cost delta.

1. **Define quality threshold** (e.g., 85% accuracy)
2. **Run evals** on Haiku/Sonnet/Opus for your task
3. **Select cheapest model** that meets threshold
4. **Monitor** costs and quality in production
5. **Optimize** based on real data, not assumptions
