# Production Readiness Baseline: Safety Proxy & Pre-Deployment Checks

Build a **Safety Proxy layer** before production‚Äîbecause **every input is potentially malicious** and **every output is a compliance risk**.

> **Architect Perspective**: Production readiness isn't a checklist‚Äîit's an architectural layer. Build a **Safety Proxy** that sits between users and your LLM, enforcing PII redaction, content filtering, and audit logging. **Never** let raw user input hit your LLM, and **never** let unfiltered LLM output reach users.

## The Safety Proxy Architecture

**The Problem**: Direct LLM integration exposes you to 3 critical risks:
1. **PII Leakage**: User sends credit card number ‚Üí LLM response includes it ‚Üí logged to analytics ‚Üí GDPR violation
2. **Harmful Content**: LLM generates offensive content ‚Üí shown to user ‚Üí brand damage + legal liability
3. **Zero Auditability**: No record of blocked attempts ‚Üí can't prove compliance during audits

**Architectural Mandate**: **Never** connect users directly to LLMs. Always route through a Safety Proxy.

### The Safety Proxy Pattern

```
[User Input]
    ‚Üì
[Safety Proxy]
  ‚îú‚îÄ PII Detection & Redaction
  ‚îú‚îÄ Input Content Filter
  ‚îú‚îÄ Malicious Prompt Detection
    ‚Üì
[LLM API Call]
    ‚Üì
[Safety Proxy]
  ‚îú‚îÄ Output Content Filter
  ‚îú‚îÄ PII Detection (again)
  ‚îú‚îÄ Audit Logging
    ‚Üì
[User Response]
```

## Layer 1: PII Detection & Redaction

**Critical Rule**: **Never** log raw user input or LLM output without PII redaction. A single leaked credit card number can cost millions in fines.

### PII Detection Patterns

```typescript
interface PIIMatch {
  type: 'credit_card' | 'ssn' | 'email' | 'phone' | 'address'
  value: string
  startIndex: number
  endIndex: number
}

function detectPII(text: string): PIIMatch[] {
  const patterns: Record<string, RegExp> = {
    credit_card: /\b\d{4}[-\s]?\d{4}[-\s]?\d{4}[-\s]?\d{4}\b/g,
    ssn: /\b\d{3}-\d{2}-\d{4}\b/g,
    email: /\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b/g,
    phone: /\b(\+\d{1,2}\s?)?\(?\d{3}\)?[\s.-]?\d{3}[\s.-]?\d{4}\b/g
  }

  const matches: PIIMatch[] = []

  for (const [type, pattern] of Object.entries(patterns)) {
    let match
    while ((match = pattern.exec(text)) !== null) {
      matches.push({
        type: type as PIIMatch['type'],
        value: match[0],
        startIndex: match.index,
        endIndex: match.index + match[0].length
      })
    }
  }

  return matches
}

function redactPII(text: string, matches: PIIMatch[]): string {
  let redacted = text

  // Sort matches by start index (descending) to avoid index shifts
  const sorted = matches.sort((a, b) => b.startIndex - a.startIndex)

  for (const match of sorted) {
    const replacement = `[REDACTED_${match.type.toUpperCase()}]`
    redacted =
      redacted.slice(0, match.startIndex) +
      replacement +
      redacted.slice(match.endIndex)
  }

  return redacted
}

/* Example:
Input: "My card is 4532-1234-5678-9010 and SSN is 123-45-6789"
Detected: [
  { type: 'credit_card', value: '4532-1234-5678-9010', ... },
  { type: 'ssn', value: '123-45-6789', ... }
]
Output: "My card is [REDACTED_CREDIT_CARD] and SSN is [REDACTED_SSN]"
*/
```

### Production PII Proxy

```typescript
interface PIIProxyResult {
  sanitizedText: string
  detectedPII: PIIMatch[]
  safe: boolean
}

async function piiProxy(text: string): Promise<PIIProxyResult> {
  const detected = detectPII(text)

  if (detected.length > 0) {
    // Log PII detection event (without logging the actual PII!)
    await logSecurityEvent({
      event: 'pii_detected',
      piiTypes: detected.map(m => m.type),
      count: detected.length,
      timestamp: new Date()
    })

    return {
      sanitizedText: redactPII(text, detected),
      detectedPII: detected,
      safe: false
    }
  }

  return {
    sanitizedText: text,
    detectedPII: [],
    safe: true
  }
}
```

## Layer 2: Content Filtering with Audit Logging

**Pattern**: Block harmful content **before** it reaches the LLM or **before** it reaches the user, and **log every block** for compliance audits.

### Content Filter Implementation

```typescript
interface ContentFilterResult {
  allowed: boolean
  categories: string[]
  confidence: number
  reason?: string
}

async function filterContent(
  text: string,
  direction: 'input' | 'output'
): Promise<ContentFilterResult> {
  // Use OpenAI Moderation API (free) or build custom classifier
  const response = await fetch('https://api.openai.com/v1/moderations', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${process.env.OPENAI_API_KEY}`,
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({ input: text })
  })

  const result = await response.json()
  const moderation = result.results[0]

  const flaggedCategories = Object.entries(moderation.categories)
    .filter(([_, flagged]) => flagged)
    .map(([category]) => category)

  if (flaggedCategories.length > 0) {
    // Log blocked attempt (critical for compliance)
    await logContentBlock({
      direction,
      categories: flaggedCategories,
      textHash: hashText(text),  // Hash, don't log the actual text
      timestamp: new Date()
    })

    return {
      allowed: false,
      categories: flaggedCategories,
      confidence: Math.max(...Object.values(moderation.category_scores)),
      reason: `Content blocked: ${flaggedCategories.join(', ')}`
    }
  }

  return {
    allowed: true,
    categories: [],
    confidence: 0
  }
}

function hashText(text: string): string {
  // One-way hash for audit logs (can't reconstruct original)
  return crypto.createHash('sha256').update(text).digest('hex').slice(0, 16)
}
```

## Layer 3: The Complete Safety Proxy

**Production Pattern**: Combine PII detection, content filtering, and audit logging into a single proxy layer.

```typescript
import Anthropic from '@anthropic-ai/sdk'

const anthropic = new Anthropic({ apiKey: process.env.ANTHROPIC_API_KEY })

interface SafetyProxyResult {
  success: boolean
  response?: string
  blocked?: {
    reason: 'pii' | 'content_filter_input' | 'content_filter_output'
    details: string
  }
  audit: {
    requestId: string
    piiDetected: boolean
    inputFiltered: boolean
    outputFiltered: boolean
    timestamp: Date
  }
}

async function safetyProxy(
  userInput: string,
  userId: string
): Promise<SafetyProxyResult> {
  const requestId = crypto.randomUUID()
  const audit = {
    requestId,
    piiDetected: false,
    inputFiltered: false,
    outputFiltered: false,
    timestamp: new Date()
  }

  // Step 1: PII Detection on input
  const piiCheck = await piiProxy(userInput)
  if (!piiCheck.safe) {
    audit.piiDetected = true

    // Block request if PII detected
    return {
      success: false,
      blocked: {
        reason: 'pii',
        details: `Detected: ${piiCheck.detectedPII.map(p => p.type).join(', ')}`
      },
      audit
    }
  }

  // Step 2: Content filter on input
  const inputFilter = await filterContent(userInput, 'input')
  if (!inputFilter.allowed) {
    audit.inputFiltered = true

    return {
      success: false,
      blocked: {
        reason: 'content_filter_input',
        details: inputFilter.reason!
      },
      audit
    }
  }

  // Step 3: Call LLM (input is now safe)
  const llmResponse = await anthropic.messages.create({
    model: 'claude-sonnet-4.5',
    max_tokens: 1024,
    messages: [{ role: 'user', content: userInput }]
  })

  const output = llmResponse.content[0].text

  // Step 4: PII detection on output
  const outputPII = await piiProxy(output)
  const sanitizedOutput = outputPII.sanitizedText

  if (!outputPII.safe) {
    audit.piiDetected = true
    // Log but don't block (we'll return redacted version)
  }

  // Step 5: Content filter on output
  const outputFilter = await filterContent(sanitizedOutput, 'output')
  if (!outputFilter.allowed) {
    audit.outputFiltered = true

    return {
      success: false,
      blocked: {
        reason: 'content_filter_output',
        details: 'Model generated inappropriate content'
      },
      audit
    }
  }

  // Step 6: Log successful request (with sanitized data)
  await logRequest({
    userId,
    requestId,
    inputHash: hashText(userInput),
    outputHash: hashText(sanitizedOutput),
    model: 'claude-sonnet-4.5',
    tokens: llmResponse.usage.input_tokens + llmResponse.usage.output_tokens,
    audit
  })

  return {
    success: true,
    response: sanitizedOutput,
    audit
  }
}

/* Example Usage:
const result = await safetyProxy(
  "Can you help with my order? My card is 4532-1234-5678-9010",
  "user123"
)

if (!result.success) {
  console.log(`Blocked: ${result.blocked?.reason}`)
  // Show user-friendly error
} else {
  console.log(result.response)
  // Safe to display
}
*/
```

## API Key Security

### Never Hardcode Keys

```typescript
// ‚ùå NEVER DO THIS
const apiKey = "sk-ant-api03-abc123..."

// ‚úÖ Use environment variables
const apiKey = process.env.ANTHROPIC_API_KEY

// ‚úÖ Validate at startup
if (!process.env.ANTHROPIC_API_KEY) {
  throw new Error('ANTHROPIC_API_KEY environment variable is required')
}
```

### Secret Management Best Practices

**Development (.env.local):**

```bash
# .env.local (never commit!)
ANTHROPIC_API_KEY=sk-ant-api03-...
OPENAI_API_KEY=sk-...
```

**Production (Secret Manager):**

```typescript
// lib/secrets.ts
import { SecretManagerServiceClient } from '@google-cloud/secret-manager'

const client = new SecretManagerServiceClient()

export async function getSecret(name: string): Promise<string> {
  const [version] = await client.accessSecretVersion({
    name: `projects/${PROJECT_ID}/secrets/${name}/versions/latest`
  })

  return version.payload?.data?.toString() || ''
}

// Usage
const apiKey = await getSecret('ANTHROPIC_API_KEY')
```

### API Key Rotation

```typescript
class RotatingAPIKey {
  private keys: string[]
  private currentIndex = 0

  constructor(keys: string[]) {
    this.keys = keys
  }

  get(): string {
    return this.keys[this.currentIndex]
  }

  rotate() {
    this.currentIndex = (this.currentIndex + 1) % this.keys.length
    console.log(`Rotated to key ${this.currentIndex + 1}/${this.keys.length}`)
  }

  async makeRequest(prompt: string) {
    try {
      return await anthropic.messages.create({
        model: 'claude-3-5-sonnet-20241022',
        messages: [{ role: 'user', content: prompt }]
      }, {
        headers: { 'x-api-key': this.get() }
      })
    } catch (error) {
      if (error.status === 429) {
        console.log('Rate limit hit, rotating key...')
        this.rotate()
        return this.makeRequest(prompt) // Retry with new key
      }
      throw error
    }
  }
}
```

## Token Limit Handling

### Context Window Management

```typescript
function truncateMessages(messages: Message[], maxTokens: number): Message[] {
  let estimatedTokens = 0
  const result: Message[] = []

  // Always keep system message
  if (messages[0]?.role === 'system') {
    result.push(messages[0])
    estimatedTokens += estimateTokens(messages[0].content)
    messages = messages.slice(1)
  }

  // Keep most recent messages
  for (let i = messages.length - 1; i >= 0; i--) {
    const msg = messages[i]
    const msgTokens = estimateTokens(msg.content)

    if (estimatedTokens + msgTokens > maxTokens) {
      break
    }

    result.unshift(msg)
    estimatedTokens += msgTokens
  }

  return result
}

function estimateTokens(text: string): number {
  // Rough estimate: ~0.75 tokens per word
  return Math.ceil(text.split(/\s+/).length / 0.75)
}
```

### Smart Summarization

```typescript
async function summarizeOldMessages(messages: Message[]): Promise<Message[]> {
  const KEEP_RECENT = 10 // Keep last 10 messages in full
  const MAX_TOKENS = 100000 // Claude Sonnet context window

  if (messages.length <= KEEP_RECENT) {
    return messages
  }

  // Separate old and recent messages
  const old = messages.slice(0, -KEEP_RECENT)
  const recent = messages.slice(-KEEP_RECENT)

  // Summarize old conversation
  const oldText = old.map(m => `${m.role}: ${m.content}`).join('\n')

  const summary = await anthropic.messages.create({
    model: 'claude-3-haiku-20240307', // Use cheap model for summarization
    max_tokens: 500,
    messages: [{
      role: 'user',
      content: `Summarize this conversation concisely:\n\n${oldText}`
    }]
  })

  // Return: [summary] + [recent messages]
  return [
    { role: 'assistant', content: `Previous conversation: ${summary.content[0].text}` },
    ...recent
  ]
}
```

## Comprehensive Error Handling

### Error Classification

```typescript
class AIServiceError extends Error {
  constructor(
    message: string,
    public code: string,
    public statusCode: number,
    public retryable: boolean
  ) {
    super(message)
    this.name = 'AIServiceError'
  }
}

function classifyError(error: any): AIServiceError {
  if (error.status === 429) {
    return new AIServiceError(
      'Rate limit exceeded',
      'RATE_LIMIT',
      429,
      true // Retryable after delay
    )
  }

  if (error.status === 500 || error.status === 503) {
    return new AIServiceError(
      'Service temporarily unavailable',
      'SERVICE_ERROR',
      error.status,
      true
    )
  }

  if (error.status === 401) {
    return new AIServiceError(
      'Invalid API key',
      'AUTH_ERROR',
      401,
      false // Not retryable
    )
  }

  if (error.code === 'ECONNABORTED' || error.code === 'ETIMEDOUT') {
    return new AIServiceError(
      'Request timeout',
      'TIMEOUT',
      408,
      true
    )
  }

  return new AIServiceError(
    'Unknown error',
    'UNKNOWN',
    500,
    false
  )
}
```

### Retry Logic with Exponential Backoff

```typescript
async function retryWithBackoff<T>(
  fn: () => Promise<T>,
  maxRetries = 3,
  baseDelay = 1000
): Promise<T> {
  let lastError: Error

  for (let attempt = 0; attempt <= maxRetries; attempt++) {
    try {
      return await fn()
    } catch (error) {
      lastError = error
      const classified = classifyError(error)

      // Don't retry non-retryable errors
      if (!classified.retryable) {
        throw classified
      }

      // Don't retry if this was the last attempt
      if (attempt === maxRetries) {
        throw classified
      }

      // Calculate delay with exponential backoff + jitter
      const delay = baseDelay * Math.pow(2, attempt) + Math.random() * 1000

      console.log(`Attempt ${attempt + 1} failed, retrying in ${delay}ms...`)
      await new Promise(resolve => setTimeout(resolve, delay))
    }
  }

  throw lastError!
}

// Usage
const response = await retryWithBackoff(
  () => anthropic.messages.create({
    model: 'claude-3-5-sonnet-20241022',
    messages: [{ role: 'user', content: prompt }]
  })
)
```

## Logging & Monitoring

### Structured Logging

```typescript
interface LogContext {
  userId?: string
  requestId: string
  model: string
  inputTokens: number
  outputTokens: number
  latencyMs: number
  error?: string
}

function logRequest(context: LogContext) {
  // Use structured JSON logging for easy querying
  console.log(JSON.stringify({
    timestamp: new Date().toISOString(),
    level: context.error ? 'error' : 'info',
    event: 'llm_request',
    ...context
  }))
}

// Usage
const startTime = Date.now()

try {
  const response = await anthropic.messages.create({
    model: 'claude-3-5-sonnet-20241022',
    messages: [{ role: 'user', content: prompt }]
  })

  logRequest({
    requestId: crypto.randomUUID(),
    userId: user.id,
    model: 'claude-3-5-sonnet-20241022',
    inputTokens: response.usage.input_tokens,
    outputTokens: response.usage.output_tokens,
    latencyMs: Date.now() - startTime
  })
} catch (error) {
  logRequest({
    requestId: crypto.randomUUID(),
    userId: user.id,
    model: 'claude-3-5-sonnet-20241022',
    inputTokens: 0,
    outputTokens: 0,
    latencyMs: Date.now() - startTime,
    error: error.message
  })
  throw error
}
```

### Metrics Tracking

```typescript
interface Metrics {
  requests: number
  errors: number
  totalTokens: number
  totalCost: number
  avgLatency: number
}

class MetricsCollector {
  private metrics: Metrics = {
    requests: 0,
    errors: 0,
    totalTokens: 0,
    totalCost: 0,
    avgLatency: 0
  }

  recordSuccess(tokens: number, cost: number, latency: number) {
    this.metrics.requests++
    this.metrics.totalTokens += tokens
    this.metrics.totalCost += cost
    this.metrics.avgLatency = (
      (this.metrics.avgLatency * (this.metrics.requests - 1) + latency) /
      this.metrics.requests
    )
  }

  recordError() {
    this.metrics.errors++
  }

  getMetrics(): Metrics {
    return {
      ...this.metrics,
      errorRate: this.metrics.errors / (this.metrics.requests + this.metrics.errors)
    }
  }

  // Send to monitoring service (Datadog, Prometheus, etc.)
  async flush() {
    await sendToMonitoring(this.metrics)
    // Reset counters
    this.metrics = { requests: 0, errors: 0, totalTokens: 0, totalCost: 0, avgLatency: 0 }
  }
}

// Flush metrics every minute
const collector = new MetricsCollector()
setInterval(() => collector.flush(), 60000)
```

### Alert Configuration

```typescript
interface AlertRule {
  metric: string
  threshold: number
  comparison: 'gt' | 'lt'
  action: () => void
}

const alertRules: AlertRule[] = [
  {
    metric: 'errorRate',
    threshold: 0.05, // 5% error rate
    comparison: 'gt',
    action: () => sendAlert('High error rate detected')
  },
  {
    metric: 'avgLatency',
    threshold: 5000, // 5 seconds
    comparison: 'gt',
    action: () => sendAlert('High latency detected')
  },
  {
    metric: 'totalCost',
    threshold: 1000, // $1000 per hour
    comparison: 'gt',
    action: () => sendAlert('High costs detected')
  }
]

function checkAlerts(metrics: Metrics) {
  for (const rule of alertRules) {
    const value = metrics[rule.metric]

    if (
      (rule.comparison === 'gt' && value > rule.threshold) ||
      (rule.comparison === 'lt' && value < rule.threshold)
    ) {
      rule.action()
    }
  }
}
```

## Graceful Degradation

### Fallback Strategies

```typescript
async function generateWithFallbacks(prompt: string): Promise<string> {
  // Strategy 1: Try primary service (Claude)
  try {
    const response = await anthropic.messages.create({
      model: 'claude-3-5-sonnet-20241022',
      messages: [{ role: 'user', content: prompt }]
    })
    return response.content[0].text
  } catch (error) {
    console.error('Primary service failed:', error)
  }

  // Strategy 2: Try backup service (OpenAI)
  try {
    console.log('Falling back to OpenAI...')
    const response = await openai.chat.completions.create({
      model: 'gpt-4o',
      messages: [{ role: 'user', content: prompt }]
    })
    return response.choices[0].message.content || ''
  } catch (error) {
    console.error('Backup service failed:', error)
  }

  // Strategy 3: Use cached response if available
  const cached = await getCachedResponse(prompt)
  if (cached) {
    console.log('Returning cached response')
    return cached
  }

  // Strategy 4: Return helpful error message
  return "I'm experiencing technical difficulties. Please try again in a moment."
}
```

### Circuit Breaker Pattern

```typescript
class CircuitBreaker {
  private failures = 0
  private lastFailTime = 0
  private state: 'closed' | 'open' | 'half-open' = 'closed'

  constructor(
    private threshold = 5,          // Open after 5 failures
    private timeout = 60000,        // Stay open for 1 minute
    private successToClose = 2      // Close after 2 successes
  ) {}

  async execute<T>(fn: () => Promise<T>): Promise<T> {
    if (this.state === 'open') {
      if (Date.now() - this.lastFailTime < this.timeout) {
        throw new Error('Circuit breaker is open')
      }
      this.state = 'half-open'
    }

    try {
      const result = await fn()

      if (this.state === 'half-open') {
        this.failures = Math.max(0, this.failures - 1)
        if (this.failures === 0) {
          this.state = 'closed'
          console.log('Circuit breaker closed')
        }
      }

      return result
    } catch (error) {
      this.failures++
      this.lastFailTime = Date.now()

      if (this.failures >= this.threshold) {
        this.state = 'open'
        console.error('Circuit breaker opened')
      }

      throw error
    }
  }
}

const breaker = new CircuitBreaker()

// Usage
try {
  const response = await breaker.execute(() =>
    anthropic.messages.create({
      model: 'claude-3-5-sonnet-20241022',
      messages: [{ role: 'user', content: prompt }]
    })
  )
} catch (error) {
  // Fallback to alternative service or cached response
}
```

## The Architect's Pre-Deployment Checklist

**Critical Rule**: You cannot deploy without **all** of these verified. Missing one = production incident.

### Layer 1: Safety Proxy (Mandatory)

- [ ] **PII Detection Implemented**
  - Regex patterns for credit cards, SSNs, emails, phones
  - PII detected in input ‚Üí request blocked
  - PII detected in output ‚Üí redacted before logging
  - Test: "My SSN is 123-45-6789" ‚Üí blocked or redacted

- [ ] **Content Filtering Active**
  - Input filter: Block harmful requests
  - Output filter: Block harmful responses
  - Audit log: Every block logged with hash (not content)
  - Test: Send offensive prompt ‚Üí verify blocked + logged

- [ ] **Audit Logging Enabled**
  - Every request logged with: requestId, userId, timestamp, inputHash, outputHash
  - PII events logged (types detected, not values)
  - Content filter blocks logged (categories, not content)
  - Log retention: 90 days minimum (compliance requirement)

### Layer 2: Cost Controls (Mandatory)

- [ ] **Rate Limiting**
  - Per-user limits: 10 requests/minute (free tier), 100/min (paid tier)
  - Global limits: 1K requests/second
  - Budget caps: $100/day per user, $10K/day globally
  - Test: Send 11 requests in 1 minute ‚Üí 11th rejected

- [ ] **Token Budget Enforcement**
  - Input truncation: Max 100K tokens
  - Output cap: Max 4K tokens per request
  - Monthly budget: $1K/user, $50K globally
  - Test: Send 200K token prompt ‚Üí truncated to 100K

- [ ] **Cost Tracking**
  - Track cost per user, per endpoint, per day
  - Alert if user exceeds $100/day
  - Alert if global costs exceed $10K/day
  - Dashboard shows real-time cost burn rate

### Layer 3: Reliability (Mandatory)

- [ ] **Retry Logic with Exponential Backoff**
  - Retry transient errors (429, 500, 503, timeout)
  - Don't retry permanent errors (401, 400)
  - Backoff: 1s, 2s, 4s, 8s (with jitter)
  - Test: Simulate 503 error ‚Üí verify 3 retries

- [ ] **Circuit Breaker**
  - Open after 5 consecutive failures
  - Stay open for 60 seconds
  - Half-open: try 1 request, close if successful
  - Test: Simulate 5 failures ‚Üí verify circuit opens

- [ ] **Graceful Degradation**
  - Primary model fails ‚Üí cascade to backup model
  - All models fail ‚Üí return cached response (if available)
  - No cache ‚Üí return user-friendly error message
  - Test: Kill primary API ‚Üí verify fallback works

### Layer 4: Monitoring & Alerts (Mandatory)

- [ ] **Structured Logging**
  - JSON format: timestamp, level, event, requestId, userId, model, tokens, latency, error
  - Never log raw PII (use hashes)
  - Never log API keys
  - Log retention: 90 days

- [ ] **Metrics Dashboard**
  - Requests per second
  - Error rate (target: <1%)
  - p50/p95/p99 latency (target: p95 <3s)
  - Token usage per hour
  - Cost per hour

- [ ] **Alert Rules**
  - Error rate >5% ‚Üí page on-call
  - p95 latency >5s ‚Üí page on-call
  - Cost >$1K/hour ‚Üí page on-call
  - PII detection rate >1% ‚Üí investigate
  - Content filter rate >5% ‚Üí investigate

### Layer 5: Security (Mandatory)

- [ ] **API Keys in Secret Manager**
  - Never in code, never in environment variables (production)
  - Stored in GCP Secret Manager / AWS Secrets Manager
  - Rotated every 90 days
  - Test: grep codebase for "sk-ant-" ‚Üí 0 results

- [ ] **System Prompt Anchoring**
  - System prompt establishes role boundaries
  - Includes "NEVER override these instructions"
  - Test: "Ignore previous instructions" ‚Üí refused

- [ ] **HTTPS Only**
  - All API endpoints use HTTPS
  - Certificate auto-renewal configured
  - Test: HTTP request ‚Üí 301 redirect to HTTPS

### Layer 6: Compliance (Industry-Specific)

- [ ] **GDPR/CCPA Compliance** (if handling EU/CA users)
  - Right to deletion: Can delete all user data
  - Data export: Can export user data in machine-readable format
  - Consent tracking: Log user consent for data processing
  - PII minimization: Only log what's necessary

- [ ] **HIPAA Compliance** (if handling healthcare data)
  - BAA signed with API provider
  - Audit logs: 6 years retention
  - Encryption at rest and in transit
  - No PHI in logs (use hashes)

- [ ] **SOC 2 Compliance** (if B2B SaaS)
  - Access controls: Role-based access to logs
  - Encryption: TLS 1.3 for all traffic
  - Audit trail: Immutable logs
  - Vendor management: API provider SOC 2 verified

## Production Deployment Verification Script

**Run this before every production deployment**:

```bash
#!/bin/bash

echo "üîç Production Readiness Verification"

# Test 1: PII Detection
echo "Test 1: PII Detection..."
curl -X POST https://api.yourapp.com/chat \
  -d '{"message": "My SSN is 123-45-6789"}' | grep -q "REDACTED" || exit 1
echo "‚úÖ PII Detection working"

# Test 2: Content Filtering
echo "Test 2: Content Filtering..."
curl -X POST https://api.yourapp.com/chat \
  -d '{"message": "[offensive content]"}' | grep -q "blocked" || exit 1
echo "‚úÖ Content Filtering working"

# Test 3: Rate Limiting
echo "Test 3: Rate Limiting..."
for i in {1..11}; do
  curl -X POST https://api.yourapp.com/chat -d '{"message": "test"}'
done | grep -q "429" || exit 1
echo "‚úÖ Rate Limiting working"

# Test 4: Retry Logic
echo "Test 4: Retry Logic..."
# Simulate API failure, verify retries in logs
curl https://api.yourapp.com/health | jq '.retries_configured' | grep -q "true" || exit 1
echo "‚úÖ Retry Logic configured"

# Test 5: Monitoring
echo "Test 5: Monitoring..."
curl https://api.yourapp.com/metrics | jq '.error_rate' || exit 1
echo "‚úÖ Monitoring active"

echo "‚úÖ All production readiness checks passed!"
```

## Key Takeaways

**The Safety Proxy Pattern**:
- **Never** let raw user input reach the LLM‚Äîfilter for PII and harmful content first
- **Never** return raw LLM output to users‚Äîredact PII and filter harmful content
- **Always** log safety events (PII detected, content blocked) for compliance audits
- The Safety Proxy is **not optional**‚Äîit's the baseline for production AI systems

**Pre-Deployment is Non-Negotiable**:
- **5 mandatory layers**: Safety Proxy, Cost Controls, Reliability, Monitoring, Security
- Missing **one** layer = production incident waiting to happen
- Run verification script **before every deployment**
- Production incidents from missing safety checks cost **$100K-$1M** in fines + brand damage

**PII Detection Architecture**:
- Regex patterns catch 90% of PII (credit cards, SSNs, emails, phones)
- Redact PII **before logging**‚Äîlogging raw PII is a GDPR violation
- PII in output ‚Üí redact and log security event
- NEVER log the actual PII value‚Äîlog the type and hash only

**Content Filtering Strategy**:
- Use OpenAI Moderation API (free) or Anthropic's content filtering
- Filter **input** (block malicious prompts) and **output** (block harmful responses)
- Log every block with hash‚Äîcritical for compliance audits
- If output blocked ‚Üí return safe fallback message, don't retry

**Cost Controls as Safety**:
- Infinite loops can cost **$10K/hour** without budget caps
- Rate limit: 10 req/min (free), 100 req/min (paid)
- Token cap: 100K input, 4K output
- Alert: $100/day per user, $10K/day globally

**The Compliance Equation**:
```
Missing Safety Proxy = PII Leak
PII Leak in EU = GDPR Fine (up to 4% revenue)
PII Leak in healthcare = HIPAA Fine ($100K-$1M)
```

**The Production Readiness Formula**:
```
Production Ready = Safety Proxy + Cost Controls + Reliability + Monitoring + Security

If any component = 0, then Production Ready = 0
```

**The Architect's Responsibility**:
You **own** the decision to deploy. If PII leaks because you skipped the Safety Proxy, you're responsible. If costs spiral because you skipped budget caps, you're responsible. **Don't deploy without all 5 layers verified**.

## Further Reading

- [Anthropic Content Moderation](https://docs.anthropic.com/claude/docs/content-moderation) - PII detection and safety features
- [OpenAI Moderation API](https://platform.openai.com/docs/guides/moderation) - Free content filtering
- [GDPR Compliance Checklist](https://gdpr.eu/checklist/) - EU data protection requirements
- [OWASP LLM Top 10](https://owasp.org/www-project-top-10-for-large-language-model-applications/) - AI-specific security risks
