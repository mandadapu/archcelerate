# API Resilience & Integration: Fault Tolerance & Model Cascade

Engineering the "connection" for production-grade reliability—because **APIs will fail**, and your system must survive.

> **Architect Perspective**: API integration isn't about "making requests"—it's about building **fault-tolerant systems** that maintain 99.9% uptime despite rate limits, network failures, and provider outages.

## The Reliability Problem

**Reality Check**: LLM APIs fail frequently
- Rate limits: 429 errors at scale
- Transient failures: Network timeouts, 503s
- Provider outages: Even Claude/OpenAI go down
- Latency spikes: p99 can be 10x p50

**Architectural Mandate**: Your application **must not crash** when the API fails.

## REST API Basics (With Production Guardrails)

### Basic Request Structure

```typescript
// Anthropic Claude API example
import Anthropic from '@anthropic-ai/sdk'

const client = new Anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY
})

async function generateText(prompt: string) {
  const response = await client.messages.create({
    model: 'claude-3-5-sonnet-20241022',
    max_tokens: 1024,
    messages: [
      { role: 'user', content: prompt }
    ]
  })

  return response.content[0].text
}
```

### Request Parameters

| Parameter | Purpose | Typical Values |
|-----------|---------|----------------|
| `model` | Which model to use | 'claude-3-5-sonnet-20241022' |
| `max_tokens` | Maximum response length | 1024-4096 for most tasks |
| `temperature` | Randomness (0-1) | 0.7 for creative, 0.2 for factual |
| `messages` | Conversation history | Array of `{role, content}` objects |
| `system` | System prompt | Role and behavior instructions |

## Streaming Responses

For better UX, stream responses token-by-token instead of waiting for the full response.

### Server-Sent Events (SSE)

```typescript
// Backend: Stream from Claude API
async function* streamChatResponse(messages: Message[]) {
  const stream = await client.messages.create({
    model: 'claude-3-5-sonnet-20241022',
    max_tokens: 1024,
    messages,
    stream: true
  })

  for await (const chunk of stream) {
    if (chunk.type === 'content_block_delta' &&
        chunk.delta.type === 'text_delta') {
      yield chunk.delta.text
    }
  }
}

// API Route (Next.js)
export async function POST(req: Request) {
  const { messages } = await req.json()

  const encoder = new TextEncoder()
  const stream = new ReadableStream({
    async start(controller) {
      for await (const text of streamChatResponse(messages)) {
        controller.enqueue(encoder.encode(text))
      }
      controller.close()
    }
  })

  return new Response(stream, {
    headers: {
      'Content-Type': 'text/plain; charset=utf-8',
      'Transfer-Encoding': 'chunked'
    }
  })
}
```

### Frontend: Consuming SSE

```typescript
// React component
async function sendMessage(message: string) {
  setLoading(true)
  setResponse('')

  const res = await fetch('/api/chat', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ messages: [...history, { role: 'user', content: message }] })
  })

  const reader = res.body?.getReader()
  const decoder = new TextDecoder()

  while (true) {
    const { done, value } = await reader!.read()
    if (done) break

    const text = decoder.decode(value)
    setResponse(prev => prev + text)
  }

  setLoading(false)
}
```

## Fault Tolerance Architecture: Exponential Backoff with Jitter

**Architect Perspective**: Retries without backoff cause **thundering herd** problems. Exponential backoff with jitter is the only production-safe pattern.

### The Retry Physics

```typescript
// ❌ Anti-Pattern: Naive Retry (causes cascading failures)
async function callAPIBad(prompt: string) {
  for (let i = 0; i < 3; i++) {
    try {
      return await api.call(prompt)
    } catch {
      // Instant retry → hammers the API during outages
      continue
    }
  }
}

// ✅ Production Pattern: Exponential Backoff + Jitter
async function callAPIProduction<T>(
  fn: () => Promise<T>,
  maxRetries = 3
): Promise<T> {
  for (let attempt = 0; attempt < maxRetries; attempt++) {
    try {
      return await fn()
    } catch (error) {
      if (!isRetryable(error)) throw error  // Don't retry auth errors
      if (attempt === maxRetries - 1) throw error

      // Exponential backoff: 1s, 2s, 4s, 8s...
      const baseDelay = Math.pow(2, attempt) * 1000

      // Jitter: Add randomness to prevent thundering herd
      const jitter = Math.random() * 1000
      const delay = baseDelay + jitter

      console.log(`Retry ${attempt + 1}/${maxRetries} after ${delay}ms`)
      await sleep(delay)
    }
  }
  throw new Error('All retries failed')
}

function isRetryable(error: any): boolean {
  // Retry transient failures only
  const retryableCodes = [429, 500, 502, 503, 504]
  return retryableCodes.includes(error.status) || error.code === 'ETIMEDOUT'
}
```

### The Jitter Calculation

```typescript
// Why jitter matters: Prevents synchronized retries
const RETRY_CONFIG = {
  baseDelay: 1000,       // 1 second base
  maxDelay: 32000,       // 32 second max
  jitterFactor: 0.3      // 30% randomness
}

function calculateDelay(attempt: number): number {
  // Exponential: 1s → 2s → 4s → 8s → 16s → 32s (capped)
  const exponential = Math.min(
    RETRY_CONFIG.baseDelay * Math.pow(2, attempt),
    RETRY_CONFIG.maxDelay
  )

  // Jitter: ±30% randomness
  const jitter = exponential * RETRY_CONFIG.jitterFactor * (Math.random() - 0.5)

  return exponential + jitter
}

// Result: 1000 clients retry at different times (no thundering herd)
```

## Model Cascade: Fallback Strategy for 99.9% Uptime

**Architectural Pattern**: If primary model fails, **automatically cascade** to backup models to maintain service.

### The Cascade Architecture

```typescript
interface ModelConfig {
  name: string
  priority: number  // Lower = higher priority
  maxRetries: number
}

const MODEL_CASCADE: ModelConfig[] = [
  { name: 'claude-opus-4.5', priority: 1, maxRetries: 2 },
  { name: 'claude-sonnet-4.5', priority: 2, maxRetries: 2 },
  { name: 'claude-haiku-4.5', priority: 3, maxRetries: 1 }
]

async function callWithCascade(prompt: string): Promise<string> {
  const errors: Error[] = []

  for (const model of MODEL_CASCADE) {
    try {
      console.log(`Trying ${model.name}...`)
      return await callAPIProduction(
        () => callModel(model.name, prompt),
        model.maxRetries
      )
    } catch (error) {
      errors.push(error)
      console.warn(`${model.name} failed, cascading to next...`)
      continue  // Try next model in cascade
    }
  }

  // All models failed
  throw new AggregateError(errors, 'All models in cascade failed')
}
```

### Production Cascade Patterns

**Pattern 1: Quality → Cost Cascade**
```typescript
// Start with best quality, fall back to cheaper
const cascade = ['opus-4.5', 'sonnet-4.5', 'haiku-4.5']
// 90% succeed on Opus, 9% on Sonnet, 1% on Haiku
```

**Pattern 2: Provider Diversity Cascade**
```typescript
// Avoid single-provider dependency
const cascade = [
  { provider: 'anthropic', model: 'claude-sonnet-4.5' },
  { provider: 'openai', model: 'gpt-4o' },
  { provider: 'anthropic', model: 'claude-haiku-4.5' }
]
// If Anthropic is down, OpenAI provides fallback
```

**Pattern 3: The Circuit Breaker**
```typescript
class CircuitBreaker {
  private failures = 0
  private state: 'closed' | 'open' | 'half-open' = 'closed'
  private lastFailure = 0

  async call<T>(fn: () => Promise<T>): Promise<T> {
    // If circuit open, fail fast
    if (this.state === 'open') {
      if (Date.now() - this.lastFailure < 60000) {
        throw new Error('Circuit breaker open')
      }
      this.state = 'half-open'  // Try again after 1 min
    }

    try {
      const result = await fn()
      this.onSuccess()
      return result
    } catch (error) {
      this.onFailure()
      throw error
    }
  }

  private onSuccess() {
    this.failures = 0
    this.state = 'closed'
  }

  private onFailure() {
    this.failures++
    this.lastFailure = Date.now()

    if (this.failures >= 5) {
      this.state = 'open'  // Stop hitting failing service
      console.warn('Circuit breaker opened')
    }
  }
}
```

### Retry Logic with Exponential Backoff

```typescript
async function callWithRetry<T>(
  fn: () => Promise<T>,
  maxRetries = 3,
  baseDelay = 1000
): Promise<T> {
  let lastError: Error

  for (let i = 0; i < maxRetries; i++) {
    try {
      return await fn()
    } catch (error: any) {
      lastError = error

      // Don't retry on client errors (400, 401, 403)
      if (error.status >= 400 && error.status < 500 && error.status !== 429) {
        throw error
      }

      // Calculate delay with exponential backoff + jitter
      const delay = baseDelay * Math.pow(2, i) + Math.random() * 1000
      console.log(`Retry ${i + 1}/${maxRetries} after ${delay}ms`)

      await new Promise(resolve => setTimeout(resolve, delay))
    }
  }

  throw lastError!
}

// Usage
const response = await callWithRetry(() =>
  client.messages.create({
    model: 'claude-3-5-sonnet-20241022',
    max_tokens: 1024,
    messages: [{ role: 'user', content: prompt }]
  })
)
```

### Graceful Degradation

```typescript
async function generateWithFallback(prompt: string): Promise<string> {
  try {
    // Try primary model (GPT-4)
    return await callGPT4(prompt)
  } catch (error) {
    console.error('GPT-4 failed, falling back to Claude', error)
    try {
      // Fallback to Claude
      return await callClaude(prompt)
    } catch (error2) {
      console.error('Claude failed, falling back to GPT-3.5', error2)
      // Final fallback to cheaper model
      return await callGPT35(prompt)
    }
  }
}
```

## Rate Limiting

Prevent API abuse and control costs with rate limiting.

### Redis-Based Rate Limiter

```typescript
// lib/rate-limit.ts
import { Redis } from '@upstash/redis'

const redis = new Redis({
  url: process.env.UPSTASH_REDIS_REST_URL!,
  token: process.env.UPSTASH_REDIS_REST_TOKEN!
})

export async function checkRateLimit(
  userId: string,
  limit: number = 10,
  windowSeconds: number = 60
): Promise<{ allowed: boolean; remaining: number }> {
  const key = `rate_limit:${userId}`
  const now = Date.now()
  const windowStart = now - windowSeconds * 1000

  // Remove old entries
  await redis.zremrangebyscore(key, 0, windowStart)

  // Count requests in current window
  const count = await redis.zcard(key)

  if (count >= limit) {
    return { allowed: false, remaining: 0 }
  }

  // Add current request
  await redis.zadd(key, { score: now, member: `${now}` })
  await redis.expire(key, windowSeconds)

  return { allowed: true, remaining: limit - count - 1 }
}

// API route usage
export async function POST(req: Request) {
  const session = await getSession()
  const { allowed, remaining } = await checkRateLimit(session.user.id)

  if (!allowed) {
    return new Response('Rate limit exceeded', {
      status: 429,
      headers: { 'X-RateLimit-Remaining': '0' }
    })
  }

  // Process request...
  return new Response(result, {
    headers: { 'X-RateLimit-Remaining': remaining.toString() }
  })
}
```

## Cost Controls

### Budget Tracking

```typescript
// Track costs per user
async function trackCost(userId: string, tokens: number, model: string) {
  const cost = calculateCost(tokens, model)

  await db.aiUsage.create({
    data: {
      userId,
      tokens,
      cost,
      model,
      timestamp: new Date()
    }
  })

  // Check if user exceeded budget
  const monthlyUsage = await db.aiUsage.aggregate({
    where: {
      userId,
      timestamp: { gte: startOfMonth(new Date()) }
    },
    _sum: { cost: true }
  })

  if (monthlyUsage._sum.cost! > USER_BUDGET_LIMIT) {
    throw new Error('Monthly budget exceeded')
  }
}

function calculateCost(tokens: number, model: string): number {
  const pricing: Record<string, { input: number; output: number }> = {
    'claude-3-5-sonnet': { input: 0.003, output: 0.015 }, // per 1K tokens
    'gpt-4-turbo': { input: 0.01, output: 0.03 },
    'gpt-3.5-turbo': { input: 0.0005, output: 0.0015 }
  }

  const { input, output } = pricing[model] || pricing['gpt-3.5-turbo']
  // Simplified: assume 50/50 input/output split
  return (tokens / 1000) * ((input + output) / 2)
}
```

## API Key Management

### Environment-Based Configuration

```typescript
// lib/ai-client.ts
import Anthropic from '@anthropic-ai/sdk'

export function getAIClient() {
  const apiKey = process.env.ANTHROPIC_API_KEY

  if (!apiKey) {
    throw new Error('ANTHROPIC_API_KEY not configured')
  }

  return new Anthropic({ apiKey })
}

// Usage
const client = getAIClient()
```

### Security Best Practices

1. **Never expose API keys in client-side code**
   - Always call LLM APIs from server-side routes

2. **Use environment variables**
   ```bash
   # .env.local
   ANTHROPIC_API_KEY=sk-ant-...
   OPENAI_API_KEY=sk-...
   ```

3. **Rotate keys regularly**
   - Set reminders to rotate API keys every 90 days

4. **Monitor for leaked keys**
   - Use tools like `git-secrets` to prevent commits with keys
   - Enable GitHub secret scanning

## Practical Exercises

1. **Build a streaming chat**: Implement SSE streaming in a Next.js API route
2. **Add retry logic**: Wrap API calls with exponential backoff
3. **Implement rate limiting**: Use Redis to limit requests per user
4. **Cost dashboard**: Track and display API costs per user

## Code Example: Complete Integration

```typescript
// lib/ai-service.ts
import Anthropic from '@anthropic-ai/sdk'
import { checkRateLimit } from './rate-limit'
import { trackCost } from './cost-tracking'
import { callWithRetry } from './retry'

export class AIService {
  private client: Anthropic

  constructor() {
    this.client = new Anthropic({
      apiKey: process.env.ANTHROPIC_API_KEY!
    })
  }

  async chat(userId: string, messages: Message[]): Promise<string> {
    // Check rate limit
    const { allowed } = await checkRateLimit(userId, 10, 60)
    if (!allowed) throw new Error('Rate limit exceeded')

    // Call API with retry
    const response = await callWithRetry(() =>
      this.client.messages.create({
        model: 'claude-3-5-sonnet-20241022',
        max_tokens: 1024,
        messages
      })
    )

    // Track cost
    await trackCost(
      userId,
      response.usage.input_tokens + response.usage.output_tokens,
      'claude-3-5-sonnet'
    )

    return response.content[0].text
  }

  async *streamChat(userId: string, messages: Message[]) {
    const { allowed } = await checkRateLimit(userId, 10, 60)
    if (!allowed) throw new Error('Rate limit exceeded')

    const stream = await this.client.messages.create({
      model: 'claude-3-5-sonnet-20241022',
      max_tokens: 1024,
      messages,
      stream: true
    })

    let totalTokens = 0
    for await (const chunk of stream) {
      if (chunk.type === 'content_block_delta') {
        yield chunk.delta.text
      }
      if (chunk.type === 'message_delta') {
        totalTokens = chunk.usage.output_tokens
      }
    }

    await trackCost(userId, totalTokens, 'claude-3-5-sonnet')
  }
}
```

## Further Reading

- [Anthropic API Documentation](https://docs.anthropic.com/claude/reference)
- [OpenAI API Reference](https://platform.openai.com/docs/api-reference)
- [Server-Sent Events Specification](https://html.spec.whatwg.org/multipage/server-sent-events.html)
