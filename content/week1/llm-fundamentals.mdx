# LLM Fundamentals

Learn how Large Language Models work and how to choose the right model for your use case.

> **Note**: Pricing and model information updated as of February 2026. The Claude 4.5 series and GPT-5 represent the latest frontier models.

## How LLMs Work

Large Language Models (LLMs) like Claude and GPT-4 are neural networks trained on vast amounts of text data. They work by:

1. **Transformers Architecture**: Using attention mechanisms to understand context
2. **Token Processing**: Breaking text into tokens (subwords)
3. **Next Token Prediction**: Predicting the most likely next token given context

### Key Concepts

**Context Windows**: The amount of text an LLM can "remember" at once.
- Claude Sonnet 4.5: 200K tokens standard, 1M tokens extended (~750K words)
- GPT-5: 400K tokens
- GPT-4o: 128K tokens
- Practical consideration: Cost increases with context size

**Tokens**: The fundamental unit LLMs process.
- Rule of thumb: 1 token ≈ 0.75 words (English)
- Example: "Hello world!" = ~3 tokens
- Why it matters: Pricing is per token (input + output)

## Model Selection

Choosing the right model involves balancing three factors:

| Model | Cost | Latency | Quality | Best For |
|-------|------|---------|---------|----------|
| Claude Opus 4.5 | High | Slow | Excellent | Complex reasoning, agents, coding |
| GPT-5 | Medium-High | Medium | Excellent | Deep reasoning, sophisticated agents |
| Claude Sonnet 4.5 | Medium | Fast | Excellent | Balanced performance, coding, agents |
| GPT-4o | Medium | Fast | Very Good | General purpose, multimodal tasks |
| Claude Haiku 4.5 | Low | Very Fast | Good | Fast responses, classification, simple tasks |

### Cost/Latency/Quality Tradeoffs

**When to prioritize cost**: High-volume, simple tasks (classification, extraction)
**When to prioritize latency**: Real-time user interactions, chat applications
**When to prioritize quality**: Complex reasoning, code generation, critical decisions

## Token Economics

Understanding token costs is critical for production AI systems:

**Example**: Chat application with 1000 daily users
- Average conversation: 10 messages
- Average message: 100 tokens input, 200 tokens output
- Total daily: 1000 users × 10 msgs × (100 input + 200 output) = 1M input + 2M output/day
- Monthly cost (Claude Sonnet 4.5): (1M × $3 + 2M × $15) / 1M × 30 = $33/day = $990/month

**Optimization strategies**:
1. Use cheaper models for simple tasks
2. Implement caching for common queries
3. Summarize long conversations to reduce context
4. Stream responses to improve perceived latency

## Context Management

Effective context window management:

1. **Prioritize recent messages**: Keep last N messages in full context
2. **Summarize old context**: Compress older messages into summary
3. **Remove irrelevant content**: Strip formatting, redundant info
4. **Smart truncation**: Cut from middle, keep beginning and end

## Practical Exercises

### Exercise 1: Token Counting

**Goal**: Learn how to count tokens in different types of text using the Anthropic API.

**Why This Matters**: Token counting is critical for:
- Estimating costs before making API calls
- Ensuring prompts fit within context windows
- Optimizing prompt efficiency

**Step-by-Step Guide**:

```typescript
import Anthropic from '@anthropic-ai/sdk'

const anthropic = new Anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY
})

async function countTokens(text: string): Promise<number> {
  // Anthropic's token counting endpoint
  const response = await anthropic.messages.countTokens({
    model: 'claude-sonnet-4-5-20251101',
    messages: [{ role: 'user', content: text }]
  })

  return response.input_tokens
}

// Test with different text types
async function runTokenCountingExercise() {
  const testCases = [
    {
      name: 'Short message',
      text: 'Hello, how are you?'
    },
    {
      name: 'Code snippet',
      text: `
function fibonacci(n) {
  if (n <= 1) return n
  return fibonacci(n - 1) + fibonacci(n - 2)
}
      `.trim()
    },
    {
      name: 'Markdown documentation',
      text: `
# API Documentation

## Authentication

All API requests require authentication using an API key:

\`\`\`bash
curl -H "Authorization: Bearer YOUR_API_KEY" https://api.example.com/data
\`\`\`

## Rate Limits

- Free tier: 100 requests/hour
- Pro tier: 1000 requests/hour
      `.trim()
    },
    {
      name: 'JSON data',
      text: JSON.stringify({
        users: [
          { id: 1, name: 'Alice', email: 'alice@example.com' },
          { id: 2, name: 'Bob', email: 'bob@example.com' }
        ]
      }, null, 2)
    }
  ]

  console.log('Token Counting Results:\n')

  for (const testCase of testCases) {
    const tokens = await countTokens(testCase.text)
    const charCount = testCase.text.length
    const tokensPerChar = (tokens / charCount).toFixed(3)

    console.log(`${testCase.name}:`)
    console.log(`  Characters: ${charCount}`)
    console.log(`  Tokens: ${tokens}`)
    console.log(`  Tokens/Char: ${tokensPerChar}`)
    console.log(`  Cost (input): $${(tokens / 1000 * 0.003).toFixed(6)}`)
    console.log()
  }
}

// Run the exercise
runTokenCountingExercise()

// Expected output:
// Short message:
//   Characters: 19
//   Tokens: 6
//   Tokens/Char: 0.316
//   Cost (input): $0.000018
//
// Code snippet:
//   Characters: 94
//   Tokens: 35
//   Tokens/Char: 0.372
//   Cost (input): $0.000105
// ...
```

**Key Insights You'll Discover**:
- Code typically uses more tokens per character than plain text
- JSON/structured data is relatively token-efficient
- Whitespace and formatting affect token count

### Exercise 2: Cost Calculation

**Goal**: Calculate the actual cost of running a real AI application.

**Scenario**: You're building a customer support chatbot that:
- Receives 1000 conversations per day
- Each conversation has 5 messages on average
- Each message is ~100 tokens input, ~200 tokens output

```typescript
interface CostEstimate {
  scenario: string
  dailyConversations: number
  messagesPerConversation: number
  avgInputTokens: number
  avgOutputTokens: number
  model: string
  inputCostPerMillion: number
  outputCostPerMillion: number
}

function calculateMonthlyCost(estimate: CostEstimate) {
  // Calculate total tokens
  const dailyMessages = estimate.dailyConversations * estimate.messagesPerConversation
  const dailyInputTokens = dailyMessages * estimate.avgInputTokens
  const dailyOutputTokens = dailyMessages * estimate.avgOutputTokens

  // Calculate daily cost
  const dailyInputCost = (dailyInputTokens / 1_000_000) * estimate.inputCostPerMillion
  const dailyOutputCost = (dailyOutputTokens / 1_000_000) * estimate.outputCostPerMillion
  const dailyCost = dailyInputCost + dailyOutputCost

  // Calculate monthly cost (30 days)
  const monthlyCost = dailyCost * 30

  return {
    dailyMetrics: {
      conversations: estimate.dailyConversations,
      messages: dailyMessages,
      inputTokens: dailyInputTokens,
      outputTokens: dailyOutputTokens,
      cost: dailyCost
    },
    monthlyMetrics: {
      conversations: estimate.dailyConversations * 30,
      messages: dailyMessages * 30,
      inputTokens: dailyInputTokens * 30,
      outputTokens: dailyOutputTokens * 30,
      cost: monthlyCost
    },
    costPerConversation: dailyCost / estimate.dailyConversations,
    breakdown: {
      inputCost: dailyInputCost,
      outputCost: dailyOutputCost,
      inputPercentage: (dailyInputCost / dailyCost * 100).toFixed(1) + '%',
      outputPercentage: (dailyOutputCost / dailyCost * 100).toFixed(1) + '%'
    }
  }
}

// Run the exercise
const scenarios: CostEstimate[] = [
  {
    scenario: 'Customer Support (Claude Sonnet 4.5)',
    dailyConversations: 1000,
    messagesPerConversation: 5,
    avgInputTokens: 100,
    avgOutputTokens: 200,
    model: 'claude-sonnet-4-5-20251101',
    inputCostPerMillion: 3,    // $3 per million input tokens
    outputCostPerMillion: 15   // $15 per million output tokens
  },
  {
    scenario: 'Customer Support (Claude Haiku 4.5)',
    dailyConversations: 1000,
    messagesPerConversation: 5,
    avgInputTokens: 100,
    avgOutputTokens: 200,
    model: 'claude-haiku-4-5-20250514',
    inputCostPerMillion: 1,
    outputCostPerMillion: 5
  },
  {
    scenario: 'Code Assistant (GPT-5)',
    dailyConversations: 5000,
    messagesPerConversation: 3,
    avgInputTokens: 500,  // Code context is larger
    avgOutputTokens: 300,
    model: 'gpt-5',
    inputCostPerMillion: 1.25,
    outputCostPerMillion: 10
  }
]

console.log('Cost Estimation Exercise\n')

for (const scenario of scenarios) {
  const results = calculateMonthlyCost(scenario)

  console.log(`${scenario.scenario}:`)
  console.log(`  Model: ${scenario.model}`)
  console.log(`  Daily: ${results.dailyMetrics.conversations.toLocaleString()} conversations`)
  console.log(`  Daily Cost: $${results.dailyMetrics.cost.toFixed(2)}`)
  console.log(`  Monthly Cost: $${results.monthlyMetrics.cost.toFixed(2)}`)
  console.log(`  Cost per Conversation: $${results.costPerConversation.toFixed(4)}`)
  console.log(`  Breakdown: ${results.breakdown.inputPercentage} input, ${results.breakdown.outputPercentage} output`)
  console.log()
}

// Expected output:
// Customer Support (Claude Sonnet 4.5):
//   Model: claude-sonnet-4-5-20251101
//   Daily: 1,000 conversations
//   Daily Cost: $16.50
//   Monthly Cost: $495.00
//   Cost per Conversation: $0.0165
//   Breakdown: 9.1% input, 90.9% output
//
// Customer Support (Claude Haiku 4.5):
//   Model: claude-haiku-4-5-20250514
//   Daily: 1,000 conversations
//   Daily Cost: $5.50
//   Monthly Cost: $165.00
//   Cost per Conversation: $0.0055
//   Breakdown: 9.1% input, 90.9% output
// ...
```

**Key Insights**:
- Output tokens cost more than input tokens (typically 5x more)
- Model choice dramatically affects cost (Haiku 4.5 vs Opus 4.5: ~6x difference)
- Volume adds up quickly - optimize for high-traffic applications
- The Claude 4.5 series offers 67% cost reduction over previous generations

### Exercise 3: Model Comparison

**Goal**: Compare different models on the same task to understand quality/speed/cost tradeoffs.

```typescript
interface ModelConfig {
  name: string
  model: string
  provider: 'anthropic' | 'openai'
}

interface ComparisonResult {
  model: string
  response: string
  latency: number  // milliseconds
  inputTokens: number
  outputTokens: number
  cost: number
  quality?: number  // Optional: your subjective rating 1-10
}

async function compareModels(prompt: string): Promise<ComparisonResult[]> {
  const models: ModelConfig[] = [
    { name: 'Claude Opus 4.5', model: 'claude-opus-4-5-20251101', provider: 'anthropic' },
    { name: 'Claude Sonnet 4.5', model: 'claude-sonnet-4-5-20251101', provider: 'anthropic' },
    { name: 'Claude Haiku 4.5', model: 'claude-haiku-4-5-20250514', provider: 'anthropic' },
    { name: 'GPT-5', model: 'gpt-5', provider: 'openai' },
    { name: 'GPT-4o', model: 'gpt-4o', provider: 'openai' }
  ]

  const results: ComparisonResult[] = []

  for (const modelConfig of models) {
    console.log(`Testing ${modelConfig.name}...`)

    const startTime = Date.now()

    try {
      if (modelConfig.provider === 'anthropic') {
        const response = await anthropic.messages.create({
          model: modelConfig.model,
          max_tokens: 1024,
          messages: [{ role: 'user', content: prompt }]
        })

        const latency = Date.now() - startTime

        // Calculate cost based on model
        let inputCost: number, outputCost: number
        if (modelConfig.model.includes('opus')) {
          inputCost = 5 / 1_000_000
          outputCost = 25 / 1_000_000
        } else if (modelConfig.model.includes('haiku')) {
          inputCost = 1 / 1_000_000
          outputCost = 5 / 1_000_000
        } else { // Sonnet
          inputCost = 3 / 1_000_000
          outputCost = 15 / 1_000_000
        }

        const cost =
          response.usage.input_tokens * inputCost +
          response.usage.output_tokens * outputCost

        results.push({
          model: modelConfig.name,
          response: response.content[0].text,
          latency,
          inputTokens: response.usage.input_tokens,
          outputTokens: response.usage.output_tokens,
          cost
        })
      } else {
        // OpenAI implementation
        const response = await openai.chat.completions.create({
          model: modelConfig.model,
          max_tokens: 1024,
          messages: [{ role: 'user', content: prompt }]
        })

        const latency = Date.now() - startTime

        // GPT pricing (2026)
        let inputCost: number, outputCost: number
        if (modelConfig.model === 'gpt-5') {
          inputCost = 1.25 / 1_000_000
          outputCost = 10 / 1_000_000
        } else { // gpt-4o
          inputCost = 2.5 / 1_000_000
          outputCost = 10 / 1_000_000
        }

        const cost =
          response.usage.prompt_tokens * inputCost +
          response.usage.completion_tokens * outputCost

        results.push({
          model: modelConfig.name,
          response: response.choices[0].message.content,
          latency,
          inputTokens: response.usage.prompt_tokens,
          outputTokens: response.usage.completion_tokens,
          cost
        })
      }
    } catch (error) {
      console.error(`Error with ${modelConfig.name}:`, error)
    }
  }

  return results
}

// Run the comparison
async function runModelComparison() {
  const testPrompt = `Explain the difference between synchronous and asynchronous programming in JavaScript. Provide a simple code example for each.`

  console.log('Model Comparison Exercise\n')
  console.log(`Prompt: "${testPrompt}"\n`)

  const results = await compareModels(testPrompt)

  // Display results in a table
  console.log('Results:')
  console.log('─'.repeat(100))
  console.log(
    'Model'.padEnd(25),
    'Latency'.padEnd(12),
    'Input Tokens'.padEnd(15),
    'Output Tokens'.padEnd(15),
    'Cost'.padEnd(12)
  )
  console.log('─'.repeat(100))

  for (const result of results) {
    console.log(
      result.model.padEnd(25),
      `${result.latency}ms`.padEnd(12),
      result.inputTokens.toString().padEnd(15),
      result.outputTokens.toString().padEnd(15),
      `$${result.cost.toFixed(6)}`.padEnd(12)
    )
  }

  console.log('─'.repeat(100))
  console.log('\nResponses:\n')

  for (const result of results) {
    console.log(`${result.model}:`)
    console.log(result.response.substring(0, 200) + '...\n')
  }

  // Calculate averages
  const avgLatency = results.reduce((sum, r) => sum + r.latency, 0) / results.length
  const avgCost = results.reduce((sum, r) => sum + r.cost, 0) / results.length

  console.log('Summary:')
  console.log(`  Average latency: ${avgLatency.toFixed(0)}ms`)
  console.log(`  Average cost: $${avgCost.toFixed(6)}`)
  console.log(`  Fastest: ${results.sort((a, b) => a.latency - b.latency)[0].model}`)
  console.log(`  Cheapest: ${results.sort((a, b) => a.cost - b.cost)[0].model}`)
}

runModelComparison()

// Expected output:
// Results:
// ────────────────────────────────────────────────────────────────────
// Model                    Latency     Input Tokens   Output Tokens   Cost
// ────────────────────────────────────────────────────────────────────
// Claude Opus 4.5          3100ms      28             250             $0.006390
// Claude Sonnet 4.5        2200ms      28             245             $0.003759
// Claude Haiku 4.5         1600ms      28             220             $0.001128
// GPT-5                    2400ms      27             240             $0.002434
// GPT-4o                   1900ms      27             235             $0.002418
// ────────────────────────────────────────────────────────────────────
//
// Summary:
//   Average latency: 2240ms
//   Average cost: $0.003226
//   Fastest: Claude Haiku 4.5
//   Cheapest: Claude Haiku 4.5
```

**Key Insights You'll Discover**:
- Latency varies significantly between models (1.6-3.1 seconds for typical requests)
- Cost can vary by 6x between models (Haiku 4.5 vs Opus 4.5)
- Cheaper/faster models like Haiku 4.5 now match previous-gen flagship performance
- GPT-5 offers competitive pricing for sophisticated reasoning tasks
- Balance quality, speed, and cost based on your use case

**Your Turn**: Try these experiments:
1. Test with different prompt complexities (simple vs complex reasoning)
2. Compare the same model with different max_tokens settings
3. Test multilingual prompts to see if token counts change

## Further Reading

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Original Transformers paper
- [Claude 4.5 Models Overview](https://platform.claude.com/docs/en/about-claude/models/overview) - Latest Claude models and capabilities
- [Anthropic API Pricing 2026](https://www.metacto.com/blogs/anthropic-api-pricing-a-full-breakdown-of-costs-and-integration) - Current Claude pricing
- [OpenAI GPT-5 Documentation](https://platform.openai.com/docs/models/gpt-5) - GPT-5 features and pricing
- [OpenAI Tokenizer](https://platform.openai.com/tokenizer) - Interactive token counting tool
