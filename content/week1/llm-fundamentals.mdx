import { CodePlayground } from '@/components/curriculum/CodePlayground'

# LLM Fundamentals

Learn how Large Language Models work and how to choose the right model for your use case.

> **Note**: Pricing and model information updated as of February 2026. The Claude 4.5 series and GPT-5 represent the latest frontier models.

## How LLMs Work

Large Language Models (LLMs) like Claude and GPT-4 are neural networks trained on vast amounts of text data. They work by:

1. **Transformers Architecture**: Using attention mechanisms to understand context
2. **Token Processing**: Breaking text into tokens (subwords)
3. **Next Token Prediction**: Predicting the most likely next token given context

### Key Concepts

**Context Windows**: The amount of text an LLM can "remember" at once.
- Claude Sonnet 4.5: 200K tokens standard, 1M tokens extended (~750K words)
- GPT-5: 400K tokens
- GPT-4o: 128K tokens
- Practical consideration: Cost increases with context size

**Tokens**: The fundamental unit LLMs process.
- Rule of thumb: 1 token ≈ 0.75 words (English)
- Example: "Hello world!" = ~3 tokens
- Why it matters: Pricing is per token (input + output)

## Model Selection

Choosing the right model involves balancing three factors:

| Model | Cost | Latency | Quality | Best For |
|-------|------|---------|---------|----------|
| Claude Opus 4.5 | High | Slow | Excellent | Complex reasoning, agents, coding |
| GPT-5 | Medium-High | Medium | Excellent | Deep reasoning, sophisticated agents |
| Claude Sonnet 4.5 | Medium | Fast | Excellent | Balanced performance, coding, agents |
| GPT-4o | Medium | Fast | Very Good | General purpose, multimodal tasks |
| Claude Haiku 4.5 | Low | Very Fast | Good | Fast responses, classification, simple tasks |

### Cost/Latency/Quality Tradeoffs

Choose your optimization focus based on your application's primary constraint:

| Priority | When to Use | Example Use Cases | Recommended Model |
|----------|-------------|-------------------|-------------------|
| **Cost** | High-volume, simple tasks | • Email classification<br/>• Data extraction<br/>• Sentiment analysis<br/>• Tag generation | Claude Haiku 4.5 |
| **Latency** | Real-time user interactions | • Chat applications<br/>• Auto-complete features<br/>• Live customer support<br/>• Interactive tutoring | Claude Haiku 4.5<br/>GPT-4o |
| **Quality** | Complex reasoning required | • Code generation<br/>• Legal document analysis<br/>• Medical diagnosis support<br/>• Strategic planning | Claude Opus 4.5<br/>GPT-5 |

**Decision Framework:**
1. **Start with cost optimization** - Use Haiku 4.5 as your baseline
2. **Test quality** - If accuracy is insufficient, upgrade to Sonnet 4.5
3. **Evaluate latency** - If responses are too slow for UX, consider Haiku or optimize prompts
4. **Reserve premium models** - Only use Opus/GPT-5 when quality justifies the 3-6x cost increase

## Token Economics

Understanding token costs is critical for production AI systems:

**Example**: Chat application with 1000 daily users
- Average conversation: 10 messages
- Average message: 100 tokens input, 200 tokens output
- Total daily: 1000 users × 10 msgs × (100 input + 200 output) = 1M input + 2M output/day
- Monthly cost (Claude Sonnet 4.5): (1M × $3 + 2M × $15) / 1M × 30 = $33/day = $990/month

**Optimization strategies**:
1. Use cheaper models for simple tasks
2. Implement caching for common queries
3. Summarize long conversations to reduce context
4. Stream responses to improve perceived latency

## Context Management

Effective context window management:

1. **Prioritize recent messages**: Keep last N messages in full context
2. **Summarize old context**: Compress older messages into summary
3. **Remove irrelevant content**: Strip formatting, redundant info
4. **Smart truncation**: Cut from middle, keep beginning and end

## Practical Exercises

### Exercise 1: Token Counting

**Goal**: Learn how to count tokens in different types of text using the Anthropic API.

**Why This Matters**: Token counting is critical for:
- Estimating costs before making API calls
- Ensuring prompts fit within context windows
- Optimizing prompt efficiency

**Try It Yourself**: Click "Run Code" to execute this token counting example. You can edit the code to test your own text!

<CodePlayground
  title="Interactive Token Counting"
  description="Edit the test cases below and click Run to see how different types of text are tokenized."
  exerciseType="token-counting"
  code={`import Anthropic from '@anthropic-ai/sdk'

const anthropic = new Anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY
})

async function countTokens(text: string): Promise<number> {
  const response = await anthropic.messages.countTokens({
    model: 'claude-sonnet-4-5-20251101',
    messages: [{ role: 'user', content: text }]
  })
  return response.input_tokens
}

// Test with different text types
async function runTokenCountingExercise() {
  const testCases = [
    {
      name: 'Short message',
      text: 'Hello, how are you?'
    },
    {
      name: 'Code snippet',
      text: \`function fibonacci(n) {
  if (n <= 1) return n
  return fibonacci(n - 1) + fibonacci(n - 2)
}\`
    },
    {
      name: 'Markdown documentation',
      text: \`# API Documentation

## Authentication

All API requests require authentication using an API key:

\\\`\\\`\\\`bash
curl -H "Authorization: Bearer YOUR_API_KEY" https://api.example.com/data
\\\`\\\`\\\`

## Rate Limits

- Free tier: 100 requests/hour
- Pro tier: 1000 requests/hour\`
    },
    {
      name: 'JSON data',
      text: JSON.stringify({
        users: [
          { id: 1, name: 'Alice', email: 'alice@example.com' },
          { id: 2, name: 'Bob', email: 'bob@example.com' }
        ]
      }, null, 2)
    }
  ]

  console.log('Token Counting Results:\\n')

  for (const testCase of testCases) {
    const tokens = await countTokens(testCase.text)
    const charCount = testCase.text.length
    const tokensPerChar = (tokens / charCount).toFixed(3)

    console.log(\`\${testCase.name}:\`)
    console.log(\`  Characters: \${charCount}\`)
    console.log(\`  Tokens: \${tokens}\`)
    console.log(\`  Tokens/Char: \${tokensPerChar}\`)
    console.log(\`  Cost (input): $\${(tokens / 1000 * 0.003).toFixed(6)}\`)
    console.log()
  }
}

runTokenCountingExercise()`}
/>

**Key Insights You'll Discover**:
- Code typically uses more tokens per character than plain text
- JSON/structured data is relatively token-efficient
- Whitespace and formatting affect token count

### Exercise 2: Cost Calculation

**Goal**: Calculate the actual cost of running a real AI application.

**Scenario**: You're building a customer support chatbot that:
- Receives 1000 conversations per day
- Each conversation has 5 messages on average
- Each message is ~100 tokens input, ~200 tokens output

**Try It Yourself**: Click "Run Code" to see cost estimates for different AI application scenarios. You can edit the scenarios to match your use case!

<CodePlayground
  title="Interactive Cost Calculator"
  description="Edit the scenarios below to calculate costs for your AI application. Try changing conversation volumes, message lengths, or models."
  exerciseType="cost-calculation"
  code={`interface CostEstimate {
  scenario: string
  dailyConversations: number
  messagesPerConversation: number
  avgInputTokens: number
  avgOutputTokens: number
  model: string
  inputCostPerMillion: number
  outputCostPerMillion: number
}

function calculateMonthlyCost(estimate: CostEstimate) {
  // Calculate total tokens
  const dailyMessages = estimate.dailyConversations * estimate.messagesPerConversation
  const dailyInputTokens = dailyMessages * estimate.avgInputTokens
  const dailyOutputTokens = dailyMessages * estimate.avgOutputTokens

  // Calculate daily cost
  const dailyInputCost = (dailyInputTokens / 1_000_000) * estimate.inputCostPerMillion
  const dailyOutputCost = (dailyOutputTokens / 1_000_000) * estimate.outputCostPerMillion
  const dailyCost = dailyInputCost + dailyOutputCost

  // Calculate monthly cost (30 days)
  const monthlyCost = dailyCost * 30

  return {
    dailyMetrics: {
      conversations: estimate.dailyConversations,
      messages: dailyMessages,
      inputTokens: dailyInputTokens,
      outputTokens: dailyOutputTokens,
      cost: dailyCost
    },
    monthlyMetrics: {
      conversations: estimate.dailyConversations * 30,
      messages: dailyMessages * 30,
      inputTokens: dailyInputTokens * 30,
      outputTokens: dailyOutputTokens * 30,
      cost: monthlyCost
    },
    costPerConversation: dailyCost / estimate.dailyConversations,
    breakdown: {
      inputCost: dailyInputCost,
      outputCost: dailyOutputCost,
      inputPercentage: (dailyInputCost / dailyCost * 100).toFixed(1) + '%',
      outputPercentage: (dailyOutputCost / dailyCost * 100).toFixed(1) + '%'
    }
  }
}

// Run the exercise
const scenarios: CostEstimate[] = [
  {
    scenario: 'Chat Application (1K users/day)',
    dailyConversations: 1000,
    messagesPerConversation: 10,
    avgInputTokens: 100,
    avgOutputTokens: 200,
    model: 'claude-sonnet-4-5-20251101',
    inputCostPerMillion: 3,
    outputCostPerMillion: 15
  },
  {
    scenario: 'Code Assistant (100 requests/day)',
    dailyConversations: 100,
    messagesPerConversation: 5,
    avgInputTokens: 500,
    avgOutputTokens: 1000,
    model: 'claude-sonnet-4-5-20251101',
    inputCostPerMillion: 3,
    outputCostPerMillion: 15
  },
  {
    scenario: 'Content Generation (500 articles/day)',
    dailyConversations: 500,
    messagesPerConversation: 4,
    avgInputTokens: 1000,
    avgOutputTokens: 2500,
    model: 'claude-sonnet-4-5-20251101',
    inputCostPerMillion: 3,
    outputCostPerMillion: 15
  }
]

console.log('Cost Estimation Results:\\n')

for (const scenario of scenarios) {
  const results = calculateMonthlyCost(scenario)

  console.log(\`\${scenario.scenario}:\`)
  console.log(\`  Daily: \${results.dailyMetrics.conversations.toLocaleString()} conversations\`)
  console.log(\`  Daily Cost: $\${results.dailyMetrics.cost.toFixed(2)}\`)
  console.log(\`  Monthly Cost: $\${results.monthlyMetrics.cost.toFixed(2)}\`)
  console.log(\`  Cost per Conversation: $\${results.costPerConversation.toFixed(4)}\`)
  console.log(\`  Breakdown: \${results.breakdown.inputPercentage} input, \${results.breakdown.outputPercentage} output\`)
  console.log()
}`}
/>

**Key Insights**:
- Output tokens cost more than input tokens (typically 5x more)
- Model choice dramatically affects cost (Haiku 4.5 vs Opus 4.5: ~6x difference)
- Volume adds up quickly - optimize for high-traffic applications
- The Claude 4.5 series offers 67% cost reduction over previous generations

### Exercise 3: Model Comparison

**Goal**: Compare different models on the same task to understand quality/speed/cost tradeoffs.

**Try It Yourself**: Click "Run Code" to compare Claude Haiku and Sonnet on the same task. Watch how they differ in speed, cost, and response quality!

<CodePlayground
  title="Interactive Model Comparison"
  description="Compare Claude models on the same task. The demo uses Haiku (fast/cheap) vs Sonnet (balanced) to show real tradeoffs."
  exerciseType="model-comparison"
  code={`import Anthropic from '@anthropic-ai/sdk'

const anthropic = new Anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY
})

interface ModelConfig {
  name: string
  model: string
  inputPrice: number  // per million tokens
  outputPrice: number
}

interface ComparisonResult {
  model: string
  response: string
  latency: number
  inputTokens: number
  outputTokens: number
  cost: number
}

async function compareModels(prompt: string): Promise<ComparisonResult[]> {
  const models: ModelConfig[] = [
    {
      name: 'Claude Haiku 4.5',
      model: 'claude-3-haiku-20240307',
      inputPrice: 1,
      outputPrice: 5
    },
    {
      name: 'Claude Sonnet 4.5',
      model: 'claude-3-5-sonnet-20240620',
      inputPrice: 3,
      outputPrice: 15
    }
  ]

  const results: ComparisonResult[] = []

  for (const modelConfig of models) {
    const start = Date.now()

    const response = await anthropic.messages.create({
      model: modelConfig.model,
      max_tokens: 150,
      messages: [{ role: 'user', content: prompt }]
    })

    const latency = Date.now() - start
    const cost = (
      response.usage.input_tokens * modelConfig.inputPrice +
      response.usage.output_tokens * modelConfig.outputPrice
    ) / 1_000_000

    results.push({
      model: modelConfig.name,
      response: response.content[0].text,
      latency,
      inputTokens: response.usage.input_tokens,
      outputTokens: response.usage.output_tokens,
      cost
    })
  }

  return results
}

// Run comparison
async function runModelComparison() {
  const testPrompt = 'Explain API rate limiting in 2 sentences.'

  console.log('Model Comparison Results:\\n')
  console.log(\`Prompt: "\${testPrompt}"\\n\`)

  const results = await compareModels(testPrompt)

  // Display comparison table
  console.log('Metrics:')
  console.log('─'.repeat(80))

  for (const result of results) {
    console.log(\`\${result.model}:\`)
    console.log(\`  Latency: \${result.latency}ms\`)
    console.log(\`  Tokens: \${result.inputTokens} in / \${result.outputTokens} out\`)
    console.log(\`  Cost: $\${result.cost.toFixed(6)}\`)
    console.log(\`  Response: \${result.response.substring(0, 100)}...\`)
    console.log()
  }

  console.log('─'.repeat(80))
  console.log('\\nKey Insights:')
  console.log('  • Haiku is faster and cheaper for simple tasks')
  console.log('  • Sonnet provides more detailed responses')
  console.log('  • Cost difference: ~3x')
  console.log('  • Choose based on your priorities!')
}

runModelComparison()`}
/>

**Key Insights You'll Discover**:
- Latency varies significantly between models (1.6-3.1 seconds for typical requests)
- Cost can vary by 6x between models (Haiku 4.5 vs Opus 4.5)
- Cheaper/faster models like Haiku 4.5 now match previous-gen flagship performance
- GPT-5 offers competitive pricing for sophisticated reasoning tasks
- Balance quality, speed, and cost based on your use case

**Your Turn**: Try these experiments:
1. Test with different prompt complexities (simple vs complex reasoning)
2. Compare the same model with different max_tokens settings
3. Test multilingual prompts to see if token counts change

## Key Takeaways

After completing this lesson, you should understand:

### Core Concepts
- **LLMs process text as tokens**, not words (~0.75 words per token for English)
- **Context windows** determine how much text an LLM can process at once (ranging from 128K to 1M tokens)
- **Transformers use attention mechanisms** to understand relationships between tokens
- **Next token prediction** is the fundamental operation behind text generation

### Model Selection
- **Choose models based on your specific needs**: Cost, latency, and quality all matter
- **Claude Haiku 4.5** for fast, cost-effective tasks (classification, simple queries)
- **Claude Sonnet 4.5** for balanced performance (general purpose, coding)
- **Claude Opus 4.5 / GPT-5** for complex reasoning, sophisticated agents, critical decisions
- **Model costs vary by 6x** - choosing the right model can dramatically reduce expenses

### Token Economics
- **Output tokens cost 5x more** than input tokens across all providers
- **Volume adds up quickly** - 1000 daily conversations = $500-3000/month depending on model choice
- **Optimization is critical**: Use caching, cheaper models for simple tasks, and context summarization
- **Always count tokens before deployment** to estimate real-world costs accurately

### Production Best Practices
- **Start with cheaper models** (Haiku 4.5) and upgrade only when needed
- **Implement caching** for frequently requested information
- **Summarize long conversations** to stay within context limits
- **Monitor token usage** in production to catch unexpected cost increases
- **Stream responses** to improve perceived latency for users
- **Test multiple models** to find the best quality/cost/speed balance for your use case

### Cost Comparison (2026)
For a typical customer support chatbot (1000 conversations/day, 5 messages each):
- **Claude Haiku 4.5**: ~$165/month
- **Claude Sonnet 4.5**: ~$495/month
- **Claude Opus 4.5**: ~$990/month

The right choice depends on whether you need simple responses (Haiku) or complex reasoning (Opus).

## Further Reading

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Original Transformers paper
- [Claude 4.5 Models Overview](https://platform.claude.com/docs/en/about-claude/models/overview) - Latest Claude models and capabilities
- [Anthropic API Pricing 2026](https://www.metacto.com/blogs/anthropic-api-pricing-a-full-breakdown-of-costs-and-integration) - Current Claude pricing
- [OpenAI GPT-5 Documentation](https://platform.openai.com/docs/models/gpt-5) - GPT-5 features and pricing
- [OpenAI Tokenizer](https://platform.openai.com/tokenizer) - Interactive token counting tool
