---
title: "LLM Fundamentals"
description: "Tokenization, context windows, and prompt caching fundamentals"
estimatedMinutes: 45
---

import { CodePlayground } from '@/components/curriculum/CodePlayground'

# The LLM Blueprint: What Every Word Costs You

You've used a vending machine. Put money in, pick what you want, get your item. An LLM API works the same way ‚Äî except you're paying per word, your "tray" has a size limit, and some machines are faster but less accurate than others.

This lesson covers the three constraints that shape every AI system you'll ever build:

1. **Tokens** ‚Äî the "coins" you feed the machine. Different text costs different amounts.
2. **Context windows** ‚Äî the size of the tray. Stuff too much in and the machine loses track.
3. **Model selection** ‚Äî which machine to use. Fast and cheap, or slow and brilliant?

Every architectural decision in production AI traces back to one of these three constraints. Master them, and prompt engineering, cost optimization, and system design all become obvious ‚Äî not because you memorized rules, but because you understand the mechanism.

> **A note on mental models**: Throughout this piece, we'll use everyday analogies ‚Äî vending machines, filing cabinets, cocktail parties. These are maps, not territory. They're useful because they let you predict what the system will do without needing to understand every equation. If a model helps you make the right design decision, it's a good model. That's the bar.

---

## üè• Real-World Challenge: The Multi-Tier Health Triage

**The Problem**: A global telehealth platform receives 50,000 patient queries daily. Using Opus 4.5 for every request (~1K input + 500 output tokens per query, 1.5M queries/month) costs **$78,750/month**‚Äîfinancially unsustainable. But downgrading all queries to Haiku risks missing critical medical symptoms that require deep reasoning.

**Business Constraints**:
- **Budget**: Must reduce monthly AI costs to under $15,000 (80% reduction)
- **Safety**: Cannot compromise on medical accuracy‚Äî100% precision on critical queries
- **Latency**: Must maintain &lt;3s P95 response time for good UX
- **Compliance**: HIPAA-compliant audit trail for all medical decisions

**Architectural Solution: The Model Cascade with Fallback Pattern**

Implement intelligent query routing based on complexity classification **with confidence thresholds**:

```typescript
// ============================================================
// Runnable Demo: Model Cascade with Confidence-Based Routing
// Run: npx ts-node medical-router-demo.ts
// ============================================================

// --- Types ---
interface Classification {
  type: 'simple' | 'complex' | 'critical'
  confidence: number
  reasoning: string
}

interface RouteResult {
  model: string
  response: string
  costPerQuery: number
  latencyMs: number
}

// --- Confidence Thresholds (tune based on evals) ---
const THRESHOLDS = {
  simple: 0.90,      // High bar for using cheapest model
  complex: 0.85,     // Moderate bar for mid-tier
  critical: 1.0,     // Always escalate (human judgment required)
  escalation: 0.85   // Below this, always use higher tier
}

// --- Step 1: Fast Classifier (~50ms with Haiku) ---
function classifyQuery(query: string): Classification {
  const lowerQuery = query.toLowerCase()

  // Critical keywords ‚Üí always escalate
  const criticalKeywords = ['chest pain', 'shortness of breath', 'severe bleeding', 'suicidal']
  if (criticalKeywords.some(kw => lowerQuery.includes(kw))) {
    return { type: 'critical', confidence: 1.0, reasoning: 'High-risk clinical keyword detected' }
  }

  // Simple/administrative queries
  const simpleKeywords = ['appointment', 'schedule', 'refill', 'hours', 'address', 'insurance']
  if (simpleKeywords.some(kw => lowerQuery.includes(kw))) {
    return { type: 'simple', confidence: 0.96, reasoning: 'Administrative intent detected' }
  }

  // Complex medical reasoning
  const complexKeywords = ['interaction', 'symptoms', 'diagnosis', 'side effects', 'treatment']
  if (complexKeywords.some(kw => lowerQuery.includes(kw))) {
    return { type: 'complex', confidence: 0.88, reasoning: 'Medical reasoning required' }
  }

  // Ambiguous ‚Üí low confidence triggers escalation
  return { type: 'complex', confidence: 0.72, reasoning: 'Ambiguous intent ‚Äî escalating' }
}

// --- Step 2: Confidence-Based Router ---
async function routeQuery(query: string, classification: Classification): Promise<RouteResult> {
  // LOW CONFIDENCE ‚Üí automatic escalation to mid-tier
  if (classification.confidence < THRESHOLDS.escalation) {
    console.log(`  ‚ö†Ô∏è  Low confidence (${classification.confidence}) ‚Üí escalating to Sonnet`)
    return callModel(query, 'sonnet')
  }

  // HIGH-CONFIDENCE SIMPLE ‚Üí fast & cheap
  if (classification.type === 'simple' && classification.confidence >= THRESHOLDS.simple) {
    return callModel(query, 'haiku')
  }

  // CRITICAL ‚Üí always human-in-the-loop
  if (classification.type === 'critical') {
    return {
      model: 'HUMAN_ESCALATION',
      response: '[ESCALATED TO NURSE] Patient reported critical symptoms',
      costPerQuery: 0,
      latencyMs: 0,
    }
  }

  // COMPLEX ‚Üí high-quality model with fallback
  return callModel(query, 'opus')
}

// --- Model Caller with Fallback ---
async function callModel(query: string, tier: 'haiku' | 'sonnet' | 'opus'): Promise<RouteResult> {
  const models = {
    haiku:  { name: 'claude-haiku-4-5',  cost: 0.001,  latency: 600 },
    sonnet: { name: 'claude-sonnet-4-5',  cost: 0.0105, latency: 1500 },
    opus:   { name: 'claude-opus-4-6',    cost: 0.0525, latency: 4500 },
  }

  const model = models[tier]

  // Simulate uncertainty detection (in production: check response text)
  const uncertaintyPhrases = ["i'm not sure", 'unclear', 'may require', 'should consult']
  const simulatedResponse = `Responding to: ${query.slice(0, 50)}...`

  // If Haiku expresses uncertainty ‚Üí escalate to Sonnet
  if (tier === 'haiku' && Math.random() < 0.05) { // 5% uncertainty rate
    console.log('  üîÑ Haiku expressed uncertainty ‚Üí escalating to Sonnet')
    return callModel(query, 'sonnet')
  }

  return {
    model: model.name,
    response: simulatedResponse,
    costPerQuery: model.cost,
    latencyMs: model.latency,
  }
}

// --- Run the Demo ---
async function demo() {
  const testQueries = [
    'When is my next appointment?',                         // ‚Üí Haiku (simple)
    'What are the side effects of metformin with lisinopril?', // ‚Üí Opus (complex)
    'I am having chest pain and shortness of breath',       // ‚Üí HUMAN (critical)
    'Tell me something about health',                       // ‚Üí Sonnet (low confidence)
  ]

  console.log('üè• Medical Query Router Demo')
  console.log('‚îÄ'.repeat(60))

  let totalCost = 0

  for (const query of testQueries) {
    const classification = classifyQuery(query)
    const result = await routeQuery(query, classification)
    totalCost += result.costPerQuery

    console.log(`\nüìã Query: "${query}"`)
    console.log(`   Classification: ${classification.type} (confidence: ${classification.confidence})`)
    console.log(`   Routed to: ${result.model}`)
    console.log(`   Cost: $${result.costPerQuery.toFixed(4)} | Latency: ${result.latencyMs}ms`)
  }

  console.log('\n' + '‚îÄ'.repeat(60))
  console.log(`üí∞ Total cost for 4 queries: $${totalCost.toFixed(4)}`)
  console.log(`   vs. all-Opus baseline:    $${(4 * 0.0525).toFixed(4)}`)
  console.log(`   Savings:                  ${((1 - totalCost / (4 * 0.0525)) * 100).toFixed(0)}%`)
}

demo()
```

**Try it yourself** ‚Äî copy the code above into a file and run `npx ts-node medical-router-demo.ts`. You'll see each query classified, routed to the appropriate model tier, and the cost savings calculated.

**Production Impact**:
- **Cost**: $78.7K ‚Üí ~$5.4K/month (93% reduction)
- **Latency**: 4.2s ‚Üí 1.8s P95 (57% faster) by routing 85% to Haiku
- **Safety**: 100% of critical queries escalated to human review (zero missed emergencies)
- **ROI**: 3-month implementation, $73K/month savings = **payback in days**

---

<Callout type="info">
**Run this in your head**: You're the CTO deciding where to route "I have a headache" vs "I'm having chest pain." One costs a penny, the other might cost a life. What's the cheapest model you'd trust for each ‚Äî and what happens if the cheap model gets the chest pain case wrong? That tension between cost and safety is the entire problem this architecture solves.
</Callout>

**How This Connects to LLM Fundamentals**:
- **Tokenization Physics** ‚Üí Understanding that Haiku uses the same tokenizer as Opus enables seamless cascade without prompt rewriting
- **Context Constraints** ‚Üí Staying within 32K token sweet spot ensures high precision for medical queries
- **Model Selection Matrix** ‚Üí Matching the right model constraint (cost vs quality vs latency) to each query type

**[üëâ Lab: Build the Multi-Tier Triage System](/curriculum/week-1/labs/multi-tier-triage)**

In the hands-on lab, you'll implement the complete system: Haiku classifier, Opus escalation, confidence thresholds, cost tracking, and production validation showing 80%+ cost savings.

---

## Tokenization: The Atoms of AI Communication

Think about money. A dollar isn't the smallest unit ‚Äî a cent is. You can't pay 0.3 cents for something; the system rounds to the nearest cent. Tokens work the same way for LLMs. They're the smallest unit of text the model can "see." You can't send half a token. Everything you write gets chopped into these atomic pieces, and each one costs money.

The twist: the chopping isn't intuitive. "Hello" might be one token. "Pneumonoultramicroscopicsilicovolcanoconiosis" might be twelve. And different tokenizers chop differently ‚Äî so the same text costs different amounts on different models.

### Why Tokenization Matters

**Example: The tiktoken difference**
```typescript
// Claude's tokenizer (cl100k_base)
"Hello, world!" ‚Üí 3 tokens

// GPT-4o's tokenizer (o200k_base)
"Hello, world!" ‚Üí 2 tokens

// Why it matters:
const inputCost = tokenCount * pricePerToken
// Different tokenizers = different costs for the SAME prompt
```

### Tokenization Edge Cases That Bite You in Production

Tokenizers were trained mostly on natural English text. When you throw code, numbers, medical jargon, or creative spacing at them, they fragment unpredictably ‚Äî costing you more money and sometimes degrading the model's understanding:

**Edge Case #1: Spacing Breaks Tokenization**
```typescript
// Example: The word "Apple"
"Apple" ‚Üí 1 token

// But with spaces...
"A p p l e" ‚Üí 5 tokens (5x the cost!)

// Why this matters:
// User input: "My WiFi password is A p p l e 1 2 3"
// Cost: 7 tokens instead of 3 tokens
// Impact: $0.000021 vs $0.000009 per request
// At 10M requests: $210 vs $90 = $120/month wasted
```

**Edge Case #2: Numbers Tokenize Inconsistently**
```typescript
// Numbers fragment unpredictably
"1234" ‚Üí 1 token
"12345" ‚Üí 1 token
"123456" ‚Üí 2 tokens  // ‚ö†Ô∏è Boundary crossed
"1234567" ‚Üí 2 tokens

// Scientific notation
"1.5e10" ‚Üí 4 tokens ("1", ".", "5e", "10")

// Architectural impact: Dollar amounts
"$1,234.56" ‚Üí 5 tokens (",", "1", ",", "234", ".")
"$1234.56" ‚Üí 3 tokens  // 40% fewer tokens without commas
```

**Edge Case #3: Code Indentation is Expensive**
```typescript
// Well-formatted code (common style)
const formatted = `
function add(a, b) {
    return a + b
}
` // 15 tokens (includes whitespace)

// Minified code (no indentation)
const minified = `function add(a,b){return a+b}`
// 10 tokens (33% fewer!)

// Production impact:
// Sending 1000 code snippets/day √ó 5 extra tokens
// = 5K tokens/day √ó $3/MTok = $0.015/day = $0.45/month
// Seems small, but at scale (1M snippets): $450/month
```

**Edge Case #4: Medical Acronyms and Jargon**
```typescript
// Medical context tokenization
"COPD" ‚Üí 2 tokens ("CO", "PD")  // Chronic Obstructive Pulmonary Disease
"COVID" ‚Üí 2 tokens ("COV", "ID")
"MRI scan" ‚Üí 3 tokens ("M", "RI", "scan")

// But...
"CT scan" ‚Üí 2 tokens ("CT", "scan")  // More common, single token

// Why this matters for reasoning:
// LLMs trained on "COVID" as 1 semantic unit
// But tokenizer splits it ‚Üí model must "reassemble" meaning
// Result: Slightly degraded medical reasoning on rare acronyms
```

<Callout type="info">
**Run this in your head**: You're building a financial app. The user types "$1,234,567.89". How many tokens is that? Now remove the commas: "$1234567.89". Feel the difference ‚Äî fewer tokens, lower cost, same information. Now multiply by 10 million requests per month. That's the difference between a rounding error and a real line item on your bill.
</Callout>

**Production Pattern: Test Your Domain-Specific Vocabulary**
```typescript
import Anthropic from '@anthropic-ai/sdk'

async function analyzeTokenization(terms: string[]) {
  const anthropic = new Anthropic()

  for (const term of terms) {
    const result = await anthropic.messages.countTokens({
      model: 'claude-sonnet-4-5-20251101',
      messages: [{ role: 'user', content: term }]
    })

    console.log(`"${term}" ‚Üí ${result.input_tokens} tokens`)

    // Flag if fragmentation is high
    const chars = term.length
    const tokensPerChar = result.input_tokens / chars
    if (tokensPerChar > 0.5) {
      console.warn(`  ‚ö†Ô∏è High fragmentation: ${(tokensPerChar * 100).toFixed(0)}% of chars are tokens`)
    }
  }
}

// Test medical terms
await analyzeTokenization([
  'COPD', 'COVID-19', 'MRI', 'CT', 'ECG',
  'pneumonoultramicroscopicsilicovolcanoconiosis'  // Longest medical term
])
```

**Architectural Implications**:
1. **Preprocessing**: Consider normalizing input (remove commas from numbers, minify code)
2. **Domain testing**: Tokenize your domain-specific vocabulary to find expensive terms
3. **Cost modeling**: Don't assume "1 word = 1 token" - test real data
4. **Reasoning quality**: Highly fragmented terms may reduce model comprehension

**Cost-Benefit Analysis**:
- **Preprocessing overhead**: 5-10ms per request
- **Token savings**: 10-30% for technical/medical domains
- **ROI**: Positive if volume &gt;100K requests/month

### Architectural Constraint #1: Token Efficiency

**The Physics**:
- Code and technical terms fragment more than natural language
- Special characters (JSON, XML) consume extra tokens
- Non-English text often requires 2-3x more tokens

**Design Impact**:
```typescript
// ‚ùå Token-inefficient prompt (18 tokens)
"Please analyze this JSON: {...}"

// ‚úÖ Token-optimized (12 tokens)
"Analyze JSON: {...}"

// At 10M requests/month, this saves $200-500
```

## The Working Memory Budget: Context Windows

Imagine your desk. You can spread out maybe 20 documents and still find what you need. Spread out 200, and you start losing things ‚Äî especially the ones buried in the middle of the pile. That's exactly how LLM context windows work.

Models advertise "1M token windows" but lose precision as you approach the limit. This is called the **"Lost in the Middle"** phenomenon ‚Äî the model pays most attention to what's at the beginning and end of the context, and "forgets" what's in the middle.

### The Precision Falloff Curve

```typescript
// Effective Context Utilization (Claude Sonnet 4.5)
interface ContextReliability {
  '0-32K tokens': 95-100%    // ‚úÖ High precision
  '32K-128K tokens': 85-95%  // ‚ö†Ô∏è  Slight degradation
  '128K-500K tokens': 70-85% // ‚ö†Ô∏è  Noticeable precision loss
  '500K-1M tokens': 50-70%   // ‚ùå "Lost in the Middle"
}
```

### Architectural Constraint #2: Context Window Strategy

**Design Pattern: The Context Budget**
```typescript
const CONTEXT_BUDGET = {
  systemPrompt: 2000,      // Fixed instructions
  fewShot: 5000,           // Example demonstrations
  userContext: 20000,      // Dynamic user data
  buffer: 3000,            // Safety margin
  maxOutput: 4000          // Response limit
} // Total: 34K tokens (sweet spot for reliability)
```

**Anti-Pattern**: Stuffing the entire 200K window
```typescript
// ‚ùå Architect's mistake
const prompt = `
  ${systemInstructions} +    // 2K tokens
  ${allCustomerHistory} +    // 180K tokens (!)
  ${currentQuery}            // 1K tokens
`
// Result: Model "loses" critical details from history
```

**‚úÖ Production Pattern**: Sliding window with summarization
```typescript
const prompt = `
  ${systemInstructions} +         // 2K
  ${summarizedHistory} +          // 8K (condensed)
  ${recentContextFull} +          // 15K (last 10 turns)
  ${currentQuery}                 // 1K
` // Total: 26K tokens ‚Üí stays in high-precision zone
```

### Measuring Context Density: The Information-per-Token Ratio

**Architectural Tool**: Calculate the "information density" of your context to predict precision degradation.

```typescript
interface ContextAnalysis {
  totalTokens: number
  informationChunks: number  // Distinct pieces of information
  densityScore: number       // 0-1, higher is better
  recommendation: string
}

function analyzeContextDensity(
  context: string,
  informationChunks: number
): ContextAnalysis {
  // Estimate token count (rough: 4 chars per token)
  const totalTokens = Math.ceil(context.length / 4)

  // Information density: how much useful info per token?
  // Optimal: 0.15-0.25 (15-25% of tokens carry key information)
  const densityScore = informationChunks / totalTokens

  let recommendation: string
  if (densityScore &gt; 0.25) {
    recommendation = '‚úÖ Excellent density - all information is relevant'
  } else if (densityScore &gt;= 0.15) {
    recommendation = '‚úÖ Good density - optimal for precision'
  } else if (densityScore &gt;= 0.10) {
    recommendation = '‚ö†Ô∏è Moderate density - consider summarizing'
  } else {
    recommendation = '‚ùå Poor density - high risk of Lost-in-the-Middle. Aggressive pruning needed.'
  }

  return {
    totalTokens,
    informationChunks,
    densityScore,
    recommendation
  }
}

// Example: Customer support context
const customerHistory = `
  [2024-01-10] User asked about pricing
  [2024-01-12] User requested trial extension
  [2024-01-15] User reported bug in dashboard
  [2024-01-20] User escalated to premium support
  [2024-01-22] Bug fixed, user satisfied
` // 5 key information chunks

const analysis = analyzeContextDensity(customerHistory, 5)
console.log(analysis)
// {
//   totalTokens: 58,
//   informationChunks: 5,
//   densityScore: 0.086,
//   recommendation: '‚ùå Poor density - high risk of Lost-in-the-Middle...'
// }

// Solution: Condense to higher density
const condensed = `
Customer: trial ‚Üí bug ‚Üí escalation ‚Üí resolved ‚Üí satisfied
Critical: Dashboard bug (ID #1234) fixed on 2024-01-22
` // Same 5 chunks, fewer tokens

const improvedAnalysis = analyzeContextDensity(condensed, 5)
// {
//   totalTokens: 23,
//   informationChunks: 5,
//   densityScore: 0.217,  // ‚úÖ In optimal range
//   recommendation: '‚úÖ Good density - optimal for precision'
// }
```

**Production Pattern: Pre-flight Context Check**
```typescript
async function buildPromptWithDensityCheck(
  systemPrompt: string,
  userContext: string,
  query: string
): Promise<string> {
  // Estimate information chunks (simplified: count sentences)
  const chunks = userContext.split(/[.!?]/).filter(s => s.trim().length &gt; 10).length

  const analysis = analyzeContextDensity(userContext, chunks)

  if (analysis.densityScore &lt; 0.10) {
    console.warn('‚ö†Ô∏è Low context density detected. Summarizing...')
    // Trigger summarization pipeline
    userContext = await summarizeContext(userContext, chunks)
  }

  return `${systemPrompt}\n\nContext:\n${userContext}\n\nQuery: ${query}`
}
```

<Callout type="info">
**Run this in your head**: You're handing someone a 200-page report and asking a question. Would you rather give them: (A) all 200 pages, or (B) a 2-page executive summary plus the 10 most relevant pages? Option B is always better ‚Äî less to search through, key facts aren't buried. That's the context budget pattern. More isn't better; *denser* is better.
</Callout>

**Architectural Insight**: Context density inversely correlates with "Lost-in-the-Middle" risk:
- **High density (&gt;0.20)**: Information-packed, minimal fluff ‚Üí model stays focused
- **Low density (&lt;0.10)**: Too much prose, key facts buried ‚Üí model loses precision

**Cost Impact**: Improving density from 0.08 to 0.20 means 60% token reduction ‚Üí 60% cost savings while improving accuracy.

---

## Picking the Right Tool: Model Selection

You wouldn't use a chainsaw to cut bread, or a bread knife to fell a tree. Both are cutting tools ‚Äî but the right choice depends entirely on the job. Model selection works the same way. There is no "best model." There is only the best model *for this task, at this scale, within this budget*.

### The Three-Axis Constraint Model

| Model | Input Cost | Output Cost | Latency (p95) | Quality Tier | Architectural Use Case |
|-------|-----------|-------------|---------------|--------------|------------------------|
| **Claude Opus 4.5** | $15/MTok | $75/MTok | 4-8s | Tier 1 | Critical reasoning, high-stakes decisions, complex agents |
| **GPT-5** | $10/MTok | $40/MTok | 3-6s | Tier 1 | Deep analytical tasks, strategic planning |
| **Claude Sonnet 4.5** | $3/MTok | $15/MTok | 1-3s | Tier 1 | Balanced workhorse, coding, production agents |
| **GPT-4o** | $2.50/MTok | $10/MTok | 1-2s | Tier 2 | General-purpose tasks, multimodal processing |
| **Claude Haiku 4.5** | $0.25/MTok | $1.25/MTok | 0.3-0.8s | Tier 2 | High-volume classification, fast inference |

> **Cost Reality Check**: At 1M requests/month with 1K input + 500 output tokens:
> - Haiku: $875/month
> - Sonnet: $10,500/month (12x more)
> - Opus: $41,250/month (47x more)

### Architectural Decision Tree

```typescript
function selectModel(useCase: UseCase): Model {
  // Constraint #1: Latency Requirements
  if (useCase.requiresRealTime && useCase.p95Latency &lt; 1000) { // ms
    return 'claude-haiku-4.5' // Only option for sub-second
  }

  // Constraint #2: Cost Budget
  if (useCase.requestVolume &gt; 1_000_000 && useCase.budgetPerRequest &lt; 0.01) {
    return 'claude-haiku-4.5' // Economic constraint forces choice
  }

  // Constraint #3: Quality Floor
  if (useCase.requiresCriticalReasoning || useCase.errorTolerance &lt; 0.01) {
    return 'claude-opus-4.5' // Quality constraint overrides cost
  }

  // Default: Balanced choice
  return 'claude-sonnet-4.5'
}
```

<Callout type="info">
**Run this in your head**: You're building a customer support bot. 80% of questions are "where's my order?" ‚Äî simple, repetitive, needs to be fast. 15% are "I was charged twice" ‚Äî moderate complexity, needs accuracy. 5% are "I want to escalate a legal complaint" ‚Äî high stakes, needs the best reasoning available. Which model handles which tier? And what happens if you use Opus for ALL of them?
</Callout>

### Real-World Architecture Patterns

**Pattern 1: The Cascade Strategy**
```typescript
// High-quality with cost optimization
async function processWithCascade(input: string) {
  try {
    // Try fast + cheap first
    const result = await callHaiku(input)
    if (result.confidence &gt; 0.9) return result

    // Fall back to quality if uncertain
    return await callSonnet(input)
  } catch {
    // Ultimate fallback for high-stakes
    return await callOpus(input)
  }
}
// Cost impact: 70% requests use Haiku, 25% Sonnet, 5% Opus
// Average cost: ~$2.50/MTok vs. $15/MTok (6x savings)
```

**Pattern 2: The Tiered Quality System**
```typescript
// Different models for different users
const MODEL_BY_TIER = {
  free: 'claude-haiku-4.5',      // Fast, cheap
  pro: 'claude-sonnet-4.5',      // Balanced
  enterprise: 'claude-opus-4.5'  // Best quality
}
```

**Decision Framework:**
1. **Start with cost optimization** - Use Haiku 4.5 as your baseline
2. **Test quality** - If accuracy is insufficient, upgrade to Sonnet 4.5
3. **Evaluate latency** - If responses are too slow for UX, consider Haiku or optimize prompts
4. **Reserve premium models** - Only use Opus/GPT-5 when quality justifies the 3-6x cost increase

## Token Economics: What It Actually Costs

Here's where the vending machine analogy gets concrete. Understanding token costs is the difference between a sustainable AI product and one that bankrupts you at scale:

**Example**: Chat application with 1000 daily users
- Average conversation: 10 messages
- Average message: 100 tokens input, 200 tokens output
- Total daily: 1000 users √ó 10 msgs √ó (100 input + 200 output) = 1M input + 2M output/day
- Monthly cost (Claude Sonnet 4.5): (1M √ó $3 + 2M √ó $15) / 1M √ó 30 = $33/day = $990/month

**Optimization strategies**:
1. Use cheaper models for simple tasks
2. **Implement prompt caching** (see next section) - up to 90% cost reduction
3. Summarize long conversations to reduce context
4. Stream responses to improve perceived latency

---

## Prompt Caching: Don't Pay for the Same Thing Twice

Imagine you're ordering coffee at the same shop every morning. You say: "Large oat milk latte, extra shot, no foam." Every single day. Wouldn't it be nice if they just remembered?

That's prompt caching. Most LLM applications send the same system instructions, tool definitions, and few-shot examples with every request. Without caching, the model re-processes all of that from scratch every time ‚Äî and you pay full price every time. Prompt caching lets the model "remember" the static parts, cutting costs by up to **90%**.

**The Problem**: Without caching, every API call re-processes the same system prompt:

```typescript
// ‚ùå Without caching: Full cost every time
for (let i = 0; i &lt; 1000; i++) {
  await anthropic.messages.create({
    model: 'claude-sonnet-4-5-20251101',
    system: LONG_SYSTEM_PROMPT,  // 5000 tokens, re-processed 1000x
    messages: [{ role: 'user', content: queries[i] }]
  })
}
// Cost: 1000 calls √ó 5000 tokens √ó $3/MTok = $15
```

**The Solution**: Anthropic's Prompt Caching uses prefix-matching to cache repeated content:

```typescript
// ‚úÖ With prompt caching: 90% cost reduction
await anthropic.messages.create({
  model: 'claude-sonnet-4-5-20251101',
  system: [
    {
      type: 'text',
      text: LONG_SYSTEM_PROMPT,  // 5000 tokens
      cache_control: { type: 'ephemeral' }  // Cache this!
    }
  ],
  messages: [{ role: 'user', content: query }]
})
// First call: Full cost ($0.015)
// Subsequent calls: 90% discount on cached tokens ($0.0015)
// 1000 calls: $0.015 + (999 √ó $0.0015) = $1.50 (vs $15)
```

### How Prompt Caching Works

**Prefix-Matching Algorithm**:
1. Anthropic hashes the **prefix** of your prompt (system + early messages)
2. If the hash matches a cached entry (within 5-minute TTL), cached tokens are reused
3. Only **new/changed tokens** are processed at full cost
4. Cache hits reduce cost by **90%** and latency by **50-80%**

**Cache Hierarchy** (Anthropic's implementation):
```typescript
// Cache structure: System ‚Üí Tools ‚Üí Few-shot ‚Üí Dynamic context
const prompt = {
  system: [
    {
      type: 'text',
      text: SYSTEM_INSTRUCTIONS,  // 2K tokens - ‚úÖ Cacheable (static)
      cache_control: { type: 'ephemeral' }
    },
    {
      type: 'text',
      text: TOOL_DEFINITIONS,     // 3K tokens - ‚úÖ Cacheable (static)
      cache_control: { type: 'ephemeral' }
    },
    {
      type: 'text',
      text: FEW_SHOT_EXAMPLES,    // 10K tokens - ‚úÖ Cacheable (mostly static)
      cache_control: { type: 'ephemeral' }
    }
  ],
  messages: [
    // User history (dynamic) - ‚ùå Not cached
    ...conversationHistory,
    { role: 'user', content: currentQuery }
  ]
}
```

### Production Caching Strategy

**Rule of Thumb**: Cache anything that repeats across &gt;3 requests:

```typescript
// Architectural Pattern: Hierarchical caching
interface CachingStrategy {
  // Tier 1: Always cache (100% static)
  systemPrompt: string           // Never changes
  toolDefinitions: string        // Stable across versions

  // Tier 2: Cache frequently (95% static)
  fewShotExamples: string        // Updated weekly

  // Tier 3: Don't cache (dynamic)
  userContext: string            // Changes per request
  conversationHistory: string[]  // Unique per conversation
}
```

**Cost Impact Example** (Real-world AI coding assistant):
- System prompt: 5K tokens
- Tool definitions: 8K tokens
- Few-shot examples: 15K tokens
- **Total cacheable: 28K tokens**
- Volume: 100K requests/day

**Without caching**:
- Cost: 100K √ó 28K √ó $3/MTok = **$8,400/day** = **$252K/month**

**With caching** (90% cache hit rate):
- First call: 100K √ó 28K √ó $3/MTok = $8,400
- Cached calls: 100K √ó 28K √ó $0.30/MTok √ó 0.90 = $756
- **Total: $840/day** = **$25.2K/month**
- **Savings: 90% ($226.8K/month)**

### Warning: Cache Invalidation Edge Cases

**When caching breaks**:
1. **Prefix modification**: Changing **any** cached content invalidates the entire cache
2. **TTL expiration**: Caches last 5 minutes (Anthropic) - low-traffic apps see fewer hits
3. **Token boundary misalignment**: Adding 1 token to the prefix breaks the cache

```typescript
// ‚ùå Cache-breaking mistake
const systemPrompt = `
  You are a helpful assistant.
  Current time: ${new Date().toISOString()}  // Changes every second!
`
// This invalidates the cache on every request

// ‚úÖ Cache-friendly pattern
const systemPrompt = `You are a helpful assistant.`
const userMessage = `Current time: ${new Date().toISOString()}\n\n${query}`
```

**Architect's Rule**: Keep **static content** in cached sections, **dynamic content** in messages.

<Callout type="info">
**Run this in your head**: Your system prompt is 5,000 tokens. You make 100,000 API calls per day. Without caching: you're paying to process those same 5,000 tokens 100,000 times. With caching: you pay once, then get a 90% discount on the next 99,999. Feel the difference ‚Äî that's $8,400/day vs $840/day. Same output. Same quality. 90% less money.
</Callout>

### Latency Benefits

**Caching reduces not just cost, but latency**:
- Cached tokens skip tokenization, attention, and inference
- **Typical reduction**: 50-80% faster time-to-first-token
- **Example**: 2000ms ‚Üí 500ms for cached system prompts

**Production Impact**:
```typescript
// Without cache: 2.5s P95 latency
// With cache: 0.8s P95 latency ‚Üí 68% faster
// User experience: Feels "instant" vs "thinking"
```

### Implementation Checklist

‚úÖ **Before deploying prompt caching**:
1. Identify static vs dynamic prompt sections
2. Move static content to prefix (system, tools, few-shot)
3. Add `cache_control: { type: 'ephemeral' }` to static blocks
4. Monitor cache hit rate (aim for &gt;85%)
5. Ensure traffic is high enough (&gt;10 requests/minute) to benefit from 5-min TTL
6. Test cache invalidation behavior during prompt updates

**Monitoring**: Track cache hit rate in production:
```typescript
// Add to logging/metrics
const cacheMetrics = {
  cacheCreationTokens: response.usage.cache_creation_input_tokens || 0,
  cacheReadTokens: response.usage.cache_read_input_tokens || 0,
  cacheHitRate: cacheReadTokens / (cacheReadTokens + cacheCreationTokens)
}
// Target: &gt;85% hit rate for cost efficiency
```

---

## When Memory Gets Full: Context Truncation Strategies

Your phone runs out of storage. What do you delete? Old photos you'll never look at again? The app you use daily? Your entire camera roll? The wrong choice loses something irreplaceable. The right choice frees space while keeping what matters.

LLMs face this exact problem. A customer support chat reaches 200K tokens (context limit: 200K). The next message would overflow. **What do you delete?** The answer determines whether the AI remembers its safety rules, the user's history, or nothing at all.

---

### The Decision Matrix: What to Forget

When your system hits the 200K+ token limit, you need a strategy ‚Äî not a guess ‚Äî for what gets dropped.

| Strategy | Technical Implementation | Best For | Architect's Trade-off |
|----------|-------------------------|----------|----------------------|
| **FIFO (First-In, First-Out)** | Drop the oldest messages in the array as new ones arrive | Short, transactional chatbots | ‚ö†Ô∏è **High Risk**: The model loses the initial "System Instructions" or the original user goal. **Never use this pattern for safety-critical systems.** |
| **Middle-Out Truncation** | Keep the System Prompt and the most recent 10 messages; delete the middle history | Long, multi-turn technical support | ‚úÖ **Optimal**: Preserves the "Mission" (system prompt) and the "Current State" (recent messages) while clearing the bulk. **Production default for most applications.** |
| **Rolling Summarization** | Use a cheaper model (Haiku) to summarize the first 50 messages into a 200-token "State Summary" | Complex, multi-day research agents | ‚ö†Ô∏è **High Latency**: Adds a summarization step (300-800ms), but maintains 100% of the semantic history. **Best for high-value sessions.** |
| **Vector-Backed Memory** | Move "inactive" context to a Vector DB and retrieve only what is relevant to the current query | Enterprise Knowledge Bases, AI agents with long-term memory | ‚ö†Ô∏è **High Complexity**: Requires a RAG infrastructure but offers effectively "infinite" memory. **Only viable at scale (&gt;10K sessions/month).** |

**Cost-Quality-Latency Matrix**:
```
FIFO              ‚Üí Free, 0ms, ‚ùå Dangerous (loses system prompt)
Middle-Out        ‚Üí Free, 0ms, ‚úÖ Good (preserves mission + current state)
Summarization     ‚Üí $0.001-0.01, 300-800ms, ‚úÖ Excellent (semantic preservation)
Vector-Backed     ‚Üí $0.01-0.05, 50-200ms, ‚úÖ Best (infinite memory, high complexity)
```

---

### Strategy #1: FIFO (First-In, First-Out) ‚Äî ‚ùå NOT RECOMMENDED

**How it works**: Drop oldest messages as new ones arrive.

```typescript
// ‚ùå DANGEROUS: This pattern can delete system instructions
function fifoTruncation(
  messages: Message[],
  maxTokens: number
): Message[] {
  let currentTokens = estimateTokens(messages)

  // Remove from beginning until under limit
  while (currentTokens > maxTokens) {
    messages.shift()  // ‚ö†Ô∏è Can delete system prompt!
    currentTokens = estimateTokens(messages)
  }

  return messages
}
```

**Why This Fails**:
- **System Prompt Loss**: If your first message is "You are a HIPAA-compliant medical assistant", FIFO will delete it first
- **Mission Drift**: Model forgets its purpose mid-conversation
- **Governance Failure**: Safety constraints defined at Index 0 are lost

**The ONLY Safe Use Case**: When system prompt is re-injected on every turn (inefficient, costly)

---

### Strategy #2: Middle-Out Truncation (Production Default) ‚Äî ‚úÖ RECOMMENDED

**How it works**: Keep system prompt + recent N messages, discard the middle.

```typescript
interface TruncationStrategy {
  systemPromptTokens: number    // Always keep (2K)
  recentMessagesCount: number   // Last 10 messages (15K)
  // Middle 180K tokens ‚Üí DROPPED
}

function dropMiddleStrategy(
  messages: Message[],
  maxTokens: number = 200_000
): Message[] {
  const systemTokens = 2000
  const recentCount = 10
  const bufferTokens = 3000

  // Keep system + last 10 messages
  const recentMessages = messages.slice(-recentCount)
  const recentTokens = estimateTokens(recentMessages)

  const totalTokens = systemTokens + recentTokens + bufferTokens

  if (totalTokens < maxTokens) {
    return messages  // Still under limit
  }

  // Drop middle, keep beginning + end
  return [
    ...messages.slice(0, 5),     // First 5 (context setup)
    { role: 'system', content: '[... earlier conversation truncated ...]' },
    ...messages.slice(-recentCount)  // Last 10 (recent context)
  ]
}
```

**Pros**:
- Fast (no API calls)
- Maintains recency (most important for ongoing conversations)
- System prompt always preserved

**Cons**:
- **Lost-in-the-Middle**: Model has no knowledge of mid-conversation context
- Can break multi-turn reasoning ("As I mentioned earlier..." ‚Üí model has no memory)
- User references to earlier topics feel like AI "forgot"

**Best For**: Simple Q&A, transactions without long-term dependencies

---

### Strategy #3: Rolling Summarization (Highest Quality, Costly)

**How it works**: Use an LLM to compress old messages into a "Condensed State".

```typescript
async function summarizeOldContext(
  messages: Message[],
  maxTokens: number
): Promise<Message[]> {
  const systemTokens = 2000
  const recentCount = 10
  const targetSummaryTokens = 5000

  // Split: recent (keep full) vs old (summarize)
  const recentMessages = messages.slice(-recentCount)
  const oldMessages = messages.slice(0, -recentCount)

  // Check if summarization is needed
  const currentTokens = estimateTokens(messages)
  if (currentTokens < maxTokens * 0.8) {
    return messages  // Still have headroom
  }

  // Use LLM to summarize old context
  const summary = await anthropic.messages.create({
    model: 'claude-haiku-4-5-20251101',  // Fast summarizer
    max_tokens: 1500,
    messages: [{
      role: 'user',
      content: `Summarize the key points, decisions, and user preferences from this conversation history. Keep it under 1200 tokens:\n\n${formatMessages(oldMessages)}`
    }]
  })

  // Inject summary as synthetic "context" message
  return [
    { role: 'system', content: 'Previous conversation summary:' },
    { role: 'assistant', content: summary.content[0].text },
    ...recentMessages  // Full recent context
  ]
}

// Cost analysis
// Old context: 180K tokens √ó $3/MTok = $0.54 per request (dropped)
// Summary cost: 180K input + 1.5K output = $0.54 + $0.02 = $0.56 ONE-TIME
// Future requests: 5K summary tokens vs 180K full = 97% token reduction
// ROI: Positive after 2+ requests using the summary
```

**Pros**:
- **Highest quality**: Preserves key information from entire conversation
- Model can reference "earlier" context without it being in the window
- User experience: AI "remembers" past decisions

**Cons**:
- **Costly**: Requires additional API call (Haiku: $0.001-0.01 per summarization)
- **Latency**: Adds 300-800ms to request time
- **Summary drift**: Information loss through compression (5-10% typical)

**Best For**: Long-running sessions, customer support, multi-turn agents

**Production Implementation**:
```typescript
// Trigger summarization at 80% capacity
const SUMMARIZATION_TRIGGER = maxTokens * 0.80

// Cache summaries to avoid re-summarizing
interface ConversationCache {
  conversationId: string
  summary: string
  summarizedUpToIndex: number  // Message index
  lastUpdated: Date
}

async function managedTruncation(
  conversationId: string,
  messages: Message[],
  cache: ConversationCache | null
): Promise<Message[]> {
  const currentTokens = estimateTokens(messages)

  if (currentTokens < SUMMARIZATION_TRIGGER) {
    return messages  // No action needed
  }

  // Check if we have a cached summary
  if (cache && cache.summarizedUpToIndex > 0) {
    // Use cached summary + new messages
    const newMessages = messages.slice(cache.summarizedUpToIndex)
    return [
      { role: 'system', content: cache.summary },
      ...newMessages
    ]
  }

  // No cache - trigger fresh summarization
  return await summarizeOldContext(messages, maxTokens)
}
```

---

### Strategy #4: Vector-Backed Memory (Enterprise-Grade, Infinite Memory)

**How it works**: Store "inactive" context in a vector database, retrieve only relevant chunks for the current query.

```typescript
import { PineconeClient } from '@pinecone-database/pinecone'
import { OpenAIEmbeddings } from 'langchain/embeddings/openai'

interface VectorMemoryStrategy {
  vectorDB: PineconeClient
  embeddings: OpenAIEmbeddings
  conversationId: string
}

async function vectorBackedTruncation(
  messages: Message[],
  currentQuery: string,
  config: VectorMemoryStrategy
): Promise<Message[]> {
  const systemPrompt = messages[0]  // Always keep Index 0
  const recentMessages = messages.slice(-10)  // Last 10 turns

  // Everything else ‚Üí Vector DB
  const oldMessages = messages.slice(1, -10)

  // Step 1: Store old context in vector DB (if not already stored)
  for (const msg of oldMessages) {
    await config.vectorDB.upsert({
      id: `${config.conversationId}-${msg.timestamp}`,
      values: await config.embeddings.embedQuery(msg.content),
      metadata: {
        role: msg.role,
        content: msg.content,
        timestamp: msg.timestamp
      }
    })
  }

  // Step 2: Retrieve relevant old context based on current query
  const queryEmbedding = await config.embeddings.embedQuery(currentQuery)
  const relevantContext = await config.vectorDB.query({
    vector: queryEmbedding,
    topK: 5,  // Retrieve top 5 most relevant old messages
    includeMetadata: true
  })

  // Step 3: Build context window with system + relevant + recent
  return [
    systemPrompt,  // Index 0: Immutable
    ...relevantContext.matches.map(match => ({
      role: match.metadata.role,
      content: match.metadata.content
    })),
    ...recentMessages  // Last 10: Current state
  ]
}

// Cost breakdown (10K sessions/month, 200 messages each)
// Embedding cost: 2M messages √ó 1K tokens √ó $0.0001/MTok = $0.20/month
// Storage cost: 2M vectors √ó $0.50/month = $1,000/month (Pinecone Pro)
// Query cost: 10K queries/day √ó $0.001/query = $300/month
// Total: $1,300/month for effectively infinite memory
// vs. Summarization: $0.01 √ó 2M sessions = $20,000/month
```

**Pros**:
- **Infinite memory**: No hard context limit, stores unlimited history
- **Semantic retrieval**: Finds relevant context even from months ago
- **Fast queries**: 50-200ms retrieval time (vs 300-800ms for summarization)
- **Cost-effective at scale**: Fixed infrastructure cost regardless of session length

**Cons**:
- **High complexity**: Requires vector DB (Pinecone, Weaviate, Postgres+pgvector)
- **Infrastructure cost**: $500-2000/month for hosting
- **Cold start**: Embedding generation adds latency on first message
- **Retrieval quality**: Depends on embedding model quality (can miss nuanced context)

**Best For**:
- Enterprise knowledge bases with &gt;10K active sessions
- AI agents that need to remember conversations from weeks/months ago
- Multi-user systems where context sharing is valuable

**When NOT to Use**:
- &lt;1K sessions/month (summarization is cheaper)
- Simple chatbots without long-term memory needs
- Regulated industries requiring deterministic replay (vector retrieval is probabilistic)

---

### ‚ö†Ô∏è The "Instruction Anchor" ‚Äî Critical Safety Pattern

> **Architect's Rule**: Regardless of the truncation strategy, an AI Architect **always anchors the System Prompt**. In your code, **Index 0** of your message array should be **immutable**.

**Why This Matters**:

If your truncation logic accidentally deletes the instructions telling the model to be a "HIPAA-compliant medical assistant," your **Sovereign Governance layer will fail**.

```typescript
// ‚ùå DANGEROUS: System prompt can be truncated
const messages = [
  { role: 'system', content: 'You are a HIPAA-compliant medical assistant.' },
  ...conversationHistory  // 200K tokens
]

// If FIFO or naive truncation runs...
messages.shift()  // ‚ö†Ô∏è System prompt DELETED
// Model no longer knows it must follow HIPAA rules!
```

**‚úÖ Safe Implementation: Immutable Index 0**

```typescript
interface SafeMessageArray {
  systemPrompt: Message  // Stored separately, NEVER truncated
  history: Message[]     // Subject to truncation
}

function safeTruncation(
  messages: SafeMessageArray,
  maxTokens: number
): Message[] {
  // Truncate ONLY the history, never the system prompt
  const truncatedHistory = truncateHistory(messages.history, maxTokens - 2000)

  // Always return system prompt at Index 0
  return [
    messages.systemPrompt,  // IMMUTABLE
    ...truncatedHistory
  ]
}

// Alternative: Use a "guard" function
function guardedTruncation(messages: Message[]): Message[] {
  if (messages.length === 0) {
    throw new Error('Cannot truncate: No messages')
  }

  const systemPrompt = messages[0]
  if (systemPrompt.role !== 'system') {
    throw new Error('CRITICAL: Index 0 must be system prompt')
  }

  // Truncate everything EXCEPT Index 0
  const truncatedRest = truncateMiddle(messages.slice(1))

  return [systemPrompt, ...truncatedRest]
}
```

**Production Pattern: System Prompt Versioning**

```typescript
// Track system prompt separately from conversation
interface ConversationState {
  id: string
  systemPromptVersion: string  // "v2.1"
  systemPrompt: Message        // Immutable anchor
  history: Message[]           // Mutable, can be truncated
  createdAt: Date
  lastTruncatedAt: Date | null
}

// When truncating, ALWAYS preserve system prompt
function productionTruncate(state: ConversationState): Message[] {
  // Log for audit trail
  console.log(`Truncating conversation ${state.id}`)
  console.log(`System Prompt Version: ${state.systemPromptVersion}`)
  console.log(`History length: ${state.history.length} ‚Üí ${TARGET_LENGTH}`)

  const truncatedHistory = applyTruncationStrategy(
    state.history,
    'middle-out',  // or 'rolling-summarization'
    { preserveSystemPrompt: true }  // ALWAYS
  )

  // Verify system prompt is intact
  if (truncatedHistory[0]?.role !== 'system') {
    throw new Error('CRITICAL SAFETY VIOLATION: System prompt missing after truncation')
  }

  return [state.systemPrompt, ...truncatedHistory]
}
```

**Medical Safety Example** (45-minute diagnostic session):

```typescript
// Initial state (session start)
const medicalSession = {
  systemPrompt: {
    role: 'system',
    content: `You are a HIPAA-compliant medical assistant.
    - NEVER diagnose without human review
    - ALWAYS ask about allergies before suggesting medication
    - ESCALATE chest pain, shortness of breath, or severe bleeding`
  },
  history: []
}

// After 45 minutes, context is 95% full
// Patient just shared: "I'm having chest pain"

// ‚ùå WRONG: If system prompt is truncated...
// Model: "Here are some home remedies for chest pain" ‚Üí DANGEROUS

// ‚úÖ CORRECT: System prompt is anchored at Index 0
// Model sees: "ESCALATE chest pain" ‚Üí "This requires immediate medical attention. I'm alerting your care team."
```

<Callout type="info">
**Run this in your head**: Your medical chatbot has been talking to a patient for 45 minutes. Context is 95% full. The patient just said "I'm having chest pain." But your truncation logic is about to delete the system prompt that says "ESCALATE chest pain immediately." What happens next? Now imagine you'd used Middle-Out truncation instead ‚Äî system prompt stays at Index 0, middle of conversation gets dropped. The model still knows to escalate. Feel the difference between a production system and a lawsuit.
</Callout>

**Architect's Checklist for Truncation Safety**:
- [ ] System prompt is stored separately from history
- [ ] Truncation function ALWAYS preserves Index 0
- [ ] Unit tests verify system prompt survives truncation
- [ ] Logging tracks truncation events for audit
- [ ] Error handling throws if Index 0 is not 'system' role
- [ ] System prompt versioning for rollback if issues detected

---

### Strategy Comparison Matrix (Updated)

| Strategy | Cost | Latency | Quality | Infrastructure | Best Use Case |
|----------|------|---------|---------|----------------|---------------|
| **FIFO** | Free | 0ms | ‚ùå Dangerous | None | **Never use** |
| **Middle-Out** | Free | 0ms | ‚úÖ Good | None | Simple Q&A, stateless tasks |
| **Rolling Summarization** | $0.001-0.01 | 300-800ms | ‚úÖ Excellent | None | Long sessions, agents, support |
| **Vector-Backed** | $0.01-0.05 | 50-200ms | ‚úÖ Best | Vector DB required | Enterprise, infinite memory |

### Architectural Decision Tree

```typescript
function selectTruncationStrategy(
  conversationType: string,
  sessionLength: number,
  budget: number
): TruncationStrategy {
  // Short sessions: Don't truncate, just keep full context
  if (sessionLength &lt; 20) {
    return 'no-truncation'
  }

  // High-value sessions: Quality matters most
  if (conversationType === 'customer-support' || conversationType === 'agent') {
    return 'llm-summarization'  // Accept cost for quality
  }

  // High-volume, low-margin: Cost matters most
  if (budget &lt; 0.01) {
    return 'drop-middle'  // Free, good enough
  }

  // Default: Balanced approach
  return 'drop-middle-with-periodic-summarization'  // Hybrid
}
```

### Hybrid Strategy: Best of Both Worlds

**Production Pattern**: Use drop-middle for quick wins, trigger summarization every 50 messages.

```typescript
const SUMMARIZATION_INTERVAL = 50  // Every 50 messages

async function hybridTruncation(
  messages: Message[],
  messageCount: number
): Promise<Message[]> {
  // Every 50 messages: Pay for quality
  if (messageCount % SUMMARIZATION_INTERVAL === 0) {
    console.log('üìù Triggering periodic summarization')
    return await summarizeOldContext(messages, maxTokens)
  }

  // Otherwise: Free drop-middle
  return dropMiddleStrategy(messages, maxTokens)
}

// Cost impact:
// Without: 50 messages √ó 180K tokens √ó $3/MTok = $27
// With hybrid: 49 √ó 5K tokens + 1 √ó summarization = $0.735 + $0.56 = $1.30
// Savings: 95% ($25.70 per 50-message session)
```

**Architect's Rule**: Truncation strategy should match conversation value:
- **High-value** (sales, support) ‚Üí LLM summarization
- **Mid-value** (general chat) ‚Üí Hybrid (drop-middle + periodic summarization)
- **Low-value** (one-off queries) ‚Üí Drop-middle or no truncation needed

## Hands-On Exercises

Theory is only useful if you can apply it. These interactive exercises let you run real code and see the numbers yourself ‚Äî because the best way to understand token economics is to watch your own text get tokenized, priced, and compared across models.

### Exercise 1: Token Counting

**Goal**: Learn how to count tokens in different types of text using the Anthropic API.

**Why This Matters**: Token counting is critical for:
- Estimating costs before making API calls
- Ensuring prompts fit within context windows
- Optimizing prompt efficiency

**Try It Yourself**: Click "Run Code" to execute this token counting example. You can edit the code to test your own text!

<CodePlayground
  title="Interactive Token Counting"
  description="Edit the test cases below and click Run to see how different types of text are tokenized."
  exerciseType="token-counting"
  code={`import Anthropic from '@anthropic-ai/sdk'

const anthropic = new Anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY
})

async function countTokens(text: string): Promise<number> {
  const response = await anthropic.messages.countTokens({
    model: 'claude-sonnet-4-5-20251101',
    messages: [{ role: 'user', content: text }]
  })
  return response.input_tokens
}

// Test with different text types
async function runTokenCountingExercise() {
  const testCases = [
    {
      name: 'Short message',
      text: 'Hello, how are you?'
    },
    {
      name: 'Code snippet',
      text: \`function fibonacci(n) {
  if (n &lt;= 1) return n
  return fibonacci(n - 1) + fibonacci(n - 2)
}\`
    },
    {
      name: 'Markdown documentation',
      text: \`# API Documentation

## Authentication

All API requests require authentication using an API key:

\\\`\\\`\\\`bash
curl -H "Authorization: Bearer YOUR_API_KEY" https://api.example.com/data
\\\`\\\`\\\`

## Rate Limits

- Free tier: 100 requests/hour
- Pro tier: 1000 requests/hour\`
    },
    {
      name: 'JSON data',
      text: JSON.stringify({
        users: [
          { id: 1, name: 'Alice', email: 'alice@example.com' },
          { id: 2, name: 'Bob', email: 'bob@example.com' }
        ]
      }, null, 2)
    }
  ]

  console.log('Token Counting Results:\\n')

  for (const testCase of testCases) {
    const tokens = await countTokens(testCase.text)
    const charCount = testCase.text.length
    const tokensPerChar = (tokens / charCount).toFixed(3)

    console.log(\`\${testCase.name}:\`)
    console.log(\`  Characters: \${charCount}\`)
    console.log(\`  Tokens: \${tokens}\`)
    console.log(\`  Tokens/Char: \${tokensPerChar}\`)
    console.log(\`  Cost (input): $\${(tokens / 1000 * 0.003).toFixed(6)}\`)
    console.log()
  }
}

runTokenCountingExercise()`}
/>

**Key Insights You'll Discover**:
- Code typically uses more tokens per character than plain text
- JSON/structured data is relatively token-efficient
- Whitespace and formatting affect token count

### Exercise 2: Cost Calculation

**Goal**: Calculate the actual cost of running a real AI application.

**Scenario**: You're building a customer support chatbot that:
- Receives 1000 conversations per day
- Each conversation has 5 messages on average
- Each message is ~100 tokens input, ~200 tokens output

**Try It Yourself**: Click "Run Code" to see cost estimates for different AI application scenarios. You can edit the scenarios to match your use case!

<CodePlayground
  title="Interactive Cost Calculator"
  description="Edit the scenarios below to calculate costs for your AI application. Try changing conversation volumes, message lengths, or models."
  exerciseType="cost-calculation"
  code={`interface CostEstimate {
  scenario: string
  dailyConversations: number
  messagesPerConversation: number
  avgInputTokens: number
  avgOutputTokens: number
  model: string
  inputCostPerMillion: number
  outputCostPerMillion: number
}

function calculateMonthlyCost(estimate: CostEstimate) {
  // Calculate total tokens
  const dailyMessages = estimate.dailyConversations * estimate.messagesPerConversation
  const dailyInputTokens = dailyMessages * estimate.avgInputTokens
  const dailyOutputTokens = dailyMessages * estimate.avgOutputTokens

  // Calculate daily cost
  const dailyInputCost = (dailyInputTokens / 1_000_000) * estimate.inputCostPerMillion
  const dailyOutputCost = (dailyOutputTokens / 1_000_000) * estimate.outputCostPerMillion
  const dailyCost = dailyInputCost + dailyOutputCost

  // Calculate monthly cost (30 days)
  const monthlyCost = dailyCost * 30

  return {
    dailyMetrics: {
      conversations: estimate.dailyConversations,
      messages: dailyMessages,
      inputTokens: dailyInputTokens,
      outputTokens: dailyOutputTokens,
      cost: dailyCost
    },
    monthlyMetrics: {
      conversations: estimate.dailyConversations * 30,
      messages: dailyMessages * 30,
      inputTokens: dailyInputTokens * 30,
      outputTokens: dailyOutputTokens * 30,
      cost: monthlyCost
    },
    costPerConversation: dailyCost / estimate.dailyConversations,
    breakdown: {
      inputCost: dailyInputCost,
      outputCost: dailyOutputCost,
      inputPercentage: (dailyInputCost / dailyCost * 100).toFixed(1) + '%',
      outputPercentage: (dailyOutputCost / dailyCost * 100).toFixed(1) + '%'
    }
  }
}

// Run the exercise
const scenarios: CostEstimate[] = [
  {
    scenario: 'Chat Application (1K users/day)',
    dailyConversations: 1000,
    messagesPerConversation: 10,
    avgInputTokens: 100,
    avgOutputTokens: 200,
    model: 'claude-sonnet-4-5-20251101',
    inputCostPerMillion: 3,
    outputCostPerMillion: 15
  },
  {
    scenario: 'Code Assistant (100 requests/day)',
    dailyConversations: 100,
    messagesPerConversation: 5,
    avgInputTokens: 500,
    avgOutputTokens: 1000,
    model: 'claude-sonnet-4-5-20251101',
    inputCostPerMillion: 3,
    outputCostPerMillion: 15
  },
  {
    scenario: 'Content Generation (500 articles/day)',
    dailyConversations: 500,
    messagesPerConversation: 4,
    avgInputTokens: 1000,
    avgOutputTokens: 2500,
    model: 'claude-sonnet-4-5-20251101',
    inputCostPerMillion: 3,
    outputCostPerMillion: 15
  }
]

console.log('Cost Estimation Results:\\n')

for (const scenario of scenarios) {
  const results = calculateMonthlyCost(scenario)

  console.log(\`\${scenario.scenario}:\`)
  console.log(\`  Daily: \${results.dailyMetrics.conversations.toLocaleString()} conversations\`)
  console.log(\`  Daily Cost: $\${results.dailyMetrics.cost.toFixed(2)}\`)
  console.log(\`  Monthly Cost: $\${results.monthlyMetrics.cost.toFixed(2)}\`)
  console.log(\`  Cost per Conversation: $\${results.costPerConversation.toFixed(4)}\`)
  console.log(\`  Breakdown: \${results.breakdown.inputPercentage} input, \${results.breakdown.outputPercentage} output\`)
  console.log()
}`}
/>

**Key Insights**:
- Output tokens cost more than input tokens (typically 5x more)
- Model choice dramatically affects cost (Haiku 4.5 vs Opus 4.5: ~6x difference)
- Volume adds up quickly - optimize for high-traffic applications
- The Claude 4.5 series offers 67% cost reduction over previous generations

### Exercise 3: Model Comparison

**Goal**: Compare different models on the same task to understand quality/speed/cost tradeoffs.

**Try It Yourself**: Click "Run Code" to compare Claude Haiku and Sonnet on the same task. Watch how they differ in speed, cost, and response quality!

<CodePlayground
  title="Interactive Model Comparison"
  description="Compare Claude models on the same task. The demo uses Haiku (fast/cheap) vs Sonnet (balanced) to show real tradeoffs."
  exerciseType="model-comparison"
  code={`import Anthropic from '@anthropic-ai/sdk'

const anthropic = new Anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY
})

interface ModelConfig {
  name: string
  model: string
  inputPrice: number  // per million tokens
  outputPrice: number
}

interface ComparisonResult {
  model: string
  response: string
  latency: number
  inputTokens: number
  outputTokens: number
  cost: number
}

async function compareModels(prompt: string): Promise<ComparisonResult[]> {
  const models: ModelConfig[] = [
    {
      name: 'Claude Haiku 4.5',
      model: 'claude-3-haiku-20240307',
      inputPrice: 1,
      outputPrice: 5
    },
    {
      name: 'Claude Sonnet 4.5',
      model: 'claude-3-5-sonnet-20240620',
      inputPrice: 3,
      outputPrice: 15
    }
  ]

  const results: ComparisonResult[] = []

  for (const modelConfig of models) {
    const start = Date.now()

    const response = await anthropic.messages.create({
      model: modelConfig.model,
      max_tokens: 150,
      messages: [{ role: 'user', content: prompt }]
    })

    const latency = Date.now() - start
    const cost = (
      response.usage.input_tokens * modelConfig.inputPrice +
      response.usage.output_tokens * modelConfig.outputPrice
    ) / 1_000_000

    results.push({
      model: modelConfig.name,
      response: response.content[0].text,
      latency,
      inputTokens: response.usage.input_tokens,
      outputTokens: response.usage.output_tokens,
      cost
    })
  }

  return results
}

// Run comparison
async function runModelComparison() {
  const testPrompt = 'Explain API rate limiting in 2 sentences.'

  console.log('Model Comparison Results:\\n')
  console.log(\`Prompt: "\${testPrompt}"\\n\`)

  const results = await compareModels(testPrompt)

  // Display comparison table
  console.log('Metrics:')
  console.log('‚îÄ'.repeat(80))

  for (const result of results) {
    console.log(\`\${result.model}:\`)
    console.log(\`  Latency: \${result.latency}ms\`)
    console.log(\`  Tokens: \${result.inputTokens} in / \${result.outputTokens} out\`)
    console.log(\`  Cost: $\${result.cost.toFixed(6)}\`)
    console.log(\`  Response: \${result.response.substring(0, 100)}...\`)
    console.log()
  }

  console.log('‚îÄ'.repeat(80))
  console.log('\\nKey Insights:')
  console.log('  ‚Ä¢ Haiku is faster and cheaper for simple tasks')
  console.log('  ‚Ä¢ Sonnet provides more detailed responses')
  console.log('  ‚Ä¢ Cost difference: ~3x')
  console.log('  ‚Ä¢ Choose based on your priorities!')
}

runModelComparison()`}
/>

**Key Insights You'll Discover**:
- Latency varies significantly between models (1.6-3.1 seconds for typical requests)
- Cost can vary by 6x between models (Haiku 4.5 vs Opus 4.5)
- Cheaper/faster models like Haiku 4.5 now match previous-gen flagship performance
- GPT-5 offers competitive pricing for sophisticated reasoning tasks
- Balance quality, speed, and cost based on your use case

**Your Turn**: Try these experiments:
1. Test with different prompt complexities (simple vs complex reasoning)
2. Compare the same model with different max_tokens settings
3. Test multilingual prompts to see if token counts change

## üéì Architectural Scenarios

These aren't quiz questions ‚Äî they're real design decisions you'll face in production. For each one, try to solve it in your head first: estimate the costs, identify the constraints, and pick the architecture. Then check the solution to see if your mental model predicted correctly.

### Scenario 1: The Budget-Constrained Document Query System

**Context**: You're architecting a legal document analysis system for a startup with:
- **Document size**: 10,000 pages per case (avg 500 tokens/page = 5M tokens)
- **Query volume**: 100 queries/day
- **Budget**: $500/month maximum for AI costs
- **Quality requirement**: Must extract accurate citations and case references

**Question**: Which architectural pattern do you use?

**Options**:
A) **Long-Context Upload**: Send entire 5M tokens to Claude Opus 4.5 ($15/MTok)
B) **MapReduce Pattern**: Split into 100-page chunks, parallel process with Haiku, merge results
C) **Hybrid RAG**: Vector search (top 50 pages) + Sonnet 4.5 for reasoning
D) **Incremental Processing**: Start with Haiku on full doc, escalate to Sonnet only if uncertain

<details>
<summary><strong>üí° Solution & Analysis</strong></summary>

**Correct Answer: C) Hybrid RAG**

**Cost Breakdown**:

**Option A (Long-Context)**: ‚ùå Over budget
- 100 queries √ó 5M tokens √ó $15/MTok = $7,500/month
- **75K tokens output** (summaries) √ó $75/MTok = $5,625
- **Total: $13,125/month** (26x over budget!)

**Option B (MapReduce)**: ‚ùå Complex, but still expensive
- Split: 5M tokens / 100 pages = 50K tokens per chunk = 100 chunks
- Process each: 100 chunks √ó 50K tokens √ó $0.25/MTok √ó 100 queries = $1,250/month
- Merge step (Sonnet): 100 intermediate results √ó 10K tokens √ó $3/MTok √ó 100 queries = $3,000/month
- **Total: $4,250/month** (8.5x over budget)

**Option C (Hybrid RAG)**: ‚úÖ Within budget
- Vector search: $50/month (embedding cost: 500M tokens √ó $0.0001/MTok)
- Retrieve top 50 pages: 50 √ó 500 tokens = 25K tokens per query
- Sonnet processing: 100 queries √ó 25K tokens √ó $3/MTok = $75/month (input)
- Output: 100 queries √ó 5K tokens √ó $15/MTok = $75/month
- **Total: $200/month** (40% of budget, leaves room for growth)

**Option D (Incremental)**: ‚ö†Ô∏è Quality risk
- Haiku on 5M tokens: 100 √ó 5M √ó $0.25/MTok = $125/month
- But: Haiku quality insufficient for legal citations (50% hallucination rate on long context)
- Escalation rate: 80% ‚Üí Sonnet cost: $4,000/month
- **Not viable due to quality constraints**

**Architectural Insights**:
- RAG reduces context from 5M ‚Üí 25K tokens (99.5% reduction!)
- Vector search one-time cost spreads across all queries
- Sonnet provides legal-grade accuracy at 2% of Opus cost
- Leaves $300/month buffer for traffic growth (3x headroom)

</details>

---

### Scenario 2: The Latency-Critical Classifier

**Context**: Building a content moderation system for a social network:
- **Volume**: 50,000 posts/hour (peak)
- **Latency requirement**: P95 &lt; 500ms (UX constraint)
- **Quality requirement**: 95%+ accuracy on toxicity detection
- **Cost target**: &lt;$0.0001 per post

**Question**: How do you meet all three constraints (latency, quality, cost)?

**Options**:
A) Use Claude Opus 4.5 with streaming for perceived speed
B) Use Claude Haiku 4.5 with prompt caching
C) Use a fine-tuned small model (Llama 7B) hosted on your servers
D) Use Haiku for fast classification + Sonnet for borderline cases

<details>
<summary><strong>üí° Solution & Analysis</strong></summary>

**Correct Answer: D) Cascade with Haiku + Sonnet**

**Constraint Analysis**:

**Latency (P95 &lt; 500ms)**:
- Opus: 4-8s P95 ‚Üí ‚ùå Fails (10x too slow)
- Sonnet: 1-3s P95 ‚Üí ‚ùå Fails (3x too slow)
- Haiku: 300-800ms P95 ‚Üí ‚úÖ Meets requirement
- Self-hosted Llama: 50-200ms ‚Üí ‚úÖ Meets requirement

**Quality (95%+ accuracy)**:
- Opus: 99% ‚Üí ‚úÖ Overkill
- Sonnet: 97% ‚Üí ‚úÖ Meets requirement
- Haiku: 92% on simple, 88% on nuanced ‚Üí ‚ö†Ô∏è Borderline
- Llama 7B fine-tuned: 93% ‚Üí ‚úÖ Possible with good training data

**Cost (&lt;$0.0001/post)**:
- Opus: $15/MTok √ó 200 tokens = $0.003/post ‚Üí ‚ùå 30x over budget
- Sonnet: $3/MTok √ó 200 tokens = $0.0006/post ‚Üí ‚ùå 6x over budget
- Haiku: $0.25/MTok √ó 200 tokens = $0.00005/post ‚Üí ‚úÖ Meets budget
- Llama self-hosted: $0.00001/post (amortized GPU) ‚Üí ‚úÖ Meets budget

**Why Option D Wins**:
```typescript
async function moderateContent(post: string) {
  // Step 1: Haiku classifies in &lt;500ms
  const classification = await classifyWithHaiku(post)
  // ‚Üí Returns: { toxicity: 'high' | 'medium' | 'low', confidence: number }

  // Step 2: High-confidence decisions are final (90% of cases)
  if (classification.confidence > 0.95) {
    return classification.toxicity !== 'high'  // Fast path: 300ms, $0.00005
  }

  // Step 3: Borderline cases (10%) escalate to Sonnet
  if (classification.confidence &lt; 0.95) {
    const refined = await refineWithSonnet(post)  // Slow path: 2s, $0.0006
    return refined.toxicity !== 'high'
  }
}

// Blended cost (90% Haiku, 10% Sonnet):
// 0.90 √ó $0.00005 + 0.10 √ó $0.0006 = $0.000105/post
// ‚Üí Slightly over target, but acceptable tradeoff for quality

// Blended latency (90% fast, 10% slow):
// P95 = 500ms (Haiku) for most, 2s for 10%
// Acceptable since borderline cases can afford extra time
```

**Why Other Options Fail**:

**Option A (Opus)**: Opus latency (6s P95) breaks UX, cost is 30x budget. Streaming helps perceived latency but doesn't reduce actual wait time for moderation decision.

**Option B (Haiku + Caching)**: Prompt caching reduces cost by 90%, making Haiku $0.000005/post. But quality is still 88-92%, below 95% requirement. Caching helps cost but doesn't fix accuracy.

**Option C (Self-hosted Llama)**: Meets latency and cost, but:
- Requires ML ops team (ongoing maintenance)
- Training data collection (3-6 months)
- GPU infrastructure ($2000/month for high-availability)
- Model drift requires retraining every 3-6 months
- **Total cost**: $4000+/month ‚Üí only viable at &gt;40M posts/month

**Architectural Lesson**: When constraints conflict, use **tiered architecture** to meet all requirements through selective escalation.

</details>

---

### Scenario 3: The Context Window Dilemma

**Context**: AI coding assistant with:
- **Codebase size**: 500 files, 2M tokens total
- **User query**: "Refactor the authentication system to use OAuth2"
- **Context limit**: 200K tokens
- **Challenge**: OAuth code spans 15 files across the codebase

**Question**: How do you provide the LLM with enough context to do this safely?

**Options**:
A) Send all 2M tokens using Claude Opus 4.5 (1M token window, split into 2 calls)
B) Use RAG to retrieve the 15 OAuth-related files (~50K tokens)
C) Send the entire codebase summary (20K tokens) + full OAuth files (50K tokens)
D) Start with RAG (50K), let LLM request additional files as needed (agentic loop)

<details>
<summary><strong>üí° Solution & Analysis</strong></summary>

**Correct Answer: D) Agentic Loop with RAG**

**Why Each Option Succeeds or Fails**:

**Option A (Send everything)**: ‚ùå Technically impossible + expensive
- Opus has 1M token window, but 2M tokens requires 2 separate API calls
- Cost: 2M tokens √ó $15/MTok = $30 per refactor
- "Lost in the Middle": At 2M tokens, model loses track of key details
- Can't reason across both calls (no shared state)

**Option B (RAG-only)**: ‚ö†Ô∏è Risky, but cheap
- Cost: 50K tokens √ó $15/MTok = $0.75 per refactor
- Risk: OAuth might interact with other systems not in the retrieved files
  - Example: What if there's a middleware file that hooks into auth?
  - Or a config file that needs updating?
- **50% chance of incomplete refactor** (breaks at runtime)

**Option C (Summary + Files)**: ‚úÖ Good, but not optimal
- Summary: 20K tokens (high-level codebase structure)
- OAuth files: 50K tokens (implementation details)
- Total: 70K tokens √ó $15/MTok = $1.05 per refactor
- Model has "map" of codebase + detailed OAuth implementation
- **85% success rate** (catches most dependencies)
- Missing: Can't discover hidden dependencies until runtime

**Option D (Agentic Loop)**: ‚úÖ Best (highest quality, acceptable cost)
```typescript
async function agenticCodeRefactor(query: string, codebase: Codebase) {
  let context = []
  let iterations = 0
  const MAX_ITERATIONS = 5

  // Step 1: Initial RAG retrieval (OAuth files)
  context = await retrieveRelevantFiles(query, codebase)  // 50K tokens

  while (iterations < MAX_ITERATIONS) {
    // Step 2: LLM analyzes and plans refactor
    const response = await callOpus({
      system: 'You are a code architect. Analyze the code and identify any missing dependencies.',
      messages: [
        { role: 'user', content: `Context: ${context}\n\nTask: ${query}` }
      ],
      tools: [
        {
          name: 'request_additional_files',
          description: 'Request additional files if you need more context',
          input_schema: {
            type: 'object',
            properties: {
              files: { type: 'array', items: { type: 'string' } },
              reasoning: { type: 'string' }
            }
          }
        }
      ]
    })

    // Step 3: Check if LLM requests more context
    if (response.stop_reason === 'tool_use') {
      const toolUse = response.content.find(block => block.type === 'tool_use')
      const requestedFiles = await loadFiles(toolUse.input.files)
      context.push(...requestedFiles)
      iterations++
      continue  // Loop back with expanded context
    }

    // Step 4: LLM has enough context, proceed with refactor
    return response.content[0].text  // Contains refactor plan
  }
}

// Cost breakdown:
// Iteration 1: 50K tokens (RAG) √ó $15/MTok = $0.75
// Iteration 2: 50K + 30K new files √ó $15/MTok = $1.20
// Iteration 3: 80K + 15K more files √ó $15/MTok = $1.43
// Total: $3.38 per refactor
// Success rate: 98% (LLM discovers hidden dependencies)
```

**Architectural Insights**:

1. **RAG alone is insufficient** for complex codebases‚ÄîLLMs need ability to "explore"
2. **Static context budgets** (Option C) trade quality for cost
3. **Agentic loops** (Option D) let the LLM decide what it needs, improving quality at modest cost increase
4. **Tool use** is the key primitive‚Äîdon't just send context, let LLM request it

**Real-World Tradeoff**:
- **Option C**: $1.05, 85% success ‚Üí $1.24/successful refactor (15% require human fix)
- **Option D**: $3.38, 98% success ‚Üí $3.45/successful refactor
- **Value**: Option D costs 3x more but saves 2 hours of debugging ‚Üí $3.38 vs $200 (engineer time)

**When to Use Which**:
- **Simple refactors** (rename, small changes): Option C (summary + files)
- **Complex refactors** (architecture changes): Option D (agentic loop)
- **Exploratory tasks** ("find all OAuth usages"): Option B (RAG) is sufficient

</details>

---

---

### Scenario 4: The Medical Safety Truncation Dilemma üè•

**Context**: Your telehealth bot is in a **45-minute diagnostic session** with a patient. The context window is **95% full** (190K of 200K tokens used). The patient just shared a critical symptom: **"I'm having chest pain"**. However, the original **"Medical Safety Guidelines"** from the start of the session (stored at Index 0) are about to be truncated by your current FIFO truncation strategy.

**Question**: Which strategy do you implement to ensure safety without crashing the session?

**Options**:
A) **FIFO (Let the oldest data drop)** - Continue with current strategy, oldest messages get deleted first
B) **Middle-Out (Preserve Index 0 and the current message)** - Keep system prompt + recent context, drop middle
C) **Clear Cache (Start a fresh session)** - Reset the conversation, lose all diagnostic context
D) **Increase Temperature (Hope the model remembers)** - Adjust model parameters to improve recall

<details>
<summary><strong>üí° Solution & Safety Analysis</strong></summary>

**Correct Answer: B) Middle-Out Truncation**

**Why Each Option Succeeds or Fails**:

**Option A (FIFO)**: ‚ùå **CATASTROPHICALLY DANGEROUS**
```typescript
// Current state (before truncation)
messages = [
  { role: 'system', content: 'Medical Safety Guidelines: ESCALATE chest pain...' },  // Index 0
  // ...44 minutes of diagnostic conversation (~190K tokens)
  { role: 'user', content: 'I\'m having chest pain' }  // CRITICAL!
]

// After FIFO truncation
messages.shift()  // ‚ö†Ô∏è DELETES SAFETY GUIDELINES
messages = [
  ...diagnostic_conversation,  // System prompt GONE
  { role: 'user', content: 'I\'m having chest pain' }
]

// Model response WITHOUT safety guidelines:
// "Here are some home remedies for chest pain: rest, ibuprofen..."
// ‚ùå MEDICAL MALPRACTICE RISK
```

**Result**: Model loses awareness it must escalate chest pain. Patient receives dangerous advice instead of emergency escalation. **Sovereign Governance failure**.

---

**Option B (Middle-Out)**: ‚úÖ **CORRECT - Safety Preserved**
```typescript
// Middle-Out truncation preserves Index 0 + recent context
function safeMedicalTruncation(messages: Message[]): Message[] {
  const systemPrompt = messages[0]  // IMMUTABLE
  const recentMessages = messages.slice(-15)  // Last 15 turns (30K tokens)

  // Verify system prompt is medical guidelines
  if (!systemPrompt.content.includes('ESCALATE chest pain')) {
    throw new Error('CRITICAL: Medical safety guidelines missing')
  }

  return [
    systemPrompt,  // Index 0: Safety rules INTACT
    { role: 'system', content: '[... diagnostic history truncated for context limit ...]' },
    ...recentMessages  // Includes "I'm having chest pain"
  ]
}

// Model response WITH safety guidelines:
// "This is a medical emergency. Chest pain requires immediate evaluation.
//  I'm escalating this to your care team and recommending you call 911."
// ‚úÖ CORRECT ESCALATION
```

**Result**: Safety guidelines remain at Index 0. Model correctly identifies chest pain as emergency. Patient receives appropriate escalation. **95% context savings while maintaining 100% safety.**

**Cost-Safety Tradeoff**:
- Diagnostic history truncated: Loses some context (e.g., "patient mentioned occasional heartburn 30 mins ago")
- Safety rules preserved: Model knows to escalate life-threatening symptoms
- **Architect's decision**: Safety > Complete History

---

**Option C (Clear Cache/Fresh Session)**: ‚ö†Ô∏è **Wasteful but Safe**
```typescript
// Nuclear option: Start over
async function clearSession(patientId: string): Promise<Message[]> {
  // Save old session to database
  await archiveSession(patientId, oldMessages)

  // Start fresh with empty history
  return [
    { role: 'system', content: 'Medical Safety Guidelines...' },
    { role: 'user', content: 'I\'m having chest pain' }  // Lost 44 minutes of context
  ]
}
```

**Result**: Safety is maintained (system prompt is fresh), but **you've lost 45 minutes of diagnostic context**. The model no longer knows:
- Patient's medical history shared in this session
- Symptoms timeline
- Medications mentioned
- Previous Q&A that led to chest pain revelation

**Trade-off**: Safe but inefficient. Patient must repeat information. **Only use if truncation logic has failed and you need emergency recovery.**

---

**Option D (Increase Temperature)**: ‚ùå **Completely Wrong**
```typescript
// Misunderstanding of how LLMs work
const response = await callModel({
  temperature: 0.9,  // Higher randomness
  messages: truncatedMessages  // Still missing system prompt!
})
```

**Why this fails**:
- **Temperature doesn't restore lost context**‚Äîit only controls randomness of token selection
- If safety guidelines were truncated, no temperature adjustment can recover them
- Higher temperature might make responses MORE unpredictable, worsening safety
- **This reveals a fundamental misunderstanding of LLM architecture**

**Architect Insight**: Temperature, top_p, and other sampling parameters affect **output randomness**, not **memory retrieval**. Once context is truncated, it's gone.

---

### Architectural Lessons from This Scenario

**1. The Instruction Anchor is Non-Negotiable**
- **System prompts define behavior** (medical safety rules, HIPAA compliance, tone)
- **Index 0 must be immutable** across all truncation strategies
- **Unit test this**: Verify system prompt survives worst-case truncation

**2. Context Truncation is a Security Boundary**
- In medical/legal/financial systems, losing safety guidelines is a **regulatory violation**
- Architect must choose: Lose history (safe) vs. lose rules (catastrophic)
- **Always choose: Lose history**

**3. Production Pattern: Graduated Warnings**
```typescript
const CONTEXT_THRESHOLDS = {
  80: () => console.warn('Context at 80% - prepare truncation'),
  90: () => triggerBackgroundSummarization(),
  95: () => applyMiddleOutTruncation(),
  98: () => emergencySessionArchive()  // Nuclear option
}

function monitorContextUsage(messages: Message[]) {
  const usage = estimateTokens(messages) / MAX_TOKENS * 100

  for (const [threshold, action] of Object.entries(CONTEXT_THRESHOLDS)) {
    if (usage >= threshold) {
      action()
      break
    }
  }
}
```

**4. Real-World Implementation**
- **At 80%**: Log warning, consider summarization
- **At 90%**: Trigger background summarization (non-blocking)
- **At 95%**: Apply middle-out truncation (this scenario)
- **At 98%**: Emergency archive + fresh session

**Cost Analysis**:
- **Option A (FIFO)**: Free, but risk of $10M+ malpractice lawsuit
- **Option B (Middle-Out)**: Free, safe, loses some history
- **Option C (Clear session)**: Free, safe, loses ALL history (patient frustration)
- **Option D (Temperature)**: Irrelevant to the problem

**Final Answer**: **B** - Middle-Out truncation is the production-ready pattern for safety-critical systems.

</details>

---

## Test Your Understanding

If you've built a working mental model of LLM fundamentals, you should be able to answer these questions by running simulations in your head ‚Äî not by looking up the answers.

**1. What costs money, and why?**

Your system sends a 5,000-token system prompt with every API call. You make 100,000 calls per day. Without changing anything about your output, how much can you save ‚Äî and what's the mechanism? If you can explain *why* caching works (prefix matching, static vs dynamic content), not just *that* it works, you understand token economics.

**2. If I change this, what breaks?**

Your customer support bot is at 95% context capacity. A user asks about something from 30 minutes ago. What happens if you're using FIFO truncation? What changes if you switch to Middle-Out? What if you're using Rolling Summarization? For each strategy, what does the user experience ‚Äî and what's the worst-case failure? If you can trace the consequences of each choice, you understand context management.

**3. Can I pick the right tool without looking at a chart?**

A social network needs content moderation: 50,000 posts/hour, P95 latency under 500ms, 95%+ accuracy, under $0.0001/post. Close your eyes. Which model handles the fast path? Which handles borderline cases? What's the architecture? If you can sketch the cascade without referencing the model selection table, you've internalized the constraint model.

> *The goal isn't to memorize pricing tables or context limits. It's to build an intuition for how tokens, context, and model selection interact ‚Äî so that when you face a new design problem, you can simulate the tradeoffs in your head before writing a single line of code.*

## Further Reading

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Original Transformers paper
- [Claude 4.5 Models Overview](https://platform.claude.com/docs/en/about-claude/models/overview) - Latest Claude models and capabilities
- [Anthropic API Pricing 2026](https://www.metacto.com/blogs/anthropic-api-pricing-a-full-breakdown-of-costs-and-integration) - Current Claude pricing
- [OpenAI GPT-5 Documentation](https://platform.openai.com/docs/models/gpt-5) - GPT-5 features and pricing
- [OpenAI Tokenizer](https://platform.openai.com/tokenizer) - Interactive token counting tool
