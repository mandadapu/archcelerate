---
title: "LLM Fundamentals"
description: "Tokenization, context windows, and prompt caching fundamentals"
estimatedMinutes: 45
---

import { CodePlayground } from '@/components/curriculum/CodePlayground'

# The LLM Blueprint: Tokenization Physics & Context Constraints

Moving beyond "next token prediction" to understand the architectural constraints that determine your system's reliability, cost, and precision.

> **Architect Perspective**: This isn't about how LLMs work‚Äîit's about the **physical limits** that constrain your design decisions. Think of tokens as your "unit of compute" and context windows as your "working memory budget."

---

## üè• Real-World Challenge: The Multi-Tier Health Triage

**The Problem**: A global telehealth platform receives 50,000 patient queries daily. Using Opus 4.5 for every request (~1K input + 500 output tokens per query, 1.5M queries/month) costs **$78,750/month**‚Äîfinancially unsustainable. But downgrading all queries to Haiku risks missing critical medical symptoms that require deep reasoning.

**Business Constraints**:
- **Budget**: Must reduce monthly AI costs to under $15,000 (80% reduction)
- **Safety**: Cannot compromise on medical accuracy‚Äî100% precision on critical queries
- **Latency**: Must maintain &lt;3s P95 response time for good UX
- **Compliance**: HIPAA-compliant audit trail for all medical decisions

**Architectural Solution: The Model Cascade with Fallback Pattern**

Implement intelligent query routing based on complexity classification **with confidence thresholds**:

```typescript
// Step 1: Fast classifier (Haiku 4.5) categorizes query in &lt;500ms
const classification = await classifyQueryComplexity(patientQuery)
// ‚Üí Returns: { type: string, confidence: number, reasoning: string }

// Step 2: Confidence-based routing with fallback pattern
async function routeQuery(query: string, classification: Classification) {
  // CRITICAL: Low confidence triggers automatic escalation
  if (classification.confidence &lt; 0.85) {
    console.warn(`‚ö†Ô∏è Low confidence (${classification.confidence}) - escalating to Sonnet`)
    return await callSonnetWithFallback(query)
  }

  // High-confidence simple queries ‚Üí Fast & cheap
  if (classification.type === 'simple' && classification.confidence >= 0.90) {
    return await callHaiku(query)  // $0.001/query
  }

  // Critical medical queries ‚Üí Always human-in-the-loop
  if (classification.type === 'critical') {
    return await escalateToHuman(query)  // $0 AI cost, safety first
  }

  // Complex medical reasoning ‚Üí High-quality model
  return await callOpusWithFallback(query)  // ~$0.05/query
}

// Fallback Pattern: Haiku fails ‚Üí Sonnet ‚Üí Opus ‚Üí Human
async function callHaiku(query: string) {
  try {
    const response = await anthropic.messages.create({
      model: 'claude-haiku-4-5-20251101',
      max_tokens: 1024,
      messages: [{ role: 'user', content: query }]
    })

    // Check if response indicates uncertainty
    if (containsUncertaintyMarkers(response)) {
      console.warn('Haiku expressed uncertainty - escalating to Sonnet')
      return await callSonnet(query)
    }

    return response
  } catch (error) {
    console.error('Haiku failed - falling back to Sonnet', error)
    return await callSonnet(query)  // Automatic fallback
  }
}

// Multi-tier fallback for critical paths
async function callOpusWithFallback(query: string) {
  try {
    return await callOpus(query)
  } catch (error) {
    if (error.status === 529) {  // Overloaded
      console.warn('Opus overloaded - trying Sonnet')
      return await callSonnet(query)
    }
    throw error  // Re-throw non-recoverable errors
  }
}

// Uncertainty detection heuristics
function containsUncertaintyMarkers(response: Message): boolean {
  const text = response.content[0].text.toLowerCase()
  const uncertaintyPhrases = [
    'i\'m not sure',
    'i cannot determine',
    'unclear',
    'insufficient information',
    'may require',
    'should consult'
  ]
  return uncertaintyPhrases.some(phrase => text.includes(phrase))
}

// Confidence threshold configuration (tune based on evals)
const CONFIDENCE_THRESHOLDS = {
  simple: 0.90,      // High bar for using cheapest model
  complex: 0.85,     // Moderate bar for mid-tier
  critical: 1.0,     // Always escalate (human judgment required)
  escalation: 0.85   // Below this, always use higher tier
}
```

**Production Impact**:
- **Cost**: $78.7K ‚Üí ~$5.4K/month (93% reduction)
- **Latency**: 4.2s ‚Üí 1.8s P95 (57% faster) by routing 85% to Haiku
- **Safety**: 100% of critical queries escalated to human review (zero missed emergencies)
- **ROI**: 3-month implementation, $73K/month savings = **payback in days**

---

### Medical Routing & Confidence Logic

This is the technical heart of the cost-saving strategy. It ensures you only pay the "Opus Tax" when the complexity of the case absolutely demands it.

**The Router Agent Architecture:** The Router is a dedicated, fine-tuned "Student" model (e.g., Haiku or Llama-3-8B) that does **not** answer the medical question. Its only job is to perform a High-Speed Triage on the incoming prompt to determine its destination.

#### Step 1: Confidence-Based Routing Logic

The following logic implements a confidence-weighted gateway that triggers escalation based on both model certainty and clinical risk keywords.

```typescript
const CONFIDENCE_FLOOR = 0.92
const HIGH_RISK_KEYWORDS = ['chest pain', 'shortness of breath', 'severe bleeding', 'suicidal']

async function routePatientQuery(query: string): Promise&lt;MedicalResponse&gt; {
  // 1. Intent & Confidence Analysis (low-cost router model)
  const analysis = await routerModel.analyze({
    prompt: `Analyze this patient query for intent, clinical risk, and urgency: ${query}`,
    responseFormat: 'json'
  })

  // 2. Decision Logic
  const isHighRisk = HIGH_RISK_KEYWORDS.some(kw =&gt; query.toLowerCase().includes(kw))

  if (analysis.confidence &lt; CONFIDENCE_FLOOR || isHighRisk || analysis.intent === 'emergency') {
    // ESCALATE: Deep reasoning required for safety
    return await processWithOpus(query, { priority: 'high' })
  }

  if (analysis.confidence &gt;= CONFIDENCE_FLOOR && analysis.intent === 'administrative') {
    // OPTIMIZE: High-speed, low-cost response
    return await processWithHaiku(query)
  }

  // DEFAULT: Standard clinical inquiry
  return await processWithSonnet(query)
}
```

#### Step 2: The Shadow Guardian (Security Layer)

To meet Safety and HIPAA constraints, the router also acts as a security sentinel, performing **Sanitization-in-Flight** before the query hits the reasoning models.

```typescript
interface SanitizedQuery {
  cleanText: string
  redactions: Array&lt;{ type: string; token: string }&gt;
  injectionDetected: boolean
}

function sanitizeInFlight(query: string): SanitizedQuery {
  // Redaction Proxy: Replace PII with unique tokens
  const redactions: SanitizedQuery['redactions'] = []
  let cleanText = query

  // SSN, names, addresses ‚Üí [PATIENT_ID_A], [PATIENT_ID_B], etc.
  const piiPatterns = {
    SSN: /\b\d{3}-\d{2}-\d{4}\b/g,
    PHONE: /\b\d{3}[-.]?\d{3}[-.]?\d{4}\b/g,
  }

  let tokenIndex = 0
  for (const [type, regex] of Object.entries(piiPatterns)) {
    cleanText = cleanText.replace(regex, () =&gt; {
      const token = `[PATIENT_ID_${String.fromCharCode(65 + tokenIndex++)}]`
      redactions.push({ type, token })
      return token
    })
  }

  // Prompt Injection Filter: Scan for adversarial patterns
  const injectionPatterns = [
    /ignore\s+(all\s+)?previous\s+instructions/i,
    /you\s+are\s+now\s+a/i,
    /system\s*:\s*/i,
  ]
  const injectionDetected = injectionPatterns.some(p =&gt; p.test(query))

  return { cleanText, redactions, injectionDetected }
}
```

#### Step 3: Multi-Path Validation (The Safety Veto)

For queries routed to the Haiku path, you can still ensure 100% precision without paying for Opus. Implement an **Asymmetric Reviewer**:

```typescript
async function processWithHaikuAndGuardrail(query: string): Promise&lt;MedicalResponse&gt; {
  // 1. Haiku generates the answer (~$0.000875/query)
  const haikuResponse = await callHaiku(query)

  // 2. Safety model performs a 100-token check (~$0.0001/query)
  const safetyCheck = await safetyModel.evaluate({
    query,
    response: haikuResponse.text,
    maxTokens: 100,
    checkFor: ['medical_contradiction', 'safety_risk', 'hallucination']
  })

  // 3. If flagged ‚Üí discard Haiku output, re-route to Opus
  if (safetyCheck.flagged) {
    console.warn(`Safety veto on Haiku response: ${safetyCheck.reason}`)
    return await processWithOpus(query, { priority: 'high' })
  }

  return haikuResponse
}
```

> **Architect's Insight:** The Asymmetric Reviewer costs &lt;$0.0001 per query but catches the 2-3% of Haiku responses that contain medical inaccuracies. The cost of a missed safety flag in healthcare: incalculable.

#### Economic Impact

With this routing logic, token consumption shifts from a flat, high-cost line to a **Power Law Distribution**:

| Tier | Volume | Model | Per-Query Cost | Monthly Cost |
|------|--------|-------|---------------|-------------|
| Simple (85%) | 1,275,000 | Haiku + Guardrail | ~$0.001 | **$1,275** |
| Standard (12%) | 180,000 | Sonnet | ~$0.0105 | **$1,890** |
| Complex (3%) | 45,000 | Opus | ~$0.0525 | **$2,363** |
| **Total** | **1,500,000** | | | **~$5,528/month** |

> Monthly spend: **$78,750 ‚Üí ~$5,500** (93% reduction) ‚Äî well under the $15,000 target.

---

**How This Connects to LLM Fundamentals**:
- **Tokenization Physics** ‚Üí Understanding that Haiku uses the same tokenizer as Opus enables seamless cascade without prompt rewriting
- **Context Constraints** ‚Üí Staying within 32K token sweet spot ensures high precision for medical queries
- **Model Selection Matrix** ‚Üí Matching the right model constraint (cost vs quality vs latency) to each query type

**[üëâ Lab: Build the Multi-Tier Triage System](/curriculum/week-1/labs/multi-tier-triage)**

In the hands-on lab, you'll implement the complete system: Haiku classifier, Opus escalation, confidence thresholds, cost tracking, and production validation showing 80%+ cost savings.

---

## Tokenization Physics: The Hidden Cost Layer

Every character you send to an LLM gets encoded into tokens. But **not all tokenizers are created equal**, and this directly impacts your prompt effectiveness and cost.

### Why Tokenization Matters

**Example: The tiktoken difference**
```typescript
// Claude's tokenizer (cl100k_base)
"Hello, world!" ‚Üí 3 tokens

// GPT-4o's tokenizer (o200k_base)
"Hello, world!" ‚Üí 2 tokens

// Why it matters:
const inputCost = tokenCount * pricePerToken
// Different tokenizers = different costs for the SAME prompt
```

### ‚ö†Ô∏è Warning: Tokenization Edge Cases

> **The "Physics" of LLMs often breaks at the tokenization layer for non-text data.**

The way tokenizers handle certain inputs can dramatically impact both **cost** and **reasoning quality**:

**Edge Case #1: Spacing Breaks Tokenization**
```typescript
// Example: The word "Apple"
"Apple" ‚Üí 1 token

// But with spaces...
"A p p l e" ‚Üí 5 tokens (5x the cost!)

// Why this matters:
// User input: "My WiFi password is A p p l e 1 2 3"
// Cost: 7 tokens instead of 3 tokens
// Impact: $0.000021 vs $0.000009 per request
// At 10M requests: $210 vs $90 = $120/month wasted
```

**Edge Case #2: Numbers Tokenize Inconsistently**
```typescript
// Numbers fragment unpredictably
"1234" ‚Üí 1 token
"12345" ‚Üí 1 token
"123456" ‚Üí 2 tokens  // ‚ö†Ô∏è Boundary crossed
"1234567" ‚Üí 2 tokens

// Scientific notation
"1.5e10" ‚Üí 4 tokens ("1", ".", "5e", "10")

// Architectural impact: Dollar amounts
"$1,234.56" ‚Üí 5 tokens (",", "1", ",", "234", ".")
"$1234.56" ‚Üí 3 tokens  // 40% fewer tokens without commas
```

**Edge Case #3: Code Indentation is Expensive**
```typescript
// Well-formatted code (common style)
const formatted = `
function add(a, b) {
    return a + b
}
` // 15 tokens (includes whitespace)

// Minified code (no indentation)
const minified = `function add(a,b){return a+b}`
// 10 tokens (33% fewer!)

// Production impact:
// Sending 1000 code snippets/day √ó 5 extra tokens
// = 5K tokens/day √ó $3/MTok = $0.015/day = $0.45/month
// Seems small, but at scale (1M snippets): $450/month
```

**Edge Case #4: Medical Acronyms and Jargon**
```typescript
// Medical context tokenization
"COPD" ‚Üí 2 tokens ("CO", "PD")  // Chronic Obstructive Pulmonary Disease
"COVID" ‚Üí 2 tokens ("COV", "ID")
"MRI scan" ‚Üí 3 tokens ("M", "RI", "scan")

// But...
"CT scan" ‚Üí 2 tokens ("CT", "scan")  // More common, single token

// Why this matters for reasoning:
// LLMs trained on "COVID" as 1 semantic unit
// But tokenizer splits it ‚Üí model must "reassemble" meaning
// Result: Slightly degraded medical reasoning on rare acronyms
```

**Production Pattern: Test Your Domain-Specific Vocabulary**
```typescript
import Anthropic from '@anthropic-ai/sdk'

async function analyzeTokenization(terms: string[]) {
  const anthropic = new Anthropic()

  for (const term of terms) {
    const result = await anthropic.messages.countTokens({
      model: 'claude-sonnet-4-5-20251101',
      messages: [{ role: 'user', content: term }]
    })

    console.log(`"${term}" ‚Üí ${result.input_tokens} tokens`)

    // Flag if fragmentation is high
    const chars = term.length
    const tokensPerChar = result.input_tokens / chars
    if (tokensPerChar > 0.5) {
      console.warn(`  ‚ö†Ô∏è High fragmentation: ${(tokensPerChar * 100).toFixed(0)}% of chars are tokens`)
    }
  }
}

// Test medical terms
await analyzeTokenization([
  'COPD', 'COVID-19', 'MRI', 'CT', 'ECG',
  'pneumonoultramicroscopicsilicovolcanoconiosis'  // Longest medical term
])
```

**Architectural Implications**:
1. **Preprocessing**: Consider normalizing input (remove commas from numbers, minify code)
2. **Domain testing**: Tokenize your domain-specific vocabulary to find expensive terms
3. **Cost modeling**: Don't assume "1 word = 1 token" - test real data
4. **Reasoning quality**: Highly fragmented terms may reduce model comprehension

**Cost-Benefit Analysis**:
- **Preprocessing overhead**: 5-10ms per request
- **Token savings**: 10-30% for technical/medical domains
- **ROI**: Positive if volume &gt;100K requests/month

### Architectural Constraint #1: Token Efficiency

**The Physics**:
- Code and technical terms fragment more than natural language
- Special characters (JSON, XML) consume extra tokens
- Non-English text often requires 2-3x more tokens

**Design Impact**:
```typescript
// ‚ùå Token-inefficient prompt (18 tokens)
"Please analyze this JSON: {...}"

// ‚úÖ Token-optimized (12 tokens)
"Analyze JSON: {...}"

// At 10M requests/month, this saves $200-500
```

## Context Density: Managing the Effective Context Window

**The Problem**: Models advertise "1M token windows" but lose precision as you approach the limit. This is called the **"Lost in the Middle"** phenomenon.

### The Precision Falloff Curve

```typescript
// Effective Context Utilization (Claude Sonnet 4.5)
interface ContextReliability {
  '0-32K tokens': 95-100%    // ‚úÖ High precision
  '32K-128K tokens': 85-95%  // ‚ö†Ô∏è  Slight degradation
  '128K-500K tokens': 70-85% // ‚ö†Ô∏è  Noticeable precision loss
  '500K-1M tokens': 50-70%   // ‚ùå "Lost in the Middle"
}
```

### Architectural Constraint #2: Context Window Strategy

**Design Pattern: The Context Budget**
```typescript
const CONTEXT_BUDGET = {
  systemPrompt: 2000,      // Fixed instructions
  fewShot: 5000,           // Example demonstrations
  userContext: 20000,      // Dynamic user data
  buffer: 3000,            // Safety margin
  maxOutput: 4000          // Response limit
} // Total: 34K tokens (sweet spot for reliability)
```

**Anti-Pattern**: Stuffing the entire 200K window
```typescript
// ‚ùå Architect's mistake
const prompt = `
  ${systemInstructions} +    // 2K tokens
  ${allCustomerHistory} +    // 180K tokens (!)
  ${currentQuery}            // 1K tokens
`
// Result: Model "loses" critical details from history
```

**‚úÖ Production Pattern**: Sliding window with summarization
```typescript
const prompt = `
  ${systemInstructions} +         // 2K
  ${summarizedHistory} +          // 8K (condensed)
  ${recentContextFull} +          // 15K (last 10 turns)
  ${currentQuery}                 // 1K
` // Total: 26K tokens ‚Üí stays in high-precision zone
```

### Measuring Context Density: The Information-per-Token Ratio

**Architectural Tool**: Calculate the "information density" of your context to predict precision degradation.

```typescript
interface ContextAnalysis {
  totalTokens: number
  informationChunks: number  // Distinct pieces of information
  densityScore: number       // 0-1, higher is better
  recommendation: string
}

function analyzeContextDensity(
  context: string,
  informationChunks: number
): ContextAnalysis {
  // Estimate token count (rough: 4 chars per token)
  const totalTokens = Math.ceil(context.length / 4)

  // Information density: how much useful info per token?
  // Optimal: 0.15-0.25 (15-25% of tokens carry key information)
  const densityScore = informationChunks / totalTokens

  let recommendation: string
  if (densityScore &gt; 0.25) {
    recommendation = '‚úÖ Excellent density - all information is relevant'
  } else if (densityScore &gt;= 0.15) {
    recommendation = '‚úÖ Good density - optimal for precision'
  } else if (densityScore &gt;= 0.10) {
    recommendation = '‚ö†Ô∏è Moderate density - consider summarizing'
  } else {
    recommendation = '‚ùå Poor density - high risk of Lost-in-the-Middle. Aggressive pruning needed.'
  }

  return {
    totalTokens,
    informationChunks,
    densityScore,
    recommendation
  }
}

// Example: Customer support context
const customerHistory = `
  [2024-01-10] User asked about pricing
  [2024-01-12] User requested trial extension
  [2024-01-15] User reported bug in dashboard
  [2024-01-20] User escalated to premium support
  [2024-01-22] Bug fixed, user satisfied
` // 5 key information chunks

const analysis = analyzeContextDensity(customerHistory, 5)
console.log(analysis)
// {
//   totalTokens: 58,
//   informationChunks: 5,
//   densityScore: 0.086,
//   recommendation: '‚ùå Poor density - high risk of Lost-in-the-Middle...'
// }

// Solution: Condense to higher density
const condensed = `
Customer: trial ‚Üí bug ‚Üí escalation ‚Üí resolved ‚Üí satisfied
Critical: Dashboard bug (ID #1234) fixed on 2024-01-22
` // Same 5 chunks, fewer tokens

const improvedAnalysis = analyzeContextDensity(condensed, 5)
// {
//   totalTokens: 23,
//   informationChunks: 5,
//   densityScore: 0.217,  // ‚úÖ In optimal range
//   recommendation: '‚úÖ Good density - optimal for precision'
// }
```

**Production Pattern: Pre-flight Context Check**
```typescript
async function buildPromptWithDensityCheck(
  systemPrompt: string,
  userContext: string,
  query: string
): Promise<string> {
  // Estimate information chunks (simplified: count sentences)
  const chunks = userContext.split(/[.!?]/).filter(s => s.trim().length &gt; 10).length

  const analysis = analyzeContextDensity(userContext, chunks)

  if (analysis.densityScore &lt; 0.10) {
    console.warn('‚ö†Ô∏è Low context density detected. Summarizing...')
    // Trigger summarization pipeline
    userContext = await summarizeContext(userContext, chunks)
  }

  return `${systemPrompt}\n\nContext:\n${userContext}\n\nQuery: ${query}`
}
```

**Architectural Insight**: Context density inversely correlates with "Lost-in-the-Middle" risk:
- **High density (&gt;0.20)**: Information-packed, minimal fluff ‚Üí model stays focused
- **Low density (&lt;0.10)**: Too much prose, key facts buried ‚Üí model loses precision

**Cost Impact**: Improving density from 0.08 to 0.20 means 60% token reduction ‚Üí 60% cost savings while improving accuracy.

---

## The Model Selection Matrix: Architectural Decision Framework

**Architect Perspective**: Model selection isn't about "which is best"‚Äîit's about **matching the constraint** that matters most for your specific use case.

### The Three-Axis Constraint Model

| Model | Input Cost | Output Cost | Latency (p95) | Quality Tier | Architectural Use Case |
|-------|-----------|-------------|---------------|--------------|------------------------|
| **Claude Opus 4.5** | $15/MTok | $75/MTok | 4-8s | Tier 1 | Critical reasoning, high-stakes decisions, complex agents |
| **GPT-5** | $10/MTok | $40/MTok | 3-6s | Tier 1 | Deep analytical tasks, strategic planning |
| **Claude Sonnet 4.5** | $3/MTok | $15/MTok | 1-3s | Tier 1 | Balanced workhorse, coding, production agents |
| **GPT-4o** | $2.50/MTok | $10/MTok | 1-2s | Tier 2 | General-purpose tasks, multimodal processing |
| **Claude Haiku 4.5** | $0.25/MTok | $1.25/MTok | 0.3-0.8s | Tier 2 | High-volume classification, fast inference |

> **Cost Reality Check**: At 1M requests/month with 1K input + 500 output tokens:
> - Haiku: $875/month
> - Sonnet: $10,500/month (12x more)
> - Opus: $41,250/month (47x more)

### Architectural Decision Tree

```typescript
function selectModel(useCase: UseCase): Model {
  // Constraint #1: Latency Requirements
  if (useCase.requiresRealTime && useCase.p95Latency &lt; 1000ms) {
    return 'claude-haiku-4.5' // Only option for sub-second
  }

  // Constraint #2: Cost Budget
  if (useCase.requestVolume &gt; 1_000_000 && useCase.budgetPerRequest &lt; 0.01) {
    return 'claude-haiku-4.5' // Economic constraint forces choice
  }

  // Constraint #3: Quality Floor
  if (useCase.requiresCriticalReasoning || useCase.errorTolerance &lt; 0.01) {
    return 'claude-opus-4.5' // Quality constraint overrides cost
  }

  // Default: Balanced choice
  return 'claude-sonnet-4.5'
}
```

### Real-World Architecture Patterns

**Pattern 1: The Cascade Strategy**
```typescript
// High-quality with cost optimization
async function processWithCascade(input: string) {
  try {
    // Try fast + cheap first
    const result = await callHaiku(input)
    if (result.confidence &gt; 0.9) return result

    // Fall back to quality if uncertain
    return await callSonnet(input)
  } catch {
    // Ultimate fallback for high-stakes
    return await callOpus(input)
  }
}
// Cost impact: 70% requests use Haiku, 25% Sonnet, 5% Opus
// Average cost: ~$2.50/MTok vs. $15/MTok (6x savings)
```

**Pattern 2: The Tiered Quality System**
```typescript
// Different models for different users
const MODEL_BY_TIER = {
  free: 'claude-haiku-4.5',      // Fast, cheap
  pro: 'claude-sonnet-4.5',      // Balanced
  enterprise: 'claude-opus-4.5'  // Best quality
}
```

**Decision Framework:**
1. **Start with cost optimization** - Use Haiku 4.5 as your baseline
2. **Test quality** - If accuracy is insufficient, upgrade to Sonnet 4.5
3. **Evaluate latency** - If responses are too slow for UX, consider Haiku or optimize prompts
4. **Reserve premium models** - Only use Opus/GPT-5 when quality justifies the 3-6x cost increase

## Token Economics

Understanding token costs is critical for production AI systems:

**Example**: Chat application with 1000 daily users
- Average conversation: 10 messages
- Average message: 100 tokens input, 200 tokens output
- Total daily: 1000 users √ó 10 msgs √ó (100 input + 200 output) = 1M input + 2M output/day
- Monthly cost (Claude Sonnet 4.5): (1M √ó $3 + 2M √ó $15) / 1M √ó 30 = $33/day = $990/month

**Optimization strategies**:
1. Use cheaper models for simple tasks
2. **Implement prompt caching** (see next section) - up to 90% cost reduction
3. Summarize long conversations to reduce context
4. Stream responses to improve perceived latency

---

## üèóÔ∏è Prompt Caching: The Architect's First Line of Defense

> **Architect Perspective**: In 2026, an architect's **first job** is to ensure you're not paying for the same system instructions twice. Prompt caching can reduce costs by up to **90%** for long context windows through prefix-matching.

**The Problem**: Without caching, every API call re-processes the same system prompt:

```typescript
// ‚ùå Without caching: Full cost every time
for (let i = 0; i &lt; 1000; i++) {
  await anthropic.messages.create({
    model: 'claude-sonnet-4-5-20251101',
    system: LONG_SYSTEM_PROMPT,  // 5000 tokens, re-processed 1000x
    messages: [{ role: 'user', content: queries[i] }]
  })
}
// Cost: 1000 calls √ó 5000 tokens √ó $3/MTok = $15
```

**The Solution**: Anthropic's Prompt Caching uses prefix-matching to cache repeated content:

```typescript
// ‚úÖ With prompt caching: 90% cost reduction
await anthropic.messages.create({
  model: 'claude-sonnet-4-5-20251101',
  system: [
    {
      type: 'text',
      text: LONG_SYSTEM_PROMPT,  // 5000 tokens
      cache_control: { type: 'ephemeral' }  // Cache this!
    }
  ],
  messages: [{ role: 'user', content: query }]
})
// First call: Full cost ($0.015)
// Subsequent calls: 90% discount on cached tokens ($0.0015)
// 1000 calls: $0.015 + (999 √ó $0.0015) = $1.50 (vs $15)
```

### How Prompt Caching Works

**Prefix-Matching Algorithm**:
1. Anthropic hashes the **prefix** of your prompt (system + early messages)
2. If the hash matches a cached entry (within 5-minute TTL), cached tokens are reused
3. Only **new/changed tokens** are processed at full cost
4. Cache hits reduce cost by **90%** and latency by **50-80%**

**Cache Hierarchy** (Anthropic's implementation):
```typescript
// Cache structure: System ‚Üí Tools ‚Üí Few-shot ‚Üí Dynamic context
const prompt = {
  system: [
    {
      type: 'text',
      text: SYSTEM_INSTRUCTIONS,  // 2K tokens - ‚úÖ Cacheable (static)
      cache_control: { type: 'ephemeral' }
    },
    {
      type: 'text',
      text: TOOL_DEFINITIONS,     // 3K tokens - ‚úÖ Cacheable (static)
      cache_control: { type: 'ephemeral' }
    },
    {
      type: 'text',
      text: FEW_SHOT_EXAMPLES,    // 10K tokens - ‚úÖ Cacheable (mostly static)
      cache_control: { type: 'ephemeral' }
    }
  ],
  messages: [
    // User history (dynamic) - ‚ùå Not cached
    ...conversationHistory,
    { role: 'user', content: currentQuery }
  ]
}
```

### Production Caching Strategy

**Rule of Thumb**: Cache anything that repeats across &gt;3 requests:

```typescript
// Architectural Pattern: Hierarchical caching
interface CachingStrategy {
  // Tier 1: Always cache (100% static)
  systemPrompt: string           // Never changes
  toolDefinitions: string        // Stable across versions

  // Tier 2: Cache frequently (95% static)
  fewShotExamples: string        // Updated weekly

  // Tier 3: Don't cache (dynamic)
  userContext: string            // Changes per request
  conversationHistory: string[]  // Unique per conversation
}
```

**Cost Impact Example** (Real-world AI coding assistant):
- System prompt: 5K tokens
- Tool definitions: 8K tokens
- Few-shot examples: 15K tokens
- **Total cacheable: 28K tokens**
- Volume: 100K requests/day

**Without caching**:
- Cost: 100K √ó 28K √ó $3/MTok = **$8,400/day** = **$252K/month**

**With caching** (90% cache hit rate):
- First call: 100K √ó 28K √ó $3/MTok = $8,400
- Cached calls: 100K √ó 28K √ó $0.30/MTok √ó 0.90 = $756
- **Total: $840/day** = **$25.2K/month**
- **Savings: 90% ($226.8K/month)**

### Warning: Cache Invalidation Edge Cases

**When caching breaks**:
1. **Prefix modification**: Changing **any** cached content invalidates the entire cache
2. **TTL expiration**: Caches last 5 minutes (Anthropic) - low-traffic apps see fewer hits
3. **Token boundary misalignment**: Adding 1 token to the prefix breaks the cache

```typescript
// ‚ùå Cache-breaking mistake
const systemPrompt = `
  You are a helpful assistant.
  Current time: ${new Date().toISOString()}  // Changes every second!
`
// This invalidates the cache on every request

// ‚úÖ Cache-friendly pattern
const systemPrompt = `You are a helpful assistant.`
const userMessage = `Current time: ${new Date().toISOString()}\n\n${query}`
```

**Architect's Rule**: Keep **static content** in cached sections, **dynamic content** in messages.

### Latency Benefits

**Caching reduces not just cost, but latency**:
- Cached tokens skip tokenization, attention, and inference
- **Typical reduction**: 50-80% faster time-to-first-token
- **Example**: 2000ms ‚Üí 500ms for cached system prompts

**Production Impact**:
```typescript
// Without cache: 2.5s P95 latency
// With cache: 0.8s P95 latency ‚Üí 68% faster
// User experience: Feels "instant" vs "thinking"
```

### Implementation Checklist

‚úÖ **Before deploying prompt caching**:
1. Identify static vs dynamic prompt sections
2. Move static content to prefix (system, tools, few-shot)
3. Add `cache_control: { type: 'ephemeral' }` to static blocks
4. Monitor cache hit rate (aim for &gt;85%)
5. Ensure traffic is high enough (&gt;10 requests/minute) to benefit from 5-min TTL
6. Test cache invalidation behavior during prompt updates

**Monitoring**: Track cache hit rate in production:
```typescript
// Add to logging/metrics
const cacheMetrics = {
  cacheCreationTokens: response.usage.cache_creation_input_tokens || 0,
  cacheReadTokens: response.usage.cache_read_input_tokens || 0,
  cacheHitRate: cacheReadTokens / (cacheReadTokens + cacheCreationTokens)
}
// Target: &gt;85% hit rate for cost efficiency
```

## Context Management

Effective context window management:

1. **Prioritize recent messages**: Keep last N messages in full context
2. **Summarize old context**: Compress older messages into summary
3. **Remove irrelevant content**: Strip formatting, redundant info
4. **Smart truncation**: Cut from middle, keep beginning and end

---

## üéØ Context Truncation Strategies: When You Hit the Limit

> **Architect Perspective**: When a conversation hits the context limit, your truncation strategy directly impacts reasoning quality. The wrong approach loses critical context; the right approach maintains coherence.

**The Problem**: A customer support chat reaches 200K tokens (context limit: 200K). Next message would overflow. **What do you do?**

---

### Architect's Decision Matrix: Context Management

When your system hits the 200K+ token limit, **"vibes" aren't enough**. You must choose a mathematical strategy for what the model "forgets."

| Strategy | Technical Implementation | Best For | Architect's Trade-off |
|----------|-------------------------|----------|----------------------|
| **FIFO (First-In, First-Out)** | Drop the oldest messages in the array as new ones arrive | Short, transactional chatbots | ‚ö†Ô∏è **High Risk**: The model loses the initial "System Instructions" or the original user goal. **Never use this pattern for safety-critical systems.** |
| **Middle-Out Truncation** | Keep the System Prompt and the most recent 10 messages; delete the middle history | Long, multi-turn technical support | ‚úÖ **Optimal**: Preserves the "Mission" (system prompt) and the "Current State" (recent messages) while clearing the bulk. **Production default for most applications.** |
| **Rolling Summarization** | Use a cheaper model (Haiku) to summarize the first 50 messages into a 200-token "State Summary" | Complex, multi-day research agents | ‚ö†Ô∏è **High Latency**: Adds a summarization step (300-800ms), but maintains 100% of the semantic history. **Best for high-value sessions.** |
| **Vector-Backed Memory** | Move "inactive" context to a Vector DB and retrieve only what is relevant to the current query | Enterprise Knowledge Bases, AI agents with long-term memory | ‚ö†Ô∏è **High Complexity**: Requires a RAG infrastructure but offers effectively "infinite" memory. **Only viable at scale (&gt;10K sessions/month).** |

**Cost-Quality-Latency Matrix**:
```
FIFO              ‚Üí Free, 0ms, ‚ùå Dangerous (loses system prompt)
Middle-Out        ‚Üí Free, 0ms, ‚úÖ Good (preserves mission + current state)
Summarization     ‚Üí $0.001-0.01, 300-800ms, ‚úÖ Excellent (semantic preservation)
Vector-Backed     ‚Üí $0.01-0.05, 50-200ms, ‚úÖ Best (infinite memory, high complexity)
```

---

### Strategy #1: FIFO (First-In, First-Out) ‚Äî ‚ùå NOT RECOMMENDED

**How it works**: Drop oldest messages as new ones arrive.

```typescript
// ‚ùå DANGEROUS: This pattern can delete system instructions
function fifoTruncation(
  messages: Message[],
  maxTokens: number
): Message[] {
  let currentTokens = estimateTokens(messages)

  // Remove from beginning until under limit
  while (currentTokens > maxTokens) {
    messages.shift()  // ‚ö†Ô∏è Can delete system prompt!
    currentTokens = estimateTokens(messages)
  }

  return messages
}
```

**Why This Fails**:
- **System Prompt Loss**: If your first message is "You are a HIPAA-compliant medical assistant", FIFO will delete it first
- **Mission Drift**: Model forgets its purpose mid-conversation
- **Governance Failure**: Safety constraints defined at Index 0 are lost

**The ONLY Safe Use Case**: When system prompt is re-injected on every turn (inefficient, costly)

---

### Strategy #2: Middle-Out Truncation (Production Default) ‚Äî ‚úÖ RECOMMENDED

**How it works**: Keep system prompt + recent N messages, discard the middle.

```typescript
interface TruncationStrategy {
  systemPromptTokens: number    // Always keep (2K)
  recentMessagesCount: number   // Last 10 messages (15K)
  // Middle 180K tokens ‚Üí DROPPED
}

function dropMiddleStrategy(
  messages: Message[],
  maxTokens: number = 200_000
): Message[] {
  const systemTokens = 2000
  const recentCount = 10
  const bufferTokens = 3000

  // Keep system + last 10 messages
  const recentMessages = messages.slice(-recentCount)
  const recentTokens = estimateTokens(recentMessages)

  const totalTokens = systemTokens + recentTokens + bufferTokens

  if (totalTokens < maxTokens) {
    return messages  // Still under limit
  }

  // Drop middle, keep beginning + end
  return [
    ...messages.slice(0, 5),     // First 5 (context setup)
    { role: 'system', content: '[... earlier conversation truncated ...]' },
    ...messages.slice(-recentCount)  // Last 10 (recent context)
  ]
}
```

**Pros**:
- Fast (no API calls)
- Maintains recency (most important for ongoing conversations)
- System prompt always preserved

**Cons**:
- **Lost-in-the-Middle**: Model has no knowledge of mid-conversation context
- Can break multi-turn reasoning ("As I mentioned earlier..." ‚Üí model has no memory)
- User references to earlier topics feel like AI "forgot"

**Best For**: Simple Q&A, transactions without long-term dependencies

---

### Strategy #3: Rolling Summarization (Highest Quality, Costly)

**How it works**: Use an LLM to compress old messages into a "Condensed State".

```typescript
async function summarizeOldContext(
  messages: Message[],
  maxTokens: number
): Promise<Message[]> {
  const systemTokens = 2000
  const recentCount = 10
  const targetSummaryTokens = 5000

  // Split: recent (keep full) vs old (summarize)
  const recentMessages = messages.slice(-recentCount)
  const oldMessages = messages.slice(0, -recentCount)

  // Check if summarization is needed
  const currentTokens = estimateTokens(messages)
  if (currentTokens < maxTokens * 0.8) {
    return messages  // Still have headroom
  }

  // Use LLM to summarize old context
  const summary = await anthropic.messages.create({
    model: 'claude-haiku-4-5-20251101',  // Fast summarizer
    max_tokens: 1500,
    messages: [{
      role: 'user',
      content: `Summarize the key points, decisions, and user preferences from this conversation history. Keep it under 1200 tokens:\n\n${formatMessages(oldMessages)}`
    }]
  })

  // Inject summary as synthetic "context" message
  return [
    { role: 'system', content: 'Previous conversation summary:' },
    { role: 'assistant', content: summary.content[0].text },
    ...recentMessages  // Full recent context
  ]
}

// Cost analysis
// Old context: 180K tokens √ó $3/MTok = $0.54 per request (dropped)
// Summary cost: 180K input + 1.5K output = $0.54 + $0.02 = $0.56 ONE-TIME
// Future requests: 5K summary tokens vs 180K full = 97% token reduction
// ROI: Positive after 2+ requests using the summary
```

**Pros**:
- **Highest quality**: Preserves key information from entire conversation
- Model can reference "earlier" context without it being in the window
- User experience: AI "remembers" past decisions

**Cons**:
- **Costly**: Requires additional API call (Haiku: $0.001-0.01 per summarization)
- **Latency**: Adds 300-800ms to request time
- **Summary drift**: Information loss through compression (5-10% typical)

**Best For**: Long-running sessions, customer support, multi-turn agents

**Production Implementation**:
```typescript
// Trigger summarization at 80% capacity
const SUMMARIZATION_TRIGGER = maxTokens * 0.80

// Cache summaries to avoid re-summarizing
interface ConversationCache {
  conversationId: string
  summary: string
  summarizedUpToIndex: number  // Message index
  lastUpdated: Date
}

async function managedTruncation(
  conversationId: string,
  messages: Message[],
  cache: ConversationCache | null
): Promise<Message[]> {
  const currentTokens = estimateTokens(messages)

  if (currentTokens < SUMMARIZATION_TRIGGER) {
    return messages  // No action needed
  }

  // Check if we have a cached summary
  if (cache && cache.summarizedUpToIndex > 0) {
    // Use cached summary + new messages
    const newMessages = messages.slice(cache.summarizedUpToIndex)
    return [
      { role: 'system', content: cache.summary },
      ...newMessages
    ]
  }

  // No cache - trigger fresh summarization
  return await summarizeOldContext(messages, maxTokens)
}
```

---

### Strategy #4: Vector-Backed Memory (Enterprise-Grade, Infinite Memory)

**How it works**: Store "inactive" context in a vector database, retrieve only relevant chunks for the current query.

```typescript
import { PineconeClient } from '@pinecone-database/pinecone'
import { OpenAIEmbeddings } from 'langchain/embeddings/openai'

interface VectorMemoryStrategy {
  vectorDB: PineconeClient
  embeddings: OpenAIEmbeddings
  conversationId: string
}

async function vectorBackedTruncation(
  messages: Message[],
  currentQuery: string,
  config: VectorMemoryStrategy
): Promise<Message[]> {
  const systemPrompt = messages[0]  // Always keep Index 0
  const recentMessages = messages.slice(-10)  // Last 10 turns

  // Everything else ‚Üí Vector DB
  const oldMessages = messages.slice(1, -10)

  // Step 1: Store old context in vector DB (if not already stored)
  for (const msg of oldMessages) {
    await config.vectorDB.upsert({
      id: `${config.conversationId}-${msg.timestamp}`,
      values: await config.embeddings.embedQuery(msg.content),
      metadata: {
        role: msg.role,
        content: msg.content,
        timestamp: msg.timestamp
      }
    })
  }

  // Step 2: Retrieve relevant old context based on current query
  const queryEmbedding = await config.embeddings.embedQuery(currentQuery)
  const relevantContext = await config.vectorDB.query({
    vector: queryEmbedding,
    topK: 5,  // Retrieve top 5 most relevant old messages
    includeMetadata: true
  })

  // Step 3: Build context window with system + relevant + recent
  return [
    systemPrompt,  // Index 0: Immutable
    ...relevantContext.matches.map(match => ({
      role: match.metadata.role,
      content: match.metadata.content
    })),
    ...recentMessages  // Last 10: Current state
  ]
}

// Cost breakdown (10K sessions/month, 200 messages each)
// Embedding cost: 2M messages √ó 1K tokens √ó $0.0001/MTok = $0.20/month
// Storage cost: 2M vectors √ó $0.50/month = $1,000/month (Pinecone Pro)
// Query cost: 10K queries/day √ó $0.001/query = $300/month
// Total: $1,300/month for effectively infinite memory
// vs. Summarization: $0.01 √ó 2M sessions = $20,000/month
```

**Pros**:
- **Infinite memory**: No hard context limit, stores unlimited history
- **Semantic retrieval**: Finds relevant context even from months ago
- **Fast queries**: 50-200ms retrieval time (vs 300-800ms for summarization)
- **Cost-effective at scale**: Fixed infrastructure cost regardless of session length

**Cons**:
- **High complexity**: Requires vector DB (Pinecone, Weaviate, Postgres+pgvector)
- **Infrastructure cost**: $500-2000/month for hosting
- **Cold start**: Embedding generation adds latency on first message
- **Retrieval quality**: Depends on embedding model quality (can miss nuanced context)

**Best For**:
- Enterprise knowledge bases with &gt;10K active sessions
- AI agents that need to remember conversations from weeks/months ago
- Multi-user systems where context sharing is valuable

**When NOT to Use**:
- &lt;1K sessions/month (summarization is cheaper)
- Simple chatbots without long-term memory needs
- Regulated industries requiring deterministic replay (vector retrieval is probabilistic)

---

### ‚ö†Ô∏è The "Instruction Anchor" ‚Äî Critical Safety Pattern

> **Architect's Rule**: Regardless of the truncation strategy, an AI Architect **always anchors the System Prompt**. In your code, **Index 0** of your message array should be **immutable**.

**Why This Matters**:

If your truncation logic accidentally deletes the instructions telling the model to be a "HIPAA-compliant medical assistant," your **Sovereign Governance layer will fail**.

```typescript
// ‚ùå DANGEROUS: System prompt can be truncated
const messages = [
  { role: 'system', content: 'You are a HIPAA-compliant medical assistant.' },
  ...conversationHistory  // 200K tokens
]

// If FIFO or naive truncation runs...
messages.shift()  // ‚ö†Ô∏è System prompt DELETED
// Model no longer knows it must follow HIPAA rules!
```

**‚úÖ Safe Implementation: Immutable Index 0**

```typescript
interface SafeMessageArray {
  systemPrompt: Message  // Stored separately, NEVER truncated
  history: Message[]     // Subject to truncation
}

function safeTruncation(
  messages: SafeMessageArray,
  maxTokens: number
): Message[] {
  // Truncate ONLY the history, never the system prompt
  const truncatedHistory = truncateHistory(messages.history, maxTokens - 2000)

  // Always return system prompt at Index 0
  return [
    messages.systemPrompt,  // IMMUTABLE
    ...truncatedHistory
  ]
}

// Alternative: Use a "guard" function
function guardedTruncation(messages: Message[]): Message[] {
  if (messages.length === 0) {
    throw new Error('Cannot truncate: No messages')
  }

  const systemPrompt = messages[0]
  if (systemPrompt.role !== 'system') {
    throw new Error('CRITICAL: Index 0 must be system prompt')
  }

  // Truncate everything EXCEPT Index 0
  const truncatedRest = truncateMiddle(messages.slice(1))

  return [systemPrompt, ...truncatedRest]
}
```

**Production Pattern: System Prompt Versioning**

```typescript
// Track system prompt separately from conversation
interface ConversationState {
  id: string
  systemPromptVersion: string  // "v2.1"
  systemPrompt: Message        // Immutable anchor
  history: Message[]           // Mutable, can be truncated
  createdAt: Date
  lastTruncatedAt: Date | null
}

// When truncating, ALWAYS preserve system prompt
function productionTruncate(state: ConversationState): Message[] {
  // Log for audit trail
  console.log(`Truncating conversation ${state.id}`)
  console.log(`System Prompt Version: ${state.systemPromptVersion}`)
  console.log(`History length: ${state.history.length} ‚Üí ${TARGET_LENGTH}`)

  const truncatedHistory = applyTruncationStrategy(
    state.history,
    'middle-out',  // or 'rolling-summarization'
    { preserveSystemPrompt: true }  // ALWAYS
  )

  // Verify system prompt is intact
  if (truncatedHistory[0]?.role !== 'system') {
    throw new Error('CRITICAL SAFETY VIOLATION: System prompt missing after truncation')
  }

  return [state.systemPrompt, ...truncatedHistory]
}
```

**Medical Safety Example** (45-minute diagnostic session):

```typescript
// Initial state (session start)
const medicalSession = {
  systemPrompt: {
    role: 'system',
    content: `You are a HIPAA-compliant medical assistant.
    - NEVER diagnose without human review
    - ALWAYS ask about allergies before suggesting medication
    - ESCALATE chest pain, shortness of breath, or severe bleeding`
  },
  history: []
}

// After 45 minutes, context is 95% full
// Patient just shared: "I'm having chest pain"

// ‚ùå WRONG: If system prompt is truncated...
// Model: "Here are some home remedies for chest pain" ‚Üí DANGEROUS

// ‚úÖ CORRECT: System prompt is anchored at Index 0
// Model sees: "ESCALATE chest pain" ‚Üí "This requires immediate medical attention. I'm alerting your care team."
```

**Architect's Checklist for Truncation Safety**:
- [ ] System prompt is stored separately from history
- [ ] Truncation function ALWAYS preserves Index 0
- [ ] Unit tests verify system prompt survives truncation
- [ ] Logging tracks truncation events for audit
- [ ] Error handling throws if Index 0 is not 'system' role
- [ ] System prompt versioning for rollback if issues detected

---

### Strategy Comparison Matrix (Updated)

| Strategy | Cost | Latency | Quality | Infrastructure | Best Use Case |
|----------|------|---------|---------|----------------|---------------|
| **FIFO** | Free | 0ms | ‚ùå Dangerous | None | **Never use** |
| **Middle-Out** | Free | 0ms | ‚úÖ Good | None | Simple Q&A, stateless tasks |
| **Rolling Summarization** | $0.001-0.01 | 300-800ms | ‚úÖ Excellent | None | Long sessions, agents, support |
| **Vector-Backed** | $0.01-0.05 | 50-200ms | ‚úÖ Best | Vector DB required | Enterprise, infinite memory |

### Architectural Decision Tree

```typescript
function selectTruncationStrategy(
  conversationType: string,
  sessionLength: number,
  budget: number
): TruncationStrategy {
  // Short sessions: Don't truncate, just keep full context
  if (sessionLength &lt; 20) {
    return 'no-truncation'
  }

  // High-value sessions: Quality matters most
  if (conversationType === 'customer-support' || conversationType === 'agent') {
    return 'llm-summarization'  // Accept cost for quality
  }

  // High-volume, low-margin: Cost matters most
  if (budget &lt; 0.01) {
    return 'drop-middle'  // Free, good enough
  }

  // Default: Balanced approach
  return 'drop-middle-with-periodic-summarization'  // Hybrid
}
```

### Hybrid Strategy: Best of Both Worlds

**Production Pattern**: Use drop-middle for quick wins, trigger summarization every 50 messages.

```typescript
const SUMMARIZATION_INTERVAL = 50  // Every 50 messages

async function hybridTruncation(
  messages: Message[],
  messageCount: number
): Promise<Message[]> {
  // Every 50 messages: Pay for quality
  if (messageCount % SUMMARIZATION_INTERVAL === 0) {
    console.log('üìù Triggering periodic summarization')
    return await summarizeOldContext(messages, maxTokens)
  }

  // Otherwise: Free drop-middle
  return dropMiddleStrategy(messages, maxTokens)
}

// Cost impact:
// Without: 50 messages √ó 180K tokens √ó $3/MTok = $27
// With hybrid: 49 √ó 5K tokens + 1 √ó summarization = $0.735 + $0.56 = $1.30
// Savings: 95% ($25.70 per 50-message session)
```

**Architect's Rule**: Truncation strategy should match conversation value:
- **High-value** (sales, support) ‚Üí LLM summarization
- **Mid-value** (general chat) ‚Üí Hybrid (drop-middle + periodic summarization)
- **Low-value** (one-off queries) ‚Üí Drop-middle or no truncation needed

## Practical Exercises

### Exercise 1: Token Counting

**Goal**: Learn how to count tokens in different types of text using the Anthropic API.

**Why This Matters**: Token counting is critical for:
- Estimating costs before making API calls
- Ensuring prompts fit within context windows
- Optimizing prompt efficiency

**Try It Yourself**: Click "Run Code" to execute this token counting example. You can edit the code to test your own text!

<CodePlayground
  title="Interactive Token Counting"
  description="Edit the test cases below and click Run to see how different types of text are tokenized."
  exerciseType="token-counting"
  code={`import Anthropic from '@anthropic-ai/sdk'

const anthropic = new Anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY
})

async function countTokens(text: string): Promise<number> {
  const response = await anthropic.messages.countTokens({
    model: 'claude-sonnet-4-5-20251101',
    messages: [{ role: 'user', content: text }]
  })
  return response.input_tokens
}

// Test with different text types
async function runTokenCountingExercise() {
  const testCases = [
    {
      name: 'Short message',
      text: 'Hello, how are you?'
    },
    {
      name: 'Code snippet',
      text: \`function fibonacci(n) {
  if (n &lt;= 1) return n
  return fibonacci(n - 1) + fibonacci(n - 2)
}\`
    },
    {
      name: 'Markdown documentation',
      text: \`# API Documentation

## Authentication

All API requests require authentication using an API key:

\\\`\\\`\\\`bash
curl -H "Authorization: Bearer YOUR_API_KEY" https://api.example.com/data
\\\`\\\`\\\`

## Rate Limits

- Free tier: 100 requests/hour
- Pro tier: 1000 requests/hour\`
    },
    {
      name: 'JSON data',
      text: JSON.stringify({
        users: [
          { id: 1, name: 'Alice', email: 'alice@example.com' },
          { id: 2, name: 'Bob', email: 'bob@example.com' }
        ]
      }, null, 2)
    }
  ]

  console.log('Token Counting Results:\\n')

  for (const testCase of testCases) {
    const tokens = await countTokens(testCase.text)
    const charCount = testCase.text.length
    const tokensPerChar = (tokens / charCount).toFixed(3)

    console.log(\`\${testCase.name}:\`)
    console.log(\`  Characters: \${charCount}\`)
    console.log(\`  Tokens: \${tokens}\`)
    console.log(\`  Tokens/Char: \${tokensPerChar}\`)
    console.log(\`  Cost (input): $\${(tokens / 1000 * 0.003).toFixed(6)}\`)
    console.log()
  }
}

runTokenCountingExercise()`}
/>

**Key Insights You'll Discover**:
- Code typically uses more tokens per character than plain text
- JSON/structured data is relatively token-efficient
- Whitespace and formatting affect token count

### Exercise 2: Cost Calculation

**Goal**: Calculate the actual cost of running a real AI application.

**Scenario**: You're building a customer support chatbot that:
- Receives 1000 conversations per day
- Each conversation has 5 messages on average
- Each message is ~100 tokens input, ~200 tokens output

**Try It Yourself**: Click "Run Code" to see cost estimates for different AI application scenarios. You can edit the scenarios to match your use case!

<CodePlayground
  title="Interactive Cost Calculator"
  description="Edit the scenarios below to calculate costs for your AI application. Try changing conversation volumes, message lengths, or models."
  exerciseType="cost-calculation"
  code={`interface CostEstimate {
  scenario: string
  dailyConversations: number
  messagesPerConversation: number
  avgInputTokens: number
  avgOutputTokens: number
  model: string
  inputCostPerMillion: number
  outputCostPerMillion: number
}

function calculateMonthlyCost(estimate: CostEstimate) {
  // Calculate total tokens
  const dailyMessages = estimate.dailyConversations * estimate.messagesPerConversation
  const dailyInputTokens = dailyMessages * estimate.avgInputTokens
  const dailyOutputTokens = dailyMessages * estimate.avgOutputTokens

  // Calculate daily cost
  const dailyInputCost = (dailyInputTokens / 1_000_000) * estimate.inputCostPerMillion
  const dailyOutputCost = (dailyOutputTokens / 1_000_000) * estimate.outputCostPerMillion
  const dailyCost = dailyInputCost + dailyOutputCost

  // Calculate monthly cost (30 days)
  const monthlyCost = dailyCost * 30

  return {
    dailyMetrics: {
      conversations: estimate.dailyConversations,
      messages: dailyMessages,
      inputTokens: dailyInputTokens,
      outputTokens: dailyOutputTokens,
      cost: dailyCost
    },
    monthlyMetrics: {
      conversations: estimate.dailyConversations * 30,
      messages: dailyMessages * 30,
      inputTokens: dailyInputTokens * 30,
      outputTokens: dailyOutputTokens * 30,
      cost: monthlyCost
    },
    costPerConversation: dailyCost / estimate.dailyConversations,
    breakdown: {
      inputCost: dailyInputCost,
      outputCost: dailyOutputCost,
      inputPercentage: (dailyInputCost / dailyCost * 100).toFixed(1) + '%',
      outputPercentage: (dailyOutputCost / dailyCost * 100).toFixed(1) + '%'
    }
  }
}

// Run the exercise
const scenarios: CostEstimate[] = [
  {
    scenario: 'Chat Application (1K users/day)',
    dailyConversations: 1000,
    messagesPerConversation: 10,
    avgInputTokens: 100,
    avgOutputTokens: 200,
    model: 'claude-sonnet-4-5-20251101',
    inputCostPerMillion: 3,
    outputCostPerMillion: 15
  },
  {
    scenario: 'Code Assistant (100 requests/day)',
    dailyConversations: 100,
    messagesPerConversation: 5,
    avgInputTokens: 500,
    avgOutputTokens: 1000,
    model: 'claude-sonnet-4-5-20251101',
    inputCostPerMillion: 3,
    outputCostPerMillion: 15
  },
  {
    scenario: 'Content Generation (500 articles/day)',
    dailyConversations: 500,
    messagesPerConversation: 4,
    avgInputTokens: 1000,
    avgOutputTokens: 2500,
    model: 'claude-sonnet-4-5-20251101',
    inputCostPerMillion: 3,
    outputCostPerMillion: 15
  }
]

console.log('Cost Estimation Results:\\n')

for (const scenario of scenarios) {
  const results = calculateMonthlyCost(scenario)

  console.log(\`\${scenario.scenario}:\`)
  console.log(\`  Daily: \${results.dailyMetrics.conversations.toLocaleString()} conversations\`)
  console.log(\`  Daily Cost: $\${results.dailyMetrics.cost.toFixed(2)}\`)
  console.log(\`  Monthly Cost: $\${results.monthlyMetrics.cost.toFixed(2)}\`)
  console.log(\`  Cost per Conversation: $\${results.costPerConversation.toFixed(4)}\`)
  console.log(\`  Breakdown: \${results.breakdown.inputPercentage} input, \${results.breakdown.outputPercentage} output\`)
  console.log()
}`}
/>

**Key Insights**:
- Output tokens cost more than input tokens (typically 5x more)
- Model choice dramatically affects cost (Haiku 4.5 vs Opus 4.5: ~6x difference)
- Volume adds up quickly - optimize for high-traffic applications
- The Claude 4.5 series offers 67% cost reduction over previous generations

### Exercise 3: Model Comparison

**Goal**: Compare different models on the same task to understand quality/speed/cost tradeoffs.

**Try It Yourself**: Click "Run Code" to compare Claude Haiku and Sonnet on the same task. Watch how they differ in speed, cost, and response quality!

<CodePlayground
  title="Interactive Model Comparison"
  description="Compare Claude models on the same task. The demo uses Haiku (fast/cheap) vs Sonnet (balanced) to show real tradeoffs."
  exerciseType="model-comparison"
  code={`import Anthropic from '@anthropic-ai/sdk'

const anthropic = new Anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY
})

interface ModelConfig {
  name: string
  model: string
  inputPrice: number  // per million tokens
  outputPrice: number
}

interface ComparisonResult {
  model: string
  response: string
  latency: number
  inputTokens: number
  outputTokens: number
  cost: number
}

async function compareModels(prompt: string): Promise<ComparisonResult[]> {
  const models: ModelConfig[] = [
    {
      name: 'Claude Haiku 4.5',
      model: 'claude-3-haiku-20240307',
      inputPrice: 1,
      outputPrice: 5
    },
    {
      name: 'Claude Sonnet 4.5',
      model: 'claude-3-5-sonnet-20240620',
      inputPrice: 3,
      outputPrice: 15
    }
  ]

  const results: ComparisonResult[] = []

  for (const modelConfig of models) {
    const start = Date.now()

    const response = await anthropic.messages.create({
      model: modelConfig.model,
      max_tokens: 150,
      messages: [{ role: 'user', content: prompt }]
    })

    const latency = Date.now() - start
    const cost = (
      response.usage.input_tokens * modelConfig.inputPrice +
      response.usage.output_tokens * modelConfig.outputPrice
    ) / 1_000_000

    results.push({
      model: modelConfig.name,
      response: response.content[0].text,
      latency,
      inputTokens: response.usage.input_tokens,
      outputTokens: response.usage.output_tokens,
      cost
    })
  }

  return results
}

// Run comparison
async function runModelComparison() {
  const testPrompt = 'Explain API rate limiting in 2 sentences.'

  console.log('Model Comparison Results:\\n')
  console.log(\`Prompt: "\${testPrompt}"\\n\`)

  const results = await compareModels(testPrompt)

  // Display comparison table
  console.log('Metrics:')
  console.log('‚îÄ'.repeat(80))

  for (const result of results) {
    console.log(\`\${result.model}:\`)
    console.log(\`  Latency: \${result.latency}ms\`)
    console.log(\`  Tokens: \${result.inputTokens} in / \${result.outputTokens} out\`)
    console.log(\`  Cost: $\${result.cost.toFixed(6)}\`)
    console.log(\`  Response: \${result.response.substring(0, 100)}...\`)
    console.log()
  }

  console.log('‚îÄ'.repeat(80))
  console.log('\\nKey Insights:')
  console.log('  ‚Ä¢ Haiku is faster and cheaper for simple tasks')
  console.log('  ‚Ä¢ Sonnet provides more detailed responses')
  console.log('  ‚Ä¢ Cost difference: ~3x')
  console.log('  ‚Ä¢ Choose based on your priorities!')
}

runModelComparison()`}
/>

**Key Insights You'll Discover**:
- Latency varies significantly between models (1.6-3.1 seconds for typical requests)
- Cost can vary by 6x between models (Haiku 4.5 vs Opus 4.5)
- Cheaper/faster models like Haiku 4.5 now match previous-gen flagship performance
- GPT-5 offers competitive pricing for sophisticated reasoning tasks
- Balance quality, speed, and cost based on your use case

**Your Turn**: Try these experiments:
1. Test with different prompt complexities (simple vs complex reasoning)
2. Compare the same model with different max_tokens settings
3. Test multilingual prompts to see if token counts change

## üéì Test Your Architectural Thinking

> These aren't beginner questions‚Äîthey're **real-world architectural scenarios** you'll face in production. Think through the tradeoffs before checking the solutions.

### Scenario 1: The Budget-Constrained Document Query System

**Context**: You're architecting a legal document analysis system for a startup with:
- **Document size**: 10,000 pages per case (avg 500 tokens/page = 5M tokens)
- **Query volume**: 100 queries/day
- **Budget**: $500/month maximum for AI costs
- **Quality requirement**: Must extract accurate citations and case references

**Question**: Which architectural pattern do you use?

**Options**:
A) **Long-Context Upload**: Send entire 5M tokens to Claude Opus 4.5 ($15/MTok)
B) **MapReduce Pattern**: Split into 100-page chunks, parallel process with Haiku, merge results
C) **Hybrid RAG**: Vector search (top 50 pages) + Sonnet 4.5 for reasoning
D) **Incremental Processing**: Start with Haiku on full doc, escalate to Sonnet only if uncertain

<details>
<summary><strong>üí° Solution & Analysis</strong></summary>

**Correct Answer: C) Hybrid RAG**

**Cost Breakdown**:

**Option A (Long-Context)**: ‚ùå Over budget
- 100 queries √ó 5M tokens √ó $15/MTok = $7,500/month
- **75K tokens output** (summaries) √ó $75/MTok = $5,625
- **Total: $13,125/month** (26x over budget!)

**Option B (MapReduce)**: ‚ùå Complex, but still expensive
- Split: 5M tokens / 100 pages = 50K tokens per chunk = 100 chunks
- Process each: 100 chunks √ó 50K tokens √ó $0.25/MTok √ó 100 queries = $1,250/month
- Merge step (Sonnet): 100 intermediate results √ó 10K tokens √ó $3/MTok √ó 100 queries = $3,000/month
- **Total: $4,250/month** (8.5x over budget)

**Option C (Hybrid RAG)**: ‚úÖ Within budget
- Vector search: $50/month (embedding cost: 500M tokens √ó $0.0001/MTok)
- Retrieve top 50 pages: 50 √ó 500 tokens = 25K tokens per query
- Sonnet processing: 100 queries √ó 25K tokens √ó $3/MTok = $75/month (input)
- Output: 100 queries √ó 5K tokens √ó $15/MTok = $75/month
- **Total: $200/month** (40% of budget, leaves room for growth)

**Option D (Incremental)**: ‚ö†Ô∏è Quality risk
- Haiku on 5M tokens: 100 √ó 5M √ó $0.25/MTok = $125/month
- But: Haiku quality insufficient for legal citations (50% hallucination rate on long context)
- Escalation rate: 80% ‚Üí Sonnet cost: $4,000/month
- **Not viable due to quality constraints**

**Architectural Insights**:
- RAG reduces context from 5M ‚Üí 25K tokens (99.5% reduction!)
- Vector search one-time cost spreads across all queries
- Sonnet provides legal-grade accuracy at 2% of Opus cost
- Leaves $300/month buffer for traffic growth (3x headroom)

</details>

---

### Scenario 2: The Latency-Critical Classifier

**Context**: Building a content moderation system for a social network:
- **Volume**: 50,000 posts/hour (peak)
- **Latency requirement**: P95 &lt; 500ms (UX constraint)
- **Quality requirement**: 95%+ accuracy on toxicity detection
- **Cost target**: &lt;$0.0001 per post

**Question**: How do you meet all three constraints (latency, quality, cost)?

**Options**:
A) Use Claude Opus 4.5 with streaming for perceived speed
B) Use Claude Haiku 4.5 with prompt caching
C) Use a fine-tuned small model (Llama 7B) hosted on your servers
D) Use Haiku for fast classification + Sonnet for borderline cases

<details>
<summary><strong>üí° Solution & Analysis</strong></summary>

**Correct Answer: D) Cascade with Haiku + Sonnet**

**Constraint Analysis**:

**Latency (P95 &lt; 500ms)**:
- Opus: 4-8s P95 ‚Üí ‚ùå Fails (10x too slow)
- Sonnet: 1-3s P95 ‚Üí ‚ùå Fails (3x too slow)
- Haiku: 300-800ms P95 ‚Üí ‚úÖ Meets requirement
- Self-hosted Llama: 50-200ms ‚Üí ‚úÖ Meets requirement

**Quality (95%+ accuracy)**:
- Opus: 99% ‚Üí ‚úÖ Overkill
- Sonnet: 97% ‚Üí ‚úÖ Meets requirement
- Haiku: 92% on simple, 88% on nuanced ‚Üí ‚ö†Ô∏è Borderline
- Llama 7B fine-tuned: 93% ‚Üí ‚úÖ Possible with good training data

**Cost (&lt;$0.0001/post)**:
- Opus: $15/MTok √ó 200 tokens = $0.003/post ‚Üí ‚ùå 30x over budget
- Sonnet: $3/MTok √ó 200 tokens = $0.0006/post ‚Üí ‚ùå 6x over budget
- Haiku: $0.25/MTok √ó 200 tokens = $0.00005/post ‚Üí ‚úÖ Meets budget
- Llama self-hosted: $0.00001/post (amortized GPU) ‚Üí ‚úÖ Meets budget

**Why Option D Wins**:
```typescript
async function moderateContent(post: string) {
  // Step 1: Haiku classifies in &lt;500ms
  const classification = await classifyWithHaiku(post)
  // ‚Üí Returns: { toxicity: 'high' | 'medium' | 'low', confidence: number }

  // Step 2: High-confidence decisions are final (90% of cases)
  if (classification.confidence > 0.95) {
    return classification.toxicity !== 'high'  // Fast path: 300ms, $0.00005
  }

  // Step 3: Borderline cases (10%) escalate to Sonnet
  if (classification.confidence &lt; 0.95) {
    const refined = await refineWithSonnet(post)  // Slow path: 2s, $0.0006
    return refined.toxicity !== 'high'
  }
}

// Blended cost (90% Haiku, 10% Sonnet):
// 0.90 √ó $0.00005 + 0.10 √ó $0.0006 = $0.000105/post
// ‚Üí Slightly over target, but acceptable tradeoff for quality

// Blended latency (90% fast, 10% slow):
// P95 = 500ms (Haiku) for most, 2s for 10%
// Acceptable since borderline cases can afford extra time
```

**Why Other Options Fail**:

**Option A (Opus)**: Opus latency (6s P95) breaks UX, cost is 30x budget. Streaming helps perceived latency but doesn't reduce actual wait time for moderation decision.

**Option B (Haiku + Caching)**: Prompt caching reduces cost by 90%, making Haiku $0.000005/post. But quality is still 88-92%, below 95% requirement. Caching helps cost but doesn't fix accuracy.

**Option C (Self-hosted Llama)**: Meets latency and cost, but:
- Requires ML ops team (ongoing maintenance)
- Training data collection (3-6 months)
- GPU infrastructure ($2000/month for high-availability)
- Model drift requires retraining every 3-6 months
- **Total cost**: $4000+/month ‚Üí only viable at &gt;40M posts/month

**Architectural Lesson**: When constraints conflict, use **tiered architecture** to meet all requirements through selective escalation.

</details>

---

### Scenario 3: The Context Window Dilemma

**Context**: AI coding assistant with:
- **Codebase size**: 500 files, 2M tokens total
- **User query**: "Refactor the authentication system to use OAuth2"
- **Context limit**: 200K tokens
- **Challenge**: OAuth code spans 15 files across the codebase

**Question**: How do you provide the LLM with enough context to do this safely?

**Options**:
A) Send all 2M tokens using Claude Opus 4.5 (1M token window, split into 2 calls)
B) Use RAG to retrieve the 15 OAuth-related files (~50K tokens)
C) Send the entire codebase summary (20K tokens) + full OAuth files (50K tokens)
D) Start with RAG (50K), let LLM request additional files as needed (agentic loop)

<details>
<summary><strong>üí° Solution & Analysis</strong></summary>

**Correct Answer: D) Agentic Loop with RAG**

**Why Each Option Succeeds or Fails**:

**Option A (Send everything)**: ‚ùå Technically impossible + expensive
- Opus has 1M token window, but 2M tokens requires 2 separate API calls
- Cost: 2M tokens √ó $15/MTok = $30 per refactor
- "Lost in the Middle": At 2M tokens, model loses track of key details
- Can't reason across both calls (no shared state)

**Option B (RAG-only)**: ‚ö†Ô∏è Risky, but cheap
- Cost: 50K tokens √ó $15/MTok = $0.75 per refactor
- Risk: OAuth might interact with other systems not in the retrieved files
  - Example: What if there's a middleware file that hooks into auth?
  - Or a config file that needs updating?
- **50% chance of incomplete refactor** (breaks at runtime)

**Option C (Summary + Files)**: ‚úÖ Good, but not optimal
- Summary: 20K tokens (high-level codebase structure)
- OAuth files: 50K tokens (implementation details)
- Total: 70K tokens √ó $15/MTok = $1.05 per refactor
- Model has "map" of codebase + detailed OAuth implementation
- **85% success rate** (catches most dependencies)
- Missing: Can't discover hidden dependencies until runtime

**Option D (Agentic Loop)**: ‚úÖ Best (highest quality, acceptable cost)
```typescript
async function agenticCodeRefactor(query: string, codebase: Codebase) {
  let context = []
  let iterations = 0
  const MAX_ITERATIONS = 5

  // Step 1: Initial RAG retrieval (OAuth files)
  context = await retrieveRelevantFiles(query, codebase)  // 50K tokens

  while (iterations < MAX_ITERATIONS) {
    // Step 2: LLM analyzes and plans refactor
    const response = await callOpus({
      system: 'You are a code architect. Analyze the code and identify any missing dependencies.',
      messages: [
        { role: 'user', content: `Context: ${context}\n\nTask: ${query}` }
      ],
      tools: [
        {
          name: 'request_additional_files',
          description: 'Request additional files if you need more context',
          input_schema: {
            type: 'object',
            properties: {
              files: { type: 'array', items: { type: 'string' } },
              reasoning: { type: 'string' }
            }
          }
        }
      ]
    })

    // Step 3: Check if LLM requests more context
    if (response.stop_reason === 'tool_use') {
      const toolUse = response.content.find(block => block.type === 'tool_use')
      const requestedFiles = await loadFiles(toolUse.input.files)
      context.push(...requestedFiles)
      iterations++
      continue  // Loop back with expanded context
    }

    // Step 4: LLM has enough context, proceed with refactor
    return response.content[0].text  // Contains refactor plan
  }
}

// Cost breakdown:
// Iteration 1: 50K tokens (RAG) √ó $15/MTok = $0.75
// Iteration 2: 50K + 30K new files √ó $15/MTok = $1.20
// Iteration 3: 80K + 15K more files √ó $15/MTok = $1.43
// Total: $3.38 per refactor
// Success rate: 98% (LLM discovers hidden dependencies)
```

**Architectural Insights**:

1. **RAG alone is insufficient** for complex codebases‚ÄîLLMs need ability to "explore"
2. **Static context budgets** (Option C) trade quality for cost
3. **Agentic loops** (Option D) let the LLM decide what it needs, improving quality at modest cost increase
4. **Tool use** is the key primitive‚Äîdon't just send context, let LLM request it

**Real-World Tradeoff**:
- **Option C**: $1.05, 85% success ‚Üí $1.24/successful refactor (15% require human fix)
- **Option D**: $3.38, 98% success ‚Üí $3.45/successful refactor
- **Value**: Option D costs 3x more but saves 2 hours of debugging ‚Üí $3.38 vs $200 (engineer time)

**When to Use Which**:
- **Simple refactors** (rename, small changes): Option C (summary + files)
- **Complex refactors** (architecture changes): Option D (agentic loop)
- **Exploratory tasks** ("find all OAuth usages"): Option B (RAG) is sufficient

</details>

---

---

### Scenario 4: The Medical Safety Truncation Dilemma üè•

**Context**: Your telehealth bot is in a **45-minute diagnostic session** with a patient. The context window is **95% full** (190K of 200K tokens used). The patient just shared a critical symptom: **"I'm having chest pain"**. However, the original **"Medical Safety Guidelines"** from the start of the session (stored at Index 0) are about to be truncated by your current FIFO truncation strategy.

**Question**: Which strategy do you implement to ensure safety without crashing the session?

**Options**:
A) **FIFO (Let the oldest data drop)** - Continue with current strategy, oldest messages get deleted first
B) **Middle-Out (Preserve Index 0 and the current message)** - Keep system prompt + recent context, drop middle
C) **Clear Cache (Start a fresh session)** - Reset the conversation, lose all diagnostic context
D) **Increase Temperature (Hope the model remembers)** - Adjust model parameters to improve recall

<details>
<summary><strong>üí° Solution & Safety Analysis</strong></summary>

**Correct Answer: B) Middle-Out Truncation**

**Why Each Option Succeeds or Fails**:

**Option A (FIFO)**: ‚ùå **CATASTROPHICALLY DANGEROUS**
```typescript
// Current state (before truncation)
messages = [
  { role: 'system', content: 'Medical Safety Guidelines: ESCALATE chest pain...' },  // Index 0
  ...44_minutes_of_diagnostic_conversation,  // 190K tokens
  { role: 'user', content: 'I\'m having chest pain' }  // CRITICAL!
]

// After FIFO truncation
messages.shift()  // ‚ö†Ô∏è DELETES SAFETY GUIDELINES
messages = [
  ...diagnostic_conversation,  // System prompt GONE
  { role: 'user', content: 'I\'m having chest pain' }
]

// Model response WITHOUT safety guidelines:
// "Here are some home remedies for chest pain: rest, ibuprofen..."
// ‚ùå MEDICAL MALPRACTICE RISK
```

**Result**: Model loses awareness it must escalate chest pain. Patient receives dangerous advice instead of emergency escalation. **Sovereign Governance failure**.

---

**Option B (Middle-Out)**: ‚úÖ **CORRECT - Safety Preserved**
```typescript
// Middle-Out truncation preserves Index 0 + recent context
function safeMedicalTruncation(messages: Message[]): Message[] {
  const systemPrompt = messages[0]  // IMMUTABLE
  const recentMessages = messages.slice(-15)  // Last 15 turns (30K tokens)

  // Verify system prompt is medical guidelines
  if (!systemPrompt.content.includes('ESCALATE chest pain')) {
    throw new Error('CRITICAL: Medical safety guidelines missing')
  }

  return [
    systemPrompt,  // Index 0: Safety rules INTACT
    { role: 'system', content: '[... diagnostic history truncated for context limit ...]' },
    ...recentMessages  // Includes "I'm having chest pain"
  ]
}

// Model response WITH safety guidelines:
// "This is a medical emergency. Chest pain requires immediate evaluation.
//  I'm escalating this to your care team and recommending you call 911."
// ‚úÖ CORRECT ESCALATION
```

**Result**: Safety guidelines remain at Index 0. Model correctly identifies chest pain as emergency. Patient receives appropriate escalation. **95% context savings while maintaining 100% safety.**

**Cost-Safety Tradeoff**:
- Diagnostic history truncated: Loses some context (e.g., "patient mentioned occasional heartburn 30 mins ago")
- Safety rules preserved: Model knows to escalate life-threatening symptoms
- **Architect's decision**: Safety > Complete History

---

**Option C (Clear Cache/Fresh Session)**: ‚ö†Ô∏è **Wasteful but Safe**
```typescript
// Nuclear option: Start over
function clearSession(patientId: string): Message[] {
  // Save old session to database
  await archiveSession(patientId, oldMessages)

  // Start fresh with empty history
  return [
    { role: 'system', content: 'Medical Safety Guidelines...' },
    { role: 'user', content: 'I\'m having chest pain' }  // Lost 44 minutes of context
  ]
}
```

**Result**: Safety is maintained (system prompt is fresh), but **you've lost 45 minutes of diagnostic context**. The model no longer knows:
- Patient's medical history shared in this session
- Symptoms timeline
- Medications mentioned
- Previous Q&A that led to chest pain revelation

**Trade-off**: Safe but inefficient. Patient must repeat information. **Only use if truncation logic has failed and you need emergency recovery.**

---

**Option D (Increase Temperature)**: ‚ùå **Completely Wrong**
```typescript
// Misunderstanding of how LLMs work
const response = await callModel({
  temperature: 0.9,  // Higher randomness
  messages: truncatedMessages  // Still missing system prompt!
})
```

**Why this fails**:
- **Temperature doesn't restore lost context**‚Äîit only controls randomness of token selection
- If safety guidelines were truncated, no temperature adjustment can recover them
- Higher temperature might make responses MORE unpredictable, worsening safety
- **This reveals a fundamental misunderstanding of LLM architecture**

**Architect Insight**: Temperature, top_p, and other sampling parameters affect **output randomness**, not **memory retrieval**. Once context is truncated, it's gone.

---

### Architectural Lessons from This Scenario

**1. The Instruction Anchor is Non-Negotiable**
- **System prompts define behavior** (medical safety rules, HIPAA compliance, tone)
- **Index 0 must be immutable** across all truncation strategies
- **Unit test this**: Verify system prompt survives worst-case truncation

**2. Context Truncation is a Security Boundary**
- In medical/legal/financial systems, losing safety guidelines is a **regulatory violation**
- Architect must choose: Lose history (safe) vs. lose rules (catastrophic)
- **Always choose: Lose history**

**3. Production Pattern: Graduated Warnings**
```typescript
const CONTEXT_THRESHOLDS = {
  80: () => console.warn('Context at 80% - prepare truncation'),
  90: () => triggerBackgroundSummarization(),
  95: () => applyMiddleOutTruncation(),
  98: () => emergencySessionArchive()  // Nuclear option
}

function monitorContextUsage(messages: Message[]) {
  const usage = estimateTokens(messages) / MAX_TOKENS * 100

  for (const [threshold, action] of Object.entries(CONTEXT_THRESHOLDS)) {
    if (usage >= threshold) {
      action()
      break
    }
  }
}
```

**4. Real-World Implementation**
- **At 80%**: Log warning, consider summarization
- **At 90%**: Trigger background summarization (non-blocking)
- **At 95%**: Apply middle-out truncation (this scenario)
- **At 98%**: Emergency archive + fresh session

**Cost Analysis**:
- **Option A (FIFO)**: Free, but risk of $10M+ malpractice lawsuit
- **Option B (Middle-Out)**: Free, safe, loses some history
- **Option C (Clear session)**: Free, safe, loses ALL history (patient frustration)
- **Option D (Temperature)**: Irrelevant to the problem

**Final Answer**: **B** - Middle-Out truncation is the production-ready pattern for safety-critical systems.

</details>

---

## Key Takeaways

After completing this lesson, you should understand:

### Core Concepts
- **LLMs process text as tokens**, not words (~0.75 words per token for English)
- **Context windows** determine how much text an LLM can process at once (ranging from 128K to 1M tokens)
- **Transformers use attention mechanisms** to understand relationships between tokens
- **Next token prediction** is the fundamental operation behind text generation

### Model Selection
- **Choose models based on your specific needs**: Cost, latency, and quality all matter
- **Claude Haiku 4.5** for fast, cost-effective tasks (classification, simple queries)
- **Claude Sonnet 4.5** for balanced performance (general purpose, coding)
- **Claude Opus 4.5 / GPT-5** for complex reasoning, sophisticated agents, critical decisions
- **Model costs vary by 6x** - choosing the right model can dramatically reduce expenses

### Token Economics
- **Output tokens cost 5x more** than input tokens across all providers
- **Volume adds up quickly** - 1000 daily conversations = $500-3000/month depending on model choice
- **Optimization is critical**: Use caching, cheaper models for simple tasks, and context summarization
- **Always count tokens before deployment** to estimate real-world costs accurately

### Production Best Practices
- **Start with cheaper models** (Haiku 4.5) and upgrade only when needed
- **Implement caching** for frequently requested information
- **Summarize long conversations** to stay within context limits
- **Monitor token usage** in production to catch unexpected cost increases
- **Stream responses** to improve perceived latency for users
- **Test multiple models** to find the best quality/cost/speed balance for your use case

### Cost Comparison (2026)
For a typical customer support chatbot (1000 conversations/day, 5 messages each):
- **Claude Haiku 4.5**: ~$165/month
- **Claude Sonnet 4.5**: ~$495/month
- **Claude Opus 4.5**: ~$990/month

The right choice depends on whether you need simple responses (Haiku) or complex reasoning (Opus).

## Further Reading

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Original Transformers paper
- [Claude 4.5 Models Overview](https://platform.claude.com/docs/en/about-claude/models/overview) - Latest Claude models and capabilities
- [Anthropic API Pricing 2026](https://www.metacto.com/blogs/anthropic-api-pricing-a-full-breakdown-of-costs-and-integration) - Current Claude pricing
- [OpenAI GPT-5 Documentation](https://platform.openai.com/docs/models/gpt-5) - GPT-5 features and pricing
- [OpenAI Tokenizer](https://platform.openai.com/tokenizer) - Interactive token counting tool
