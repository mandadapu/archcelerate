import { CodePlayground } from '@/components/curriculum/CodePlayground'

# The LLM Blueprint: Tokenization Physics & Context Constraints

Moving beyond "next token prediction" to understand the architectural constraints that determine your system's reliability, cost, and precision.

> **Architect Perspective**: This isn't about how LLMs work‚Äîit's about the **physical limits** that constrain your design decisions. Think of tokens as your "unit of compute" and context windows as your "working memory budget."

---

## üè• Real-World Challenge: The Multi-Tier Health Triage

**The Problem**: A global telehealth platform receives 50,000 patient queries daily. Using Opus 4.5 for every request costs **$225,000/month**‚Äîfinancially unsustainable. But downgrading all queries to Haiku risks missing critical medical symptoms that require deep reasoning.

**Business Constraints**:
- **Budget**: Must reduce monthly AI costs to under $30,000 (80% reduction)
- **Safety**: Cannot compromise on medical accuracy‚Äî100% precision on critical queries
- **Latency**: Must maintain <3s P95 response time for good UX
- **Compliance**: HIPAA-compliant audit trail for all medical decisions

**Architectural Solution: The Model Cascade**

Implement intelligent query routing based on complexity classification:

```typescript
// Step 1: Fast classifier (Haiku 4.5) categorizes query in <500ms
const classification = await classifyQueryComplexity(patientQuery)
// ‚Üí "simple" (password reset), "complex" (drug interaction), or "critical" (chest pain)

// Step 2: Route to appropriate model based on classification
if (classification.type === 'simple' && classification.confidence > 0.90) {
  return await callHaiku(patientQuery)      // Fast & cheap: $0.001/query
} else if (classification.type === 'critical') {
  return await escalateToHuman(patientQuery) // Safety: Always HITL
} else {
  return await callOpus(patientQuery)       // Quality: $0.15/query
}
```

**Production Impact**:
- **Cost**: $225K ‚Üí $28K/month (87.5% reduction)
- **Latency**: 4.2s ‚Üí 1.8s P95 (57% faster) by routing 70% to Haiku
- **Safety**: 100% of critical queries escalated to human review (zero missed emergencies)
- **ROI**: 3-month implementation, $197K/month savings = **22-day payback period**

**Routing Distribution** (50K queries/day):
- 70% ‚Üí Haiku (administrative, simple questions): **$35K savings**
- 25% ‚Üí Opus (medical reasoning, drug interactions): **$162K spend**
- 5% ‚Üí Human review (emergencies, uncertain cases): **$0 AI cost**

**How This Connects to LLM Fundamentals**:
- **Tokenization Physics** ‚Üí Understanding that Haiku uses the same tokenizer as Opus enables seamless cascade without prompt rewriting
- **Context Constraints** ‚Üí Staying within 32K token sweet spot ensures high precision for medical queries
- **Model Selection Matrix** ‚Üí Matching the right model constraint (cost vs quality vs latency) to each query type

**[üëâ Lab: Build the Multi-Tier Triage System](/curriculum/week-1/labs/multi-tier-triage)**

In the hands-on lab, you'll implement the complete system: Haiku classifier, Opus escalation, confidence thresholds, cost tracking, and production validation showing 80%+ cost savings.

---

## Tokenization Physics: The Hidden Cost Layer

Every character you send to an LLM gets encoded into tokens. But **not all tokenizers are created equal**, and this directly impacts your prompt effectiveness and cost.

### Why Tokenization Matters

**Example: The tiktoken difference**
```typescript
// Claude's tokenizer (cl100k_base)
"Hello, world!" ‚Üí 3 tokens

// GPT-4o's tokenizer (o200k_base)
"Hello, world!" ‚Üí 2 tokens

// Why it matters:
const inputCost = tokenCount * pricePerToken
// Different tokenizers = different costs for the SAME prompt
```

### Architectural Constraint #1: Token Efficiency

**The Physics**:
- Code and technical terms fragment more than natural language
- Special characters (JSON, XML) consume extra tokens
- Non-English text often requires 2-3x more tokens

**Design Impact**:
```typescript
// ‚ùå Token-inefficient prompt (18 tokens)
"Please analyze this JSON: {...}"

// ‚úÖ Token-optimized (12 tokens)
"Analyze JSON: {...}"

// At 10M requests/month, this saves $200-500
```

## Context Density: Managing the Effective Context Window

**The Problem**: Models advertise "1M token windows" but lose precision as you approach the limit. This is called the **"Lost in the Middle"** phenomenon.

### The Precision Falloff Curve

```typescript
// Effective Context Utilization (Claude Sonnet 4.5)
interface ContextReliability {
  '0-32K tokens': 95-100%    // ‚úÖ High precision
  '32K-128K tokens': 85-95%  // ‚ö†Ô∏è  Slight degradation
  '128K-500K tokens': 70-85% // ‚ö†Ô∏è  Noticeable precision loss
  '500K-1M tokens': 50-70%   // ‚ùå "Lost in the Middle"
}
```

### Architectural Constraint #2: Context Window Strategy

**Design Pattern: The Context Budget**
```typescript
const CONTEXT_BUDGET = {
  systemPrompt: 2000,      // Fixed instructions
  fewShot: 5000,           // Example demonstrations
  userContext: 20000,      // Dynamic user data
  buffer: 3000,            // Safety margin
  maxOutput: 4000          // Response limit
} // Total: 34K tokens (sweet spot for reliability)
```

**Anti-Pattern**: Stuffing the entire 200K window
```typescript
// ‚ùå Architect's mistake
const prompt = `
  ${systemInstructions} +    // 2K tokens
  ${allCustomerHistory} +    // 180K tokens (!)
  ${currentQuery}            // 1K tokens
`
// Result: Model "loses" critical details from history
```

**‚úÖ Production Pattern**: Sliding window with summarization
```typescript
const prompt = `
  ${systemInstructions} +         // 2K
  ${summarizedHistory} +          // 8K (condensed)
  ${recentContextFull} +          // 15K (last 10 turns)
  ${currentQuery}                 // 1K
` // Total: 26K tokens ‚Üí stays in high-precision zone
```

### Measuring Context Density: The Information-per-Token Ratio

**Architectural Tool**: Calculate the "information density" of your context to predict precision degradation.

```typescript
interface ContextAnalysis {
  totalTokens: number
  informationChunks: number  // Distinct pieces of information
  densityScore: number       // 0-1, higher is better
  recommendation: string
}

function analyzeContextDensity(
  context: string,
  informationChunks: number
): ContextAnalysis {
  // Estimate token count (rough: 4 chars per token)
  const totalTokens = Math.ceil(context.length / 4)

  // Information density: how much useful info per token?
  // Optimal: 0.15-0.25 (15-25% of tokens carry key information)
  const densityScore = informationChunks / totalTokens

  let recommendation: string
  if (densityScore > 0.25) {
    recommendation = '‚úÖ Excellent density - all information is relevant'
  } else if (densityScore >= 0.15) {
    recommendation = '‚úÖ Good density - optimal for precision'
  } else if (densityScore >= 0.10) {
    recommendation = '‚ö†Ô∏è Moderate density - consider summarizing'
  } else {
    recommendation = '‚ùå Poor density - high risk of Lost-in-the-Middle. Aggressive pruning needed.'
  }

  return {
    totalTokens,
    informationChunks,
    densityScore,
    recommendation
  }
}

// Example: Customer support context
const customerHistory = `
  [2024-01-10] User asked about pricing
  [2024-01-12] User requested trial extension
  [2024-01-15] User reported bug in dashboard
  [2024-01-20] User escalated to premium support
  [2024-01-22] Bug fixed, user satisfied
` // 5 key information chunks

const analysis = analyzeContextDensity(customerHistory, 5)
console.log(analysis)
// {
//   totalTokens: 58,
//   informationChunks: 5,
//   densityScore: 0.086,
//   recommendation: '‚ùå Poor density - high risk of Lost-in-the-Middle...'
// }

// Solution: Condense to higher density
const condensed = `
Customer: trial ‚Üí bug ‚Üí escalation ‚Üí resolved ‚Üí satisfied
Critical: Dashboard bug (ID #1234) fixed on 2024-01-22
` // Same 5 chunks, fewer tokens

const improvedAnalysis = analyzeContextDensity(condensed, 5)
// {
//   totalTokens: 23,
//   informationChunks: 5,
//   densityScore: 0.217,  // ‚úÖ In optimal range
//   recommendation: '‚úÖ Good density - optimal for precision'
// }
```

**Production Pattern: Pre-flight Context Check**
```typescript
async function buildPromptWithDensityCheck(
  systemPrompt: string,
  userContext: string,
  query: string
): Promise<string> {
  // Estimate information chunks (simplified: count sentences)
  const chunks = userContext.split(/[.!?]/).filter(s => s.trim().length > 10).length

  const analysis = analyzeContextDensity(userContext, chunks)

  if (analysis.densityScore < 0.10) {
    console.warn('‚ö†Ô∏è Low context density detected. Summarizing...')
    // Trigger summarization pipeline
    userContext = await summarizeContext(userContext, chunks)
  }

  return `${systemPrompt}\n\nContext:\n${userContext}\n\nQuery: ${query}`
}
```

**Architectural Insight**: Context density inversely correlates with "Lost-in-the-Middle" risk:
- **High density (>0.20)**: Information-packed, minimal fluff ‚Üí model stays focused
- **Low density (<0.10)**: Too much prose, key facts buried ‚Üí model loses precision

**Cost Impact**: Improving density from 0.08 to 0.20 means 60% token reduction ‚Üí 60% cost savings while improving accuracy.

---

## The Model Selection Matrix: Architectural Decision Framework

**Architect Perspective**: Model selection isn't about "which is best"‚Äîit's about **matching the constraint** that matters most for your specific use case.

### The Three-Axis Constraint Model

| Model | Input Cost | Output Cost | Latency (p95) | Quality Tier | Architectural Use Case |
|-------|-----------|-------------|---------------|--------------|------------------------|
| **Claude Opus 4.5** | $15/MTok | $75/MTok | 4-8s | Tier 1 | Critical reasoning, high-stakes decisions, complex agents |
| **GPT-5** | $10/MTok | $40/MTok | 3-6s | Tier 1 | Deep analytical tasks, strategic planning |
| **Claude Sonnet 4.5** | $3/MTok | $15/MTok | 1-3s | Tier 1 | Balanced workhorse, coding, production agents |
| **GPT-4o** | $2.50/MTok | $10/MTok | 1-2s | Tier 2 | General-purpose tasks, multimodal processing |
| **Claude Haiku 4.5** | $0.25/MTok | $1.25/MTok | 0.3-0.8s | Tier 2 | High-volume classification, fast inference |

> **Cost Reality Check**: At 1M requests/month with 1K input + 500 output tokens:
> - Haiku: $875/month
> - Sonnet: $10,500/month (12x more)
> - Opus: $41,250/month (47x more)

### Architectural Decision Tree

```typescript
function selectModel(useCase: UseCase): Model {
  // Constraint #1: Latency Requirements
  if (useCase.requiresRealTime && useCase.p95Latency < 1000ms) {
    return 'claude-haiku-4.5' // Only option for sub-second
  }

  // Constraint #2: Cost Budget
  if (useCase.requestVolume > 1_000_000 && useCase.budgetPerRequest < 0.01) {
    return 'claude-haiku-4.5' // Economic constraint forces choice
  }

  // Constraint #3: Quality Floor
  if (useCase.requiresCriticalReasoning || useCase.errorTolerance < 0.01) {
    return 'claude-opus-4.5' // Quality constraint overrides cost
  }

  // Default: Balanced choice
  return 'claude-sonnet-4.5'
}
```

### Real-World Architecture Patterns

**Pattern 1: The Cascade Strategy**
```typescript
// High-quality with cost optimization
async function processWithCascade(input: string) {
  try {
    // Try fast + cheap first
    const result = await callHaiku(input)
    if (result.confidence > 0.9) return result

    // Fall back to quality if uncertain
    return await callSonnet(input)
  } catch {
    // Ultimate fallback for high-stakes
    return await callOpus(input)
  }
}
// Cost impact: 70% requests use Haiku, 25% Sonnet, 5% Opus
// Average cost: ~$2.50/MTok vs. $15/MTok (6x savings)
```

**Pattern 2: The Tiered Quality System**
```typescript
// Different models for different users
const MODEL_BY_TIER = {
  free: 'claude-haiku-4.5',      // Fast, cheap
  pro: 'claude-sonnet-4.5',      // Balanced
  enterprise: 'claude-opus-4.5'  // Best quality
}
```

**Decision Framework:**
1. **Start with cost optimization** - Use Haiku 4.5 as your baseline
2. **Test quality** - If accuracy is insufficient, upgrade to Sonnet 4.5
3. **Evaluate latency** - If responses are too slow for UX, consider Haiku or optimize prompts
4. **Reserve premium models** - Only use Opus/GPT-5 when quality justifies the 3-6x cost increase

## Token Economics

Understanding token costs is critical for production AI systems:

**Example**: Chat application with 1000 daily users
- Average conversation: 10 messages
- Average message: 100 tokens input, 200 tokens output
- Total daily: 1000 users √ó 10 msgs √ó (100 input + 200 output) = 1M input + 2M output/day
- Monthly cost (Claude Sonnet 4.5): (1M √ó $3 + 2M √ó $15) / 1M √ó 30 = $33/day = $990/month

**Optimization strategies**:
1. Use cheaper models for simple tasks
2. Implement caching for common queries
3. Summarize long conversations to reduce context
4. Stream responses to improve perceived latency

## Context Management

Effective context window management:

1. **Prioritize recent messages**: Keep last N messages in full context
2. **Summarize old context**: Compress older messages into summary
3. **Remove irrelevant content**: Strip formatting, redundant info
4. **Smart truncation**: Cut from middle, keep beginning and end

## Practical Exercises

### Exercise 1: Token Counting

**Goal**: Learn how to count tokens in different types of text using the Anthropic API.

**Why This Matters**: Token counting is critical for:
- Estimating costs before making API calls
- Ensuring prompts fit within context windows
- Optimizing prompt efficiency

**Try It Yourself**: Click "Run Code" to execute this token counting example. You can edit the code to test your own text!

<CodePlayground
  title="Interactive Token Counting"
  description="Edit the test cases below and click Run to see how different types of text are tokenized."
  exerciseType="token-counting"
  code={`import Anthropic from '@anthropic-ai/sdk'

const anthropic = new Anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY
})

async function countTokens(text: string): Promise<number> {
  const response = await anthropic.messages.countTokens({
    model: 'claude-sonnet-4-5-20251101',
    messages: [{ role: 'user', content: text }]
  })
  return response.input_tokens
}

// Test with different text types
async function runTokenCountingExercise() {
  const testCases = [
    {
      name: 'Short message',
      text: 'Hello, how are you?'
    },
    {
      name: 'Code snippet',
      text: \`function fibonacci(n) {
  if (n <= 1) return n
  return fibonacci(n - 1) + fibonacci(n - 2)
}\`
    },
    {
      name: 'Markdown documentation',
      text: \`# API Documentation

## Authentication

All API requests require authentication using an API key:

\\\`\\\`\\\`bash
curl -H "Authorization: Bearer YOUR_API_KEY" https://api.example.com/data
\\\`\\\`\\\`

## Rate Limits

- Free tier: 100 requests/hour
- Pro tier: 1000 requests/hour\`
    },
    {
      name: 'JSON data',
      text: JSON.stringify({
        users: [
          { id: 1, name: 'Alice', email: 'alice@example.com' },
          { id: 2, name: 'Bob', email: 'bob@example.com' }
        ]
      }, null, 2)
    }
  ]

  console.log('Token Counting Results:\\n')

  for (const testCase of testCases) {
    const tokens = await countTokens(testCase.text)
    const charCount = testCase.text.length
    const tokensPerChar = (tokens / charCount).toFixed(3)

    console.log(\`\${testCase.name}:\`)
    console.log(\`  Characters: \${charCount}\`)
    console.log(\`  Tokens: \${tokens}\`)
    console.log(\`  Tokens/Char: \${tokensPerChar}\`)
    console.log(\`  Cost (input): $\${(tokens / 1000 * 0.003).toFixed(6)}\`)
    console.log()
  }
}

runTokenCountingExercise()`}
/>

**Key Insights You'll Discover**:
- Code typically uses more tokens per character than plain text
- JSON/structured data is relatively token-efficient
- Whitespace and formatting affect token count

### Exercise 2: Cost Calculation

**Goal**: Calculate the actual cost of running a real AI application.

**Scenario**: You're building a customer support chatbot that:
- Receives 1000 conversations per day
- Each conversation has 5 messages on average
- Each message is ~100 tokens input, ~200 tokens output

**Try It Yourself**: Click "Run Code" to see cost estimates for different AI application scenarios. You can edit the scenarios to match your use case!

<CodePlayground
  title="Interactive Cost Calculator"
  description="Edit the scenarios below to calculate costs for your AI application. Try changing conversation volumes, message lengths, or models."
  exerciseType="cost-calculation"
  code={`interface CostEstimate {
  scenario: string
  dailyConversations: number
  messagesPerConversation: number
  avgInputTokens: number
  avgOutputTokens: number
  model: string
  inputCostPerMillion: number
  outputCostPerMillion: number
}

function calculateMonthlyCost(estimate: CostEstimate) {
  // Calculate total tokens
  const dailyMessages = estimate.dailyConversations * estimate.messagesPerConversation
  const dailyInputTokens = dailyMessages * estimate.avgInputTokens
  const dailyOutputTokens = dailyMessages * estimate.avgOutputTokens

  // Calculate daily cost
  const dailyInputCost = (dailyInputTokens / 1_000_000) * estimate.inputCostPerMillion
  const dailyOutputCost = (dailyOutputTokens / 1_000_000) * estimate.outputCostPerMillion
  const dailyCost = dailyInputCost + dailyOutputCost

  // Calculate monthly cost (30 days)
  const monthlyCost = dailyCost * 30

  return {
    dailyMetrics: {
      conversations: estimate.dailyConversations,
      messages: dailyMessages,
      inputTokens: dailyInputTokens,
      outputTokens: dailyOutputTokens,
      cost: dailyCost
    },
    monthlyMetrics: {
      conversations: estimate.dailyConversations * 30,
      messages: dailyMessages * 30,
      inputTokens: dailyInputTokens * 30,
      outputTokens: dailyOutputTokens * 30,
      cost: monthlyCost
    },
    costPerConversation: dailyCost / estimate.dailyConversations,
    breakdown: {
      inputCost: dailyInputCost,
      outputCost: dailyOutputCost,
      inputPercentage: (dailyInputCost / dailyCost * 100).toFixed(1) + '%',
      outputPercentage: (dailyOutputCost / dailyCost * 100).toFixed(1) + '%'
    }
  }
}

// Run the exercise
const scenarios: CostEstimate[] = [
  {
    scenario: 'Chat Application (1K users/day)',
    dailyConversations: 1000,
    messagesPerConversation: 10,
    avgInputTokens: 100,
    avgOutputTokens: 200,
    model: 'claude-sonnet-4-5-20251101',
    inputCostPerMillion: 3,
    outputCostPerMillion: 15
  },
  {
    scenario: 'Code Assistant (100 requests/day)',
    dailyConversations: 100,
    messagesPerConversation: 5,
    avgInputTokens: 500,
    avgOutputTokens: 1000,
    model: 'claude-sonnet-4-5-20251101',
    inputCostPerMillion: 3,
    outputCostPerMillion: 15
  },
  {
    scenario: 'Content Generation (500 articles/day)',
    dailyConversations: 500,
    messagesPerConversation: 4,
    avgInputTokens: 1000,
    avgOutputTokens: 2500,
    model: 'claude-sonnet-4-5-20251101',
    inputCostPerMillion: 3,
    outputCostPerMillion: 15
  }
]

console.log('Cost Estimation Results:\\n')

for (const scenario of scenarios) {
  const results = calculateMonthlyCost(scenario)

  console.log(\`\${scenario.scenario}:\`)
  console.log(\`  Daily: \${results.dailyMetrics.conversations.toLocaleString()} conversations\`)
  console.log(\`  Daily Cost: $\${results.dailyMetrics.cost.toFixed(2)}\`)
  console.log(\`  Monthly Cost: $\${results.monthlyMetrics.cost.toFixed(2)}\`)
  console.log(\`  Cost per Conversation: $\${results.costPerConversation.toFixed(4)}\`)
  console.log(\`  Breakdown: \${results.breakdown.inputPercentage} input, \${results.breakdown.outputPercentage} output\`)
  console.log()
}`}
/>

**Key Insights**:
- Output tokens cost more than input tokens (typically 5x more)
- Model choice dramatically affects cost (Haiku 4.5 vs Opus 4.5: ~6x difference)
- Volume adds up quickly - optimize for high-traffic applications
- The Claude 4.5 series offers 67% cost reduction over previous generations

### Exercise 3: Model Comparison

**Goal**: Compare different models on the same task to understand quality/speed/cost tradeoffs.

**Try It Yourself**: Click "Run Code" to compare Claude Haiku and Sonnet on the same task. Watch how they differ in speed, cost, and response quality!

<CodePlayground
  title="Interactive Model Comparison"
  description="Compare Claude models on the same task. The demo uses Haiku (fast/cheap) vs Sonnet (balanced) to show real tradeoffs."
  exerciseType="model-comparison"
  code={`import Anthropic from '@anthropic-ai/sdk'

const anthropic = new Anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY
})

interface ModelConfig {
  name: string
  model: string
  inputPrice: number  // per million tokens
  outputPrice: number
}

interface ComparisonResult {
  model: string
  response: string
  latency: number
  inputTokens: number
  outputTokens: number
  cost: number
}

async function compareModels(prompt: string): Promise<ComparisonResult[]> {
  const models: ModelConfig[] = [
    {
      name: 'Claude Haiku 4.5',
      model: 'claude-3-haiku-20240307',
      inputPrice: 1,
      outputPrice: 5
    },
    {
      name: 'Claude Sonnet 4.5',
      model: 'claude-3-5-sonnet-20240620',
      inputPrice: 3,
      outputPrice: 15
    }
  ]

  const results: ComparisonResult[] = []

  for (const modelConfig of models) {
    const start = Date.now()

    const response = await anthropic.messages.create({
      model: modelConfig.model,
      max_tokens: 150,
      messages: [{ role: 'user', content: prompt }]
    })

    const latency = Date.now() - start
    const cost = (
      response.usage.input_tokens * modelConfig.inputPrice +
      response.usage.output_tokens * modelConfig.outputPrice
    ) / 1_000_000

    results.push({
      model: modelConfig.name,
      response: response.content[0].text,
      latency,
      inputTokens: response.usage.input_tokens,
      outputTokens: response.usage.output_tokens,
      cost
    })
  }

  return results
}

// Run comparison
async function runModelComparison() {
  const testPrompt = 'Explain API rate limiting in 2 sentences.'

  console.log('Model Comparison Results:\\n')
  console.log(\`Prompt: "\${testPrompt}"\\n\`)

  const results = await compareModels(testPrompt)

  // Display comparison table
  console.log('Metrics:')
  console.log('‚îÄ'.repeat(80))

  for (const result of results) {
    console.log(\`\${result.model}:\`)
    console.log(\`  Latency: \${result.latency}ms\`)
    console.log(\`  Tokens: \${result.inputTokens} in / \${result.outputTokens} out\`)
    console.log(\`  Cost: $\${result.cost.toFixed(6)}\`)
    console.log(\`  Response: \${result.response.substring(0, 100)}...\`)
    console.log()
  }

  console.log('‚îÄ'.repeat(80))
  console.log('\\nKey Insights:')
  console.log('  ‚Ä¢ Haiku is faster and cheaper for simple tasks')
  console.log('  ‚Ä¢ Sonnet provides more detailed responses')
  console.log('  ‚Ä¢ Cost difference: ~3x')
  console.log('  ‚Ä¢ Choose based on your priorities!')
}

runModelComparison()`}
/>

**Key Insights You'll Discover**:
- Latency varies significantly between models (1.6-3.1 seconds for typical requests)
- Cost can vary by 6x between models (Haiku 4.5 vs Opus 4.5)
- Cheaper/faster models like Haiku 4.5 now match previous-gen flagship performance
- GPT-5 offers competitive pricing for sophisticated reasoning tasks
- Balance quality, speed, and cost based on your use case

**Your Turn**: Try these experiments:
1. Test with different prompt complexities (simple vs complex reasoning)
2. Compare the same model with different max_tokens settings
3. Test multilingual prompts to see if token counts change

## Key Takeaways

After completing this lesson, you should understand:

### Core Concepts
- **LLMs process text as tokens**, not words (~0.75 words per token for English)
- **Context windows** determine how much text an LLM can process at once (ranging from 128K to 1M tokens)
- **Transformers use attention mechanisms** to understand relationships between tokens
- **Next token prediction** is the fundamental operation behind text generation

### Model Selection
- **Choose models based on your specific needs**: Cost, latency, and quality all matter
- **Claude Haiku 4.5** for fast, cost-effective tasks (classification, simple queries)
- **Claude Sonnet 4.5** for balanced performance (general purpose, coding)
- **Claude Opus 4.5 / GPT-5** for complex reasoning, sophisticated agents, critical decisions
- **Model costs vary by 6x** - choosing the right model can dramatically reduce expenses

### Token Economics
- **Output tokens cost 5x more** than input tokens across all providers
- **Volume adds up quickly** - 1000 daily conversations = $500-3000/month depending on model choice
- **Optimization is critical**: Use caching, cheaper models for simple tasks, and context summarization
- **Always count tokens before deployment** to estimate real-world costs accurately

### Production Best Practices
- **Start with cheaper models** (Haiku 4.5) and upgrade only when needed
- **Implement caching** for frequently requested information
- **Summarize long conversations** to stay within context limits
- **Monitor token usage** in production to catch unexpected cost increases
- **Stream responses** to improve perceived latency for users
- **Test multiple models** to find the best quality/cost/speed balance for your use case

### Cost Comparison (2026)
For a typical customer support chatbot (1000 conversations/day, 5 messages each):
- **Claude Haiku 4.5**: ~$165/month
- **Claude Sonnet 4.5**: ~$495/month
- **Claude Opus 4.5**: ~$990/month

The right choice depends on whether you need simple responses (Haiku) or complex reasoning (Opus).

## Further Reading

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Original Transformers paper
- [Claude 4.5 Models Overview](https://platform.claude.com/docs/en/about-claude/models/overview) - Latest Claude models and capabilities
- [Anthropic API Pricing 2026](https://www.metacto.com/blogs/anthropic-api-pricing-a-full-breakdown-of-costs-and-integration) - Current Claude pricing
- [OpenAI GPT-5 Documentation](https://platform.openai.com/docs/models/gpt-5) - GPT-5 features and pricing
- [OpenAI Tokenizer](https://platform.openai.com/tokenizer) - Interactive token counting tool
