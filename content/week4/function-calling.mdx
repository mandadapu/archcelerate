---
title: 'Function Calling & Tool Use'
description: 'Transform LLMs from text generators to orchestrators that call your APIs'
estimatedTime: 45
difficulty: 'intermediate'
objectives:
  - Understand the function calling loop (request â†’ intent â†’ execution â†’ response)
  - Define tools that LLMs can reliably invoke
  - Handle tool execution and error cases
  - Build multi-step tool orchestration
---

# Function Calling & Tool Use

## The Fundamental Insight

**Critical misunderstanding**: The LLM does NOT execute functions. It **requests** function calls.

Your code executes the function, then feeds the result back to the LLM.

---

## The Architecture: The Function Calling Loop

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. USER REQUEST                                             â”‚
â”‚    "What's the weather in London and should I bring an      â”‚
â”‚     umbrella?"                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. LLM INTENT (Tool Call Request)                           â”‚
â”‚    {                                                        â”‚
â”‚      "tool": "get_weather",                                 â”‚
â”‚      "arguments": {                                         â”‚
â”‚        "location": "London",                                â”‚
â”‚        "unit": "celsius"                                    â”‚
â”‚      }                                                      â”‚
â”‚    }                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. YOUR CODE EXECUTES (Not the LLM!)                        â”‚
â”‚    const result = await getWeather('London', 'celsius')     â”‚
â”‚    // Returns: { temp: 12, conditions: 'rainy',            â”‚
â”‚    //            humidity: 85 }                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. FEED RESULT BACK TO LLM                                  â”‚
â”‚    Tool result: { temp: 12, conditions: 'rainy',           â”‚
â”‚                   humidity: 85 }                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. FINAL RESPONSE (LLM synthesizes answer)                  â”‚
â”‚    "It's 12Â°C and rainy in London with 85% humidity.       â”‚
â”‚     Yes, you should definitely bring an umbrella!"         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key insight**: The LLM is the **orchestrator**, not the executor. It decides WHAT to call and WHEN, but your code does the execution.

---

## Basic Function Calling Pattern

### Step 1: Define Your Tools

```typescript
import Anthropic from '@anthropic-ai/sdk'

const anthropic = new Anthropic({ apiKey: process.env.ANTHROPIC_API_KEY })

// Define available tools (functions LLM can request)
const tools: Anthropic.Tool[] = [
  {
    name: 'get_weather',
    description: 'Get current weather for a specific location. Use this whenever the user asks about weather, temperature, or conditions.',
    input_schema: {
      type: 'object',
      properties: {
        location: {
          type: 'string',
          description: 'City name, e.g. "London", "San Francisco", "Tokyo"'
        },
        unit: {
          type: 'string',
          enum: ['celsius', 'fahrenheit'],
          description: 'Temperature unit. Use celsius for international cities, fahrenheit for US cities.'
        }
      },
      required: ['location', 'unit']
    }
  }
]
```

**Critical elements**:
- **name**: Function identifier (snake_case)
- **description**: When to use this tool (be specific!)
- **input_schema**: JSON Schema defining parameters
- **required**: Which parameters are mandatory

### Step 2: Implement the Tool Function

```typescript
interface WeatherResult {
  temperature: number
  unit: string
  conditions: string
  humidity: number
  location: string
}

// Your actual implementation (calls weather API, database, etc.)
async function getWeather(location: string, unit: 'celsius' | 'fahrenheit'): Promise<WeatherResult> {
  // In production, this would call a real weather API
  const response = await fetch(
    `https://api.weatherapi.com/v1/current.json?key=${process.env.WEATHER_API_KEY}&q=${location}`
  )

  if (!response.ok) {
    throw new Error(`Weather API error: ${response.statusText}`)
  }

  const data = await response.json()

  return {
    temperature: unit === 'celsius' ? data.current.temp_c : data.current.temp_f,
    unit,
    conditions: data.current.condition.text,
    humidity: data.current.humidity,
    location: data.location.name
  }
}
```

### Step 3: The Orchestration Loop

```typescript
async function chat(userMessage: string): Promise<string> {
  const messages: Anthropic.MessageParam[] = [
    { role: 'user', content: userMessage }
  ]

  let response = await anthropic.messages.create({
    model: 'claude-3-5-sonnet-20240620',
    max_tokens: 1024,
    tools,
    messages
  })

  // Loop until LLM stops requesting tools
  while (response.stop_reason === 'tool_use') {
    // Extract tool use requests
    const toolUses = response.content.filter(
      (block): block is Anthropic.ToolUseBlock => block.type === 'tool_use'
    )

    // Execute each tool
    const toolResults: Anthropic.ToolResultBlockParam[] = []

    for (const toolUse of toolUses) {
      console.log(`ğŸ”§ LLM requested tool: ${toolUse.name}`)
      console.log(`   Arguments:`, toolUse.input)

      try {
        let result: any

        // Route to appropriate function
        if (toolUse.name === 'get_weather') {
          result = await getWeather(
            toolUse.input.location,
            toolUse.input.unit
          )
        } else {
          throw new Error(`Unknown tool: ${toolUse.name}`)
        }

        console.log(`âœ… Tool result:`, result)

        toolResults.push({
          type: 'tool_result',
          tool_use_id: toolUse.id,
          content: JSON.stringify(result)
        })
      } catch (error) {
        console.error(`âŒ Tool execution failed:`, error)

        toolResults.push({
          type: 'tool_result',
          tool_use_id: toolUse.id,
          content: JSON.stringify({
            error: error instanceof Error ? error.message : 'Unknown error'
          }),
          is_error: true
        })
      }
    }

    // Add assistant's response and tool results to conversation
    messages.push(
      { role: 'assistant', content: response.content },
      { role: 'user', content: toolResults }
    )

    // Continue conversation with tool results
    response = await anthropic.messages.create({
      model: 'claude-3-5-sonnet-20240620',
      max_tokens: 1024,
      tools,
      messages
    })
  }

  // Extract final text response
  const textBlock = response.content.find(
    (block): block is Anthropic.TextBlock => block.type === 'text'
  )

  return textBlock?.text || 'No response generated'
}

// Usage
const answer = await chat('What\'s the weather in London? Should I bring an umbrella?')
console.log(answer)
// "It's 12Â°C and rainy in London with 85% humidity. Yes, you should definitely bring an umbrella!"
```

---

## Multi-Tool Orchestration

The real power: LLMs can chain multiple tools to accomplish complex tasks.

```typescript
const tools: Anthropic.Tool[] = [
  {
    name: 'check_inventory',
    description: 'Check if a product is in stock at a specific warehouse',
    input_schema: {
      type: 'object',
      properties: {
        product_id: {
          type: 'string',
          description: 'Product SKU or ID'
        },
        warehouse: {
          type: 'string',
          enum: ['US_WEST', 'US_EAST', 'EU', 'ASIA'],
          description: 'Warehouse location code'
        }
      },
      required: ['product_id', 'warehouse']
    }
  },
  {
    name: 'create_shipment',
    description: 'Create a shipment order for a product. Only call this AFTER checking inventory.',
    input_schema: {
      type: 'object',
      properties: {
        product_id: { type: 'string' },
        quantity: { type: 'integer', minimum: 1 },
        warehouse: { type: 'string', enum: ['US_WEST', 'US_EAST', 'EU', 'ASIA'] },
        destination: { type: 'string', description: 'Shipping address' },
        priority: { type: 'string', enum: ['standard', 'express', 'overnight'] }
      },
      required: ['product_id', 'quantity', 'warehouse', 'destination', 'priority']
    }
  },
  {
    name: 'notify_customer',
    description: 'Send email notification to customer about order status',
    input_schema: {
      type: 'object',
      properties: {
        customer_email: { type: 'string', format: 'email' },
        message: { type: 'string' },
        order_id: { type: 'string' }
      },
      required: ['customer_email', 'message']
    }
  }
]

// User: "Ship 5 units of SKU-12345 from US_WEST to 123 Main St, express, and notify john@example.com"

// LLM will automatically:
// 1. Call check_inventory(product_id='SKU-12345', warehouse='US_WEST')
// 2. If in stock, call create_shipment(...)
// 3. Then call notify_customer(customer_email='john@example.com', ...)
// 4. Summarize: "I've checked inventory (5 units available), created an express shipment, and notified John."
```

**The LLM orchestrates the sequence** based on the tool descriptions and conversation context.

---

## Designing LLM-Ready Tool Specs

### Rule: If an intern can't understand it, the LLM can't either

**âŒ Bad tool description**:
```typescript
{
  name: 'process',
  description: 'Processes data',
  input_schema: {
    type: 'object',
    properties: {
      data: { type: 'string' },
      mode: { type: 'string' }
    }
  }
}
```

**Problems**:
- "process" is vague - process how?
- "data" could be anything
- "mode" has no constraints - LLM will hallucinate values

**âœ… Good tool description**:
```typescript
{
  name: 'calculate_shipping_cost',
  description: 'Calculate shipping cost for a package based on weight, dimensions, and destination. Use this when the user asks about shipping rates, delivery costs, or wants a quote.',
  input_schema: {
    type: 'object',
    properties: {
      weight_kg: {
        type: 'number',
        description: 'Package weight in kilograms',
        minimum: 0.1,
        maximum: 500
      },
      destination_country: {
        type: 'string',
        enum: ['US', 'CA', 'UK', 'DE', 'FR', 'JP', 'AU'],
        description: 'Two-letter ISO country code for destination'
      },
      service_level: {
        type: 'string',
        enum: ['standard', 'express', 'overnight'],
        description: 'Shipping speed. standard = 5-7 days, express = 2-3 days, overnight = next day'
      }
    },
    required: ['weight_kg', 'destination_country', 'service_level']
  }
}
```

**Why this works**:
- âœ… Specific name describes exactly what it does
- âœ… Description explains WHEN to use it
- âœ… Enums prevent hallucinated values
- âœ… Constraints (min/max) prevent invalid inputs
- âœ… Units are explicit (kg, not "weight")

### Best Practices for Tool Definitions

| Do | Don't |
|----|-------|
| Use **enums** for categorical parameters | Use free-form strings ("any color") |
| Add **min/max** constraints to numbers | Leave numbers unbounded |
| Specify **units** (kg, USD, meters) | Assume units ("weight", "price") |
| Explain **when to use** the tool | Just describe what it does |
| Use **descriptive names** (calculate_tax) | Use generic names (process, handle) |
| Mark parameters as **required** | Make everything optional |
| Provide **examples** in descriptions | Leave descriptions vague |

---

## Production Patterns

### Pattern 1: Tool Execution with Retry

```typescript
async function executeToolWithRetry(
  toolName: string,
  args: any,
  maxRetries: number = 3
): Promise<any> {
  let lastError: Error | null = null

  for (let attempt = 1; attempt &lt;= maxRetries; attempt++) {
    try {
      console.log(`Attempt ${attempt}/${maxRetries} for ${toolName}`)

      // Route to function
      switch (toolName) {
        case 'get_weather':
          return await getWeather(args.location, args.unit)

        case 'check_inventory':
          return await checkInventory(args.product_id, args.warehouse)

        default:
          throw new Error(`Unknown tool: ${toolName}`)
      }
    } catch (error) {
      lastError = error instanceof Error ? error : new Error(String(error))
      console.error(`Attempt ${attempt} failed:`, lastError.message)

      if (attempt < maxRetries) {
        // Exponential backoff
        await new Promise(resolve => setTimeout(resolve, Math.pow(2, attempt) * 1000))
      }
    }
  }

  throw lastError
}
```

### Pattern 2: Tool Authorization

```typescript
interface ToolContext {
  userId: string
  permissions: string[]
}

function checkToolAuthorization(toolName: string, context: ToolContext): void {
  const requiredPermissions: Record<string, string> = {
    'create_shipment': 'shipments.create',
    'refund_customer': 'refunds.create',
    'delete_data': 'data.delete'
  }

  const required = requiredPermissions[toolName]

  if (required && !context.permissions.includes(required)) {
    throw new Error(`Unauthorized: User ${context.userId} lacks permission ${required}`)
  }
}

// Usage in tool execution
async function executeToolSafely(
  toolName: string,
  args: any,
  context: ToolContext
): Promise<any> {
  // 1. Authorization check
  checkToolAuthorization(toolName, context)

  // 2. Input validation
  const validator = toolValidators[toolName]
  const validatedArgs = validator.parse(args) // Zod validation

  // 3. Execute
  return await executeTool(toolName, validatedArgs)
}
```

### Pattern 3: Tool Observability

```typescript
interface ToolMetrics {
  toolName: string
  duration: number
  success: boolean
  error?: string
  timestamp: Date
}

async function executeToolWithMetrics(
  toolName: string,
  args: any
): Promise<any> {
  const startTime = Date.now()
  let success = false
  let error: string | undefined

  try {
    const result = await executeTool(toolName, args)
    success = true
    return result
  } catch (e) {
    error = e instanceof Error ? e.message : String(e)
    throw e
  } finally {
    const duration = Date.now() - startTime

    // Log metrics
    await logMetrics({
      toolName,
      duration,
      success,
      error,
      timestamp: new Date()
    })

    // Alert on failures
    if (!success && duration &gt; 5000) {
      await sendAlert({
        type: 'tool_failure',
        message: `Tool ${toolName} failed after ${duration}ms: ${error}`
      })
    }
  }
}
```

### Pattern 4: Intent Classification & Routing

**The Problem**: Sending all 50 tools to the LLM on every request wastes tokens ($$$) and confuses the model.

**The Solution**: Use a fast "router" model to classify intent, then send only relevant tools.

```typescript
interface IntentClassification {
  intent: string
  confidence: number
  relevantTools: string[]
  reasoning: string
}

/**
 * Two-tier architecture: Fast router â†’ Specialized handler
 */
class IntentRouter {
  private intentCatalog: Map<string, string[]>

  constructor() {
    // Map intents to relevant tool sets
    this.intentCatalog = new Map([
      ['order_management', ['create_order', 'cancel_order', 'track_order', 'modify_order']],
      ['inventory_lookup', ['check_inventory', 'search_products', 'get_product_details']],
      ['customer_support', ['create_ticket', 'search_knowledge_base', 'escalate_to_human']],
      ['account_management', ['update_profile', 'change_password', 'view_billing']],
      ['reporting', ['generate_report', 'export_data', 'query_analytics']]
    ])
  }

  /**
   * Step 1: Classify intent with fast, cheap model
   */
  async classifyIntent(userMessage: string): Promise<IntentClassification> {
    // Use Haiku (fast, cheap) for classification
    const response = await anthropic.messages.create({
      model: 'claude-3-haiku-20240307',  // 5x cheaper, 3x faster than Sonnet
      max_tokens: 200,
      system: `You are an intent classifier. Analyze the user's message and determine their intent.

Available intents:
- order_management: Creating, modifying, canceling orders
- inventory_lookup: Checking stock, searching products
- customer_support: Getting help, reporting issues
- account_management: Profile updates, settings
- reporting: Analytics, exports, data queries

Return JSON:
{
  "intent": "category_name",
  "confidence": 0.0-1.0,
  "reasoning": "brief explanation"
}`,
      messages: [{ role: 'user', content: userMessage }]
    })

    const content = response.content[0].type === 'text' ? response.content[0].text : '{}'
    const classification = JSON.parse(content)

    // Map intent to relevant tools
    const relevantTools = this.intentCatalog.get(classification.intent) || []

    return {
      intent: classification.intent,
      confidence: classification.confidence,
      relevantTools,
      reasoning: classification.reasoning
    }
  }

  /**
   * Step 2: Execute with specialized tool set
   */
  async handleRequest(userMessage: string): Promise<string> {
    // Classify intent
    const classification = await this.classifyIntent(userMessage)

    console.log(`Intent: ${classification.intent} (confidence: ${classification.confidence})`)
    console.log(`Relevant tools: ${classification.relevantTools.join(', ')}`)

    // If low confidence, use general handler with all tools
    if (classification.confidence &lt; 0.7) {
      console.warn('Low confidence, using general handler')
      return await this.generalHandler(userMessage, allTools)
    }

    // Get tools for this intent
    const tools = allTools.filter(t => classification.relevantTools.includes(t.name))

    if (tools.length === 0) {
      return await this.generalHandler(userMessage, allTools)
    }

    // Execute with specialized tool set
    return await this.executeWithTools(userMessage, tools, classification.intent)
  }

  private async executeWithTools(
    userMessage: string,
    tools: Anthropic.Tool[],
    intent: string
  ): Promise<string> {
    const response = await anthropic.messages.create({
      model: 'claude-3-5-sonnet-20240620',
      max_tokens: 1024,
      system: `You are a helpful assistant specialized in ${intent.replace('_', ' ')}.`,
      tools,  // Only relevant tools!
      messages: [{ role: 'user', content: userMessage }]
    })

    // ... handle tool use loop as before
    return 'Response'
  }

  private async generalHandler(userMessage: string, tools: Anthropic.Tool[]): Promise<string> {
    // Fallback: use all tools
    return await this.executeWithTools(userMessage, tools, 'general')
  }
}
```

**Cost Savings**:
```typescript
// âŒ Without routing: Send all 50 tools every time
// 50 tools Ã— 150 tokens each = 7,500 prompt tokens
// Cost: $0.023 per request (Sonnet input pricing)

// âœ… With routing: Send only 4-5 relevant tools
// Classification: 200 tokens (Haiku) = $0.0002
// Execution: 5 tools Ã— 150 tokens = 750 prompt tokens (Sonnet) = $0.0023
// Total: $0.0025 per request
// Savings: 90% cost reduction!
```

### Pattern 5: Prompt Decoupling

**The Problem**: Mixing conversational logic and tool-call logic in one system prompt makes it bloated and expensive.

**The Solution**: Separate prompts for different concerns.

```typescript
/**
 * Decoupled prompt architecture
 */
class DecoupledOrchestrator {
  // Conversational persona (static, can be cached)
  private conversationalPrompt = `You are a friendly, helpful customer service assistant.
- Be empathetic and understanding
- Use a warm, professional tone
- Explain things clearly without jargon
- Ask clarifying questions when needed`

  // Tool-use instructions (static, can be cached)
  private toolUsePrompt = `When using tools:
- Check inventory BEFORE creating orders
- Resolve entity IDs BEFORE calling action tools
- Validate permissions for sensitive operations
- Return helpful error messages if tools fail`

  // Dynamic context (changes per request)
  private buildDynamicContext(user: User, session: Session): string {
    return `Current user: ${user.name} (${user.email})
Account tier: ${user.tier}
Session context: ${session.lastAction}
Active conversation: ${session.conversationId}`
  }

  async handleRequest(userMessage: string, user: User, session: Session): Promise<string> {
    // Combine prompts efficiently
    const systemPrompt = [
      this.conversationalPrompt,
      '',  // Separator
      this.toolUsePrompt,
      '',
      this.buildDynamicContext(user, session)
    ].join('\n')

    const response = await anthropic.messages.create({
      model: 'claude-3-5-sonnet-20240620',
      max_tokens: 1024,
      system: systemPrompt,
      tools: relevantTools,
      messages: conversationHistory
    })

    return response.content[0].type === 'text' ? response.content[0].text : ''
  }
}
```

**Why This Works**:
1. **Static sections are cacheable** (Anthropic's prompt caching) â†’ 90% cost reduction on repeated calls
2. **Clear separation of concerns** â†’ Easier to A/B test conversational tone vs tool behavior
3. **Dynamic context is small** â†’ Only pay for what changes per request
4. **Reusable components** â†’ Share tool-use prompt across multiple endpoints

### Pattern 6: Multi-Stage Orchestration

**The Problem**: Complex workflows require multiple LLM calls with different tool sets at each stage.

**The Solution**: State machine orchestration with stage-specific tools.

```typescript
type WorkflowStage =
  | 'intent_classification'
  | 'entity_resolution'
  | 'validation'
  | 'execution'
  | 'confirmation'

interface WorkflowState {
  stage: WorkflowStage
  intent?: string
  resolvedEntities: Map<string, string>  // mention â†’ entity_id
  validationErrors: string[]
  executionResults: any[]
}

/**
 * Multi-stage workflow orchestrator
 */
class WorkflowOrchestrator {
  async orchestrate(userMessage: string): Promise<string> {
    const state: WorkflowState = {
      stage: 'intent_classification',
      resolvedEntities: new Map(),
      validationErrors: [],
      executionResults: []
    }

    // Stage 1: Classify intent (Haiku, no tools)
    const classification = await this.classifyIntent(userMessage)
    state.intent = classification.intent
    state.stage = 'entity_resolution'

    // Stage 2: Resolve entities (Haiku, search tools only)
    const entities = await this.resolveEntities(userMessage, [
      searchUsersTool,
      searchProductsTool,
      searchOrdersTool
    ])
    state.resolvedEntities = entities
    state.stage = 'validation'

    // Stage 3: Validate (Sonnet, validation tools)
    const validation = await this.validate(state, [
      checkPermissionsTool,
      validateInventoryTool,
      checkBusinessRulesTool
    ])

    if (!validation.success) {
      state.validationErrors = validation.errors
      return this.buildErrorResponse(state)
    }

    state.stage = 'execution'

    // Stage 4: Execute (Sonnet, action tools only)
    const execution = await this.execute(state, [
      createOrderTool,
      updateInventoryTool,
      notifyCustomerTool
    ])
    state.executionResults = execution.results
    state.stage = 'confirmation'

    // Stage 5: Confirm (Sonnet, no tools, generate response)
    return await this.generateConfirmation(state)
  }

  private async classifyIntent(message: string): Promise<{ intent: string }> {
    // Fast classification with Haiku, no tools
    const response = await anthropic.messages.create({
      model: 'claude-3-haiku-20240307',
      max_tokens: 100,
      system: 'Classify user intent. Return JSON: { "intent": "order" | "support" | "account" }',
      messages: [{ role: 'user', content: message }]
    })

    return JSON.parse(response.content[0].type === 'text' ? response.content[0].text : '{}')
  }

  private async resolveEntities(
    message: string,
    searchTools: Anthropic.Tool[]
  ): Promise<Map<string, string>> {
    // Sonnet with search tools only (no action tools yet)
    const response = await anthropic.messages.create({
      model: 'claude-3-5-sonnet-20240620',
      max_tokens: 512,
      system: 'Resolve all entity mentions to database IDs using search tools.',
      tools: searchTools,  // Only search, no actions
      messages: [{ role: 'user', content: message }]
    })

    // Execute search tools, build entity map
    const entities = new Map<string, string>()
    // ... tool execution logic
    return entities
  }

  private async validate(
    state: WorkflowState,
    validationTools: Anthropic.Tool[]
  ): Promise<{ success: boolean; errors: string[] }> {
    // Sonnet with validation tools only
    const response = await anthropic.messages.create({
      model: 'claude-3-5-sonnet-20240620',
      max_tokens: 512,
      system: 'Validate this request before execution.',
      tools: validationTools,
      messages: [{
        role: 'user',
        content: `Validate: Intent=${state.intent}, Entities=${JSON.stringify(Array.from(state.resolvedEntities.entries()))}`
      }]
    })

    // Check validation results
    return { success: true, errors: [] }
  }

  private async execute(
    state: WorkflowState,
    actionTools: Anthropic.Tool[]
  ): Promise<{ results: any[] }> {
    // Sonnet with action tools only (no search, no validation)
    const response = await anthropic.messages.create({
      model: 'claude-3-5-sonnet-20240620',
      max_tokens: 1024,
      system: 'Execute validated actions.',
      tools: actionTools,  // Only actions, everything else resolved
      messages: [{
        role: 'user',
        content: `Execute: ${JSON.stringify(state)}`
      }]
    })

    // Execute tools, return results
    return { results: [] }
  }

  private async generateConfirmation(state: WorkflowState): Promise<string> {
    // Final response generation (no tools needed)
    const response = await anthropic.messages.create({
      model: 'claude-3-5-sonnet-20240620',
      max_tokens: 512,
      system: 'Generate a friendly confirmation message.',
      messages: [{
        role: 'user',
        content: `Summarize: ${JSON.stringify(state.executionResults)}`
      }]
    })

    return response.content[0].type === 'text' ? response.content[0].text : ''
  }

  private buildErrorResponse(state: WorkflowState): string {
    return `I couldn't complete that request due to: ${state.validationErrors.join(', ')}`
  }
}
```

**Benefits**:
1. **Stage-specific tool sets** â†’ LLM only sees relevant tools at each stage
2. **Clear failure points** â†’ Know exactly where things went wrong
3. **Resumable workflows** â†’ Can retry failed stages without restarting
4. **Cost optimization** â†’ Use Haiku for classification/search, Sonnet for complex reasoning
5. **Testable** â†’ Each stage can be unit tested independently

**Production Metrics** (from enterprise deployment):
- **Intent classification accuracy**: 95% (Haiku)
- **Entity resolution rate**: 92% (with fuzzy search fallback)
- **Validation catch rate**: 85% of errors caught before execution
- **Cost per workflow**: $0.008 avg (5 stages, mixed Haiku/Sonnet)
- **P95 latency**: 3.2 seconds (includes tool execution time)

---

## Common Pitfalls

### Pitfall 1: Assuming the LLM executes code

```typescript
// âŒ WRONG MENTAL MODEL
// "The LLM will call my function"

// âœ… CORRECT MENTAL MODEL
// "The LLM requests a function call with JSON parameters.
//  My code parses that JSON and executes the function.
//  I feed the result back to the LLM."
```

### Pitfall 2: Vague function descriptions

```typescript
// âŒ BAD
{
  name: 'search',
  description: 'Search for things'
}

// LLM: "Should I use this for web search? Database search? File search?"
```

```typescript
// âœ… GOOD
{
  name: 'search_products',
  description: 'Search the product catalog by name, SKU, or category. Use this when user asks to find, look up, or search for products in inventory.'
}
```

### Pitfall 3: No parameter constraints

```typescript
// âŒ BAD: LLM can make up any value
{
  priority: { type: 'string' }
}

// LLM might return: "very urgent", "ASAP!!!", "high priority", etc.
```

```typescript
// âœ… GOOD: Enum forces specific values
{
  priority: {
    type: 'string',
    enum: ['low', 'medium', 'high', 'urgent']
  }
}
```

---

## Key Takeaways

### The Orchestration Model

> "The LLM is the brain that decides WHAT to do and WHEN. Your code is the hands that DO it."

**Roles**:
- **LLM**: Understands intent, plans action sequence, synthesizes results
- **Your code**: Executes functions, validates inputs, handles errors

### Function Calling vs. Prompting

| Approach | Reliability | Type Safety | Production-Ready |
|----------|-------------|-------------|------------------|
| Prompt: "Call get_weather()" | Low (parsing errors) | No | âŒ |
| Function calling | High (structured) | Yes (JSON Schema) | âœ… |

### Three Rules for Good Tools

1. **Specific descriptions** - Explain when to use, not just what it does
2. **Constrained parameters** - Use enums, min/max, required fields
3. **Error handling** - Return useful error messages to LLM

### Why This Matters

Without function calling:
- LLM can only return text
- No integration with existing systems
- Manual parsing required
- Unreliable and error-prone

With function calling:
- LLM becomes an **orchestration layer**
- Integrates with databases, APIs, services
- Type-safe and validated
- Production-ready

**Function calling transforms LLMs from chatbots into system components.**

---

## Next Steps

- [Schema Design](./schema-design.mdx) - Design robust tool specifications
- [Week 4 Lab](../lab/support-ticket-router) - Build a Support Ticket Router with multi-tool orchestration
- [Week 5: AI Agents](../../week5/) - Build autonomous agents that chain tools

---

## Further Reading

### Official Documentation
- [Anthropic: Tool Use (Function Calling)](https://docs.anthropic.com/en/docs/build-with-claude/tool-use)
- [OpenAI: Function Calling](https://platform.openai.com/docs/guides/function-calling)
- [Google: Function Calling with Gemini](https://ai.google.dev/docs/function_calling)

### Best Practices
- [Designing Effective AI Functions](https://simonwillison.net/2023/Jun/5/function-calling/)
- [Function Calling Patterns in Production](https://www.anthropic.com/research/tool-use)
