---
title: 'Structured Output & JSON Mode'
description: 'Transform LLMs from chatbots to system components with structured data'
estimatedTime: 40
difficulty: 'intermediate'
objectives:
  - Understand why structured data is critical for production AI
  - Master JSON mode and structured output patterns
  - Design type-safe schemas for LLM responses
  - Handle validation and error cases
---

# Structured Output & JSON Mode

---

## üéØ Real-World Challenge: The Enterprise Support Orchestrator

**The Problem**: An enterprise SaaS company receives 500 unstructured support emails daily. Manual triage takes 3 hours/day, and 40% of tickets are routed to the wrong team, delaying resolution by 2-3 days. Support costs are **$180K/year** in wasted labor.

**Business Constraints**:
- **Speed**: Must process emails in &lt;2 seconds (users expect instant routing)
- **Accuracy**: &gt;95% correct routing to engineering, billing, or product teams
- **Integration**: Must populate Zendesk/Salesforce with structured data (category, priority, customer_tier)
- **Zero Hallucinations**: Cannot invent ticket IDs or priority levels‚Äîdatabase expects exact enums

**The Architectural Problem**: LLMs return **text**, but your support system needs **typed data**:

```typescript
// ‚ùå What you get from raw LLM
"This is a very high priority issue from an enterprise customer about billing"

// ‚úÖ What your database needs
{
  category: 'billing',           // enum: 'technical' | 'billing' | 'feature_request'
  priority: 10,                  // integer: 1-10
  sentiment: 'frustrated',       // enum: 'satisfied' | 'neutral' | 'frustrated'
  customer_tier: 'enterprise',   // enum: 'free' | 'pro' | 'enterprise'
  product_id: 'SKU-12345'        // string: must match regex /SKU-\d{5}/
}
```

**Architectural Solution: Type-Safe Extraction with Structured Outputs**

Use Anthropic's **Structured Outputs** (strict schema enforcement) instead of JSON Mode (prompt engineering):

```typescript
import Anthropic from '@anthropic-ai/sdk'

// Step 1: Define TypeScript schema (enforced at API level)
const TicketSchema = {
  type: 'object',
  properties: {
    category: { type: 'string', enum: ['technical', 'billing', 'feature_request'] },
    priority: { type: 'integer', minimum: 1, maximum: 10 },
    sentiment: { type: 'string', enum: ['satisfied', 'neutral', 'frustrated'] },
    customer_tier: { type: 'string', enum: ['free', 'pro', 'enterprise'] },
    product_id: { type: 'string', pattern: '^SKU-\\d{5}$' }
  },
  required: ['category', 'priority', 'sentiment', 'customer_tier']
}

// Step 2: LLM enforces schema (cannot hallucinate values)
const response = await anthropic.messages.create({
  model: 'claude-3-5-sonnet-20240620',
  max_tokens: 1024,
  tools: [{
    name: 'extract_ticket',
    description: 'Extract structured ticket data',
    input_schema: TicketSchema
  }],
  tool_choice: { type: 'tool', name: 'extract_ticket' },
  messages: [{ role: 'user', content: rawEmail }]
})

// Step 3: Type-safe result (guaranteed to match schema)
const ticket: TicketData = response.content[0].input
// ‚Üí { category: 'billing', priority: 10, ... }

// Step 4: Direct database insert (no validation needed)
await db.tickets.create({ data: ticket })
```

**Production Impact**:

**Before (Manual Triage)**:
- Processing time: 3 hours/day
- Mis-routing rate: 40%
- Average resolution time: 3.2 days
- Annual cost: $180K in labor

**After (Structured Output Automation)**:
- Processing time: **3 minutes/day** (60x faster)
- Mis-routing rate: **3.8%** (10x improvement)
- Average resolution time: **0.9 days** (3.5x faster)
- Annual cost: **$12K** in API costs + $8K monitoring = **$20K total**
- **ROI**: $160K/year savings = **800% ROI**

**The Critical Difference**: JSON Mode vs Structured Outputs

| Approach | Schema Enforcement | Hallucination Risk | Validation Needed | Production Ready |
|----------|-------------------|-------------------|-------------------|------------------|
| **JSON Mode** | ‚ùå Prompt-based (unreliable) | ‚ö†Ô∏è High (invents "very high" priority) | ‚úÖ Required | ‚ö†Ô∏è Risky |
| **Structured Outputs** | ‚úÖ API-level (guaranteed) | ‚úÖ None (rejects invalid values) | ‚ùå Not needed | ‚úÖ Yes |

**Real Example of JSON Mode Failure**:
```typescript
// User: "URGENT!!! Our app keeps crashing!"

// JSON Mode (prompt engineering)
{ "priority": "very urgent" }  // ‚ùå Database expects 1-10, gets string

// Structured Outputs (API enforcement)
{ "priority": 10 }  // ‚úÖ LLM forced to use valid integer
```

**[üëâ Lab: Build the Support Ticket Router](/curriculum/week-4/labs/support-ticket-router)**

In the hands-on lab, you'll:
1. Compare JSON Mode vs Structured Outputs on 100 real support emails
2. Measure hallucination rates (JSON Mode: 18%, Structured: 0%)
3. Build production-ready Zendesk integration
4. Deploy with full error handling and self-healing retries

**Key Architectural Insight**: Structured Outputs transforms LLMs from **text generators** into **system components** that integrate seamlessly with databases, APIs, and workflows.

---

## Real-World Industry Application: Support Auto-Router with Tool Calling

### Business Context: Enterprise SaaS Customer Support Automation

**The Challenge**: A B2B SaaS company processes 500 support tickets daily. 60% are "tier check" questions ("What features do I have?", "Can I upgrade?") that require looking up customer subscription data. Manual routing wastes 4 hours/day and delays resolutions.

**Business Constraints**:
- **Target**: 40% instant resolution (no human handoff)
- **Data Privacy**: Cannot hallucinate subscription tiers (legal risk)
- **Integration**: Must query real Zendesk API for accurate data
- **Cost**: Each Zendesk API call costs $0.02, can't afford to query unnecessarily

**The Architectural Problem**: You need the LLM to:
1. **Classify** the ticket (billing, technical, feature_request)
2. **Decide** if it needs external data (subscription info)
3. **Call a tool** (query Zendesk API) only when necessary
4. **Generate** a structured response with the subscription data

**Before: Static Structured Output (50% hallucination rate)**
```typescript
// ‚ùå LLM invents subscription tier when it doesn't know
{
  "category": "billing",
  "priority": "high",
  "subscription_tier": "enterprise", // HALLUCINATED - user might be on 'free'!
  "suggested_fix": "Contact billing team"
}
```

**After: Structured Output + Tool Calling (0% hallucination, 40% instant resolution)**

### Implementation: Tool-Augmented Support Router

```typescript
import Anthropic from '@anthropic-ai/sdk'

const anthropic = new Anthropic({ apiKey: process.env.ANTHROPIC_API_KEY })

// Step 1: Define structured output schema
const SupportTicketSchema = {
  type: 'object' as const,
  properties: {
    category: {
      type: 'string',
      enum: ['billing', 'technical', 'feature_request', 'account_management']
    },
    priority: {
      type: 'integer',
      minimum: 1,
      maximum: 10
    },
    sentiment: {
      type: 'string',
      enum: ['satisfied', 'neutral', 'frustrated', 'angry']
    },
    requiresHuman: {
      type: 'boolean',
      description: 'Whether this requires human escalation'
    },
    suggestedFix: {
      type: 'string',
      description: 'Immediate resolution if possible, or next steps'
    }
  },
  required: ['category', 'priority', 'sentiment', 'requiresHuman', 'suggestedFix']
}

// Step 2: Define tool for external API lookup
const tools: Anthropic.Tool[] = [
  {
    name: 'query_zendesk_subscription',
    description: `Query Zendesk API to get the customer's current subscription tier and features.
Use this ONLY when the ticket is about billing, features, or upgrades.
DO NOT call this for general technical questions.`,
    input_schema: {
      type: 'object',
      properties: {
        user_email: {
          type: 'string',
          description: 'Customer email address from the ticket'
        },
        query_reason: {
          type: 'string',
          description: 'Why you need subscription data (e.g., "check upgrade eligibility")'
        }
      },
      required: ['user_email', 'query_reason']
    }
  },
  {
    name: 'classify_support_ticket',
    description: 'Classify and route the support ticket based on content analysis',
    input_schema: SupportTicketSchema
  }
]

// Step 3: Implement tool handlers
async function queryZendeskSubscription(
  userEmail: string,
  queryReason: string
): Promise<{ tier: string; features: string[]; renewal_date: string }> {
  // In production: Call real Zendesk API
  console.log(`[ZENDESK API] Looking up ${userEmail} (reason: ${queryReason})`)

  // Simulated API call
  const response = await fetch(`https://api.zendesk.com/v2/users/search?query=${userEmail}`, {
    headers: {
      'Authorization': `Bearer ${process.env.ZENDESK_API_KEY}`
    }
  })

  const data = await response.json()

  return {
    tier: data.subscription?.tier || 'free',
    features: data.subscription?.features || ['basic_support', 'email_integration'],
    renewal_date: data.subscription?.renewal_date || '2026-03-15'
  }
}

// Step 4: Orchestrate LLM with tool calling
async function routeSupportTicket(
  ticketContent: string,
  userEmail: string
): Promise<{
  classification: any
  subscriptionData?: any
  resolutionMessage: string
  cost: number
}> {
  let cost = 0.015 // Base LLM cost
  let subscriptionData: any = null

  // First LLM call: Analyze ticket and decide if tool use needed
  const response = await anthropic.messages.create({
    model: 'claude-3-5-sonnet-20240620',
    max_tokens: 2048,
    tools,
    tool_choice: { type: 'auto' }, // Let LLM decide which tool(s) to call
    messages: [{
      role: 'user',
      content: `Analyze this support ticket and route appropriately.

Ticket from: ${userEmail}
Content: "${ticketContent}"

If this is about billing, features, or upgrades, use query_zendesk_subscription to get accurate data.
Otherwise, classify directly using classify_support_ticket.`
    }]
  })

  // Check if LLM wants to call tools
  const toolUses = response.content.filter(block => block.type === 'tool_use')

  if (toolUses.length &gt; 0) {
    const toolResults: Anthropic.MessageParam[] = []

    for (const toolUse of toolUses) {
      if (toolUse.type === 'tool_use') {
        if (toolUse.name === 'query_zendesk_subscription') {
          // LLM decided it needs subscription data
          cost += 0.02 // Zendesk API cost
          subscriptionData = await queryZendeskSubscription(
            toolUse.input.user_email,
            toolUse.input.query_reason
          )

          toolResults.push({
            role: 'user',
            content: [{
              type: 'tool_result',
              tool_use_id: toolUse.id,
              content: JSON.stringify(subscriptionData)
            }]
          })
        }
      }
    }

    // Second LLM call: Generate final classification with tool results
    const finalResponse = await anthropic.messages.create({
      model: 'claude-3-5-sonnet-20240620',
      max_tokens: 2048,
      tools,
      tool_choice: { type: 'tool', name: 'classify_support_ticket' }, // Force classification
      messages: [
        { role: 'user', content: `Analyze this support ticket and route appropriately.\n\nTicket from: ${userEmail}\nContent: "${ticketContent}"` },
        { role: 'assistant', content: response.content },
        ...toolResults
      ]
    })

    cost += 0.015 // Second LLM call cost

    // Extract classification from tool use
    const classificationTool = finalResponse.content.find(
      block => block.type === 'tool_use' && block.name === 'classify_support_ticket'
    )

    if (classificationTool && classificationTool.type === 'tool_use') {
      const classification = classificationTool.input

      // Generate resolution message
      let resolutionMessage = classification.suggestedFix

      if (subscriptionData) {
        resolutionMessage += `\n\nYour current plan: ${subscriptionData.tier.toUpperCase()}\nFeatures: ${subscriptionData.features.join(', ')}\nRenewal: ${subscriptionData.renewal_date}`
      }

      return {
        classification,
        subscriptionData,
        resolutionMessage,
        cost
      }
    }
  }

  throw new Error('LLM failed to classify ticket')
}

// Step 5: Usage example
const result = await routeSupportTicket(
  "I want to add more users to my account but I'm not sure if my plan supports it. Can you help?",
  "john.doe@acme.com"
)

console.log('Classification:', result.classification)
// {
//   category: 'billing',
//   priority: 6,
//   sentiment: 'neutral',
//   requiresHuman: false,
//   suggestedFix: 'Checking your current plan limits...'
// }

console.log('Subscription:', result.subscriptionData)
// {
//   tier: 'professional',
//   features: ['team_collaboration', 'advanced_analytics', 'up_to_10_users'],
//   renewal_date: '2026-03-15'
// }

console.log('Resolution:', result.resolutionMessage)
// "Your Professional plan supports up to 10 users. You currently have 7 users,
//  so you can add 3 more without upgrading. To add more, consider our Enterprise plan."

console.log('Cost:', result.cost)
// 0.045 ($0.015 * 2 LLM calls + $0.02 Zendesk API)
```

### Production Outcome Metrics

**Before (Manual Routing)**:
- Processing time: 8 minutes/ticket average
- Mis-routing rate: 35% (wrong team assignment)
- Hallucination rate: 50% (invented subscription data)
- Cost: $180K/year in agent labor

**After (Tool-Augmented Auto-Router)**:
- Processing time: **12 seconds/ticket** (40x faster)
- Mis-routing rate: **2.1%** (accurate with real data)
- Hallucination rate: **0%** (API-verified data)
- Instant resolution: **41%** (exceeded 40% target)
- Cost: $28K/year ($18K API costs + $10K monitoring)
- **ROI**: $152K/year savings = **543% ROI**

**Cost Breakdown (500 tickets/day)**:
- **Category 1: Technical** (200 tickets, 40%)
  - No Zendesk lookup needed
  - Cost: 200 √ó $0.015 = $3.00/day
  - Instant resolution: 25% (50 tickets)

- **Category 2: Billing/Features** (250 tickets, 50%)
  - Requires Zendesk lookup
  - Cost: 250 √ó $0.045 = $11.25/day
  - Instant resolution: 60% (150 tickets)

- **Category 3: Complex** (50 tickets, 10%)
  - Requires human escalation
  - Cost: 50 √ó $0.015 = $0.75/day
  - Instant resolution: 0%

**Total**: $15/day = **$450/month** = **$5,400/year** in LLM costs
**Zendesk API**: 250 √ó $0.02 √ó 30 days = **$150/month** = **$1,800/year**
**Total AI costs**: **$7,200/year** vs **$180K/year** manual labor = **96% cost reduction**

### Key Architectural Decisions

**1. Tool Choice: `auto` vs `tool` vs `any`**
```typescript
// Option 1: Auto (LLM decides if tools needed)
tool_choice: { type: 'auto' }
// Use when: You trust LLM to call tools only when necessary
// Cost: Lower (fewer API calls)
// Risk: LLM might skip tools when it should call them

// Option 2: Tool (force specific tool)
tool_choice: { type: 'tool', name: 'query_zendesk_subscription' }
// Use when: You know this request MUST call a specific tool
// Cost: Higher (always calls API)
// Risk: None (deterministic behavior)

// Option 3: Any (force any tool)
tool_choice: { type: 'any' }
// Use when: LLM must use at least one tool, you don't care which
// Cost: Medium
// Risk: LLM might pick wrong tool
```

**2. Multi-Turn Orchestration Pattern**
```typescript
// Turn 1: LLM decides which tools to call
const response1 = await anthropic.messages.create({
  tools: [queryZendeskTool, classifyTicketTool],
  tool_choice: { type: 'auto' }
})

// Turn 2: Execute tools and feed results back
const toolResults = await executeTools(response1.content)

// Turn 3: LLM makes final decision with tool data
const response2 = await anthropic.messages.create({
  messages: [
    originalMessage,
    { role: 'assistant', content: response1.content },
    { role: 'user', content: toolResults }
  ],
  tool_choice: { type: 'tool', name: 'classify_support_ticket' }
})
```

**Why this works**:
- ‚úÖ LLM only calls expensive Zendesk API when genuinely needed (50% of cases)
- ‚úÖ Zero hallucinations (subscription data comes from real API)
- ‚úÖ Structured output ensures database compatibility
- ‚úÖ 40%+ instant resolution (no human handoff)
- ‚úÖ Cost-optimized ($0.015 for simple tickets, $0.045 for complex)

**Alternative Architecture (Not Recommended)**:
```typescript
// ‚ùå BAD: Always query Zendesk first
const subscriptionData = await queryZendesk(userEmail) // Wastes API call
const classification = await classifyWithData(ticket, subscriptionData)
// Problem: Queries API even for "password reset" tickets (waste of $0.02)
```

**Production Best Practice**:
```typescript
// ‚úÖ GOOD: Let LLM decide if it needs external data
const response = await anthropic.messages.create({
  tools: [queryZendeskTool, classifyTicketTool],
  tool_choice: { type: 'auto' }, // Trust LLM judgment
  messages: [{
    role: 'user',
    content: `Important: Only call query_zendesk_subscription if the ticket
             is about billing, features, or account limits.
             For technical issues or password resets, classify directly.`
  }]
})
// Result: 50% cost savings by avoiding unnecessary API calls
```

---

## The Problem: Unstructured AI

**The fundamental limitation of chatbots**: They produce text, but production systems need **data**.

```typescript
// ‚ùå What tutorials teach (unusable in production)
const response = await anthropic.messages.create({
  model: 'claude-3-5-sonnet-20240620',
  max_tokens: 1024,
  messages: [{
    role: 'user',
    content: 'What's the weather in San Francisco?'
  }]
})

// Response: "The weather in San Francisco is currently 62¬∞F and partly cloudy."
// Problem: How do you extract 62 and "partly cloudy" reliably? String parsing? Regex? üò±
```

**Why this fails in production**:
- No type safety
- Parsing errors are common ("62¬∞F" vs "62 degrees" vs "62F")
- Can't populate UI components or database fields
- No validation
- Breaks downstream systems

**The solution**: Force the LLM to output **structured JSON** that matches your schema.

---

## The Three Levels of Structured Output

### Level 1: Plain Text (Don't do this)

```typescript
const response = await anthropic.messages.create({
  model: 'claude-3-5-sonnet-20240620',
  max_tokens: 1024,
  messages: [{
    role: 'user',
    content: 'Extract the weather from this: "It\'s 62¬∞F and partly cloudy in SF"'
  }]
})

// Returns: "The temperature is 62¬∞F and conditions are partly cloudy."
// Problem: Still unstructured! You're back to string parsing.
```

**Issues**:
- LLM adds conversational fluff
- No consistent format
- Parsing is brittle

### Level 2: JSON Mode (Prompt engineering)

```typescript
const response = await anthropic.messages.create({
  model: 'claude-3-5-sonnet-20240620',
  max_tokens: 1024,
  messages: [{
    role: 'user',
    content: `Extract weather data and return ONLY valid JSON:

{
  "temperature": number,
  "unit": "F" | "C",
  "conditions": string,
  "location": string
}

Text: "It's 62¬∞F and partly cloudy in SF"

Return only the JSON, no explanation.`
  }]
})

// Returns:
// {
//   "temperature": 62,
//   "unit": "F",
//   "conditions": "partly cloudy",
//   "location": "San Francisco"
// }
```

**Better**, but still risky:
- LLM might add explanation before/after JSON
- No schema validation
- Occasional hallucinated fields
- Requires careful prompt engineering

### Level 3: Structured Output (Production-ready) ‚úÖ

**Anthropic Claude**: Use **schema-constrained generation** by defining expected output format.

```typescript
interface WeatherData {
  temperature: number
  unit: 'F' | 'C'
  conditions: string
  location: string
  timestamp: string
}

const response = await anthropic.messages.create({
  model: 'claude-3-5-sonnet-20240620',
  max_tokens: 1024,
  messages: [{
    role: 'user',
    content: 'Extract weather: "It\'s 62¬∞F and partly cloudy in SF"'
  }],
  // Force JSON output
  response_format: { type: 'json_object' },
  system: `You are a weather data extractor. Always respond with valid JSON matching this schema:
{
  "temperature": number,
  "unit": "F" | "C",
  "conditions": string,
  "location": string,
  "timestamp": ISO 8601 string
}`
})

const data: WeatherData = JSON.parse(response.content[0].text)
// Guaranteed to match schema (validate with Zod for extra safety)
```

**OpenAI GPT-4**: Native **Structured Outputs** with JSON Schema enforcement.

```typescript
import OpenAI from 'openai'
const openai = new OpenAI()

const response = await openai.chat.completions.create({
  model: 'gpt-4-turbo-preview',
  messages: [{
    role: 'user',
    content: 'Extract weather: "It\'s 62¬∞F and partly cloudy in SF"'
  }],
  response_format: {
    type: 'json_schema',
    json_schema: {
      name: 'weather_extraction',
      schema: {
        type: 'object',
        properties: {
          temperature: { type: 'number' },
          unit: { type: 'string', enum: ['F', 'C'] },
          conditions: { type: 'string' },
          location: { type: 'string' },
          timestamp: { type: 'string', format: 'date-time' }
        },
        required: ['temperature', 'unit', 'conditions', 'location', 'timestamp'],
        additionalProperties: false
      }
    }
  }
})

const data: WeatherData = JSON.parse(response.choices[0].message.content)
```

**Why Level 3 is production-ready**:
- ‚úÖ Guaranteed JSON (no parsing errors)
- ‚úÖ Type-safe with TypeScript interfaces
- ‚úÖ Validated against schema
- ‚úÖ No hallucinated fields
- ‚úÖ Direct integration with APIs/databases

### Structured Output Methodology Comparison

As an AI Architect, the choice between these methods impacts latency, cost, and reliability.

| Feature | JSON Mode | Function / Tool Calling | Structured Outputs (Strict) |
|---------|-----------|------------------------|------------------------------|
| **Primary Goal** | Valid syntax | Action / External interaction | 100% Schema Adherence |
| **Mechanism** | Constrained token generation | LLM-driven "intent" extraction | Finite State Machine (FSM) |
| **Guarantees** | Valid JSON (bracket matching) | High intent accuracy | Deterministic schema match |
| **LLM Reasoning** | Can use Chain-of-Thought (CoT) | Limited CoT within arguments | Best for direct extraction |
| **Best For** | Logging, simple data storage | API calls, Agents, database queries | Mission-critical data ingestion |
| **Provider Support** | OpenAI, Gemini, Mistral | All major frontier models | OpenAI, Gemini (2026 standard) |

**When to use each**:

- **JSON Mode**: Quick prototypes, logging, analytics data where schema drift is acceptable
- **Function Calling**: Orchestration, agentic workflows, multi-step tasks requiring tool use
- **Structured Outputs (Strict)**: Financial transactions, compliance data, API contracts where schema violations are unacceptable

### Pro-Tips for Production

**The "JSON Mode" Trap**

JSON Mode only ensures the output is **syntactically valid JSON**. It does NOT ensure the JSON has the fields you asked for.

```typescript
// ‚ùå JSON Mode (risky)
// Prompt: "Return JSON with 'temperature' and 'unit'"
// LLM might return: { "temp": 62, "measurement": "F" }
// Valid JSON ‚úì, but wrong fields ‚úó

// ‚úÖ Structured Outputs (reliable)
// Schema defines exactly: { "temperature": number, "unit": string }
// LLM MUST return those exact fields or fail
```

**For production in 2026, Structured Outputs (Strict Mode) is the gold standard for reliability.**


---

## Grammar-Constrained Decoding: The Technical Guarantee

### The Critical Difference: Prompting vs Inference-Level Constraints

**Architect's Principle**: In production, "JSON Mode" is a **suggestion** via prompting, but **Structured Outputs** with grammar constraints are a **technical guarantee** at the inference level.

### Understanding the Mechanism

**JSON Mode** (Prompt-based):
```typescript
// ‚ùå JSON Mode: Relies on prompting
const response = await anthropic.messages.create({
  model: 'claude-3-5-sonnet-20240620',
  system: 'You MUST return valid JSON. Do not add any text before or after.',
  messages: [{
    role: 'user',
    content: 'Extract data from this email...'
  }]
})

// Problem: LLM *tries* to follow instructions but might still:
// - Add conversational fluff: "Here's the JSON: {...}"
// - Use wrong field names
// - Invent values not in your enum
// - Return syntactically valid but schema-invalid JSON
```

**Structured Outputs** (Grammar-constrained):
```typescript
// ‚úÖ Structured Outputs: Grammar enforcement at decoding
const response = await anthropic.messages.create({
  model: 'claude-3-5-sonnet-20240620',
  tools: [{
    name: 'extract_data',
    input_schema: {
      type: 'object',
      properties: {
        category: { type: 'string', enum: ['billing', 'technical', 'feature'] },
        priority: { type: 'integer', minimum: 1, maximum: 10 }
      },
      required: ['category', 'priority']
    }
  }],
  tool_choice: { type: 'tool', name: 'extract_data' }
})

// Guarantee: The model's **token generation process** is constrained
// - Can only output tokens that lead to valid schema
// - Logprobs for invalid tokens are set to zero
// - Mathematically impossible to get unparseable response
```

### How Constrained Decoding Works

**Traditional LLM Generation**:
```
User: "Extract category from this ticket"
LLM sampling: [vocabulary of 50K tokens]
  ‚Üì
Token 1: "The" (probability: 0.15) ‚Üê Conversational fluff
Token 2: "category" (probability: 0.12)
Token 3: "is" (probability: 0.08)
Token 4: "billing" (probability: 0.25)
  ‚Üì
Output: "The category is billing" ‚Üê String, not JSON!
```

**Grammar-Constrained Generation**:
```
User: "Extract category"
Schema: { "category": enum["billing", "technical", "feature"] }

LLM sampling: [vocabulary filtered by grammar]
  ‚Üì
Token 1: Must be "{" (start of JSON object) ‚Üí probability: 1.0
Token 2: Must be "category" (only valid key) ‚Üí probability: 1.0
Token 3: Must be ":" (JSON syntax) ‚Üí probability: 1.0
Token 4: Must be one of ["billing", "technical", "feature"] ‚Üí pick highest prob
Token 5: Must be "}" (close object) ‚Üí probability: 1.0
  ‚Üì
Output: {"category": "billing"} ‚Üê Guaranteed valid!
```

### The Logprobs Constraint Mechanism

**Technical Deep Dive**:

```python
# Pseudo-code for constrained decoding
def constrained_sample(model, schema, prefix=""):
    while not is_complete(prefix, schema):
        # Get probability distribution over vocabulary
        logprobs = model.get_next_token_logprobs(prefix)

        # Filter tokens by schema validity
        valid_tokens = get_valid_next_tokens(prefix, schema)

        # Zero out probabilities for invalid tokens
        for token_id in range(vocab_size):
            if token_id not in valid_tokens:
                logprobs[token_id] = -inf  # Zero probability

        # Re-normalize probability distribution
        logprobs = softmax(logprobs)

        # Sample from constrained distribution
        next_token = sample(logprobs)
        prefix += next_token

    return prefix
```

**Key Insight**: By setting invalid token probabilities to `-inf` (zero after softmax), the model **cannot** generate tokens that would violate your schema.

### Production Implications

**JSON Mode**:
```typescript
// Success rate: ~97-98%
// Failure modes:
// - "Sure! Here's the data: {...}" ‚Üê conversational wrapper
// - { "priority": "very high" } ‚Üê string instead of integer
// - { "status": "unknown" } ‚Üê value not in enum

// Mitigation: Requires manual validation + retry logic
try {
  const parsed = JSON.parse(response)
  const validated = schema.parse(parsed)  // Zod validation
} catch (error) {
  // Retry with more explicit prompts
}
```

**Structured Outputs** (Grammar-constrained):
```typescript
// Success rate: 100% (mathematically guaranteed)
// Failure modes: None (impossible to violate schema)

// No validation needed:
const data = response.content[0].input  // Already schema-compliant!
await db.insert(data)  // Safe to use directly
```

### When to Use Each

| Approach | Best For | Reliability | Performance |
|----------|----------|-------------|-------------|
| **JSON Mode** | Rapid prototyping, logging, analytics | 97-98% | Faster (no constraint overhead) |
| **Structured Outputs** | Financial transactions, compliance, API contracts | 100% | Slightly slower (constraint computation) |

**Architect's Rule**: Use **Structured Outputs** for any data that flows into:
- Financial systems (invoices, payments, refunds)
- Legal/compliance databases (audit logs, user consent)
- API contracts (external integrations like Salesforce, Stripe)
- Mission-critical workflows (support ticket routing, security alerts)

### Provider Support

**OpenAI** (Structured Outputs):
```typescript
const response = await openai.chat.completions.create({
  model: 'gpt-4-turbo',
  response_format: {
    type: 'json_schema',
    json_schema: {
      name: 'data_extraction',
      strict: true,  // ‚Üê Enables grammar constraints
      schema: {
        type: 'object',
        properties: {
          field: { type: 'string', enum: ['value1', 'value2'] }
        },
        required: ['field'],
        additionalProperties: false
      }
    }
  }
})
```

**Anthropic** (Via Tool Use):
```typescript
// Tool definitions are grammar-constrained
const response = await anthropic.messages.create({
  tools: [{ name: 'extract', input_schema: yourSchema }],
  tool_choice: { type: 'tool', name: 'extract' }
})
// Tool inputs are guaranteed to match schema
```

**The ROI Calculation**:

**Scenario**: Processing 100K support tickets/month

| Metric | JSON Mode | Structured Outputs |
|--------|-----------|-------------------|
| **Success Rate** | 97.5% | 100% |
| **Failed Extractions** | 2,500/month | 0/month |
| **Manual Review Cost** | $25,000/month (10 hours/day @ $50/hr) | $0 |
| **Retry API Costs** | $750/month (2,500 retries √ó $0.003) | $0 |
| **Total Cost** | $25,750/month | $0 |

**ROI**: Structured Outputs saves $25,750/month by eliminating all schema violations.

**Tool Choice as Logic Control**

In Function Calling, setting `tool_choice: "required"` forces the LLM to act as a pure data transformer, effectively disabling its "chatty" personality.

```typescript
// Backend microservice pattern
const response = await openai.chat.completions.create({
  model: 'gpt-4o',
  messages: [{ role: 'user', content: userInput }],
  tools: [extractionTool],
  tool_choice: { type: 'function', function: { name: 'extract_data' } }
  // LLM MUST call this function, no text response allowed
})
```

This is common for backend microservices where you want deterministic extraction, not conversation.

**Token Overhead in Function Calling**

Function descriptions consume **prompt tokens**. If you have 20 complex functions, you're paying for those definitions in **every single API call**.

**Architect's optimization strategy**:
1. **Context-aware tool pruning**: Only send relevant tools based on user context
2. **Tool grouping**: Combine related functions into categories, send only relevant category
3. **Caching**: Use prompt caching for tool definitions (supported by Anthropic Claude)

```typescript
// ‚ùå Expensive: Send all 20 tools every time
const response = await anthropic.messages.create({
  tools: allTools, // 20 tools * 200 tokens each = 4,000 prompt tokens wasted
  messages: [...]
})

// ‚úÖ Optimized: Send only relevant tools
const relevantTools = getUserContextTools(user.role) // Only 3-5 tools
const response = await anthropic.messages.create({
  tools: relevantTools, // 5 tools * 200 tokens = 1,000 prompt tokens
  messages: [...]
})

// Savings: 3,000 tokens per request = 75% cost reduction on tool definitions
```

---

## Production Pattern: Type-Safe Extraction

```typescript
import { z } from 'zod'
import Anthropic from '@anthropic-ai/sdk'

const anthropic = new Anthropic({ apiKey: process.env.ANTHROPIC_API_KEY })

// 1. Define Zod schema (validation + TypeScript types)
const WeatherSchema = z.object({
  temperature: z.number().min(-100).max(150),
  unit: z.enum(['F', 'C']),
  conditions: z.string().min(1).max(100),
  location: z.string().min(1).max(100),
  timestamp: z.string().datetime(),
  confidence: z.number().min(0).max(1).optional()
})

type WeatherData = z.infer<typeof WeatherSchema>

// 2. Extract with structured output
async function extractWeather(text: string): Promise<WeatherData> {
  const response = await anthropic.messages.create({
    model: 'claude-3-5-sonnet-20240620',
    max_tokens: 500,
    system: `You are a weather data extractor. Extract weather information and return ONLY valid JSON.

Schema:
{
  "temperature": number (min: -100, max: 150),
  "unit": "F" or "C",
  "conditions": string (e.g., "partly cloudy", "sunny"),
  "location": string (city name),
  "timestamp": ISO 8601 datetime string,
  "confidence": optional number 0-1 (how confident you are in extraction)
}

Rules:
- If temperature is ambiguous, use best judgment
- If unit is missing, default to F for US cities, C for others
- If you're unsure, set confidence &lt; 0.8
- timestamp should be current time if not specified`,
    messages: [{
      role: 'user',
      content: `Extract weather data from: "${text}"`
    }]
  })

  // 3. Parse and validate
  const content = response.content[0]
  if (content.type !== 'text') {
    throw new Error('Unexpected response type')
  }

  try {
    const parsed = JSON.parse(content.text)
    const validated = WeatherSchema.parse(parsed)
    return validated
  } catch (error) {
    if (error instanceof z.ZodError) {
      console.error('Schema validation failed:', error.errors)
      throw new Error(`Invalid weather data: ${error.errors.map(e => e.message).join(', ')}`)
    }
    throw error
  }
}

// 4. Usage
const weather = await extractWeather('It\'s 62¬∞F and partly cloudy in San Francisco')

console.log(weather.temperature) // TypeScript knows this is a number
console.log(weather.conditions)  // TypeScript knows this is a string

// Safe to use in database
await db.weatherReadings.create({ data: weather })

// Safe to use in API response
return Response.json({ weather })
```

**Key benefits**:
1. **Zod schema**: Runtime validation + TypeScript types from single source
2. **Error handling**: Catch schema violations before they corrupt data
3. **Type safety**: Autocomplete and compile-time checks
4. **Production-ready**: Direct integration with DB/API

---

## Real-World Use Cases

### 1. Form Extraction from User Input

**Problem**: Users type messy natural language, but your database needs structured fields.

```typescript
const ContactSchema = z.object({
  name: z.string().min(1),
  email: z.string().email(),
  phone: z.string().regex(/^\+?[\d\s-()]+$/),
  company: z.string().optional(),
  message: z.string().min(10),
  urgency: z.enum(['low', 'medium', 'high'])
})

async function extractContactForm(userInput: string): Promise<z.infer<typeof ContactSchema>> {
  const response = await anthropic.messages.create({
    model: 'claude-3-5-sonnet-20240620',
    max_tokens: 500,
    system: `Extract contact form data from user input. Return JSON matching this schema:
{
  "name": "full name",
  "email": "valid email",
  "phone": "phone number with country code if available",
  "company": "company name if mentioned (optional)",
  "message": "the actual message/inquiry",
  "urgency": "low" | "medium" | "high" (infer from tone)
}`,
    messages: [{
      role: 'user',
      content: `Extract: "${userInput}"`
    }]
  })

  const parsed = JSON.parse(response.content[0].text)
  return ContactSchema.parse(parsed)
}

// User types:
// "Hi, I'm John Smith from Acme Corp, john@acme.com. Need help ASAP with pricing! Call me at 555-1234"

const form = await extractContactForm(userInput)
// {
//   name: "John Smith",
//   email: "john@acme.com",
//   phone: "555-1234",
//   company: "Acme Corp",
//   message: "Need help with pricing",
//   urgency: "high"
// }

// Save directly to database
await db.contacts.create({ data: form })
```

### 2. Invoice Parsing

```typescript
const InvoiceSchema = z.object({
  invoiceNumber: z.string(),
  date: z.string().datetime(),
  vendor: z.string(),
  total: z.number().positive(),
  currency: z.string().length(3), // USD, EUR, etc.
  lineItems: z.array(z.object({
    description: z.string(),
    quantity: z.number().int().positive(),
    unitPrice: z.number().positive(),
    total: z.number().positive()
  })),
  taxAmount: z.number().nonnegative().optional(),
  dueDate: z.string().datetime().optional()
})

async function parseInvoice(invoiceText: string): Promise<z.infer<typeof InvoiceSchema>> {
  // ... similar pattern
}
```

### 3. Sentiment + Classification

```typescript
const SentimentSchema = z.object({
  sentiment: z.enum(['positive', 'neutral', 'negative']),
  score: z.number().min(-1).max(1), // -1 = very negative, +1 = very positive
  categories: z.array(z.enum(['product_quality', 'customer_service', 'pricing', 'shipping'])),
  actionable: z.boolean(), // Does this require follow-up?
  priority: z.enum(['low', 'medium', 'high', 'urgent'])
})

// Analyze customer feedback
const feedback = await analyzeFeedback('The product broke after 2 days and support is ignoring me!')
// {
//   sentiment: 'negative',
//   score: -0.9,
//   categories: ['product_quality', 'customer_service'],
//   actionable: true,
//   priority: 'urgent'
// }
```

---

## Common Pitfalls

### Pitfall 1: Forgetting to Validate

```typescript
// ‚ùå BAD: Trust LLM output blindly
const response = await anthropic.messages.create({ ... })
const data = JSON.parse(response.content[0].text)
await db.save(data) // DANGEROUS! What if LLM hallucinated fields?
```

```typescript
// ‚úÖ GOOD: Always validate
const response = await anthropic.messages.create({ ... })
const parsed = JSON.parse(response.content[0].text)
const validated = MySchema.parse(parsed) // Throws if invalid
await db.save(validated)
```

### Pitfall 2: Vague Schemas

```typescript
// ‚ùå BAD: Too loose
const schema = z.object({
  data: z.any(), // LLM can return anything!
  result: z.string() // Could be empty, too long, wrong format
})
```

```typescript
// ‚úÖ GOOD: Specific constraints
const schema = z.object({
  temperature: z.number().min(-100).max(150), // Realistic bounds
  status: z.enum(['success', 'error']), // Explicit options
  message: z.string().min(1).max(500) // Length limits
})
```

### Pitfall 3: Missing Error Handling

```typescript
// ‚ùå BAD: Assume parsing always works
const data = JSON.parse(response.content[0].text)
```

```typescript
// ‚úÖ GOOD: Handle parse errors
try {
  const data = JSON.parse(response.content[0].text)
  const validated = MySchema.parse(data)
  return validated
} catch (error) {
  if (error instanceof SyntaxError) {
    console.error('LLM returned invalid JSON:', response.content[0].text)
  } else if (error instanceof z.ZodError) {
    console.error('Schema validation failed:', error.errors)
  }
  throw new Error('Failed to extract structured data')
}
```

---

## Solving Common Failures in Production

Even with the best architecture, LLMs can fail when forced into strict structures. As an architect, you must design for these edge cases to ensure system reliability.

### Failure 1: The "Max Tokens" Cutoff

**The Problem**: JSON requires a closing brace `}`. If the LLM hits its `max_tokens` limit mid-generation, the JSON is malformed and your parser will crash.

```typescript
// ‚ùå LLM hit token limit mid-JSON
{
  "name": "John Smith",
  "email": "john@example.com",
  "address": {
    "street": "123 Main St",
    "city": "San Francisco",
    // [MAX_TOKENS REACHED - JSON TRUNCATED]
```

**Architect's Fixes**:

**1. Pre-calculate token budget**
```typescript
// Estimate output size and add buffer
const estimatedOutputTokens = calculateEstimatedTokens(schema)
const maxTokens = Math.ceil(estimatedOutputTokens * 1.5) // 50% buffer

const response = await anthropic.messages.create({
  model: 'claude-3-5-sonnet-20240620',
  max_tokens: maxTokens, // Generous buffer prevents truncation
  messages: [...]
})
```

**2. Use a fallback parser**
```typescript
import { parsePartialJson } from 'partial-json-parser'

try {
  return JSON.parse(content)
} catch (error) {
  console.warn('JSON truncated, attempting repair...')

  // Attempts to add missing closing brackets/braces
  const repaired = parsePartialJson(content)

  if (repaired) {
    console.log('Successfully repaired truncated JSON')
    return repaired
  }

  throw new Error('JSON parsing failed and repair unsuccessful')
}
```

**3. Use streaming for large outputs**
```typescript
const stream = await anthropic.messages.stream({
  model: 'claude-3-5-sonnet-20240620',
  max_tokens: 4096,
  messages: [...]
})

let partialJson = ''

for await (const chunk of stream) {
  if (chunk.type === 'content_block_delta' && chunk.delta.type === 'text_delta') {
    partialJson += chunk.delta.text

    // Try parsing incrementally
    try {
      const parsed = JSON.parse(partialJson)
      // Success! We have complete JSON
      return parsed
    } catch {
      // Not complete yet, continue streaming
    }
  }
}
```

### Failure 2: Hallucinated Arguments (The "Imaginary User ID")

**The Problem**: When using Function Calling, the LLM might invent a required argument (like a `transaction_id`) that wasn't in the prompt just to satisfy the schema.

```typescript
// User: "Process a refund for John"

// ‚ùå LLM invents user_id that doesn't exist
{
  "tool": "process_refund",
  "arguments": {
    "user_id": "user_12345", // HALLUCINATED - not in prompt!
    "amount": 50.00
  }
}

// Your system tries to refund non-existent user ‚Üí failure
```

**Architect's Fixes**:

**1. Mark fields as optional where possible**
```typescript
// ‚ùå BAD: Forces LLM to hallucinate
{
  name: 'process_refund',
  input_schema: {
    properties: {
      user_id: { type: 'string' }, // LLM MUST provide this
      amount: { type: 'number' }
    },
    required: ['user_id', 'amount'] // Forces hallucination if user_id unknown
  }
}

// ‚úÖ GOOD: Let LLM admit it doesn't know
{
  name: 'process_refund',
  input_schema: {
    properties: {
      user_id: { type: 'string', description: 'User ID if known' },
      user_email: { type: 'string', description: 'Email if user_id unknown' },
      amount: { type: 'number' }
    },
    required: ['amount'], // Only require what's definitely in prompt
    // At least one of user_id or user_email should be provided
  }
}
```

**2. Runtime validation with Zod**
```typescript
import { z } from 'zod'

const RefundArgsSchema = z.object({
  user_id: z.string().regex(/^user_[0-9]+$/), // Validate format
  amount: z.number().positive().max(10000)
})

async function executeRefund(args: any): Promise<any> {
  try {
    // Validate before execution
    const validated = RefundArgsSchema.parse(args)

    // Check if user_id actually exists in database
    const user = await db.users.findUnique({ where: { id: validated.user_id } })

    if (!user) {
      throw new Error(`User ${validated.user_id} does not exist`)
    }

    return await processRefund(validated)
  } catch (error) {
    // Return error to LLM for retry
    if (error instanceof z.ZodError) {
      return {
        error: 'Invalid arguments',
        details: error.errors,
        message: 'Please provide valid user_id in format "user_12345"'
      }
    }

    return {
      error: error.message,
      message: 'User not found. Try looking up by email instead.'
    }
  }
}
```

**3. Multi-turn conversation for missing data**
```typescript
async function orchestrateRefund(userMessage: string) {
  const response = await anthropic.messages.create({
    model: 'claude-3-5-sonnet-20240620',
    max_tokens: 1024,
    tools: [processRefundTool, lookupUserTool],
    messages: [{ role: 'user', content: userMessage }]
  })

  // LLM realizes it needs user_id first
  if (response.content[0].name === 'lookup_user') {
    const user = await lookupUser(response.content[0].input.email)

    // Feed result back to LLM
    const nextResponse = await anthropic.messages.create({
      model: 'claude-3-5-sonnet-20240620',
      max_tokens: 1024,
      tools: [processRefundTool],
      messages: [
        { role: 'user', content: userMessage },
        { role: 'assistant', content: response.content },
        { role: 'user', content: [{ type: 'tool_result', tool_use_id: '...', content: JSON.stringify(user) }] }
      ]
    })

    // Now LLM can call process_refund with real user_id
  }
}
```

### Failure 3: Schema Drift

**The Problem**: You update your backend API, but forget to update the LLM's tool description. The LLM continues to send the "old" argument names.

```typescript
// ‚ùå Your code
interface OrderParams {
  productId: string      // Renamed from product_sku
  deliverySpeed: string  // Renamed from priority
}

// ‚ùå LLM's tool definition (outdated)
{
  name: 'create_order',
  input_schema: {
    properties: {
      product_sku: { type: 'string' },  // OLD NAME
      priority: { type: 'string' }      // OLD NAME
    }
  }
}

// Result: LLM sends { product_sku: "...", priority: "..." }
// Your code expects { productId: "...", deliverySpeed: "..." }
// ‚Üí Silent failures or crashes
```

**Architect's Fix: Single Source of Truth**

**Option 1: Generate tool definitions from TypeScript types**
```typescript
import { zodToJsonSchema } from 'zod-to-json-schema'
import { z } from 'zod'

// Define schema once
const OrderParamsSchema = z.object({
  productId: z.string().regex(/^SKU-[0-9]{5}$/),
  deliverySpeed: z.enum(['standard', 'express', 'overnight']),
  quantity: z.number().int().positive()
})

// Generate TypeScript type
type OrderParams = z.infer<typeof OrderParamsSchema>

// Generate JSON Schema for LLM
const jsonSchema = zodToJsonSchema(OrderParamsSchema)

// Use in tool definition
const createOrderTool: Anthropic.Tool = {
  name: 'create_order',
  description: 'Create a new product order',
  input_schema: jsonSchema // SINGLE SOURCE OF TRUTH
}

// Use in validation
function createOrder(params: unknown): Promise<Order> {
  const validated = OrderParamsSchema.parse(params) // Same schema!
  return db.orders.create({ data: validated })
}
```

**Option 2: Generate from OpenAPI spec**
```typescript
import { OpenAPIV3 } from 'openapi-types'

// Load your OpenAPI/Swagger spec
const apiSpec: OpenAPIV3.Document = await loadOpenAPISpec()

// Extract operation schema
const createOrderOperation = apiSpec.paths['/orders']?.post
const requestBodySchema = createOrderOperation?.requestBody?.content['application/json']?.schema

// Convert to Anthropic tool
const tool: Anthropic.Tool = {
  name: 'create_order',
  description: createOrderOperation?.description || '',
  input_schema: requestBodySchema as any
}

// Both LLM and API use same spec ‚Üí no drift!
```

---

## Schema Evolution: Treating LLM Outputs as API Contracts

### The Production Reality: Schemas Are Not Static

**Architect's Principle**: Your LLM output schema is an **API Contract**. If you change a `priority` field from a string to an integer, you must version your schema and provide migration scripts‚Äîjust like you would for a REST API.

### The Breaking Change Scenario

```typescript
// ‚ùå Week 1: Initial schema
const TicketSchemaV1 = {
  type: 'object',
  properties: {
    category: { type: 'string' },
    priority: { type: 'string' },  // "low", "medium", "high"
    assignee: { type: 'string' }
  }
}

// ‚ùå Week 4: You realize strings are inefficient
const TicketSchema = {
  type: 'object',
  properties: {
    category: { type: 'string', enum: ['billing', 'technical'] },  // Added enum
    priority: { type: 'integer', minimum: 1, maximum: 10 },  // Changed type!
    team: { type: 'string' }  // Renamed from assignee!
  }
}

// üö® PROBLEM: Analytics dashboard expects old schema
// All historical ticket data uses { priority: "high" }
// New LLM outputs { priority: 8 }
// Dashboard crashes: "Cannot read property 'charAt' of number"
```

### Schema Versioning Strategy

**Pattern 1: Explicit Version Field**

```typescript
// Include version in every schema
const TicketSchemaV1 = {
  type: 'object',
  properties: {
    schema_version: { type: 'string', const: 'v1' },  // Explicit version
    category: { type: 'string' },
    priority: { type: 'string' }
  },
  required: ['schema_version', 'category', 'priority']
}

const TicketSchemaV2 = {
  type: 'object',
  properties: {
    schema_version: { type: 'string', const: 'v2' },  // New version
    category: { type: 'string', enum: ['billing', 'technical', 'feature'] },
    priority: { type: 'integer', minimum: 1, maximum: 10 },
    team: { type: 'string' }
  },
  required: ['schema_version', 'category', 'priority', 'team']
}
```

**Pattern 2: Separate Schema Namespaces**

```typescript
// File: schemas/ticket/v1.ts
export const TicketSchemaV1 = { ... }

// File: schemas/ticket/v2.ts
export const TicketSchemaV2 = { ... }

// Runtime selection
import { TicketSchemaV1, TicketSchemaV2 } from './schemas/ticket'

function getTicketSchema(version: string) {
  switch (version) {
    case 'v1': return TicketSchemaV1
    case 'v2': return TicketSchemaV2
    default: throw new Error(`Unknown schema version: ${version}`)
  }
}
```

### Migration Scripts: The Semantic Translator

**The Problem**: Historical data uses old schema, but analytics expects new schema.

**The Solution**: Write **bidirectional migration functions**.

```typescript
/**
 * Migrate ticket data from v1 to v2 schema
 */
function migrateV1toV2(ticketV1: TicketV1): TicketV2 {
  return {
    schema_version: 'v2',
    category: mapCategoryV1toV2(ticketV1.category),  // Semantic mapping
    priority: mapPriorityV1toV2(ticketV1.priority),  // Type conversion
    team: ticketV1.assignee || 'unassigned'  // Field rename with default
  }
}

/**
 * Map category from free-form string to enum
 */
function mapCategoryV1toV2(category: string): 'billing' | 'technical' | 'feature' {
  // Fuzzy matching for historical data
  const normalized = category.toLowerCase()

  if (normalized.includes('bill') || normalized.includes('payment')) {
    return 'billing'
  } else if (normalized.includes('feature') || normalized.includes('request')) {
    return 'feature'
  } else {
    return 'technical'  // Default
  }
}

/**
 * Map priority from string to integer
 */
function mapPriorityV1toV2(priority: string): number {
  const mapping: Record<string, number> = {
    'low': 3,
    'medium': 6,
    'high': 9,
    'urgent': 10,
    'critical': 10
  }

  const normalized = priority.toLowerCase()
  return mapping[normalized] || 5  // Default to medium
}

/**
 * Reverse migration (v2 to v1) for backward compatibility
 */
function migrateV2toV1(ticketV2: TicketV2): TicketV1 {
  return {
    schema_version: 'v1',
    category: ticketV2.category,  // Enum is subset of string
    priority: mapPriorityV2toV1(ticketV2.priority),
    assignee: ticketV2.team
  }
}

function mapPriorityV2toV1(priority: number): string {
  if (priority >= 9) return 'high'
  if (priority >= 6) return 'medium'
  return 'low'
}
```

### Production Integration Pattern

```typescript
import { TicketSchemaV1, TicketSchemaV2 } from './schemas/ticket'

interface UnifiedTicket {
  // Internal representation (always latest schema)
  schema_version: 'v2'
  category: 'billing' | 'technical' | 'feature'
  priority: number  // 1-10
  team: string
}

class TicketProcessor {
  /**
   * Extract ticket using latest schema
   */
  async extractTicket(emailContent: string): Promise<UnifiedTicket> {
    const response = await anthropic.messages.create({
      tools: [{
        name: 'extract_ticket',
        input_schema: TicketSchemaV2  // Always use latest
      }],
      tool_choice: { type: 'tool', name: 'extract_ticket' },
      messages: [{ role: 'user', content: emailContent }]
    })

    const extracted = response.content[0].input

    return {
      schema_version: 'v2',
      ...extracted
    } as UnifiedTicket
  }

  /**
   * Load historical ticket (might be v1 or v2)
   */
  async loadTicket(ticketId: string): Promise<UnifiedTicket> {
    const raw = await db.tickets.findUnique({ where: { id: ticketId } })

    // Migrate to latest schema if needed
    if (raw.schema_version === 'v1') {
      console.log(`Migrating ticket ${ticketId} from v1 to v2`)
      return migrateV1toV2(raw)
    }

    return raw as UnifiedTicket
  }

  /**
   * Export for legacy system (still expects v1)
   */
  async exportToLegacySystem(ticket: UnifiedTicket): Promise<void> {
    // Convert to v1 for backward compatibility
    const ticketV1 = migrateV2toV1(ticket)

    await legacyAPI.createTicket(ticketV1)
  }
}
```

### Database Migration Strategy

**Approach 1: Lazy Migration** (Recommended for large datasets)

```typescript
// Don't migrate all data at once
// Migrate on read, write back in new schema

async function getTicket(ticketId: string): Promise<UnifiedTicket> {
  const raw = await db.tickets.findUnique({ where: { id: ticketId } })

  if (raw.schema_version === 'v1') {
    // Migrate on read
    const migrated = migrateV1toV2(raw)

    // Write back in new schema (lazy migration)
    await db.tickets.update({
      where: { id: ticketId },
      data: migrated
    })

    return migrated
  }

  return raw
}
```

**Approach 2: Batch Migration** (For smaller datasets or urgent changes)

```typescript
// Migrate all v1 tickets to v2 in background job

async function migrateAllTicketsV1toV2(): Promise<void> {
  const v1Tickets = await db.tickets.findMany({
    where: { schema_version: 'v1' }
  })

  console.log(`Migrating ${v1Tickets.length} tickets from v1 to v2...`)

  for (const ticket of v1Tickets) {
    const migrated = migrateV1toV2(ticket)

    await db.tickets.update({
      where: { id: ticket.id },
      data: migrated
    })
  }

  console.log('Migration complete!')
}

// Run during maintenance window
// cron: 0 2 * * 0 (Sunday 2 AM)
```

### Versioning Best Practices

1. **Semantic Versioning for Schemas**
   ```
   MAJOR.MINOR.PATCH

   MAJOR: Breaking changes (type changes, required field removals)
   MINOR: Backward-compatible additions (new optional fields)
   PATCH: Documentation/description updates only
   ```

2. **Deprecation Warnings**
   ```typescript
   const TicketSchemaV1 = {
     deprecated: true,
     deprecation_date: '2026-06-01',
     migration_guide: 'https://docs.company.com/migrations/ticket-v1-to-v2',
     ...schema
   }
   ```

3. **Testing Migrations**
   ```typescript
   // Test that migrations are reversible
   describe('Schema migrations', () => {
     it('should round-trip v1 ‚Üí v2 ‚Üí v1 without data loss', () => {
       const original: TicketV1 = { ... }

       const v2 = migrateV1toV2(original)
       const roundtrip = migrateV2toV1(v2)

       expect(roundtrip).toEqual(original)
     })

     it('should handle edge cases in priority mapping', () => {
       expect(mapPriorityV1toV2('URGENT!!!')).toBe(10)
       expect(mapPriorityV1toV2('unknown')).toBe(5)  // Default
     })
   })
   ```

4. **Monitoring Schema Usage**
   ```typescript
   // Track which schemas are still in use
   async function logSchemaUsage(ticketId: string, version: string): Promise<void> {
     await analytics.track('schema_version_used', {
       resource: 'ticket',
       resource_id: ticketId,
       schema_version: version,
       timestamp: new Date()
     })
   }

   // Alert when v1 usage drops below 5% ‚Üí safe to deprecate
   ```

### The Architect's Checklist: Schema Changes

Before deploying a schema change to production:

- [ ] Increment schema version number
- [ ] Write migration function (old ‚Üí new)
- [ ] Write reverse migration (new ‚Üí old) for rollback
- [ ] Test migrations on sample data
- [ ] Update LLM tool definitions to use new schema
- [ ] Deploy migration script (lazy or batch)
- [ ] Monitor schema version distribution
- [ ] Update analytics dashboards to handle both schemas
- [ ] Set deprecation date for old schema (e.g., 90 days)
- [ ] Document breaking changes in changelog

**The Cost of Not Versioning**:

- Analytics dashboards crash on schema mismatch
- Historical data becomes unreadable
- Rollbacks are impossible (no reverse migration)
- Integration partners break on undocumented changes

**The ROI of Proper Versioning**:

- Zero downtime deployments (old and new schemas coexist)
- Safe rollbacks (reverse migrations available)
- Historical data remains queryable (migrations handle old schemas)
- API contracts with external systems remain stable


**Option 3: Runtime schema validation layer**
```typescript
// Middleware that validates LLM arguments against current API schema
async function validateToolCall(
  toolName: string,
  args: unknown
): Promise<{ valid: boolean; error?: string }> {
  const currentSchema = await getLatestToolSchema(toolName)

  try {
    currentSchema.parse(args)
    return { valid: true }
  } catch (error) {
    if (error instanceof z.ZodError) {
      return {
        valid: false,
        error: `Schema mismatch: ${error.errors.map(e => `${e.path}: ${e.message}`).join(', ')}`
      }
    }
    throw error
  }
}
```

---

## Self-Healing Logic: The Repair Middleware

Even with perfect prompts and schemas, LLMs occasionally produce malformed output in production. Architect-level systems must **automatically recover** from these failures without human intervention.

### The Three-Layer Defense Strategy

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Layer 1: PREVENTIVE (Before LLM call)                ‚îÇ
‚îÇ - Generous max_tokens buffer                         ‚îÇ
‚îÇ - Clear schema in system prompt                      ‚îÇ
‚îÇ - Few-shot examples of correct format                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ LLM generates output
                   ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Layer 2: REACTIVE (Parse & Validate)                 ‚îÇ
‚îÇ - Try JSON.parse()                                   ‚îÇ
‚îÇ - If success, validate against Zod schema            ‚îÇ
‚îÇ - If fail, proceed to Layer 3                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ Validation failed
                   ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Layer 3: REPAIR (Auto-fix & Retry)                   ‚îÇ
‚îÇ - JSON repair (add missing brackets)                 ‚îÇ
‚îÇ - Regex cleanup (remove conversational fluff)        ‚îÇ
‚îÇ - Feedback loop (send error back to LLM)             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Production Pattern: JSON Repair Middleware

```typescript
import { z } from 'zod'
import Anthropic from '@anthropic-ai/sdk'

interface RepairResult<T> {
  success: boolean
  data?: T
  repaired: boolean
  attempts: number
  error?: string
}

/**
 * Self-healing JSON extraction with automatic repair
 */
class SelfHealingExtractor<T> {
  constructor(
    private schema: z.ZodSchema<T>,
    private anthropic: Anthropic,
    private maxRepairAttempts: number = 3
  ) {}

  async extract(
    prompt: string,
    systemPrompt: string
  ): Promise<RepairResult<T>> {
    let attempts = 0
    let lastError: string = ''

    while (attempts < this.maxRepairAttempts) {
      attempts++

      try {
        // Step 1: Get LLM response
        const response = await this.anthropic.messages.create({
          model: 'claude-3-5-sonnet-20240620',
          max_tokens: 2048,
          system: systemPrompt + (attempts &gt; 1 ? this.buildErrorFeedback(lastError) : ''),
          messages: [{ role: 'user', content: prompt }]
        })

        const content = response.content[0].type === 'text' ? response.content[0].text : ''

        // Step 2: Try direct parsing
        try {
          const parsed = JSON.parse(content)
          const validated = this.schema.parse(parsed)

          return {
            success: true,
            data: validated,
            repaired: false,
            attempts
          }
        } catch (parseError) {
          console.log(`Attempt ${attempts}: Direct parse failed, trying repair...`)

          // Step 3: Auto-repair
          const repaired = await this.repairJSON(content)

          if (repaired) {
            try {
              const parsed = JSON.parse(repaired)
              const validated = this.schema.parse(parsed)

              console.log(`‚úÖ Successfully repaired JSON on attempt ${attempts}`)

              return {
                success: true,
                data: validated,
                repaired: true,
                attempts
              }
            } catch (repairError) {
              // Repair didn't work, prepare feedback for next attempt
              lastError = this.buildErrorMessage(parseError, repairError, content)
            }
          } else {
            lastError = this.buildErrorMessage(parseError, null, content)
          }
        }
      } catch (apiError) {
        // API-level error (rate limit, network, etc.)
        return {
          success: false,
          repaired: false,
          attempts,
          error: apiError instanceof Error ? apiError.message : 'API error'
        }
      }
    }

    // All repair attempts failed
    return {
      success: false,
      repaired: false,
      attempts,
      error: `Failed after ${attempts} attempts. Last error: ${lastError}`
    }
  }

  /**
   * Attempt to repair malformed JSON
   */
  private async repairJSON(content: string): Promise<string | null> {
    // Strategy 1: Remove conversational fluff (common LLM mistake)
    let cleaned = this.removeConversationalFluff(content)

    // Strategy 2: Fix missing closing brackets (truncation)
    cleaned = this.addMissingBrackets(cleaned)

    // Strategy 3: Fix common quote issues
    cleaned = this.fixQuotes(cleaned)

    // Strategy 4: Try partial JSON parser
    try {
      const repaired = this.parsePartialJSON(cleaned)
      if (repaired) {
        return JSON.stringify(repaired)
      }
    } catch {
      // Partial parse failed
    }

    return null
  }

  /**
   * Remove text before/after JSON block
   */
  private removeConversationalFluff(content: string): string {
    // Common patterns LLMs add:
    // "Here's the JSON: {...}"
    // "```json\n{...}\n```"
    // "Sure! {...}"

    // Extract JSON from code blocks
    const codeBlockMatch = content.match(/```(?:json)?\s*(\{[\s\S]*\})\s*```/)
    if (codeBlockMatch) {
      return codeBlockMatch[1]
    }

    // Find first { and last }
    const firstBrace = content.indexOf('{')
    const lastBrace = content.lastIndexOf('}')

    if (firstBrace !== -1 && lastBrace !== -1 && lastBrace > firstBrace) {
      return content.slice(firstBrace, lastBrace + 1)
    }

    return content
  }

  /**
   * Add missing closing brackets (token limit truncation)
   */
  private addMissingBrackets(content: string): string {
    let fixed = content.trim()

    // Count brackets
    const openBraces = (fixed.match(/\{/g) || []).length
    const closeBraces = (fixed.match(/\}/g) || []).length
    const openBrackets = (fixed.match(/\[/g) || []).length
    const closeBrackets = (fixed.match(/\]/g) || []).length

    // Add missing closing braces
    for (let i = 0; i < openBraces - closeBraces; i++) {
      fixed += '}'
    }

    // Add missing closing brackets
    for (let i = 0; i < openBrackets - closeBrackets; i++) {
      fixed += ']'
    }

    return fixed
  }

  /**
   * Fix common quote issues
   */
  private fixQuotes(content: string): string {
    // Replace smart quotes with standard quotes
    return content
      .replace(/[""]/g, '"')
      .replace(/['']/g, "'")
  }

  /**
   * Parse incomplete JSON (progressive parsing)
   */
  private parsePartialJSON(content: string): any | null {
    // Try progressive truncation
    let current = content
    while (current.length &gt; 10) {
      try {
        return JSON.parse(current)
      } catch {
        // Remove last character and try again
        current = current.slice(0, -1).trim()

        // Add closing bracket if needed
        if (current.endsWith(',')) {
          current = current.slice(0, -1) + '}'
        }
      }
    }

    return null
  }

  /**
   * Build error feedback for LLM (validation error ‚Üí retry with hints)
   */
  private buildErrorFeedback(error: string): string {
    return `

IMPORTANT: Your previous response had errors. Please fix them:

Error: ${error}

Requirements:
- Return ONLY valid JSON (no explanation before/after)
- Ensure all required fields are present
- Use exact field names from schema
- Close all brackets properly
- Use double quotes for strings, not single quotes`
  }

  /**
   * Build detailed error message from parse/validation errors
   */
  private buildErrorMessage(
    parseError: any,
    repairError: any,
    content: string
  ): string {
    let message = 'JSON parsing failed. '

    if (parseError instanceof SyntaxError) {
      message += `Syntax error: ${parseError.message}. `
    } else if (parseError instanceof z.ZodError) {
      const fieldErrors = parseError.errors
        .map(e => `Field "${e.path.join('.')}" ${e.message}`)
        .join(', ')
      message += `Validation errors: ${fieldErrors}. `
    }

    if (repairError) {
      message += `Repair also failed: ${repairError instanceof Error ? repairError.message : 'Unknown error'}. `
    }

    // Include truncated content for debugging
    const preview = content.slice(0, 200) + (content.length &gt; 200 ? '...' : '')
    message += `Content preview: ${preview}`

    return message
  }
}
```

### Usage: Self-Healing Extraction

```typescript
// Define schema
const WeatherSchema = z.object({
  temperature: z.number(),
  unit: z.enum(['F', 'C']),
  conditions: z.string(),
  location: z.string()
})

// Create self-healing extractor
const extractor = new SelfHealingExtractor(
  WeatherSchema,
  anthropic,
  3 // max repair attempts
)

// Extract with automatic repair
const result = await extractor.extract(
  'Extract weather from: "It\'s 62 degrees and rainy in SF"',
  'You are a weather data extractor. Return ONLY valid JSON matching the schema.'
)

if (result.success) {
  console.log('Data:', result.data)
  console.log('Repaired:', result.repaired)
  console.log('Attempts:', result.attempts)

  // Use validated data
  await db.weather.create({ data: result.data })
} else {
  console.error('Extraction failed:', result.error)
  // Fall back to manual processing or alert human
}
```

### Advanced: Validation Error Feedback Loop

When validation fails, **send the exact error messages back to the LLM** so it can self-correct.

```typescript
/**
 * Multi-turn extraction with validation feedback
 */
async function extractWithFeedback<T>(
  schema: z.ZodSchema<T>,
  prompt: string,
  maxTurns: number = 3
): Promise<T> {
  let conversationHistory: Anthropic.MessageParam[] = [
    { role: 'user', content: prompt }
  ]

  for (let turn = 1; turn &lt;= maxTurns; turn++) {
    const response = await anthropic.messages.create({
      model: 'claude-3-5-sonnet-20240620',
      max_tokens: 1024,
      messages: conversationHistory
    })

    const content = response.content[0].type === 'text' ? response.content[0].text : ''

    try {
      // Try parsing and validation
      const parsed = JSON.parse(content)
      const validated = schema.parse(parsed)

      console.log(`‚úÖ Success on turn ${turn}`)
      return validated
    } catch (error) {
      console.log(`Turn ${turn} failed, sending feedback...`)

      // Build detailed error feedback
      let feedback = 'Your JSON has errors:\n\n'

      if (error instanceof SyntaxError) {
        feedback += `SYNTAX ERROR: ${error.message}\n\n`
        feedback += 'Common fixes:\n'
        feedback += '- Ensure all brackets are closed\n'
        feedback += '- Use double quotes for strings\n'
        feedback += '- Remove trailing commas\n'
      } else if (error instanceof z.ZodError) {
        feedback += 'VALIDATION ERRORS:\n'
        error.errors.forEach((e, i) => {
          feedback += `${i + 1}. Field "${e.path.join('.')}" - ${e.message}\n`
        })
        feedback += '\nFix these issues and return corrected JSON.'
      }

      // Add feedback to conversation
      conversationHistory.push(
        { role: 'assistant', content: response.content },
        { role: 'user', content: feedback }
      )

      if (turn === maxTurns) {
        throw new Error(`Failed after ${maxTurns} turns. Last error: ${error}`)
      }
    }
  }

  throw new Error('Extraction failed')
}
```

### Production Metrics: Repair Success Rates

Track repair effectiveness to optimize strategies:

```typescript
interface RepairMetrics {
  totalExtractions: number
  directSuccess: number
  repairedSuccess: number
  totalFailures: number
  avgAttempts: number
  commonErrors: Map<string, number>
}

class RepairMetricsTracker {
  private metrics: RepairMetrics = {
    totalExtractions: 0,
    directSuccess: 0,
    repairedSuccess: 0,
    totalFailures: 0,
    avgAttempts: 0,
    commonErrors: new Map()
  }

  recordResult(result: RepairResult<any>): void {
    this.metrics.totalExtractions++

    if (result.success) {
      if (result.repaired) {
        this.metrics.repairedSuccess++
      } else {
        this.metrics.directSuccess++
      }
    } else {
      this.metrics.totalFailures++
      if (result.error) {
        const count = this.metrics.commonErrors.get(result.error) || 0
        this.metrics.commonErrors.set(result.error, count + 1)
      }
    }

    // Update average attempts
    const total = this.metrics.directSuccess + this.metrics.repairedSuccess + this.metrics.totalFailures
    this.metrics.avgAttempts =
      ((this.metrics.avgAttempts * (total - 1)) + result.attempts) / total
  }

  getReport(): string {
    const total = this.metrics.totalExtractions
    const successRate = ((this.metrics.directSuccess + this.metrics.repairedSuccess) / total) * 100
    const repairRate = (this.metrics.repairedSuccess / total) * 100

    return `
Extraction Metrics (last ${total} calls):
  Success Rate: ${successRate.toFixed(1)}%
  Direct Success: ${this.metrics.directSuccess} (${((this.metrics.directSuccess / total) * 100).toFixed(1)}%)
  Repaired Success: ${this.metrics.repairedSuccess} (${repairRate.toFixed(1)}%)
  Failures: ${this.metrics.totalFailures} (${((this.metrics.totalFailures / total) * 100).toFixed(1)}%)
  Avg Attempts: ${this.metrics.avgAttempts.toFixed(2)}

Top Errors:
${Array.from(this.metrics.commonErrors.entries())
  .sort((a, b) => b[1] - a[1])
  .slice(0, 5)
  .map(([error, count]) => `  ${count}x - ${error.slice(0, 80)}`)
  .join('\n')}
`
  }
}
```

**Target Metrics**:
- **Direct Success Rate**: &gt; 95% (schema works well, no repair needed)
- **Repair Success Rate**: 3-4% (auto-repair saves failed calls)
- **Total Failure Rate**: &lt; 1% (alert humans for manual review)
- **Avg Attempts**: &lt; 1.1 (most succeed on first try)

**When to Alert**:
- Direct success &lt; 90% ‚Üí Schema/prompt needs improvement
- Repair success &gt; 10% ‚Üí Underlying issue with LLM behavior
- Total failure &gt; 2% ‚Üí Production reliability at risk

---


---

## Deterministic Fallback: Escaping the Loop of Death

### The Problem: Infinite Retry Loops

Even with self-healing logic, what happens when the LLM **repeatedly fails validation**? The "Loop of Death":

```typescript
// ‚ùå INFINITE RETRY LOOP (Production Killer)
let attempts = 0
while (true) {  // No exit condition!
  const response = await llm.generate(prompt)
  try {
    return schema.parse(response)
  } catch (error) {
    attempts++
    prompt = `${prompt}\n\nError: ${error}. Try again.`
    // Continues forever if LLM can't produce valid JSON
  }
}
```

### Real-World Production Incident

**System**: Healthcare appointment scheduling (HIPAA-compliant)
**Schema**: Required strict date format (YYYY-MM-DD)
**Problem**: LLM kept returning natural language dates

```typescript
// Attempt 1
LLM: { "appointment_date": "tomorrow" }
Error: Invalid date format

// Attempt 2
LLM: { "appointment_date": "next Tuesday" }
Error: Invalid date format

// Attempt 3
LLM: { "appointment_date": "in 3 days" }
Error: Invalid date format

// ... (continues for 47 attempts)
// After 2 minutes: Request timeout
// Patient abandons form
```

**Impact**:
- 12% of appointment requests timed out (infinite retry loop)
- Average time to failure: 45 seconds
- Patient abandonment rate: 68%
- **Cost**: $280,000/year in lost bookings

### The Architect's Solution: Schema-Guided Shadowing

**Core Principle**: "In a mission-critical API, a 500 error is a failure of architecture. Always have a 'Hard-Coded Default' or a 'Format-Fixer' microservice that ensures the downstream system receives something valid, even if the LLM is hallucinating."

### The Four-Tier Fallback Hierarchy

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Tier 1: Primary LLM (Sonnet, temp=0.7)                  ‚îÇ
‚îÇ Creative, flexible, handles complex cases                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ Validation failed (1st attempt)
                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Tier 2: Constrained LLM (Haiku, temp=0.0)               ‚îÇ
‚îÇ Strict, deterministic, focused on format correction     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ Validation failed (2nd attempt)
                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Tier 3: Format-Fixer Microservice (Deterministic)       ‚îÇ
‚îÇ Coerces any input into valid schema format              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ Extreme edge case
                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Tier 4: Hard-Coded Defaults (Guaranteed Valid)          ‚îÇ
‚îÇ Safe defaults for every field, never fails              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Pattern 1: Hard-Coded Defaults

```typescript
interface FallbackDefaults {
  [field: string]: any | (() => any)
}

const appointmentDefaults: FallbackDefaults = {
  priority: 'routine',  // If LLM can't determine, default to routine
  duration_minutes: 30,  // Standard appointment length
  appointment_date: () => addDays(new Date(), 1).toISOString().split('T')[0],  // Tomorrow
  notification_preference: 'email',  // Safe default
  insurance_verified: false  // Require manual verification
}

async function extractWithFallback<T>(
  text: string,
  schema: z.ZodSchema<T>,
  defaults: FallbackDefaults,
  maxAttempts: number = 3
): Promise<T> {
  let lastError: z.ZodError | null = null

  for (let attempt = 1; attempt <= maxAttempts; attempt++) {
    try {
      const response = await llm.generate(text)
      return schema.parse(response)
    } catch (error) {
      if (error instanceof z.ZodError) {
        lastError = error
        console.warn(`Attempt ${attempt} failed:`, error.errors.map(e => e.message))
      }
    }
  }

  // DETERMINISTIC FALLBACK: Use hard-coded defaults
  console.error('LLM failed after max attempts, using fallback defaults')

  const fallbackResult: any = {}

  for (const error of lastError!.errors) {
    const field = error.path[0] as string
    const defaultValue = defaults[field]

    if (typeof defaultValue === 'function') {
      fallbackResult[field] = defaultValue()
    } else if (defaultValue !== undefined) {
      fallbackResult[field] = defaultValue
    } else {
      // No default available, use schema type default
      fallbackResult[field] = getTypeDefault(error.expected)
    }
  }

  // Log fallback for monitoring
  await logFallbackUsage(text, fallbackResult, lastError!)

  return fallbackResult as T
}

function getTypeDefault(zodType: string): any {
  const defaults: Record<string, any> = {
    'string': '',
    'number': 0,
    'boolean': false,
    'array': [],
    'object': {}
  }
  return defaults[zodType] || null
}
```

### Pattern 2: Constrained Decoding Fallback

```typescript
/**
 * If primary model fails, fall back to a highly constrained model
 */
async function extractWithConstrainedFallback<T>(
  text: string,
  schema: z.ZodSchema<T>
): Promise<T> {
  // Tier 1: Primary model (Sonnet, creative)
  try {
    const response = await anthropic.messages.create({
      model: 'claude-3-5-sonnet-20240620',
      max_tokens: 1024,
      temperature: 0.7,  // Some creativity
      system: 'Extract structured data from the input.',
      messages: [{ role: 'user', content: text }]
    })

    const content = response.content[0].type === 'text' ? response.content[0].text : '{}'
    return schema.parse(JSON.parse(content))
  } catch (error) {
    console.warn('Primary model failed, trying constrained fallback')
  }

  // Tier 2: Constrained model (Haiku, strict)
  try {
    const response = await anthropic.messages.create({
      model: 'claude-3-haiku-20240307',  // 3x faster, 5x cheaper
      max_tokens: 512,
      temperature: 0.0,  // Zero creativity, maximum determinism
      system: `You are a JSON formatter. Output ONLY valid JSON matching this schema:
${JSON.stringify(schema._def.shape(), null, 2)}

NO explanations. NO markdown. ONLY the JSON object.`,
      messages: [{
        role: 'user',
        content: `Convert this to valid JSON:\n${text}`
      }]
    })

    const content = response.content[0].type === 'text' ? response.content[0].text : '{}'
    return schema.parse(JSON.parse(content))
  } catch (error) {
    console.error('Constrained model also failed, using format fixer')
  }

  // Tier 3: Format-Fixer Microservice (deterministic)
  return await formatFixerService.forceValid(text, schema)
}
```

### Pattern 3: Format-Fixer Microservice

```typescript
/**
 * Deterministic service that GUARANTEES valid output
 * Never throws, always returns something valid
 */
class FormatFixerService {
  /**
   * Coerce any input into valid schema format
   */
  forceValid<T>(input: any, schema: z.ZodSchema<T>): T {
    const schemaShape = schema._def.shape()
    const result: any = {}

    for (const [field, fieldSchema] of Object.entries(schemaShape)) {
      const value = input[field]

      // String coercion
      if (fieldSchema instanceof z.ZodString) {
        result[field] = String(value || '')

        // Enum validation
        if (fieldSchema._def.checks) {
          for (const check of fieldSchema._def.checks) {
            if (check.kind === 'enum') {
              if (!check.values.includes(result[field])) {
                result[field] = check.values[0]  // Default to first enum value
              }
            }
          }
        }

        // Date format fixing (critical for scheduling)
        if (field.includes('date')) {
          result[field] = this.coerceToISO8601(value)
        }

        // Email validation
        if (field.includes('email')) {
          result[field] = this.coerceToEmail(value)
        }
      }

      // Number coercion
      if (fieldSchema instanceof z.ZodNumber) {
        result[field] = Number(value) || 0

        // Min/max enforcement
        if (fieldSchema._def.checks) {
          for (const check of fieldSchema._def.checks) {
            if (check.kind === 'min' && result[field] < check.value) {
              result[field] = check.value
            }
            if (check.kind === 'max' && result[field] > check.value) {
              result[field] = check.value
            }
          }
        }
      }

      // Boolean coercion
      if (fieldSchema instanceof z.ZodBoolean) {
        result[field] = Boolean(value)
      }

      // Array coercion
      if (fieldSchema instanceof z.ZodArray) {
        result[field] = Array.isArray(value) ? value : []
      }
    }

    return result as T
  }

  private coerceToISO8601(value: any): string {
    const str = String(value)

    // Already ISO 8601
    if (/^\d{4}-\d{2}-\d{2}$/.test(str)) {
      return str
    }

    // MM/DD/YYYY
    if (/^\d{2}\/\d{2}\/\d{4}$/.test(str)) {
      const [month, day, year] = str.split('/')
      return `${year}-${month.padStart(2, '0')}-${day.padStart(2, '0')}`
    }

    // Natural language
    if (/tomorrow/i.test(str)) {
      return addDays(new Date(), 1).toISOString().split('T')[0]
    }

    if (/next week/i.test(str)) {
      return addDays(new Date(), 7).toISOString().split('T')[0]
    }

    const daysMatch = str.match(/in (\d+) days?/i)
    if (daysMatch) {
      return addDays(new Date(), parseInt(daysMatch[1])).toISOString().split('T')[0]
    }

    // Fallback: tomorrow
    return addDays(new Date(), 1).toISOString().split('T')[0]
  }

  private coerceToEmail(value: any): string {
    const str = String(value)

    // Already looks like email
    if (/@/.test(str)) {
      return str.toLowerCase().trim()
    }

    // Fallback: empty string (will require manual entry)
    return ''
  }
}

// Utility function
function addDays(date: Date, days: number): Date {
  const result = new Date(date)
  result.setDate(result.getDate() + days)
  return result
}
```

### Production Implementation: Complete Fallback Chain

```typescript
/**
 * Production-grade extraction with 4-tier fallback
 */
async function extractWithFullFallback<T>(
  text: string,
  schema: z.ZodSchema<T>,
  options: {
    defaults?: FallbackDefaults
    maxAttempts?: number
    enableFallback?: boolean
  } = {}
): Promise<{ data: T; tier: number; fallbackUsed: boolean }> {
  const { defaults = {}, maxAttempts = 3, enableFallback = true } = options

  let attempts = 0

  // Tier 1: Primary LLM (multiple attempts)
  while (attempts < maxAttempts) {
    attempts++
    try {
      const response = await anthropic.messages.create({
        model: 'claude-3-5-sonnet-20240620',
        max_tokens: 1024,
        temperature: 0.7,
        messages: [{ role: 'user', content: text }]
      })

      const content = response.content[0].type === 'text' ? response.content[0].text : '{}'
      const data = schema.parse(JSON.parse(content))

      return { data, tier: 1, fallbackUsed: false }
    } catch (error) {
      console.warn(`Tier 1 attempt ${attempts} failed`)
    }
  }

  if (!enableFallback) {
    throw new Error('Extraction failed and fallback is disabled')
  }

  // Tier 2: Constrained LLM (Haiku, single attempt)
  try {
    const response = await anthropic.messages.create({
      model: 'claude-3-haiku-20240307',
      max_tokens: 512,
      temperature: 0.0,
      system: 'Output ONLY valid JSON. No explanations.',
      messages: [{ role: 'user', content: `Fix to valid JSON:\n${text}` }]
    })

    const content = response.content[0].type === 'text' ? response.content[0].text : '{}'
    const data = schema.parse(JSON.parse(content))

    console.log('‚úÖ Tier 2 (Constrained LLM) succeeded')
    return { data, tier: 2, fallbackUsed: true }
  } catch (error) {
    console.warn('Tier 2 (Constrained LLM) failed')
  }

  // Tier 3: Format-Fixer (deterministic coercion)
  try {
    const formatFixer = new FormatFixerService()
    const data = formatFixer.forceValid(text, schema)

    console.log('‚úÖ Tier 3 (Format-Fixer) succeeded')
    return { data, tier: 3, fallbackUsed: true }
  } catch (error) {
    console.error('Tier 3 (Format-Fixer) failed (rare)')
  }

  // Tier 4: Hard-Coded Defaults (guaranteed success)
  const data: any = {}
  for (const [field, defaultValue] of Object.entries(defaults)) {
    if (typeof defaultValue === 'function') {
      data[field] = defaultValue()
    } else {
      data[field] = defaultValue
    }
  }

  console.log('‚ö†Ô∏è Tier 4 (Hard-Coded Defaults) used')
  return { data: data as T, tier: 4, fallbackUsed: true }
}
```

### Monitoring Fallback Usage

```typescript
interface FallbackMetrics {
  tier1Success: number  // Primary LLM
  tier2Success: number  // Constrained LLM
  tier3Success: number  // Format-Fixer
  tier4Success: number  // Hard-Coded Defaults
  totalExtractions: number
}

const metrics: FallbackMetrics = {
  tier1Success: 0,
  tier2Success: 0,
  tier3Success: 0,
  tier4Success: 0,
  totalExtractions: 0
}

async function trackFallbackUsage(tier: number): Promise<void> {
  metrics.totalExtractions++

  if (tier === 1) metrics.tier1Success++
  if (tier === 2) metrics.tier2Success++
  if (tier === 3) metrics.tier3Success++
  if (tier === 4) metrics.tier4Success++

  // Alert if fallback usage is too high
  if (metrics.totalExtractions % 100 === 0) {
    const tier1Rate = (metrics.tier1Success / metrics.totalExtractions) * 100
    const fallbackRate = ((metrics.tier2Success + metrics.tier3Success + metrics.tier4Success) / metrics.totalExtractions) * 100

    console.log(`
Fallback Metrics (last ${metrics.totalExtractions} extractions):
  Tier 1 (Primary): ${tier1Rate.toFixed(1)}%
  Tier 2 (Constrained): ${((metrics.tier2Success / metrics.totalExtractions) * 100).toFixed(1)}%
  Tier 3 (Format-Fixer): ${((metrics.tier3Success / metrics.totalExtractions) * 100).toFixed(1)}%
  Tier 4 (Defaults): ${((metrics.tier4Success / metrics.totalExtractions) * 100).toFixed(1)}%
  Total Fallback Rate: ${fallbackRate.toFixed(1)}%
`)

    // Alert if primary success drops below 90%
    if (tier1Rate < 90) {
      await sendAlert({
        type: 'extraction_quality_degradation',
        message: `Primary LLM success rate dropped to ${tier1Rate.toFixed(1)}%. Review prompts and schema.`,
        severity: 'warning'
      })
    }

    // Alert if Tier 4 (defaults) used >1%
    const tier4Rate = (metrics.tier4Success / metrics.totalExtractions) * 100
    if (tier4Rate > 1) {
      await sendAlert({
        type: 'excessive_fallback_usage',
        message: `Hard-coded defaults used ${tier4Rate.toFixed(1)}% of time. Investigate schema issues.`,
        severity: 'critical'
      })
    }
  }
}
```

### Production Results: Healthcare Scheduling System

**Before Deterministic Fallback**:
- 12% of requests timed out (infinite retry loop)
- Average time to failure: 45 seconds
- Patient abandonment rate: 68%
- **Cost**: $280,000/year in lost bookings

**After 4-Tier Fallback Hierarchy**:
- 0% timeouts (always returns valid output)
- Tier 1 (Primary) success: 94%
- Tier 2 (Constrained) success: 5%
- Tier 3 (Format-Fixer) used: 1%
- Tier 4 (Defaults) used: &lt;0.1%
- Average response time: 1.2 seconds
- Patient abandonment: 8%
- **ROI**: $260,000/year in recovered bookings

### Target Metrics

| Metric | Target | Alert Threshold |
|--------|--------|-----------------|
| **Tier 1 Success** | &gt;90% | &lt;85% (schema needs improvement) |
| **Tier 2 Usage** | &lt;8% | &gt;15% (LLM struggles with format) |
| **Tier 3 Usage** | &lt;2% | &gt;5% (systematic issues) |
| **Tier 4 Usage** | &lt;0.5% | &gt;1% (critical, investigate immediately) |
| **Total Timeouts** | 0% | &gt;0.1% (fallback chain broken) |

### When to Use Each Tier

**Tier 1 (Primary LLM)**:
- ‚úÖ Complex extraction requiring reasoning
- ‚úÖ Ambiguous input needing interpretation
- ‚úÖ Multi-field dependencies

**Tier 2 (Constrained LLM)**:
- ‚úÖ Format correction (dates, emails, phone numbers)
- ‚úÖ Simple JSON structure fixes
- ‚úÖ Fast fallback (&lt;500ms)

**Tier 3 (Format-Fixer)**:
- ‚úÖ Deterministic coercion required
- ‚úÖ Known input patterns (date formats)
- ‚úÖ Type conversions (string ‚Üí number)

**Tier 4 (Hard-Coded Defaults)**:
- ‚úÖ Graceful degradation for rare edge cases
- ‚úÖ Partial extraction (some fields valid, others default)
- ‚úÖ Better than 500 error

**Architect's Tip**: "In a mission-critical API, a 500 error is a failure of architecture. Always have a 'Hard-Coded Default' or a 'Format-Fixer' microservice that ensures the downstream system receives something valid, even if the LLM is hallucinating."

## Production Checklist

Before shipping structured output to production:

- [ ] **Define Zod schema** with constraints (min/max, enums, regex)
- [ ] **Validate all outputs** - never trust LLM blindly
- [ ] **Handle parse errors** - JSON.parse can throw
- [ ] **Handle validation errors** - schema.parse can throw
- [ ] **Test edge cases** - empty strings, nulls, unexpected formats
- [ ] **Add retry logic** - LLM might return invalid JSON occasionally
- [ ] **Log failures** - track when LLM doesn't follow schema
- [ ] **Set max_tokens** appropriately - too low = truncated JSON
- [ ] **Monitor schema adherence rate** - should be &gt;99%

---

---

## Architect Challenge: API Integrity Failure Simulation

### The Production Incident

You are the AI Architect for **EnterpriseFlow**, a B2B workflow automation platform. Your structured output system routes 10,000 support tickets/day directly into **Salesforce**.

**The Integration Contract**: Salesforce expects a `customer_id` in the format `CUST-XXXX` (e.g., `CUST-1234`).

**The Failure**: After 3 weeks in production, your ops team reports:
- **500 errors spiking** from Salesforce API (15% of tickets)
- **Manual investigation** reveals the LLM occasionally sends:
  - `ID-1234` (wrong prefix)
  - `1234` (missing prefix entirely)
  - `CUSTOMER-1234` (verbose prefix)
  - `cust-1234` (lowercase)

**Business Impact**:
- 1,500 tickets/day stuck in failed state
- Support agents manually fixing `customer_id` fields (4 hours/day)
- Salesforce API rate limits triggered by retry storms
- **$50K/month** in wasted labor + customer escalations

---

### Challenge: How Do You Solve This at the Architectural Layer?

**Option A: Add "please use CUST- prefix" to the system prompt**

*Reasoning*: More explicit prompting should improve adherence.

*Prediction*: Reduces failures from 15% to 8%, but still not production-grade.

*Why it fails*:
- LLMs are probabilistic, not deterministic
- Even with explicit prompts, edge cases slip through
- No enforcement mechanism for invalid values
- Still requires manual intervention for failures

**Option B: Use Zod/Pydantic Schema with Regex Validation + Auto-Retry Loop** ‚úÖ **CORRECT**

*Reasoning*: Code-level constraints + feedback loop enforces data contracts.

*Architecture*:

```typescript
import { z } from 'zod'
import Anthropic from '@anthropic-ai/sdk'

// Step 1: Define schema with strict regex validation
const SalesforceTicketSchema = z.object({
  customer_id: z.string().regex(/^CUST-\d{4}$/, {
    message: 'customer_id must match format CUST-XXXX (e.g., CUST-1234)'
  }),
  category: z.enum(['billing', 'technical', 'feature']),
  priority: z.number().int().min(1).max(10)
})

type SalesforceTicket = z.infer<typeof SalesforceTicketSchema>

// Step 2: Self-healing extraction with validation feedback
async function extractWithValidation(
  ticketText: string,
  maxAttempts: number = 3
): Promise<SalesforceTicket> {
  let lastError: string = ''

  for (let attempt = 1; attempt <= maxAttempts; attempt++) {
    // Build prompt with error feedback on retries
    const systemPrompt = `Extract support ticket data.

Schema requirements:
- customer_id: MUST be format "CUST-XXXX" (4 digits, e.g., CUST-1234)
- category: MUST be one of "billing", "technical", "feature"
- priority: Integer 1-10

${attempt > 1 ? `\n‚ö†Ô∏è PREVIOUS ATTEMPT FAILED:\n${lastError}\n\nFIX THE ERROR AND RETRY.` : ''}`

    const response = await anthropic.messages.create({
      model: 'claude-3-5-sonnet-20240620',
      max_tokens: 500,
      system: systemPrompt,
      messages: [{
        role: 'user',
        content: `Extract ticket data from: "${ticketText}"`
      }]
    })

    const content = response.content[0].type === 'text'
      ? response.content[0].text
      : ''

    try {
      // Parse JSON
      const parsed = JSON.parse(content)

      // Validate with Zod (throws if regex fails)
      const validated = SalesforceTicketSchema.parse(parsed)

      console.log(`‚úÖ Success on attempt ${attempt}`)
      return validated
    } catch (error) {
      if (error instanceof z.ZodError) {
        // Build detailed feedback for LLM
        lastError = error.errors
          .map(e => `Field "${e.path.join('.')}": ${e.message}`)
          .join('\n')

        console.log(`Attempt ${attempt} failed validation:`, lastError)

        if (attempt === maxAttempts) {
          throw new Error(`Validation failed after ${maxAttempts} attempts: ${lastError}`)
        }

        // Loop continues with error feedback
      } else {
        throw error
      }
    }
  }

  throw new Error('Extraction failed')
}

// Step 3: Usage with guaranteed valid output
const ticket = await extractWithValidation(
  'Customer CUST-5678 needs help with billing issue (high priority)'
)
// ‚Üí { customer_id: 'CUST-5678', category: 'billing', priority: 9 }

// Safe to send to Salesforce (guaranteed to match contract)
await salesforce.createTicket(ticket)  // No 500 errors!
```

*Why it works*:
- **Regex validation** catches all invalid formats (ID-1234, 1234, CUSTOMER-1234)
- **Auto-retry with feedback** gives LLM a second chance to self-correct
- **95% success rate** on retry (from initial 85% to 95% √ó 0.85 = ~99% total)
- **Zero manual intervention** for schema violations

*Performance*:
- Before: 15% failures (1,500/day), 4 hours manual fixing
- After: 0.2% failures (20/day), 5 minutes manual review
- **ROI**: $48K/month savings in labor

**Option C: Manually edit the IDs in Salesforce to match the LLM**

*Reasoning*: Fix the problem at the destination.

*Why it fails*:
- Unsustainable at 1,500 errors/day
- Doesn't address root cause (invalid LLM outputs)
- Creates data inconsistency between systems
- Violates single source of truth principle

**Option D: Use a more expensive model (Opus) and hope it's more accurate**

*Reasoning*: Smarter model = fewer mistakes.

*Prediction*: Reduces failures from 15% to 10%, but 5x cost increase.

*Why it fails*:
- More expensive doesn't guarantee format compliance
- Doesn't address systemic validation gap
- Cost increase: $15K/month ‚Üí $75K/month
- Still requires manual intervention for failures

---

### The Correct Answer: B

**The Architect's Principle**: Enforce data contracts at the **code level**, not the **prompt level**. LLMs are probabilistic; validation + feedback loops are deterministic.

**Production Pattern**:

```typescript
class ProductionTicketExtractor {
  private schema = SalesforceTicketSchema
  private maxAttempts = 3

  async extract(ticketText: string): Promise<{
    data: SalesforceTicket
    attempts: number
    repaired: boolean
  }> {
    for (let attempt = 1; attempt <= this.maxAttempts; attempt++) {
      try {
        const extracted = await this.attemptExtraction(ticketText, attempt)
        const validated = this.schema.parse(extracted)

        return {
          data: validated,
          attempts: attempt,
          repaired: attempt > 1
        }
      } catch (error) {
        if (attempt === this.maxAttempts) {
          // All retries exhausted - escalate to human
          await this.escalateToHuman(ticketText, error)
          throw error
        }

        // Log attempt for debugging
        await this.logValidationFailure(ticketText, attempt, error)
      }
    }

    throw new Error('Extraction failed')
  }

  private async escalateToHuman(
    ticketText: string,
    error: unknown
  ): Promise<void> {
    await db.manualReviewQueue.create({
      data: {
        ticketText,
        error: error instanceof Error ? error.message : 'Unknown error',
        timestamp: new Date(),
        priority: 'high'
      }
    })

    // Alert ops team
    await slack.send({
      channel: '#ops-alerts',
      text: `üö® Ticket extraction failed after ${this.maxAttempts} attempts. Manual review required.`
    })
  }
}
```

**Monitoring Dashboard**:

```typescript
// Track extraction success rates
interface ExtractionMetrics {
  totalAttempts: number
  firstTrySuccess: number      // 85%
  secondTrySuccess: number     // 95% of failures
  thirdTrySuccess: number      // 98% of remaining
  totalFailures: number        // 0.2%

  commonErrors: Map<string, number>  // customer_id format: 1,200 times
}

// Alert thresholds:
// - First-try success < 80% ‚Üí Review system prompt
// - Total failure rate > 1% ‚Üí Escalate to engineering
// - Specific error > 5% of total ‚Üí Add explicit validation rule
```

**ROI Summary**:

| Metric | Before (No Validation) | Option A (Prompting) | Option B (Validation + Retry) |
|--------|------------------------|----------------------|-------------------------------|
| **Failure Rate** | 15% | 8% | 0.2% |
| **Failed Tickets/Day** | 1,500 | 800 | 20 |
| **Manual Labor/Day** | 4 hours | 2 hours | 5 minutes |
| **Monthly Labor Cost** | $50,000 | $25,000 | $500 |
| **API Retry Costs** | $5,000 | $3,000 | $50 |
| **Model Costs** | $15,000 | $15,000 | $16,500 (+10% for retries) |
| **Total Monthly Cost** | **$70,000** | **$43,000** | **$17,050** |

**Savings**: $52,950/month = **$635K/year**

---

### Key Architectural Lessons

1. **Prompts are suggestions, schemas are contracts**
   - LLMs are probabilistic ‚Üí 85-95% adherence
   - Code validation is deterministic ‚Üí 100% enforcement

2. **Validation + Retry beats bigger models**
   - Claude Sonnet + retry (99%) < Opus alone (90%)
   - Cost: 10% increase vs 5x increase

3. **Fail fast, fail loud**
   - Detect failures immediately (Zod validation)
   - Auto-retry with error feedback
   - Escalate remaining 0.2% to humans

4. **Single source of truth**
   - Schema defined once (Zod)
   - Used for: LLM prompts, validation, TypeScript types, API contracts
   - Schema evolution via migrations (versioning)


## Key Takeaways

### From Chatbot to System Component

| Approach | Production-Ready? | Use Case |
|----------|------------------|----------|
| Plain text | ‚ùå No | Demos, prototypes |
| JSON mode (prompting) | ‚ö†Ô∏è Risky | Quick experiments |
| Structured output + validation | ‚úÖ Yes | Production systems |

### The Architect's Mindset

> "An LLM that can't produce structured data is just an expensive chatbot. Structured output transforms LLMs into **callable functions** that integrate with your existing systems."

**Three rules**:
1. **Always define a schema** - Zod, JSON Schema, or TypeScript interface
2. **Always validate** - Don't trust LLM output blindly
3. **Always handle errors** - Parse failures happen

### Why This Matters

- **UI components** need typed data, not strings
- **Databases** require structured fields, not paragraphs
- **APIs** expect JSON contracts, not prose
- **Type safety** catches bugs at compile time, not runtime

Without structured output, your AI is isolated from your stack. With it, AI becomes a **first-class system component**.

---

## Next Steps

- [Function Calling](./function-calling.mdx) - Let LLMs call your APIs and tools
- [Schema Design](./schema-design.mdx) - Design robust API contracts for LLMs
- [Week 5: AI Agents](../../week5/) - Build autonomous agents with tool use

---

## Further Reading

### Official Documentation
- [Anthropic: Structured Output](https://docs.anthropic.com/en/docs/build-with-claude/structured-outputs)
- [OpenAI: Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)
- [Zod Documentation](https://zod.dev/)

### Research
- [Constrained Decoding for Structured Generation](https://arxiv.org/abs/2307.09702)
- [JSON Mode vs Structured Outputs: When to Use Each](https://community.openai.com/t/json-mode-vs-structured-outputs)
