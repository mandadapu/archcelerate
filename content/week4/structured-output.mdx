---
title: 'Structured Output & JSON Mode'
description: 'Transform LLMs from chatbots to system components with structured data'
estimatedTime: 40
difficulty: 'intermediate'
objectives:
  - Understand why structured data is critical for production AI
  - Master JSON mode and structured output patterns
  - Design type-safe schemas for LLM responses
  - Handle validation and error cases
---

# Structured Output & JSON Mode

---

## ğŸ¯ Real-World Challenge: The Enterprise Support Orchestrator

**The Problem**: An enterprise SaaS company receives 500 unstructured support emails daily. Manual triage takes 3 hours/day, and 40% of tickets are routed to the wrong team, delaying resolution by 2-3 days. Support costs are **$180K/year** in wasted labor.

**Business Constraints**:
- **Speed**: Must process emails in <2 seconds (users expect instant routing)
- **Accuracy**: >95% correct routing to engineering, billing, or product teams
- **Integration**: Must populate Zendesk/Salesforce with structured data (category, priority, customer_tier)
- **Zero Hallucinations**: Cannot invent ticket IDs or priority levelsâ€”database expects exact enums

**The Architectural Problem**: LLMs return **text**, but your support system needs **typed data**:

```typescript
// âŒ What you get from raw LLM
"This is a very high priority issue from an enterprise customer about billing"

// âœ… What your database needs
{
  category: 'billing',           // enum: 'technical' | 'billing' | 'feature_request'
  priority: 10,                  // integer: 1-10
  sentiment: 'frustrated',       // enum: 'satisfied' | 'neutral' | 'frustrated'
  customer_tier: 'enterprise',   // enum: 'free' | 'pro' | 'enterprise'
  product_id: 'SKU-12345'        // string: must match regex /SKU-\d{5}/
}
```

**Architectural Solution: Type-Safe Extraction with Structured Outputs**

Use Anthropic's **Structured Outputs** (strict schema enforcement) instead of JSON Mode (prompt engineering):

```typescript
import Anthropic from '@anthropic-ai/sdk'

// Step 1: Define TypeScript schema (enforced at API level)
const TicketSchema = {
  type: 'object',
  properties: {
    category: { type: 'string', enum: ['technical', 'billing', 'feature_request'] },
    priority: { type: 'integer', minimum: 1, maximum: 10 },
    sentiment: { type: 'string', enum: ['satisfied', 'neutral', 'frustrated'] },
    customer_tier: { type: 'string', enum: ['free', 'pro', 'enterprise'] },
    product_id: { type: 'string', pattern: '^SKU-\\d{5}$' }
  },
  required: ['category', 'priority', 'sentiment', 'customer_tier']
}

// Step 2: LLM enforces schema (cannot hallucinate values)
const response = await anthropic.messages.create({
  model: 'claude-3-5-sonnet-20240620',
  max_tokens: 1024,
  tools: [{
    name: 'extract_ticket',
    description: 'Extract structured ticket data',
    input_schema: TicketSchema
  }],
  tool_choice: { type: 'tool', name: 'extract_ticket' },
  messages: [{ role: 'user', content: rawEmail }]
})

// Step 3: Type-safe result (guaranteed to match schema)
const ticket: TicketData = response.content[0].input
// â†’ { category: 'billing', priority: 10, ... }

// Step 4: Direct database insert (no validation needed)
await db.tickets.create({ data: ticket })
```

**Production Impact**:

**Before (Manual Triage)**:
- Processing time: 3 hours/day
- Mis-routing rate: 40%
- Average resolution time: 3.2 days
- Annual cost: $180K in labor

**After (Structured Output Automation)**:
- Processing time: **3 minutes/day** (60x faster)
- Mis-routing rate: **3.8%** (10x improvement)
- Average resolution time: **0.9 days** (3.5x faster)
- Annual cost: **$12K** in API costs + $8K monitoring = **$20K total**
- **ROI**: $160K/year savings = **800% ROI**

**The Critical Difference**: JSON Mode vs Structured Outputs

| Approach | Schema Enforcement | Hallucination Risk | Validation Needed | Production Ready |
|----------|-------------------|-------------------|-------------------|------------------|
| **JSON Mode** | âŒ Prompt-based (unreliable) | âš ï¸ High (invents "very high" priority) | âœ… Required | âš ï¸ Risky |
| **Structured Outputs** | âœ… API-level (guaranteed) | âœ… None (rejects invalid values) | âŒ Not needed | âœ… Yes |

**Real Example of JSON Mode Failure**:
```typescript
// User: "URGENT!!! Our app keeps crashing!"

// JSON Mode (prompt engineering)
{ "priority": "very urgent" }  // âŒ Database expects 1-10, gets string

// Structured Outputs (API enforcement)
{ "priority": 10 }  // âœ… LLM forced to use valid integer
```

**[ğŸ‘‰ Lab: Build the Support Ticket Router](/curriculum/week-4/labs/support-ticket-router)**

In the hands-on lab, you'll:
1. Compare JSON Mode vs Structured Outputs on 100 real support emails
2. Measure hallucination rates (JSON Mode: 18%, Structured: 0%)
3. Build production-ready Zendesk integration
4. Deploy with full error handling and self-healing retries

**Key Architectural Insight**: Structured Outputs transforms LLMs from **text generators** into **system components** that integrate seamlessly with databases, APIs, and workflows.

---

## Real-World Industry Application: Support Auto-Router with Tool Calling

### Business Context: Enterprise SaaS Customer Support Automation

**The Challenge**: A B2B SaaS company processes 500 support tickets daily. 60% are "tier check" questions ("What features do I have?", "Can I upgrade?") that require looking up customer subscription data. Manual routing wastes 4 hours/day and delays resolutions.

**Business Constraints**:
- **Target**: 40% instant resolution (no human handoff)
- **Data Privacy**: Cannot hallucinate subscription tiers (legal risk)
- **Integration**: Must query real Zendesk API for accurate data
- **Cost**: Each Zendesk API call costs $0.02, can't afford to query unnecessarily

**The Architectural Problem**: You need the LLM to:
1. **Classify** the ticket (billing, technical, feature_request)
2. **Decide** if it needs external data (subscription info)
3. **Call a tool** (query Zendesk API) only when necessary
4. **Generate** a structured response with the subscription data

**Before: Static Structured Output (50% hallucination rate)**
```typescript
// âŒ LLM invents subscription tier when it doesn't know
{
  "category": "billing",
  "priority": "high",
  "subscription_tier": "enterprise", // HALLUCINATED - user might be on 'free'!
  "suggested_fix": "Contact billing team"
}
```

**After: Structured Output + Tool Calling (0% hallucination, 40% instant resolution)**

### Implementation: Tool-Augmented Support Router

```typescript
import Anthropic from '@anthropic-ai/sdk'

const anthropic = new Anthropic({ apiKey: process.env.ANTHROPIC_API_KEY })

// Step 1: Define structured output schema
const SupportTicketSchema = {
  type: 'object' as const,
  properties: {
    category: {
      type: 'string',
      enum: ['billing', 'technical', 'feature_request', 'account_management']
    },
    priority: {
      type: 'integer',
      minimum: 1,
      maximum: 10
    },
    sentiment: {
      type: 'string',
      enum: ['satisfied', 'neutral', 'frustrated', 'angry']
    },
    requiresHuman: {
      type: 'boolean',
      description: 'Whether this requires human escalation'
    },
    suggestedFix: {
      type: 'string',
      description: 'Immediate resolution if possible, or next steps'
    }
  },
  required: ['category', 'priority', 'sentiment', 'requiresHuman', 'suggestedFix']
}

// Step 2: Define tool for external API lookup
const tools: Anthropic.Tool[] = [
  {
    name: 'query_zendesk_subscription',
    description: `Query Zendesk API to get the customer's current subscription tier and features.
Use this ONLY when the ticket is about billing, features, or upgrades.
DO NOT call this for general technical questions.`,
    input_schema: {
      type: 'object',
      properties: {
        user_email: {
          type: 'string',
          description: 'Customer email address from the ticket'
        },
        query_reason: {
          type: 'string',
          description: 'Why you need subscription data (e.g., "check upgrade eligibility")'
        }
      },
      required: ['user_email', 'query_reason']
    }
  },
  {
    name: 'classify_support_ticket',
    description: 'Classify and route the support ticket based on content analysis',
    input_schema: SupportTicketSchema
  }
]

// Step 3: Implement tool handlers
async function queryZendeskSubscription(
  userEmail: string,
  queryReason: string
): Promise<{ tier: string; features: string[]; renewal_date: string }> {
  // In production: Call real Zendesk API
  console.log(`[ZENDESK API] Looking up ${userEmail} (reason: ${queryReason})`)

  // Simulated API call
  const response = await fetch(`https://api.zendesk.com/v2/users/search?query=${userEmail}`, {
    headers: {
      'Authorization': `Bearer ${process.env.ZENDESK_API_KEY}`
    }
  })

  const data = await response.json()

  return {
    tier: data.subscription?.tier || 'free',
    features: data.subscription?.features || ['basic_support', 'email_integration'],
    renewal_date: data.subscription?.renewal_date || '2026-03-15'
  }
}

// Step 4: Orchestrate LLM with tool calling
async function routeSupportTicket(
  ticketContent: string,
  userEmail: string
): Promise<{
  classification: any
  subscriptionData?: any
  resolutionMessage: string
  cost: number
}> {
  let cost = 0.015 // Base LLM cost
  let subscriptionData: any = null

  // First LLM call: Analyze ticket and decide if tool use needed
  const response = await anthropic.messages.create({
    model: 'claude-3-5-sonnet-20240620',
    max_tokens: 2048,
    tools,
    tool_choice: { type: 'auto' }, // Let LLM decide which tool(s) to call
    messages: [{
      role: 'user',
      content: `Analyze this support ticket and route appropriately.

Ticket from: ${userEmail}
Content: "${ticketContent}"

If this is about billing, features, or upgrades, use query_zendesk_subscription to get accurate data.
Otherwise, classify directly using classify_support_ticket.`
    }]
  })

  // Check if LLM wants to call tools
  const toolUses = response.content.filter(block => block.type === 'tool_use')

  if (toolUses.length > 0) {
    const toolResults: Anthropic.MessageParam[] = []

    for (const toolUse of toolUses) {
      if (toolUse.type === 'tool_use') {
        if (toolUse.name === 'query_zendesk_subscription') {
          // LLM decided it needs subscription data
          cost += 0.02 // Zendesk API cost
          subscriptionData = await queryZendeskSubscription(
            toolUse.input.user_email,
            toolUse.input.query_reason
          )

          toolResults.push({
            role: 'user',
            content: [{
              type: 'tool_result',
              tool_use_id: toolUse.id,
              content: JSON.stringify(subscriptionData)
            }]
          })
        }
      }
    }

    // Second LLM call: Generate final classification with tool results
    const finalResponse = await anthropic.messages.create({
      model: 'claude-3-5-sonnet-20240620',
      max_tokens: 2048,
      tools,
      tool_choice: { type: 'tool', name: 'classify_support_ticket' }, // Force classification
      messages: [
        { role: 'user', content: `Analyze this support ticket and route appropriately.\n\nTicket from: ${userEmail}\nContent: "${ticketContent}"` },
        { role: 'assistant', content: response.content },
        ...toolResults
      ]
    })

    cost += 0.015 // Second LLM call cost

    // Extract classification from tool use
    const classificationTool = finalResponse.content.find(
      block => block.type === 'tool_use' && block.name === 'classify_support_ticket'
    )

    if (classificationTool && classificationTool.type === 'tool_use') {
      const classification = classificationTool.input

      // Generate resolution message
      let resolutionMessage = classification.suggestedFix

      if (subscriptionData) {
        resolutionMessage += `\n\nYour current plan: ${subscriptionData.tier.toUpperCase()}\nFeatures: ${subscriptionData.features.join(', ')}\nRenewal: ${subscriptionData.renewal_date}`
      }

      return {
        classification,
        subscriptionData,
        resolutionMessage,
        cost
      }
    }
  }

  throw new Error('LLM failed to classify ticket')
}

// Step 5: Usage example
const result = await routeSupportTicket(
  "I want to add more users to my account but I'm not sure if my plan supports it. Can you help?",
  "john.doe@acme.com"
)

console.log('Classification:', result.classification)
// {
//   category: 'billing',
//   priority: 6,
//   sentiment: 'neutral',
//   requiresHuman: false,
//   suggestedFix: 'Checking your current plan limits...'
// }

console.log('Subscription:', result.subscriptionData)
// {
//   tier: 'professional',
//   features: ['team_collaboration', 'advanced_analytics', 'up_to_10_users'],
//   renewal_date: '2026-03-15'
// }

console.log('Resolution:', result.resolutionMessage)
// "Your Professional plan supports up to 10 users. You currently have 7 users,
//  so you can add 3 more without upgrading. To add more, consider our Enterprise plan."

console.log('Cost:', result.cost)
// 0.045 ($0.015 * 2 LLM calls + $0.02 Zendesk API)
```

### Production Outcome Metrics

**Before (Manual Routing)**:
- Processing time: 8 minutes/ticket average
- Mis-routing rate: 35% (wrong team assignment)
- Hallucination rate: 50% (invented subscription data)
- Cost: $180K/year in agent labor

**After (Tool-Augmented Auto-Router)**:
- Processing time: **12 seconds/ticket** (40x faster)
- Mis-routing rate: **2.1%** (accurate with real data)
- Hallucination rate: **0%** (API-verified data)
- Instant resolution: **41%** (exceeded 40% target)
- Cost: $28K/year ($18K API costs + $10K monitoring)
- **ROI**: $152K/year savings = **543% ROI**

**Cost Breakdown (500 tickets/day)**:
- **Category 1: Technical** (200 tickets, 40%)
  - No Zendesk lookup needed
  - Cost: 200 Ã— $0.015 = $3.00/day
  - Instant resolution: 25% (50 tickets)

- **Category 2: Billing/Features** (250 tickets, 50%)
  - Requires Zendesk lookup
  - Cost: 250 Ã— $0.045 = $11.25/day
  - Instant resolution: 60% (150 tickets)

- **Category 3: Complex** (50 tickets, 10%)
  - Requires human escalation
  - Cost: 50 Ã— $0.015 = $0.75/day
  - Instant resolution: 0%

**Total**: $15/day = **$450/month** = **$5,400/year** in LLM costs
**Zendesk API**: 250 Ã— $0.02 Ã— 30 days = **$150/month** = **$1,800/year**
**Total AI costs**: **$7,200/year** vs **$180K/year** manual labor = **96% cost reduction**

### Key Architectural Decisions

**1. Tool Choice: `auto` vs `tool` vs `any`**
```typescript
// Option 1: Auto (LLM decides if tools needed)
tool_choice: { type: 'auto' }
// Use when: You trust LLM to call tools only when necessary
// Cost: Lower (fewer API calls)
// Risk: LLM might skip tools when it should call them

// Option 2: Tool (force specific tool)
tool_choice: { type: 'tool', name: 'query_zendesk_subscription' }
// Use when: You know this request MUST call a specific tool
// Cost: Higher (always calls API)
// Risk: None (deterministic behavior)

// Option 3: Any (force any tool)
tool_choice: { type: 'any' }
// Use when: LLM must use at least one tool, you don't care which
// Cost: Medium
// Risk: LLM might pick wrong tool
```

**2. Multi-Turn Orchestration Pattern**
```typescript
// Turn 1: LLM decides which tools to call
const response1 = await anthropic.messages.create({
  tools: [queryZendeskTool, classifyTicketTool],
  tool_choice: { type: 'auto' }
})

// Turn 2: Execute tools and feed results back
const toolResults = await executeTools(response1.content)

// Turn 3: LLM makes final decision with tool data
const response2 = await anthropic.messages.create({
  messages: [
    originalMessage,
    { role: 'assistant', content: response1.content },
    { role: 'user', content: toolResults }
  ],
  tool_choice: { type: 'tool', name: 'classify_support_ticket' }
})
```

**Why this works**:
- âœ… LLM only calls expensive Zendesk API when genuinely needed (50% of cases)
- âœ… Zero hallucinations (subscription data comes from real API)
- âœ… Structured output ensures database compatibility
- âœ… 40%+ instant resolution (no human handoff)
- âœ… Cost-optimized ($0.015 for simple tickets, $0.045 for complex)

**Alternative Architecture (Not Recommended)**:
```typescript
// âŒ BAD: Always query Zendesk first
const subscriptionData = await queryZendesk(userEmail) // Wastes API call
const classification = await classifyWithData(ticket, subscriptionData)
// Problem: Queries API even for "password reset" tickets (waste of $0.02)
```

**Production Best Practice**:
```typescript
// âœ… GOOD: Let LLM decide if it needs external data
const response = await anthropic.messages.create({
  tools: [queryZendeskTool, classifyTicketTool],
  tool_choice: { type: 'auto' }, // Trust LLM judgment
  messages: [{
    role: 'user',
    content: `Important: Only call query_zendesk_subscription if the ticket
             is about billing, features, or account limits.
             For technical issues or password resets, classify directly.`
  }]
})
// Result: 50% cost savings by avoiding unnecessary API calls
```

---

## The Problem: Unstructured AI

**The fundamental limitation of chatbots**: They produce text, but production systems need **data**.

```typescript
// âŒ What tutorials teach (unusable in production)
const response = await anthropic.messages.create({
  model: 'claude-3-5-sonnet-20240620',
  max_tokens: 1024,
  messages: [{
    role: 'user',
    content: 'What's the weather in San Francisco?'
  }]
})

// Response: "The weather in San Francisco is currently 62Â°F and partly cloudy."
// Problem: How do you extract 62 and "partly cloudy" reliably? String parsing? Regex? ğŸ˜±
```

**Why this fails in production**:
- No type safety
- Parsing errors are common ("62Â°F" vs "62 degrees" vs "62F")
- Can't populate UI components or database fields
- No validation
- Breaks downstream systems

**The solution**: Force the LLM to output **structured JSON** that matches your schema.

---

## The Three Levels of Structured Output

### Level 1: Plain Text (Don't do this)

```typescript
const response = await anthropic.messages.create({
  model: 'claude-3-5-sonnet-20240620',
  max_tokens: 1024,
  messages: [{
    role: 'user',
    content: 'Extract the weather from this: "It\'s 62Â°F and partly cloudy in SF"'
  }]
})

// Returns: "The temperature is 62Â°F and conditions are partly cloudy."
// Problem: Still unstructured! You're back to string parsing.
```

**Issues**:
- LLM adds conversational fluff
- No consistent format
- Parsing is brittle

### Level 2: JSON Mode (Prompt engineering)

```typescript
const response = await anthropic.messages.create({
  model: 'claude-3-5-sonnet-20240620',
  max_tokens: 1024,
  messages: [{
    role: 'user',
    content: `Extract weather data and return ONLY valid JSON:

{
  "temperature": number,
  "unit": "F" | "C",
  "conditions": string,
  "location": string
}

Text: "It's 62Â°F and partly cloudy in SF"

Return only the JSON, no explanation.`
  }]
})

// Returns:
// {
//   "temperature": 62,
//   "unit": "F",
//   "conditions": "partly cloudy",
//   "location": "San Francisco"
// }
```

**Better**, but still risky:
- LLM might add explanation before/after JSON
- No schema validation
- Occasional hallucinated fields
- Requires careful prompt engineering

### Level 3: Structured Output (Production-ready) âœ…

**Anthropic Claude**: Use **schema-constrained generation** by defining expected output format.

```typescript
interface WeatherData {
  temperature: number
  unit: 'F' | 'C'
  conditions: string
  location: string
  timestamp: string
}

const response = await anthropic.messages.create({
  model: 'claude-3-5-sonnet-20240620',
  max_tokens: 1024,
  messages: [{
    role: 'user',
    content: 'Extract weather: "It\'s 62Â°F and partly cloudy in SF"'
  }],
  // Force JSON output
  response_format: { type: 'json_object' },
  system: `You are a weather data extractor. Always respond with valid JSON matching this schema:
{
  "temperature": number,
  "unit": "F" | "C",
  "conditions": string,
  "location": string,
  "timestamp": ISO 8601 string
}`
})

const data: WeatherData = JSON.parse(response.content[0].text)
// Guaranteed to match schema (validate with Zod for extra safety)
```

**OpenAI GPT-4**: Native **Structured Outputs** with JSON Schema enforcement.

```typescript
import OpenAI from 'openai'
const openai = new OpenAI()

const response = await openai.chat.completions.create({
  model: 'gpt-4-turbo-preview',
  messages: [{
    role: 'user',
    content: 'Extract weather: "It\'s 62Â°F and partly cloudy in SF"'
  }],
  response_format: {
    type: 'json_schema',
    json_schema: {
      name: 'weather_extraction',
      schema: {
        type: 'object',
        properties: {
          temperature: { type: 'number' },
          unit: { type: 'string', enum: ['F', 'C'] },
          conditions: { type: 'string' },
          location: { type: 'string' },
          timestamp: { type: 'string', format: 'date-time' }
        },
        required: ['temperature', 'unit', 'conditions', 'location', 'timestamp'],
        additionalProperties: false
      }
    }
  }
})

const data: WeatherData = JSON.parse(response.choices[0].message.content)
```

**Why Level 3 is production-ready**:
- âœ… Guaranteed JSON (no parsing errors)
- âœ… Type-safe with TypeScript interfaces
- âœ… Validated against schema
- âœ… No hallucinated fields
- âœ… Direct integration with APIs/databases

### Structured Output Methodology Comparison

As an AI Architect, the choice between these methods impacts latency, cost, and reliability.

| Feature | JSON Mode | Function / Tool Calling | Structured Outputs (Strict) |
|---------|-----------|------------------------|------------------------------|
| **Primary Goal** | Valid syntax | Action / External interaction | 100% Schema Adherence |
| **Mechanism** | Constrained token generation | LLM-driven "intent" extraction | Finite State Machine (FSM) |
| **Guarantees** | Valid JSON (bracket matching) | High intent accuracy | Deterministic schema match |
| **LLM Reasoning** | Can use Chain-of-Thought (CoT) | Limited CoT within arguments | Best for direct extraction |
| **Best For** | Logging, simple data storage | API calls, Agents, database queries | Mission-critical data ingestion |
| **Provider Support** | OpenAI, Gemini, Mistral | All major frontier models | OpenAI, Gemini (2026 standard) |

**When to use each**:

- **JSON Mode**: Quick prototypes, logging, analytics data where schema drift is acceptable
- **Function Calling**: Orchestration, agentic workflows, multi-step tasks requiring tool use
- **Structured Outputs (Strict)**: Financial transactions, compliance data, API contracts where schema violations are unacceptable

### Pro-Tips for Production

**The "JSON Mode" Trap**

JSON Mode only ensures the output is **syntactically valid JSON**. It does NOT ensure the JSON has the fields you asked for.

```typescript
// âŒ JSON Mode (risky)
// Prompt: "Return JSON with 'temperature' and 'unit'"
// LLM might return: { "temp": 62, "measurement": "F" }
// Valid JSON âœ“, but wrong fields âœ—

// âœ… Structured Outputs (reliable)
// Schema defines exactly: { "temperature": number, "unit": string }
// LLM MUST return those exact fields or fail
```

**For production in 2026, Structured Outputs (Strict Mode) is the gold standard for reliability.**

**Tool Choice as Logic Control**

In Function Calling, setting `tool_choice: "required"` forces the LLM to act as a pure data transformer, effectively disabling its "chatty" personality.

```typescript
// Backend microservice pattern
const response = await openai.chat.completions.create({
  model: 'gpt-4o',
  messages: [{ role: 'user', content: userInput }],
  tools: [extractionTool],
  tool_choice: { type: 'function', function: { name: 'extract_data' } }
  // LLM MUST call this function, no text response allowed
})
```

This is common for backend microservices where you want deterministic extraction, not conversation.

**Token Overhead in Function Calling**

Function descriptions consume **prompt tokens**. If you have 20 complex functions, you're paying for those definitions in **every single API call**.

**Architect's optimization strategy**:
1. **Context-aware tool pruning**: Only send relevant tools based on user context
2. **Tool grouping**: Combine related functions into categories, send only relevant category
3. **Caching**: Use prompt caching for tool definitions (supported by Anthropic Claude)

```typescript
// âŒ Expensive: Send all 20 tools every time
const response = await anthropic.messages.create({
  tools: allTools, // 20 tools * 200 tokens each = 4,000 prompt tokens wasted
  messages: [...]
})

// âœ… Optimized: Send only relevant tools
const relevantTools = getUserContextTools(user.role) // Only 3-5 tools
const response = await anthropic.messages.create({
  tools: relevantTools, // 5 tools * 200 tokens = 1,000 prompt tokens
  messages: [...]
})

// Savings: 3,000 tokens per request = 75% cost reduction on tool definitions
```

---

## Production Pattern: Type-Safe Extraction

```typescript
import { z } from 'zod'
import Anthropic from '@anthropic-ai/sdk'

const anthropic = new Anthropic({ apiKey: process.env.ANTHROPIC_API_KEY })

// 1. Define Zod schema (validation + TypeScript types)
const WeatherSchema = z.object({
  temperature: z.number().min(-100).max(150),
  unit: z.enum(['F', 'C']),
  conditions: z.string().min(1).max(100),
  location: z.string().min(1).max(100),
  timestamp: z.string().datetime(),
  confidence: z.number().min(0).max(1).optional()
})

type WeatherData = z.infer<typeof WeatherSchema>

// 2. Extract with structured output
async function extractWeather(text: string): Promise<WeatherData> {
  const response = await anthropic.messages.create({
    model: 'claude-3-5-sonnet-20240620',
    max_tokens: 500,
    system: `You are a weather data extractor. Extract weather information and return ONLY valid JSON.

Schema:
{
  "temperature": number (min: -100, max: 150),
  "unit": "F" or "C",
  "conditions": string (e.g., "partly cloudy", "sunny"),
  "location": string (city name),
  "timestamp": ISO 8601 datetime string,
  "confidence": optional number 0-1 (how confident you are in extraction)
}

Rules:
- If temperature is ambiguous, use best judgment
- If unit is missing, default to F for US cities, C for others
- If you're unsure, set confidence < 0.8
- timestamp should be current time if not specified`,
    messages: [{
      role: 'user',
      content: `Extract weather data from: "${text}"`
    }]
  })

  // 3. Parse and validate
  const content = response.content[0]
  if (content.type !== 'text') {
    throw new Error('Unexpected response type')
  }

  try {
    const parsed = JSON.parse(content.text)
    const validated = WeatherSchema.parse(parsed)
    return validated
  } catch (error) {
    if (error instanceof z.ZodError) {
      console.error('Schema validation failed:', error.errors)
      throw new Error(`Invalid weather data: ${error.errors.map(e => e.message).join(', ')}`)
    }
    throw error
  }
}

// 4. Usage
const weather = await extractWeather('It\'s 62Â°F and partly cloudy in San Francisco')

console.log(weather.temperature) // TypeScript knows this is a number
console.log(weather.conditions)  // TypeScript knows this is a string

// Safe to use in database
await db.weatherReadings.create({ data: weather })

// Safe to use in API response
return Response.json({ weather })
```

**Key benefits**:
1. **Zod schema**: Runtime validation + TypeScript types from single source
2. **Error handling**: Catch schema violations before they corrupt data
3. **Type safety**: Autocomplete and compile-time checks
4. **Production-ready**: Direct integration with DB/API

---

## Real-World Use Cases

### 1. Form Extraction from User Input

**Problem**: Users type messy natural language, but your database needs structured fields.

```typescript
const ContactSchema = z.object({
  name: z.string().min(1),
  email: z.string().email(),
  phone: z.string().regex(/^\+?[\d\s-()]+$/),
  company: z.string().optional(),
  message: z.string().min(10),
  urgency: z.enum(['low', 'medium', 'high'])
})

async function extractContactForm(userInput: string): Promise<z.infer<typeof ContactSchema>> {
  const response = await anthropic.messages.create({
    model: 'claude-3-5-sonnet-20240620',
    max_tokens: 500,
    system: `Extract contact form data from user input. Return JSON matching this schema:
{
  "name": "full name",
  "email": "valid email",
  "phone": "phone number with country code if available",
  "company": "company name if mentioned (optional)",
  "message": "the actual message/inquiry",
  "urgency": "low" | "medium" | "high" (infer from tone)
}`,
    messages: [{
      role: 'user',
      content: `Extract: "${userInput}"`
    }]
  })

  const parsed = JSON.parse(response.content[0].text)
  return ContactSchema.parse(parsed)
}

// User types:
// "Hi, I'm John Smith from Acme Corp, john@acme.com. Need help ASAP with pricing! Call me at 555-1234"

const form = await extractContactForm(userInput)
// {
//   name: "John Smith",
//   email: "john@acme.com",
//   phone: "555-1234",
//   company: "Acme Corp",
//   message: "Need help with pricing",
//   urgency: "high"
// }

// Save directly to database
await db.contacts.create({ data: form })
```

### 2. Invoice Parsing

```typescript
const InvoiceSchema = z.object({
  invoiceNumber: z.string(),
  date: z.string().datetime(),
  vendor: z.string(),
  total: z.number().positive(),
  currency: z.string().length(3), // USD, EUR, etc.
  lineItems: z.array(z.object({
    description: z.string(),
    quantity: z.number().int().positive(),
    unitPrice: z.number().positive(),
    total: z.number().positive()
  })),
  taxAmount: z.number().nonnegative().optional(),
  dueDate: z.string().datetime().optional()
})

async function parseInvoice(invoiceText: string): Promise<z.infer<typeof InvoiceSchema>> {
  // ... similar pattern
}
```

### 3. Sentiment + Classification

```typescript
const SentimentSchema = z.object({
  sentiment: z.enum(['positive', 'neutral', 'negative']),
  score: z.number().min(-1).max(1), // -1 = very negative, +1 = very positive
  categories: z.array(z.enum(['product_quality', 'customer_service', 'pricing', 'shipping'])),
  actionable: z.boolean(), // Does this require follow-up?
  priority: z.enum(['low', 'medium', 'high', 'urgent'])
})

// Analyze customer feedback
const feedback = await analyzeFeedback('The product broke after 2 days and support is ignoring me!')
// {
//   sentiment: 'negative',
//   score: -0.9,
//   categories: ['product_quality', 'customer_service'],
//   actionable: true,
//   priority: 'urgent'
// }
```

---

## Common Pitfalls

### Pitfall 1: Forgetting to Validate

```typescript
// âŒ BAD: Trust LLM output blindly
const response = await anthropic.messages.create({ ... })
const data = JSON.parse(response.content[0].text)
await db.save(data) // DANGEROUS! What if LLM hallucinated fields?
```

```typescript
// âœ… GOOD: Always validate
const response = await anthropic.messages.create({ ... })
const parsed = JSON.parse(response.content[0].text)
const validated = MySchema.parse(parsed) // Throws if invalid
await db.save(validated)
```

### Pitfall 2: Vague Schemas

```typescript
// âŒ BAD: Too loose
const schema = z.object({
  data: z.any(), // LLM can return anything!
  result: z.string() // Could be empty, too long, wrong format
})
```

```typescript
// âœ… GOOD: Specific constraints
const schema = z.object({
  temperature: z.number().min(-100).max(150), // Realistic bounds
  status: z.enum(['success', 'error']), // Explicit options
  message: z.string().min(1).max(500) // Length limits
})
```

### Pitfall 3: Missing Error Handling

```typescript
// âŒ BAD: Assume parsing always works
const data = JSON.parse(response.content[0].text)
```

```typescript
// âœ… GOOD: Handle parse errors
try {
  const data = JSON.parse(response.content[0].text)
  const validated = MySchema.parse(data)
  return validated
} catch (error) {
  if (error instanceof SyntaxError) {
    console.error('LLM returned invalid JSON:', response.content[0].text)
  } else if (error instanceof z.ZodError) {
    console.error('Schema validation failed:', error.errors)
  }
  throw new Error('Failed to extract structured data')
}
```

---

## Solving Common Failures in Production

Even with the best architecture, LLMs can fail when forced into strict structures. As an architect, you must design for these edge cases to ensure system reliability.

### Failure 1: The "Max Tokens" Cutoff

**The Problem**: JSON requires a closing brace `}`. If the LLM hits its `max_tokens` limit mid-generation, the JSON is malformed and your parser will crash.

```typescript
// âŒ LLM hit token limit mid-JSON
{
  "name": "John Smith",
  "email": "john@example.com",
  "address": {
    "street": "123 Main St",
    "city": "San Francisco",
    // [MAX_TOKENS REACHED - JSON TRUNCATED]
```

**Architect's Fixes**:

**1. Pre-calculate token budget**
```typescript
// Estimate output size and add buffer
const estimatedOutputTokens = calculateEstimatedTokens(schema)
const maxTokens = Math.ceil(estimatedOutputTokens * 1.5) // 50% buffer

const response = await anthropic.messages.create({
  model: 'claude-3-5-sonnet-20240620',
  max_tokens: maxTokens, // Generous buffer prevents truncation
  messages: [...]
})
```

**2. Use a fallback parser**
```typescript
import { parsePartialJson } from 'partial-json-parser'

try {
  return JSON.parse(content)
} catch (error) {
  console.warn('JSON truncated, attempting repair...')

  // Attempts to add missing closing brackets/braces
  const repaired = parsePartialJson(content)

  if (repaired) {
    console.log('Successfully repaired truncated JSON')
    return repaired
  }

  throw new Error('JSON parsing failed and repair unsuccessful')
}
```

**3. Use streaming for large outputs**
```typescript
const stream = await anthropic.messages.stream({
  model: 'claude-3-5-sonnet-20240620',
  max_tokens: 4096,
  messages: [...]
})

let partialJson = ''

for await (const chunk of stream) {
  if (chunk.type === 'content_block_delta' && chunk.delta.type === 'text_delta') {
    partialJson += chunk.delta.text

    // Try parsing incrementally
    try {
      const parsed = JSON.parse(partialJson)
      // Success! We have complete JSON
      return parsed
    } catch {
      // Not complete yet, continue streaming
    }
  }
}
```

### Failure 2: Hallucinated Arguments (The "Imaginary User ID")

**The Problem**: When using Function Calling, the LLM might invent a required argument (like a `transaction_id`) that wasn't in the prompt just to satisfy the schema.

```typescript
// User: "Process a refund for John"

// âŒ LLM invents user_id that doesn't exist
{
  "tool": "process_refund",
  "arguments": {
    "user_id": "user_12345", // HALLUCINATED - not in prompt!
    "amount": 50.00
  }
}

// Your system tries to refund non-existent user â†’ failure
```

**Architect's Fixes**:

**1. Mark fields as optional where possible**
```typescript
// âŒ BAD: Forces LLM to hallucinate
{
  name: 'process_refund',
  input_schema: {
    properties: {
      user_id: { type: 'string' }, // LLM MUST provide this
      amount: { type: 'number' }
    },
    required: ['user_id', 'amount'] // Forces hallucination if user_id unknown
  }
}

// âœ… GOOD: Let LLM admit it doesn't know
{
  name: 'process_refund',
  input_schema: {
    properties: {
      user_id: { type: 'string', description: 'User ID if known' },
      user_email: { type: 'string', description: 'Email if user_id unknown' },
      amount: { type: 'number' }
    },
    required: ['amount'], // Only require what's definitely in prompt
    // At least one of user_id or user_email should be provided
  }
}
```

**2. Runtime validation with Zod**
```typescript
import { z } from 'zod'

const RefundArgsSchema = z.object({
  user_id: z.string().regex(/^user_[0-9]+$/), // Validate format
  amount: z.number().positive().max(10000)
})

async function executeRefund(args: any): Promise<any> {
  try {
    // Validate before execution
    const validated = RefundArgsSchema.parse(args)

    // Check if user_id actually exists in database
    const user = await db.users.findUnique({ where: { id: validated.user_id } })

    if (!user) {
      throw new Error(`User ${validated.user_id} does not exist`)
    }

    return await processRefund(validated)
  } catch (error) {
    // Return error to LLM for retry
    if (error instanceof z.ZodError) {
      return {
        error: 'Invalid arguments',
        details: error.errors,
        message: 'Please provide valid user_id in format "user_12345"'
      }
    }

    return {
      error: error.message,
      message: 'User not found. Try looking up by email instead.'
    }
  }
}
```

**3. Multi-turn conversation for missing data**
```typescript
async function orchestrateRefund(userMessage: string) {
  const response = await anthropic.messages.create({
    model: 'claude-3-5-sonnet-20240620',
    max_tokens: 1024,
    tools: [processRefundTool, lookupUserTool],
    messages: [{ role: 'user', content: userMessage }]
  })

  // LLM realizes it needs user_id first
  if (response.content[0].name === 'lookup_user') {
    const user = await lookupUser(response.content[0].input.email)

    // Feed result back to LLM
    const nextResponse = await anthropic.messages.create({
      model: 'claude-3-5-sonnet-20240620',
      max_tokens: 1024,
      tools: [processRefundTool],
      messages: [
        { role: 'user', content: userMessage },
        { role: 'assistant', content: response.content },
        { role: 'user', content: [{ type: 'tool_result', tool_use_id: '...', content: JSON.stringify(user) }] }
      ]
    })

    // Now LLM can call process_refund with real user_id
  }
}
```

### Failure 3: Schema Drift

**The Problem**: You update your backend API, but forget to update the LLM's tool description. The LLM continues to send the "old" argument names.

```typescript
// âŒ Your code
interface OrderParams {
  productId: string      // Renamed from product_sku
  deliverySpeed: string  // Renamed from priority
}

// âŒ LLM's tool definition (outdated)
{
  name: 'create_order',
  input_schema: {
    properties: {
      product_sku: { type: 'string' },  // OLD NAME
      priority: { type: 'string' }      // OLD NAME
    }
  }
}

// Result: LLM sends { product_sku: "...", priority: "..." }
// Your code expects { productId: "...", deliverySpeed: "..." }
// â†’ Silent failures or crashes
```

**Architect's Fix: Single Source of Truth**

**Option 1: Generate tool definitions from TypeScript types**
```typescript
import { zodToJsonSchema } from 'zod-to-json-schema'
import { z } from 'zod'

// Define schema once
const OrderParamsSchema = z.object({
  productId: z.string().regex(/^SKU-[0-9]{5}$/),
  deliverySpeed: z.enum(['standard', 'express', 'overnight']),
  quantity: z.number().int().positive()
})

// Generate TypeScript type
type OrderParams = z.infer<typeof OrderParamsSchema>

// Generate JSON Schema for LLM
const jsonSchema = zodToJsonSchema(OrderParamsSchema)

// Use in tool definition
const createOrderTool: Anthropic.Tool = {
  name: 'create_order',
  description: 'Create a new product order',
  input_schema: jsonSchema // SINGLE SOURCE OF TRUTH
}

// Use in validation
function createOrder(params: unknown): Promise<Order> {
  const validated = OrderParamsSchema.parse(params) // Same schema!
  return db.orders.create({ data: validated })
}
```

**Option 2: Generate from OpenAPI spec**
```typescript
import { OpenAPIV3 } from 'openapi-types'

// Load your OpenAPI/Swagger spec
const apiSpec: OpenAPIV3.Document = await loadOpenAPISpec()

// Extract operation schema
const createOrderOperation = apiSpec.paths['/orders']?.post
const requestBodySchema = createOrderOperation?.requestBody?.content['application/json']?.schema

// Convert to Anthropic tool
const tool: Anthropic.Tool = {
  name: 'create_order',
  description: createOrderOperation?.description || '',
  input_schema: requestBodySchema as any
}

// Both LLM and API use same spec â†’ no drift!
```

**Option 3: Runtime schema validation layer**
```typescript
// Middleware that validates LLM arguments against current API schema
async function validateToolCall(
  toolName: string,
  args: unknown
): Promise<{ valid: boolean; error?: string }> {
  const currentSchema = await getLatestToolSchema(toolName)

  try {
    currentSchema.parse(args)
    return { valid: true }
  } catch (error) {
    if (error instanceof z.ZodError) {
      return {
        valid: false,
        error: `Schema mismatch: ${error.errors.map(e => `${e.path}: ${e.message}`).join(', ')}`
      }
    }
    throw error
  }
}
```

---

## Self-Healing Logic: The Repair Middleware

Even with perfect prompts and schemas, LLMs occasionally produce malformed output in production. Architect-level systems must **automatically recover** from these failures without human intervention.

### The Three-Layer Defense Strategy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 1: PREVENTIVE (Before LLM call)                â”‚
â”‚ - Generous max_tokens buffer                         â”‚
â”‚ - Clear schema in system prompt                      â”‚
â”‚ - Few-shot examples of correct format                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚ LLM generates output
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 2: REACTIVE (Parse & Validate)                 â”‚
â”‚ - Try JSON.parse()                                   â”‚
â”‚ - If success, validate against Zod schema            â”‚
â”‚ - If fail, proceed to Layer 3                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚ Validation failed
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 3: REPAIR (Auto-fix & Retry)                   â”‚
â”‚ - JSON repair (add missing brackets)                 â”‚
â”‚ - Regex cleanup (remove conversational fluff)        â”‚
â”‚ - Feedback loop (send error back to LLM)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Production Pattern: JSON Repair Middleware

```typescript
import { z } from 'zod'
import Anthropic from '@anthropic-ai/sdk'

interface RepairResult<T> {
  success: boolean
  data?: T
  repaired: boolean
  attempts: number
  error?: string
}

/**
 * Self-healing JSON extraction with automatic repair
 */
class SelfHealingExtractor<T> {
  constructor(
    private schema: z.ZodSchema<T>,
    private anthropic: Anthropic,
    private maxRepairAttempts: number = 3
  ) {}

  async extract(
    prompt: string,
    systemPrompt: string
  ): Promise<RepairResult<T>> {
    let attempts = 0
    let lastError: string = ''

    while (attempts < this.maxRepairAttempts) {
      attempts++

      try {
        // Step 1: Get LLM response
        const response = await this.anthropic.messages.create({
          model: 'claude-3-5-sonnet-20240620',
          max_tokens: 2048,
          system: systemPrompt + (attempts > 1 ? this.buildErrorFeedback(lastError) : ''),
          messages: [{ role: 'user', content: prompt }]
        })

        const content = response.content[0].type === 'text' ? response.content[0].text : ''

        // Step 2: Try direct parsing
        try {
          const parsed = JSON.parse(content)
          const validated = this.schema.parse(parsed)

          return {
            success: true,
            data: validated,
            repaired: false,
            attempts
          }
        } catch (parseError) {
          console.log(`Attempt ${attempts}: Direct parse failed, trying repair...`)

          // Step 3: Auto-repair
          const repaired = await this.repairJSON(content)

          if (repaired) {
            try {
              const parsed = JSON.parse(repaired)
              const validated = this.schema.parse(parsed)

              console.log(`âœ… Successfully repaired JSON on attempt ${attempts}`)

              return {
                success: true,
                data: validated,
                repaired: true,
                attempts
              }
            } catch (repairError) {
              // Repair didn't work, prepare feedback for next attempt
              lastError = this.buildErrorMessage(parseError, repairError, content)
            }
          } else {
            lastError = this.buildErrorMessage(parseError, null, content)
          }
        }
      } catch (apiError) {
        // API-level error (rate limit, network, etc.)
        return {
          success: false,
          repaired: false,
          attempts,
          error: apiError instanceof Error ? apiError.message : 'API error'
        }
      }
    }

    // All repair attempts failed
    return {
      success: false,
      repaired: false,
      attempts,
      error: `Failed after ${attempts} attempts. Last error: ${lastError}`
    }
  }

  /**
   * Attempt to repair malformed JSON
   */
  private async repairJSON(content: string): Promise<string | null> {
    // Strategy 1: Remove conversational fluff (common LLM mistake)
    let cleaned = this.removeConversationalFluff(content)

    // Strategy 2: Fix missing closing brackets (truncation)
    cleaned = this.addMissingBrackets(cleaned)

    // Strategy 3: Fix common quote issues
    cleaned = this.fixQuotes(cleaned)

    // Strategy 4: Try partial JSON parser
    try {
      const repaired = this.parsePartialJSON(cleaned)
      if (repaired) {
        return JSON.stringify(repaired)
      }
    } catch {
      // Partial parse failed
    }

    return null
  }

  /**
   * Remove text before/after JSON block
   */
  private removeConversationalFluff(content: string): string {
    // Common patterns LLMs add:
    // "Here's the JSON: {...}"
    // "```json\n{...}\n```"
    // "Sure! {...}"

    // Extract JSON from code blocks
    const codeBlockMatch = content.match(/```(?:json)?\s*(\{[\s\S]*\})\s*```/)
    if (codeBlockMatch) {
      return codeBlockMatch[1]
    }

    // Find first { and last }
    const firstBrace = content.indexOf('{')
    const lastBrace = content.lastIndexOf('}')

    if (firstBrace !== -1 && lastBrace !== -1 && lastBrace > firstBrace) {
      return content.slice(firstBrace, lastBrace + 1)
    }

    return content
  }

  /**
   * Add missing closing brackets (token limit truncation)
   */
  private addMissingBrackets(content: string): string {
    let fixed = content.trim()

    // Count brackets
    const openBraces = (fixed.match(/\{/g) || []).length
    const closeBraces = (fixed.match(/\}/g) || []).length
    const openBrackets = (fixed.match(/\[/g) || []).length
    const closeBrackets = (fixed.match(/\]/g) || []).length

    // Add missing closing braces
    for (let i = 0; i < openBraces - closeBraces; i++) {
      fixed += '}'
    }

    // Add missing closing brackets
    for (let i = 0; i < openBrackets - closeBrackets; i++) {
      fixed += ']'
    }

    return fixed
  }

  /**
   * Fix common quote issues
   */
  private fixQuotes(content: string): string {
    // Replace smart quotes with standard quotes
    return content
      .replace(/[""]/g, '"')
      .replace(/['']/g, "'")
  }

  /**
   * Parse incomplete JSON (progressive parsing)
   */
  private parsePartialJSON(content: string): any | null {
    // Try progressive truncation
    let current = content
    while (current.length > 10) {
      try {
        return JSON.parse(current)
      } catch {
        // Remove last character and try again
        current = current.slice(0, -1).trim()

        // Add closing bracket if needed
        if (current.endsWith(',')) {
          current = current.slice(0, -1) + '}'
        }
      }
    }

    return null
  }

  /**
   * Build error feedback for LLM (validation error â†’ retry with hints)
   */
  private buildErrorFeedback(error: string): string {
    return `

IMPORTANT: Your previous response had errors. Please fix them:

Error: ${error}

Requirements:
- Return ONLY valid JSON (no explanation before/after)
- Ensure all required fields are present
- Use exact field names from schema
- Close all brackets properly
- Use double quotes for strings, not single quotes`
  }

  /**
   * Build detailed error message from parse/validation errors
   */
  private buildErrorMessage(
    parseError: any,
    repairError: any,
    content: string
  ): string {
    let message = 'JSON parsing failed. '

    if (parseError instanceof SyntaxError) {
      message += `Syntax error: ${parseError.message}. `
    } else if (parseError instanceof z.ZodError) {
      const fieldErrors = parseError.errors
        .map(e => `Field "${e.path.join('.')}" ${e.message}`)
        .join(', ')
      message += `Validation errors: ${fieldErrors}. `
    }

    if (repairError) {
      message += `Repair also failed: ${repairError instanceof Error ? repairError.message : 'Unknown error'}. `
    }

    // Include truncated content for debugging
    const preview = content.slice(0, 200) + (content.length > 200 ? '...' : '')
    message += `Content preview: ${preview}`

    return message
  }
}
```

### Usage: Self-Healing Extraction

```typescript
// Define schema
const WeatherSchema = z.object({
  temperature: z.number(),
  unit: z.enum(['F', 'C']),
  conditions: z.string(),
  location: z.string()
})

// Create self-healing extractor
const extractor = new SelfHealingExtractor(
  WeatherSchema,
  anthropic,
  3 // max repair attempts
)

// Extract with automatic repair
const result = await extractor.extract(
  'Extract weather from: "It\'s 62 degrees and rainy in SF"',
  'You are a weather data extractor. Return ONLY valid JSON matching the schema.'
)

if (result.success) {
  console.log('Data:', result.data)
  console.log('Repaired:', result.repaired)
  console.log('Attempts:', result.attempts)

  // Use validated data
  await db.weather.create({ data: result.data })
} else {
  console.error('Extraction failed:', result.error)
  // Fall back to manual processing or alert human
}
```

### Advanced: Validation Error Feedback Loop

When validation fails, **send the exact error messages back to the LLM** so it can self-correct.

```typescript
/**
 * Multi-turn extraction with validation feedback
 */
async function extractWithFeedback<T>(
  schema: z.ZodSchema<T>,
  prompt: string,
  maxTurns: number = 3
): Promise<T> {
  let conversationHistory: Anthropic.MessageParam[] = [
    { role: 'user', content: prompt }
  ]

  for (let turn = 1; turn <= maxTurns; turn++) {
    const response = await anthropic.messages.create({
      model: 'claude-3-5-sonnet-20240620',
      max_tokens: 1024,
      messages: conversationHistory
    })

    const content = response.content[0].type === 'text' ? response.content[0].text : ''

    try {
      // Try parsing and validation
      const parsed = JSON.parse(content)
      const validated = schema.parse(parsed)

      console.log(`âœ… Success on turn ${turn}`)
      return validated
    } catch (error) {
      console.log(`Turn ${turn} failed, sending feedback...`)

      // Build detailed error feedback
      let feedback = 'Your JSON has errors:\n\n'

      if (error instanceof SyntaxError) {
        feedback += `SYNTAX ERROR: ${error.message}\n\n`
        feedback += 'Common fixes:\n'
        feedback += '- Ensure all brackets are closed\n'
        feedback += '- Use double quotes for strings\n'
        feedback += '- Remove trailing commas\n'
      } else if (error instanceof z.ZodError) {
        feedback += 'VALIDATION ERRORS:\n'
        error.errors.forEach((e, i) => {
          feedback += `${i + 1}. Field "${e.path.join('.')}" - ${e.message}\n`
        })
        feedback += '\nFix these issues and return corrected JSON.'
      }

      // Add feedback to conversation
      conversationHistory.push(
        { role: 'assistant', content: response.content },
        { role: 'user', content: feedback }
      )

      if (turn === maxTurns) {
        throw new Error(`Failed after ${maxTurns} turns. Last error: ${error}`)
      }
    }
  }

  throw new Error('Extraction failed')
}
```

### Production Metrics: Repair Success Rates

Track repair effectiveness to optimize strategies:

```typescript
interface RepairMetrics {
  totalExtractions: number
  directSuccess: number
  repairedSuccess: number
  totalFailures: number
  avgAttempts: number
  commonErrors: Map<string, number>
}

class RepairMetricsTracker {
  private metrics: RepairMetrics = {
    totalExtractions: 0,
    directSuccess: 0,
    repairedSuccess: 0,
    totalFailures: 0,
    avgAttempts: 0,
    commonErrors: new Map()
  }

  recordResult(result: RepairResult<any>): void {
    this.metrics.totalExtractions++

    if (result.success) {
      if (result.repaired) {
        this.metrics.repairedSuccess++
      } else {
        this.metrics.directSuccess++
      }
    } else {
      this.metrics.totalFailures++
      if (result.error) {
        const count = this.metrics.commonErrors.get(result.error) || 0
        this.metrics.commonErrors.set(result.error, count + 1)
      }
    }

    // Update average attempts
    const total = this.metrics.directSuccess + this.metrics.repairedSuccess + this.metrics.totalFailures
    this.metrics.avgAttempts =
      ((this.metrics.avgAttempts * (total - 1)) + result.attempts) / total
  }

  getReport(): string {
    const total = this.metrics.totalExtractions
    const successRate = ((this.metrics.directSuccess + this.metrics.repairedSuccess) / total) * 100
    const repairRate = (this.metrics.repairedSuccess / total) * 100

    return `
Extraction Metrics (last ${total} calls):
  Success Rate: ${successRate.toFixed(1)}%
  Direct Success: ${this.metrics.directSuccess} (${((this.metrics.directSuccess / total) * 100).toFixed(1)}%)
  Repaired Success: ${this.metrics.repairedSuccess} (${repairRate.toFixed(1)}%)
  Failures: ${this.metrics.totalFailures} (${((this.metrics.totalFailures / total) * 100).toFixed(1)}%)
  Avg Attempts: ${this.metrics.avgAttempts.toFixed(2)}

Top Errors:
${Array.from(this.metrics.commonErrors.entries())
  .sort((a, b) => b[1] - a[1])
  .slice(0, 5)
  .map(([error, count]) => `  ${count}x - ${error.slice(0, 80)}`)
  .join('\n')}
`
  }
}
```

**Target Metrics**:
- **Direct Success Rate**: > 95% (schema works well, no repair needed)
- **Repair Success Rate**: 3-4% (auto-repair saves failed calls)
- **Total Failure Rate**: < 1% (alert humans for manual review)
- **Avg Attempts**: < 1.1 (most succeed on first try)

**When to Alert**:
- Direct success < 90% â†’ Schema/prompt needs improvement
- Repair success > 10% â†’ Underlying issue with LLM behavior
- Total failure > 2% â†’ Production reliability at risk

---

## Production Checklist

Before shipping structured output to production:

- [ ] **Define Zod schema** with constraints (min/max, enums, regex)
- [ ] **Validate all outputs** - never trust LLM blindly
- [ ] **Handle parse errors** - JSON.parse can throw
- [ ] **Handle validation errors** - schema.parse can throw
- [ ] **Test edge cases** - empty strings, nulls, unexpected formats
- [ ] **Add retry logic** - LLM might return invalid JSON occasionally
- [ ] **Log failures** - track when LLM doesn't follow schema
- [ ] **Set max_tokens** appropriately - too low = truncated JSON
- [ ] **Monitor schema adherence rate** - should be >99%

---

## Key Takeaways

### From Chatbot to System Component

| Approach | Production-Ready? | Use Case |
|----------|------------------|----------|
| Plain text | âŒ No | Demos, prototypes |
| JSON mode (prompting) | âš ï¸ Risky | Quick experiments |
| Structured output + validation | âœ… Yes | Production systems |

### The Architect's Mindset

> "An LLM that can't produce structured data is just an expensive chatbot. Structured output transforms LLMs into **callable functions** that integrate with your existing systems."

**Three rules**:
1. **Always define a schema** - Zod, JSON Schema, or TypeScript interface
2. **Always validate** - Don't trust LLM output blindly
3. **Always handle errors** - Parse failures happen

### Why This Matters

- **UI components** need typed data, not strings
- **Databases** require structured fields, not paragraphs
- **APIs** expect JSON contracts, not prose
- **Type safety** catches bugs at compile time, not runtime

Without structured output, your AI is isolated from your stack. With it, AI becomes a **first-class system component**.

---

## Next Steps

- [Function Calling](./function-calling.mdx) - Let LLMs call your APIs and tools
- [Schema Design](./schema-design.mdx) - Design robust API contracts for LLMs
- [Week 5: AI Agents](../../week5/) - Build autonomous agents with tool use

---

## Further Reading

### Official Documentation
- [Anthropic: Structured Output](https://docs.anthropic.com/en/docs/build-with-claude/structured-outputs)
- [OpenAI: Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)
- [Zod Documentation](https://zod.dev/)

### Research
- [Constrained Decoding for Structured Generation](https://arxiv.org/abs/2307.09702)
- [JSON Mode vs Structured Outputs: When to Use Each](https://community.openai.com/t/json-mode-vs-structured-outputs)
